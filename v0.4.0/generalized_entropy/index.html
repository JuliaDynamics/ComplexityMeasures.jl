<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Generalized entropy · Entropies.jl</title><link href="https://fonts.googleapis.com/css?family=Lato|Roboto+Mono" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.11.2/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.11.2/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.11.2/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.11.1/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL=".."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../assets/documenter.js"></script><script src="../siteinfo.js"></script><script src="../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-dark.css" data-theme-name="documenter-dark"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../assets/themeswap.js"></script><link href="https://fonts.googleapis.com/css?family=Montserrat|Source+Code+Pro&amp;display=swap" rel="stylesheet" type="text/css"/></head><body><div id="documenter"><nav class="docs-sidebar"><div class="docs-package-name"><span class="docs-autofit">Entropies.jl</span></div><form class="docs-search" action="../search/"><input class="docs-search-query" id="documenter-search-query" name="q" type="text" placeholder="Search docs"/></form><ul class="docs-menu"><li><a class="tocitem" href="../">Documentation</a></li><li><a class="tocitem" href="../histogram_estimation/">Histograms</a></li><li><a class="tocitem" href="../probabilities/">Probabilities</a></li><li class="is-active"><a class="tocitem" href>Generalized entropy</a><ul class="internal"><li><a class="tocitem" href="#For-probability-distributions"><span>For probability distributions</span></a></li><li><a class="tocitem" href="#For-real-data-(ordered-sequences,-time-series)"><span>For real data (ordered sequences, time series)</span></a></li></ul></li><li><span class="tocitem">Estimators</span><ul><li><a class="tocitem" href="../CountOccurrences/">CountOccurrences (counting)</a></li><li><a class="tocitem" href="../SymbolicPermutation/">Permutation (symbolic)</a></li><li><a class="tocitem" href="../SymbolicWeightedPermutation/">Weighted permutation (symbolic)</a></li><li><a class="tocitem" href="../SymbolicAmplitudeAwarePermutation/">Amplitude-aware permutation (symbolic)</a></li><li><a class="tocitem" href="../VisitationFrequency/">Visitation frequency (binning)</a></li></ul></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><nav class="breadcrumb"><ul class="is-hidden-mobile"><li class="is-active"><a href>Generalized entropy</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>Generalized entropy</a></li></ul></nav><div class="docs-right"><a class="docs-edit-link" href="https://github.com/kahaaga/Entropies.jl/blob/master/docs/src/generalized_entropy.md" title="Edit on GitHub"><span class="docs-icon fab"></span><span class="docs-label is-hidden-touch">Edit on GitHub</span></a><a class="docs-settings-button fas fa-cog" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-sidebar-button fa fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a></div></header><article class="content" id="documenter-page"><h1 id="Generalized-entropy"><a class="docs-heading-anchor" href="#Generalized-entropy">Generalized entropy</a><a id="Generalized-entropy-1"></a><a class="docs-heading-anchor-permalink" href="#Generalized-entropy" title="Permalink"></a></h1><h2 id="For-probability-distributions"><a class="docs-heading-anchor" href="#For-probability-distributions">For probability distributions</a><a id="For-probability-distributions-1"></a><a class="docs-heading-anchor-permalink" href="#For-probability-distributions" title="Permalink"></a></h2><p>Generalized entropy is a property of probability distributions.</p><article class="docstring"><header><a class="docstring-binding" id="Entropies.genentropy-Union{Tuple{T}, Tuple{Real,AbstractArray{T,N} where N}} where T&lt;:Real" href="#Entropies.genentropy-Union{Tuple{T}, Tuple{Real,AbstractArray{T,N} where N}} where T&lt;:Real"><code>Entropies.genentropy</code></a> — <span class="docstring-category">Method</span></header><section><div><p><strong>Generalized entropy of a probability distribution</strong></p><pre><code class="language-none">genentropy(α::Real, p::AbstractArray; base = Base.MathConstants.e)</code></pre><p>Compute the entropy, to the given <code>base</code>, of an array of probabilities <code>p</code> (assuming  that <code>p</code> is sum-normalized).</p><p>If a multivariate <code>Dataset</code> <code>x</code> is given, then the a sum-normalized histogram is obtained  directly on the elements of <code>x</code>, and the generalized entropy is computed on that  distribution.</p><p><strong>Description</strong></p><p>Let <span>$p$</span> be an array of probabilities (summing to 1). Then the Rényi entropy is</p><div>\[H_\alpha(p) = \frac{1}{1-\alpha} \log \left(\sum_i p[i]^\alpha\right)\]</div><p>and generalizes other known entropies, like e.g. the information entropy (<span>$\alpha = 1$</span>, see <sup class="footnote-reference"><a id="citeref-Shannon1948" href="#footnote-Shannon1948">[Shannon1948]</a></sup>), the maximum entropy (<span>$\alpha=0$</span>, also known as Hartley entropy), or the correlation entropy (<span>$\alpha = 2$</span>, also known as collision entropy).</p><p><strong>Example</strong></p><pre><code class="language-julia">using Entropies
p = rand(5000)
p = p ./ sum(p) # normalizing to 1 ensures we have a probability distribution

# Estimate order-1 generalized entropy to base 2 of the distribution
Entropies.genentropy(1, ps, base = 2)</code></pre><p>See also: <a href="../histogram_estimation/#Entropies.non0hist-Union{Tuple{AbstractArray{T,1}}, Tuple{T}} where T&lt;:Real"><code>non0hist</code></a>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/kahaaga/Entropies.jl/blob/1d84927452d425324261d2bb8a38682e93d29fb0/src/generalized_entropy.jl#L4-L45">source</a></section></article><h2 id="For-real-data-(ordered-sequences,-time-series)"><a class="docs-heading-anchor" href="#For-real-data-(ordered-sequences,-time-series)">For real data (ordered sequences, time series)</a><a id="For-real-data-(ordered-sequences,-time-series)-1"></a><a class="docs-heading-anchor-permalink" href="#For-real-data-(ordered-sequences,-time-series)" title="Permalink"></a></h2><p>The method above only works when you actually have access to a probability distribution. In most cases, probability distributions have to be estimated from data.</p><p>Currently, we implement the following probability estimators:</p><ul><li><a href="../CountOccurrences/#Entropies.CountOccurrences"><code>CountOccurrences</code></a></li><li><a href="../VisitationFrequency/#Entropies.VisitationFrequency"><code>VisitationFrequency</code></a></li><li><a href="../SymbolicPermutation/#Entropies.SymbolicPermutation"><code>SymbolicPermutation</code></a></li><li><a href="../SymbolicWeightedPermutation/#Entropies.SymbolicWeightedPermutation"><code>SymbolicWeightedPermutation</code></a></li><li><a href="../SymbolicAmplitudeAwarePermutation/#Entropies.SymbolicAmplitudeAwarePermutation"><code>SymbolicAmplitudeAwarePermutation</code></a></li></ul><h3 id="Getting-the-distributions"><a class="docs-heading-anchor" href="#Getting-the-distributions">Getting the distributions</a><a id="Getting-the-distributions-1"></a><a class="docs-heading-anchor-permalink" href="#Getting-the-distributions" title="Permalink"></a></h3><p>Distributions can be obtained directly for dataset <code>x</code> using the signature</p><pre><code class="language-julia">probabilities(x, estimator)</code></pre><h3 id="Computing-the-entropy"><a class="docs-heading-anchor" href="#Computing-the-entropy">Computing the entropy</a><a id="Computing-the-entropy-1"></a><a class="docs-heading-anchor-permalink" href="#Computing-the-entropy" title="Permalink"></a></h3><p>The syntax for using the different estimators to compute generalized entropy are as follows.</p><article class="docstring"><header><a class="docstring-binding" id="Entropies.genentropy-Tuple{AbstractDataset}" href="#Entropies.genentropy-Tuple{AbstractDataset}"><code>Entropies.genentropy</code></a> — <span class="docstring-category">Method</span></header><section><div><p><strong>Entropy based on counting occurrences of distinct elements</strong></p><pre><code class="language-none">genentropy(x::AbstractDataset, est::CountOccurrences, α = 1; base = Base.MathConstants.e)
genentropy(x::AbstractVector{T}, est::CountOccurrences, α = 1; base = Base.MathConstants.e) where T</code></pre><p>Compute the order-<code>α</code> generalized (Rényi) entropy<sup class="footnote-reference"><a id="citeref-Rényi1960" href="#footnote-Rényi1960">[Rényi1960]</a></sup> of a dataset <code>x</code> by counting repeated elements in <code>x</code>. Then, obtain a sum-normalized histogram from the  counts of repeated elements, and compute generalized entropy. Assumes that <code>x</code> can be sorted.</p><p><strong>Example</strong></p><pre><code class="language-julia">using Entropies, DelayEmbeddings

# A dataset with many identical state vectors
D = Dataset(rand(1:3, 5000, 3))

# Estimate order-1 generalized entropy to base 2 of the dataset
Entropies.genentropy(D, CountOccurrences(), 1, base = 2)</code></pre><pre><code class="language-julia">using Entropies, DelayEmbeddings

# A bunch of tuples, many potentially identical
x = [(rand(1:5), rand(1:5), rand(1:5)) for i = 1:10000]

# Default generalized entropy of the tuples
Entropies.genentropy(x, CountOccurrences())</code></pre><p>See also: <a href="../CountOccurrences/#Entropies.CountOccurrences"><code>CountOccurrences</code></a>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/kahaaga/Entropies.jl/blob/1d84927452d425324261d2bb8a38682e93d29fb0/src/counting_based/CountOccurrences.jl#L15-L48">source</a></section><section><div><p><strong>Permutation entropy</strong></p><pre><code class="language-none">genentropy(x::AbstractDataset, est::SymbolicPermutation, α::Real = 1; base = 2) → Real
genentropy(x::AbstractVector{&lt;:Real}, est::SymbolicPermutation, α::Real = 1; m::Int = 3, τ::Int = 1, base = 2) → Real

genentropy!(s::Vector{Int}, x::AbstractDataset, est::SymbolicPermutation, α::Real = 1; base = 2) → Real
genentropy!(s::Vector{Int}, x::AbstractVector{&lt;:Real}, est::SymbolicPermutation, α::Real = 1; m::Int = 3, τ::Int = 1, base = 2) → Real</code></pre><p>Compute the generalized order-<code>α</code> entropy over a permutation symbolization of <code>x</code>, using  symbol size/order <code>m</code>. </p><p>If <code>x</code> is a multivariate <code>Dataset</code>, then symbolization is performed directly on the state  vectors. If <code>x</code> is a univariate signal, then a delay reconstruction with embedding lag <code>τ</code>  and embedding dimension <code>m</code> is used to construct state vectors, on which symbolization is  then performed.</p><p>A pre-allocated symbol array <code>s</code> can be provided to save some memory allocations if   probabilities are to be computed for multiple data sets. If provided, it is required that  <code>length(x) == length(s)</code> if <code>x</code> is a <code>Dataset</code>, or  <code>length(s) == length(x) - (m-1)τ</code>  if <code>x</code> is a univariate signal.</p><p><strong>Probability and entropy estimation</strong></p><p>An unordered symbol frequency histogram is obtained by symbolizing the points in <code>x</code>, using <a href="../probabilities/#Entropies.probabilities-Tuple{Dataset,SymbolicPermutation}"><code>probabilities(::AbstractDataset, ::SymbolicPermutation)</code></a>. Sum-normalizing this histogram yields a probability distribution over the symbols.</p><p>After the symbolization histogram/distribution has been obtained, the order <code>α</code> generalized  entropy<sup class="footnote-reference"><a id="citeref-Rényi1960" href="#footnote-Rényi1960">[Rényi1960]</a></sup>, to the given <code>base</code>, is computed from that sum-normalized symbol  distribution, using <a href="#Entropies.genentropy-Union{Tuple{T}, Tuple{Real,AbstractArray{T,N} where N}} where T&lt;:Real"><code>genentropy</code></a>.</p><div class="admonition is-category-hint"><header class="admonition-header">Generalized entropy order vs. permutation order</header><div class="admonition-body"><p>Do not confuse the order of the generalized entropy (<code>α</code>) with the order <code>m</code> of the  permutation entropy (<code>m</code>, which controls the symbol size). Permutation entropy is usually  estimated with <code>α = 1</code>, but the implementation here allows the generalized entropy of any  dimension to be computed from the symbol frequency distribution.</p></div></div><p>See also: <a href="../SymbolicPermutation/#Entropies.SymbolicPermutation"><code>SymbolicPermutation</code></a>, <a href="#Entropies.genentropy-Union{Tuple{T}, Tuple{Real,AbstractArray{T,N} where N}} where T&lt;:Real"><code>genentropy</code></a>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/kahaaga/Entropies.jl/blob/1d84927452d425324261d2bb8a38682e93d29fb0/src/symbolic/SymbolicPermutation.jl#L245-L286">source</a></section><section><div><p><strong>Weighted permutation entropy</strong></p><pre><code class="language-none">genentropy(x::AbstractDataset, est::SymbolicWeightedPermutation, α::Real = 1; base = 2) → Real
genentropy(x::AbstractVector{&lt;:Real}, est::SymbolicWeightedPermutation, α::Real = 1; m::Int = 3, τ::Int = 1, base = 2) → Real</code></pre><p>Compute the generalized order <code>α</code> entropy based on a weighted permutation  symbolization of <code>x</code>, using symbol size/order <code>m</code> for the permutations.</p><p>If <code>x</code> is a multivariate <code>Dataset</code>, then symbolization is performed directly on the state  vectors. If <code>x</code> is a univariate signal, then a delay reconstruction with embedding lag <code>τ</code>  and embedding dimension <code>m</code> is used to construct state vectors, on which symbolization is  then performed.</p><p><strong>Probability and entropy estimation</strong></p><p>An unordered symbol frequency histogram is obtained by symbolizing the points in <code>x</code> by a weighted procedure, using <a href="../probabilities/#Entropies.probabilities-Tuple{Dataset,SymbolicPermutation}"><code>probabilities(::AbstractDataset, ::SymbolicWeightedPermutation)</code></a>. Sum-normalizing this histogram yields a probability distribution over the weighted symbols.</p><p>After the symbolization histogram/distribution has been obtained, the order <code>α</code> generalized  entropy<sup class="footnote-reference"><a id="citeref-Rényi1960" href="#footnote-Rényi1960">[Rényi1960]</a></sup>, to the given <code>base</code>, is computed from that sum-normalized symbol  distribution, using <a href="#Entropies.genentropy-Union{Tuple{T}, Tuple{Real,AbstractArray{T,N} where N}} where T&lt;:Real"><code>genentropy</code></a>.</p><div class="admonition is-category-hint"><header class="admonition-header">Generalized entropy order vs. permutation order</header><div class="admonition-body"><p>Do not confuse the order of the generalized entropy (<code>α</code>) with the order <code>m</code> of the  permutation entropy (<code>m</code>, which controls the symbol size). Permutation entropy is usually  estimated with <code>α = 1</code>, but the implementation here allows the generalized entropy of any  dimension to be computed from the symbol frequency distribution.</p></div></div><p>See also: <a href="../SymbolicWeightedPermutation/#Entropies.SymbolicWeightedPermutation"><code>SymbolicWeightedPermutation</code></a>, <a href="#Entropies.genentropy-Union{Tuple{T}, Tuple{Real,AbstractArray{T,N} where N}} where T&lt;:Real"><code>genentropy</code></a>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/kahaaga/Entropies.jl/blob/1d84927452d425324261d2bb8a38682e93d29fb0/src/symbolic/SymbolicWeightedPermutation.jl#L229-L262">source</a></section><section><div><p><strong>Amplitude-aware permutation entropy</strong></p><pre><code class="language-none">genentropy(x::AbstractDataset, est::SymbolicAmplitudeAwarePermutation, α::Real = 1; base = 2) → Real
genentropy(x::AbstractVector{&lt;:Real}, est::SymbolicAmplitudeAwarePermutation, α::Real = 1; 
    m::Int = 3, τ::Int = 1, base = 2) → Real</code></pre><p>Compute the generalized order <code>α</code> entropy based on an amplitude-sensitive permutation  symbolization of <code>x</code>, using symbol size/order <code>m</code> for the permutations.</p><p>If <code>x</code> is a multivariate <code>Dataset</code>, then symbolization is performed directly on the state  vectors. If <code>x</code> is a univariate signal, then a delay reconstruction with embedding lag <code>τ</code>  and embedding dimension <code>m</code> is used to construct state vectors, on which symbolization is  then performed.</p><p><strong>Probability and entropy estimation</strong></p><p>An unordered symbol frequency histogram is obtained by symbolizing the points in <code>x</code> by an amplitude-aware procedure, using  <a href="../probabilities/#Entropies.probabilities-Tuple{Dataset,SymbolicPermutation}"><code>probabilities(::AbstractDataset, ::SymbolicAmplitudeAwarePermutation)</code></a>. Sum-normalizing this histogram yields a probability distribution over the amplitude-encoding  symbols.</p><p>After the symbolization histogram/distribution has been obtained, the order <code>α</code> generalized  entropy<sup class="footnote-reference"><a id="citeref-Rényi1960" href="#footnote-Rényi1960">[Rényi1960]</a></sup>, to the given <code>base</code>, is computed from that sum-normalized symbol  distribution, using <a href="#Entropies.genentropy-Union{Tuple{T}, Tuple{Real,AbstractArray{T,N} where N}} where T&lt;:Real"><code>genentropy</code></a>.</p><div class="admonition is-category-hint"><header class="admonition-header">Generalized entropy order vs. permutation order</header><div class="admonition-body"><p>Do not confuse the order of the generalized entropy (<code>α</code>) with the order <code>m</code> of the  permutation entropy (<code>m</code>, which controls the symbol size). Permutation entropy is usually  estimated with <code>α = 1</code>, but the implementation here allows the generalized entropy of any  dimension to be computed from the symbol frequency distribution.</p></div></div><p>See also: <a href="../SymbolicAmplitudeAwarePermutation/#Entropies.SymbolicAmplitudeAwarePermutation"><code>SymbolicAmplitudeAwarePermutation</code></a>, <a href="#Entropies.genentropy-Union{Tuple{T}, Tuple{Real,AbstractArray{T,N} where N}} where T&lt;:Real"><code>genentropy</code></a>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/kahaaga/Entropies.jl/blob/1d84927452d425324261d2bb8a38682e93d29fb0/src/symbolic/SymbolicAmplitudeAware.jl#L165-L202">source</a></section></article><section class="footnotes is-size-7"><ul><li class="footnote" id="footnote-Rényi1960"><a class="tag is-link" href="#citeref-Rényi1960">Rényi1960</a>A. Rényi, <em>Proceedings of the fourth Berkeley Symposium on Mathematics, Statistics and Probability</em>, pp 547 (1960)</li><li class="footnote" id="footnote-Shannon1948"><a class="tag is-link" href="#citeref-Shannon1948">Shannon1948</a>C. E. Shannon, Bell Systems Technical Journal <strong>27</strong>, pp 379 (1948)</li><li class="footnote" id="footnote-Rényi1960"><a class="tag is-link" href="#citeref-Rényi1960">Rényi1960</a>A. Rényi, <em>Proceedings of the fourth Berkeley Symposium on Mathematics, Statistics and Probability</em>, pp 547 (1960)</li><li class="footnote" id="footnote-Rényi1960"><a class="tag is-link" href="#citeref-Rényi1960">Rényi1960</a>A. Rényi, <em>Proceedings of the fourth Berkeley Symposium on Mathematics, Statistics and Probability</em>, pp 547 (1960)</li><li class="footnote" id="footnote-Rényi1960"><a class="tag is-link" href="#citeref-Rényi1960">Rényi1960</a>A. Rényi, <em>Proceedings of the fourth Berkeley Symposium on Mathematics,  Statistics and Probability</em>, pp 547 (1960)</li></ul></section></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../probabilities/">« Probabilities</a><a class="docs-footer-nextpage" href="../CountOccurrences/">CountOccurrences (counting) »</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> on <span class="colophon-date" title="Thursday 5 November 2020 01:00">Thursday 5 November 2020</span>. Using Julia version 1.5.2.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
