using Test, StaticArrays, LinearAlgebra, Distributions

@testset "NN - Kraskov" begin
    m = 4
    Ï„ = 1
    Ï„s = tuple([Ï„*i for i = 0:m-1]...)
    x = rand(250)
    D = genembed(x, Ï„s)
    est = Kraskov(k = 3, w = 1)
    @test entropy(est, D) isa Real

    # Analytical test.
    XN = Dataset(randn(100000, 1));
    # For normal distribution with mean 0 and std 1, the entropy is
    h_XN_base_e = 0.5 * log(MathConstants.e, 2Ï€) + 0.5 # nats
    h_XN_base_2 = h_XN_base_e / log(2, MathConstants.e) # bits

    h_XN_kr_base_e = entropy(Kraskov(k = 3, base = MathConstants.e), XN)
    h_XN_kr_base_2 = entropy(Kraskov(k = 3, base = 2), XN)
    @test round(h_XN_base_e, digits = 1) == round(h_XN_kr_base_e, digits = 1)
    @test round(h_XN_base_2, digits = 1) == round(h_XN_kr_base_2, digits = 1)
end

@testset "NN - KozachenkoLeonenko" begin
    m = 4
    Ï„ = 1
    Ï„s = tuple([Ï„*i for i = 0:m-1]...)
    x = rand(250)
    D = genembed(x, Ï„s)
    est = KozachenkoLeonenko(w = 1)

    @test entropy(est, D) isa Real

    # Analytical test.
    XN = Dataset(randn(100000, 1));
    # For normal distribution with mean 0 and std 1, the entropy is
    h_XN_base_e = 0.5 * log(MathConstants.e, 2Ï€) + 0.5 # nats
    h_XN_base_2 = h_XN_base_e / log(2, MathConstants.e) # bits

    h_XN_kr_base_e = entropy(KozachenkoLeonenko(base = MathConstants.e), XN)
    h_XN_kr_base_2 = entropy(KozachenkoLeonenko(base = 2), XN)
    # The KozachenkoLeonenko estimator is not as precise as Kraskov, so check that we're
    # within +- 3% of the target value
    tol_e = h_XN_base_e * 0.03
    tol_2 = h_XN_base_2 * 0.03
    @test h_XN_base_e - tol_e â‰¤ h_XN_kr_base_e â‰¤ h_XN_base_e + tol_e
    @test h_XN_base_2 - tol_2 â‰¤ h_XN_kr_base_2 â‰¤ h_XN_base_2 + tol_2
end

@testset "NN - Zhu" begin

    # To ensure minimal rectangle volumes are correct, we also test internals directly here.
    # It's not feasible to construct an end-product test due to the neighbor searches.
    x = Dataset([[-1, -2], [0, -2], [3, 2]])
    y = Dataset([[3, 1], [-5, 1], [3, -2]])
    @test Entropies.volume_minimal_rect([0, 0], x) == 24
    @test Entropies.volume_minimal_rect([0, 0], y) == 40

    # Analytical tests for the estimated entropy
    DN = Dataset(randn(200000, 1))
    hN_base_e = 0.5 * log(MathConstants.e, 2Ï€) + 0.5
    hN_base_2 = hN_base_e / log(2, MathConstants.e)

    e_base_e = Zhu(k = 3, base = MathConstants.e)
    e_base_2 = Zhu(k = 3, base = 2)

    @test round(entropy(e_base_e, DN), digits = 1) == round(hN_base_e, digits = 1)
    @test round(entropy(e_base_2, DN), digits = 1) == round(hN_base_2, digits = 1)
end

@testset "NN - Zhu" begin

    # To ensure minimal rectangle volumes are correct, we also test internals directly here.
    # It's not feasible to construct an end-product test due to the neighbor searches.
    x = Dataset([[-1, -2], [0, -2], [3, 2]])
    y = Dataset([[3, 1], [-5, 1], [3, -2]])
    @test Entropies.volume_minimal_rect([0, 0], x) == 24
    @test Entropies.volume_minimal_rect([0, 0], y) == 40

    # Analytical tests for the estimated entropy
    DN = Dataset(randn(200000, 1))
    hN_base_e = 0.5 * log(MathConstants.e, 2Ï€) + 0.5
    hN_base_2 = hN_base_e / log(2, MathConstants.e)

    e_base_e = Zhu(k = 3, base = MathConstants.e)
    e_base_2 = Zhu(k = 3, base = 2)

    @test round(entropy(e_base_e, DN), digits = 1) == round(hN_base_e, digits = 1)
    @test round(entropy(e_base_2, DN), digits = 1) == round(hN_base_2, digits = 1)
end


@testset "NN - ZhuSingh" begin

    using Test, Distributions, LinearAlgebra
    # Test internals in addition to end-product, because designing an exact end-product
    # test is  a mess due to the neighbor searches. If these top-level tests fail, then
    # the issue is probability related to these functions.
    using Test
    nns = Dataset([[-1, -1], [0, -2], [3, 2.0]])
    x = @SVector [0.0, 0.0]
    dists = Entropies.maxdists(x, nns)
    vol = Entropies.volume_minimal_rect(dists)
    Î¾ = Entropies.n_borderpoints(x, nns, dists)
    @test vol == 24.0
    @test Î¾ == 2.0

    nns = Dataset([[3, 1], [3, -2], [-5, 1.0]])
    x = @SVector [0.0, 0.0]
    dists = Entropies.maxdists(x, nns)
    vol = Entropies.volume_minimal_rect(dists)
    Î¾ = Entropies.n_borderpoints(x, nns, dists)
    @test vol == 40.0
    @test Î¾ == 2.0

    nns = Dataset([[-3, 1], [3, 1], [5, -2.0]])
    x = @SVector [0.0, 0.0]
    dists = Entropies.maxdists(x, nns)
    vol = Entropies.volume_minimal_rect(dists)
    Î¾ = Entropies.n_borderpoints(x, nns, dists)
    @test vol == 40.0
    @test Î¾ == 1.0

    # Analytical tests: 1D normal distribution
    DN = Dataset(randn(100000, 1))
    Ïƒ = 1.0
    hN_base_e = 0.5 * log(MathConstants.e, 2Ï€ * Ïƒ^2) + 0.5
    hN_base_2 = hN_base_e / log(2, MathConstants.e)

    e_base_e = ZhuSingh(k = 3, base = MathConstants.e)
    e_base_2 = ZhuSingh(k = 3, base = 2)

    @test round(entropy(e_base_e, DN), digits = 1) == round(hN_base_e, digits = 1)
    @test round(entropy(e_base_2, DN), digits = 1) == round(hN_base_2, digits = 1)

    # Analytical test: 3D normal distribution
    Ïƒs = ones(3)
    Î¼s = zeros(3)
    ğ’©â‚‚ = MvNormal(Î¼s, Diagonal(Ïƒs))
    Î£ = diagm(Ïƒs)
    n = length(Î¼s)
    h_ğ’©â‚‚_base_â„¯ = 0.5n * log(â„¯, 2Ï€) + 0.5*log(â„¯, det(Î£)) + 0.5n
    h_ğ’©â‚‚_base_2 = h_ğ’©â‚‚_base_â„¯  / log(2, â„¯)

    sample = Dataset(transpose(rand(ğ’©â‚‚, 50000)))
    hZS_ğ’©â‚‚_base_â„¯ = entropy(e_base_e, sample)
    hZS_ğ’©â‚‚_base_2 = entropy(e_base_2, sample)

    # Estimation accuracy decreases for fixed N with increasing edimension, so exact comparison
    # isn't useful. Just check that values are within 1% of the target.
    tol_â„¯  = hZS_ğ’©â‚‚_base_â„¯ * 0.01
    tol_2  = hZS_ğ’©â‚‚_base_2 * 0.01
    @test h_ğ’©â‚‚_base_â„¯ - tol_â„¯ â‰¤ hZS_ğ’©â‚‚_base_â„¯ â‰¤ h_ğ’©â‚‚_base_â„¯ + tol_â„¯
    @test h_ğ’©â‚‚_base_2 - tol_2 â‰¤ hZS_ğ’©â‚‚_base_2 â‰¤ h_ğ’©â‚‚_base_2 + tol_2
end
