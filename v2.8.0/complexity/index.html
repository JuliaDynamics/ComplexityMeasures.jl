<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Complexity measures · ComplexityMeasures.jl</title><script data-outdated-warner src="../assets/warner.js"></script><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.045/juliamono.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.24/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL=".."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../assets/documenter.js"></script><script src="../siteinfo.js"></script><script src="../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../assets/themeswap.js"></script><link href="https://fonts.googleapis.com/css?family=Montserrat|Source+Code+Pro&amp;display=swap" rel="stylesheet" type="text/css"/></head><body><div id="documenter"><nav class="docs-sidebar"><div class="docs-package-name"><span class="docs-autofit"><a href="../">ComplexityMeasures.jl</a></span></div><form class="docs-search" action="../search/"><input class="docs-search-query" id="documenter-search-query" name="q" type="text" placeholder="Search docs"/></form><ul class="docs-menu"><li><a class="tocitem" href="../">ComplexityMeasures.jl</a></li><li><a class="tocitem" href="../probabilities/">Probabilities</a></li><li><a class="tocitem" href="../encodings/">Encodings</a></li><li><a class="tocitem" href="../entropies/">Entropies</a></li><li class="is-active"><a class="tocitem" href>Complexity measures</a><ul class="internal"><li><a class="tocitem" href="#Complexity-measures-API"><span>Complexity measures API</span></a></li><li><a class="tocitem" href="#Approximate-entropy"><span>Approximate entropy</span></a></li><li><a class="tocitem" href="#Sample-entropy"><span>Sample entropy</span></a></li><li><a class="tocitem" href="#Missing-dispersion-patterns"><span>Missing dispersion patterns</span></a></li><li><a class="tocitem" href="#Reverse-dispersion-entropy"><span>Reverse dispersion entropy</span></a></li><li><a class="tocitem" href="#Statistical-complexity"><span>Statistical complexity</span></a></li></ul></li><li><a class="tocitem" href="../convenience/">Convenience functions</a></li><li><a class="tocitem" href="../examples/">ComplexityMeasures.jl Examples</a></li><li><a class="tocitem" href="../devdocs/">ComplexityMeasures.jl Dev Docs</a></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><nav class="breadcrumb"><ul class="is-hidden-mobile"><li class="is-active"><a href>Complexity measures</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>Complexity measures</a></li></ul></nav><div class="docs-right"><a class="docs-edit-link" href="https://github.com/JuliaDynamics/ComplexityMeasures.jl/blob/main/docs/src/complexity.md" title="Edit on GitHub"><span class="docs-icon fab"></span><span class="docs-label is-hidden-touch">Edit on GitHub</span></a><a class="docs-settings-button fas fa-cog" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-sidebar-button fa fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a></div></header><article class="content" id="documenter-page"><h1 id="Complexity-measures"><a class="docs-heading-anchor" href="#Complexity-measures">Complexity measures</a><a id="Complexity-measures-1"></a><a class="docs-heading-anchor-permalink" href="#Complexity-measures" title="Permalink"></a></h1><p>In this page we document estimators for complexity measures that are not entropies in the strict mathematical sense. The API is almost identical to <a href="../entropies/#ComplexityMeasures.entropy-Tuple{EntropyDefinition, ProbabilitiesEstimator, Any}"><code>entropy</code></a> and is defined by:</p><ul><li><a href="#ComplexityMeasures.complexity"><code>complexity</code></a></li><li><a href="#ComplexityMeasures.complexity_normalized"><code>complexity_normalized</code></a></li><li><a href="#ComplexityMeasures.ComplexityEstimator"><code>ComplexityEstimator</code></a></li></ul><h2 id="Complexity-measures-API"><a class="docs-heading-anchor" href="#Complexity-measures-API">Complexity measures API</a><a id="Complexity-measures-API-1"></a><a class="docs-heading-anchor-permalink" href="#Complexity-measures-API" title="Permalink"></a></h2><article class="docstring"><header><a class="docstring-binding" id="ComplexityMeasures.complexity" href="#ComplexityMeasures.complexity"><code>ComplexityMeasures.complexity</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">complexity(c::ComplexityEstimator, x)</code></pre><p>Estimate a complexity measure according to <code>c</code> for <a href="../#input_data">input data</a> <code>x</code>, where <code>c</code> can be any of the following estimators:</p><ul><li><a href="#ComplexityMeasures.ReverseDispersion"><code>ReverseDispersion</code></a>.</li><li><a href="#ComplexityMeasures.ApproximateEntropy"><code>ApproximateEntropy</code></a>.</li><li><a href="#ComplexityMeasures.SampleEntropy"><code>SampleEntropy</code></a>.</li><li><a href="#ComplexityMeasures.MissingDispersionPatterns"><code>MissingDispersionPatterns</code></a>.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaDynamics/ComplexityMeasures.jl/blob/6bdeaf2132c4501e675e8817979c221f1bdea337/src/complexity.jl#L14-L25">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="ComplexityMeasures.complexity_normalized" href="#ComplexityMeasures.complexity_normalized"><code>ComplexityMeasures.complexity_normalized</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">complexity_normalized(c::ComplexityEstimator, x) → m ∈ [a, b]</code></pre><p>The same as <a href="#ComplexityMeasures.complexity"><code>complexity</code></a>, but the result is normalized to the interval <code>[a, b]</code>, where <code>[a, b]</code> depends on <code>c</code>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaDynamics/ComplexityMeasures.jl/blob/6bdeaf2132c4501e675e8817979c221f1bdea337/src/complexity.jl#L32-L37">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="ComplexityMeasures.ComplexityEstimator" href="#ComplexityMeasures.ComplexityEstimator"><code>ComplexityMeasures.ComplexityEstimator</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">ComplexityEstimator</code></pre><p>Supertype for estimators for various complexity measures that are not entropies in the strict mathematical sense. See <a href="#ComplexityMeasures.complexity"><code>complexity</code></a> for all available estimators.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaDynamics/ComplexityMeasures.jl/blob/6bdeaf2132c4501e675e8817979c221f1bdea337/src/complexity.jl#L5-L11">source</a></section></article><h2 id="Approximate-entropy"><a class="docs-heading-anchor" href="#Approximate-entropy">Approximate entropy</a><a id="Approximate-entropy-1"></a><a class="docs-heading-anchor-permalink" href="#Approximate-entropy" title="Permalink"></a></h2><article class="docstring"><header><a class="docstring-binding" id="ComplexityMeasures.ApproximateEntropy" href="#ComplexityMeasures.ApproximateEntropy"><code>ComplexityMeasures.ApproximateEntropy</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">ApproximateEntropy &lt;: ComplexityEstimator
ApproximateEntropy([x]; r = 0.2std(x), kwargs...)</code></pre><p>An estimator for the approximate entropy (ApEn; Pincus, 1991)<sup class="footnote-reference"><a id="citeref-Pincus1991" href="#footnote-Pincus1991">[Pincus1991]</a></sup> complexity measure, used with <a href="#ComplexityMeasures.complexity"><code>complexity</code></a>.</p><p>The keyword argument <code>r</code> is mandatory if an input timeseries <code>x</code> is not provided.</p><p><strong>Keyword arguments</strong></p><ul><li><code>r::Real</code>: The radius used when querying for nearest neighbors around points. Its value   should be determined from the input data, for example as some proportion of the   standard deviation of the data.</li><li><code>m::Int = 2</code>: The embedding dimension.</li><li><code>τ::Int = 1</code>: The embedding lag.</li><li><code>base::Real = MathConstants.e</code>: The base to use for the logarithm. Pincus (1991) uses the   natural logarithm.</li></ul><p><strong>Description</strong></p><p>Approximate entropy is defined as</p><p class="math-container">\[ApEn(m ,r) = \lim_{N \to \infty} \left[ \phi(x, m, r) - \phi(x, m + 1, r) \right].\]</p><p>Approximate entropy is estimated for a timeseries <code>x</code>, by first embedding <code>x</code> using embedding dimension <code>m</code> and embedding lag <code>τ</code>, then searching for similar vectors within tolerance radius <code>r</code>, using the estimator described below, with logarithms to the given <code>base</code> (natural logarithm is used in Pincus, 1991).</p><p>Specifically, for a finite-length timeseries <code>x</code>, an estimator for <span>$ApEn(m ,r)$</span> is</p><p class="math-container">\[ApEn(m, r, N) = \phi(x, m, r, N) -  \phi(x, m + 1, r, N),\]</p><p>where <code>N = length(x)</code> and</p><p class="math-container">\[\phi(x, k, r, N) =
\dfrac{1}{N-(k-1)\tau} \sum_{i=1}^{N - (k-1)\tau}
\log{\left(
    \sum_{j = 1}^{N-(k-1)\tau} \dfrac{\theta(d({\bf x}_i^m, {\bf x}_j^m) \leq r)}{N-(k-1)\tau}
    \right)}.\]</p><p>Here, <span>$\theta(\cdot)$</span> returns 1 if the argument is true and 0 otherwise,  <span>$d({\bf x}_i, {\bf x}_j)$</span> returns the Chebyshev distance between vectors  <span>${\bf x}_i$</span> and <span>${\bf x}_j$</span>, and the <code>k</code>-dimensional embedding vectors are constructed from the input timeseries <span>$x(t)$</span> as</p><p class="math-container">\[{\bf x}_i^k = (x(i), x(i+τ), x(i+2τ), \ldots, x(i+(k-1)\tau)).\]</p><div class="admonition is-info"><header class="admonition-header">Flexible embedding lag</header><div class="admonition-body"><p>In the original paper, they fix <code>τ = 1</code>. In our implementation, the normalization constant is modified to account for embeddings with <code>τ != 1</code>.</p></div></div></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaDynamics/ComplexityMeasures.jl/blob/6bdeaf2132c4501e675e8817979c221f1bdea337/src/complexity_measures/approximate_entropy.jl#L8-L70">source</a></section></article><h2 id="Sample-entropy"><a class="docs-heading-anchor" href="#Sample-entropy">Sample entropy</a><a id="Sample-entropy-1"></a><a class="docs-heading-anchor-permalink" href="#Sample-entropy" title="Permalink"></a></h2><article class="docstring"><header><a class="docstring-binding" id="ComplexityMeasures.SampleEntropy" href="#ComplexityMeasures.SampleEntropy"><code>ComplexityMeasures.SampleEntropy</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">SampleEntropy([x]; r = 0.2std(x), kwargs...) &lt;: ComplexityEstimator</code></pre><p>An estimator for the sample entropy complexity measure (Richman &amp; Moorman, 2000)<sup class="footnote-reference"><a id="citeref-Richman2000" href="#footnote-Richman2000">[Richman2000]</a></sup>, used with <a href="#ComplexityMeasures.complexity"><code>complexity</code></a> and <a href="#ComplexityMeasures.complexity_normalized"><code>complexity_normalized</code></a>.</p><p>The keyword argument <code>r</code> is mandatory if an input timeseries <code>x</code> is not provided.</p><p><strong>Keyword arguments</strong></p><ul><li><code>r::Real</code>: The radius used when querying for nearest neighbors around points. Its value   should be determined from the input data, for example as some proportion of the   standard deviation of the data.</li><li><code>m::Int = 1</code>: The embedding dimension.</li><li><code>τ::Int = 1</code>: The embedding lag.</li></ul><p><strong>Description</strong></p><p>An <em>estimator</em> for sample entropy using radius <code>r</code>, embedding dimension <code>m</code>, and embedding lag <code>τ</code> is</p><p class="math-container">\[SampEn(m,r, N) = -\ln{\dfrac{A(r, N)}{B(r, N)}}.\]</p><p>Here,</p><p class="math-container">\[\begin{aligned}
B(r, m, N) = \sum_{i = 1}^{N-m\tau} \sum_{j = 1, j \neq i}^{N-m\tau} \theta(d({\bf x}_i^m, {\bf x}_j^m) \leq r) \\
A(r, m, N) = \sum_{i = 1}^{N-m\tau} \sum_{j = 1, j \neq i}^{N-m\tau} \theta(d({\bf x}_i^{m+1}, {\bf x}_j^{m+1}) \leq r) \\
\end{aligned},\]</p><p>where <span>$\theta(\cdot)$</span> returns 1 if the argument is true and 0 otherwise, and <span>$d(x, y)$</span> computes the Chebyshev distance between <span>$x$</span> and <span>$y$</span>, and  <span>${\bf x}_i^{m}$</span> and <span>${\bf x}_i^{m+1}$</span> are <code>m</code>-dimensional and <code>m+1</code>-dimensional embedding vectors, where <code>k</code>-dimensional embedding vectors are constructed from the input timeseries <span>$x(t)$</span> as</p><p class="math-container">\[{\bf x}_i^k = (x(i), x(i+τ), x(i+2τ), \ldots, x(i+(k-1)\tau)).\]</p><p>Quoting Richman &amp; Moorman (2002): &quot;SampEn(m,r,N) will be defined except when B = 0, in which case no regularity has been detected, or when A = 0, which corresponds to a conditional probability of 0 and an infinite value of SampEn(m,r,N)&quot;. In these cases, <code>NaN</code> is returned.</p><p>If computing the normalized measure, then the resulting sample entropy is on <code>[0, 1]</code>.</p><div class="admonition is-info"><header class="admonition-header">Flexible embedding lag</header><div class="admonition-body"><p>The original algorithm fixes <code>τ = 1</code>. All formulas here are modified to account for any <code>τ</code>.</p></div></div><p>See also: <a href="../convenience/#ComplexityMeasures.entropy_sample"><code>entropy_sample</code></a>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaDynamics/ComplexityMeasures.jl/blob/6bdeaf2132c4501e675e8817979c221f1bdea337/src/complexity_measures/sample_entropy.jl#L9-L69">source</a></section></article><h2 id="Missing-dispersion-patterns"><a class="docs-heading-anchor" href="#Missing-dispersion-patterns">Missing dispersion patterns</a><a id="Missing-dispersion-patterns-1"></a><a class="docs-heading-anchor-permalink" href="#Missing-dispersion-patterns" title="Permalink"></a></h2><article class="docstring"><header><a class="docstring-binding" id="ComplexityMeasures.MissingDispersionPatterns" href="#ComplexityMeasures.MissingDispersionPatterns"><code>ComplexityMeasures.MissingDispersionPatterns</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">MissingDispersionPatterns &lt;: ComplexityEstimator
MissingDispersionPatterns(est = Dispersion())</code></pre><p>An estimator for the number of missing dispersion patterns (<span>$N_{MDP}$</span>), a complexity measure which can be used to detect nonlinearity in time series (Zhou et al., 2022)<sup class="footnote-reference"><a id="citeref-Zhou2022" href="#footnote-Zhou2022">[Zhou2022]</a></sup>.</p><p>Used with <a href="#ComplexityMeasures.complexity"><code>complexity</code></a> or <a href="#ComplexityMeasures.complexity_normalized"><code>complexity_normalized</code></a>, whose implementation uses <a href="../probabilities/#ComplexityMeasures.missing_outcomes"><code>missing_outcomes</code></a>.</p><p><strong>Description</strong></p><p>If used with <a href="#ComplexityMeasures.complexity"><code>complexity</code></a>, <span>$N_{MDP}$</span> is computed by first symbolising each <code>xᵢ ∈ x</code>, then embedding the resulting symbol sequence using the dispersion pattern estimator <code>est</code>, and computing the quantity</p><p class="math-container">\[N_{MDP} = L - N_{ODP},\]</p><p>where <code>L = total_outcomes(est)</code> (i.e. the total number of possible dispersion patterns), and <span>$N_{ODP}$</span> is defined as the number of <em>occurring</em> dispersion patterns.</p><p>If used with <a href="#ComplexityMeasures.complexity_normalized"><code>complexity_normalized</code></a>, then <span>$N_{MDP}^N = (L - N_{ODP})/L$</span> is computed. The authors recommend that <code>total_outcomes(est.symbolization)^est.m &lt;&lt; length(x) - est.m*est.τ + 1</code> to avoid undersampling.</p><div class="admonition is-info"><header class="admonition-header">Encoding</header><div class="admonition-body"><p><a href="../probabilities/#ComplexityMeasures.Dispersion"><code>Dispersion</code></a>&#39;s linear mapping from CDFs to integers is based on equidistant partitioning of the interval <code>[0, 1]</code>. This is slightly different from Zhou et al. (2022), which uses the linear mapping <span>$s_i := \text{round}(y + 0.5)$</span>.</p></div></div><p><strong>Usage</strong></p><p>In Zhou et al. (2022), <a href="#ComplexityMeasures.MissingDispersionPatterns"><code>MissingDispersionPatterns</code></a> is used to detect nonlinearity in time series by comparing the <span>$N_{MDP}$</span> for a time series <code>x</code> to <span>$N_{MDP}$</span> values for an ensemble of surrogates of <code>x</code>. If <span>$N_{MDP} &gt; q_{MDP}^{WIAAFT}$</span>, where <span>$q_{MDP}^{WIAAFT}$</span> is some <code>q</code>-th quantile of the surrogate ensemble, then it is taken as evidence for nonlinearity.</p><p>See also: <a href="../probabilities/#ComplexityMeasures.Dispersion"><code>Dispersion</code></a>, <a href="#ComplexityMeasures.ReverseDispersion"><code>ReverseDispersion</code></a>, <a href="../probabilities/#ComplexityMeasures.total_outcomes"><code>total_outcomes</code></a>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaDynamics/ComplexityMeasures.jl/blob/6bdeaf2132c4501e675e8817979c221f1bdea337/src/complexity_measures/missing_dispersion.jl#L7-L53">source</a></section></article><h2 id="Reverse-dispersion-entropy"><a class="docs-heading-anchor" href="#Reverse-dispersion-entropy">Reverse dispersion entropy</a><a id="Reverse-dispersion-entropy-1"></a><a class="docs-heading-anchor-permalink" href="#Reverse-dispersion-entropy" title="Permalink"></a></h2><article class="docstring"><header><a class="docstring-binding" id="ComplexityMeasures.ReverseDispersion" href="#ComplexityMeasures.ReverseDispersion"><code>ComplexityMeasures.ReverseDispersion</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">ReverseDispersion &lt;: ComplexityEstimator
ReverseDispersion(; c = 3, m = 2, τ = 1, check_unique = true)</code></pre><p>Estimator for the reverse dispersion entropy complexity measure (Li et al., 2019)<sup class="footnote-reference"><a id="citeref-Li2019" href="#footnote-Li2019">[Li2019]</a></sup>.</p><p><strong>Description</strong></p><p>Li et al. (2021)<sup class="footnote-reference"><a id="citeref-Li2019" href="#footnote-Li2019">[Li2019]</a></sup> defines the reverse dispersion entropy as</p><p class="math-container">\[H_{rde} = \sum_{i = 1}^{c^m} \left(p_i - \dfrac{1}{{c^m}} \right)^2 =
\left( \sum_{i=1}^{c^m} p_i^2 \right) - \dfrac{1}{c^{m}}\]</p><p>where the probabilities <span>$p_i$</span> are obtained precisely as for the <a href="../probabilities/#ComplexityMeasures.Dispersion"><code>Dispersion</code></a> probability estimator. Relative frequencies of dispersion patterns are computed using the given <code>encoding</code> scheme , which defaults to encoding using the normal cumulative distribution function (NCDF), as implemented by <a href="../encodings/#ComplexityMeasures.GaussianCDFEncoding"><code>GaussianCDFEncoding</code></a>, using embedding dimension <code>m</code> and embedding delay <code>τ</code>. Recommended parameter values<sup class="footnote-reference"><a id="citeref-Li2018" href="#footnote-Li2018">[Li2018]</a></sup> are <code>m ∈ [2, 3]</code>, <code>τ = 1</code> for the embedding, and <code>c ∈ [3, 4, …, 8]</code> categories for the Gaussian mapping.</p><p>If normalizing, then the reverse dispersion entropy is normalized to <code>[0, 1]</code>.</p><p>The minimum value of <span>$H_{rde}$</span> is zero and occurs precisely when the dispersion pattern distribution is flat, which occurs when all <span>$p_i$</span>s are equal to <span>$1/c^m$</span>. Because <span>$H_{rde} \geq 0$</span>, <span>$H_{rde}$</span> can therefore be said to be a measure of how far the dispersion pattern probability distribution is from white noise.</p><p><strong>Data requirements</strong></p><p>The input must have more than one unique element for the default <a href="@ref"><code>GaussianEncoding</code></a> to be well-defined. Li et al. (2018) recommends that <code>x</code> has at least 1000 data points.</p><p>If <code>check_unique == true</code> (default), then it is checked that the input has more than one unique value. If <code>check_unique == false</code> and the input only has one unique element, then a <code>InexactError</code> is thrown when trying to compute probabilities.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaDynamics/ComplexityMeasures.jl/blob/6bdeaf2132c4501e675e8817979c221f1bdea337/src/complexity_measures/reverse_dispersion_entropy.jl#L6-L47">source</a></section></article><h2 id="Statistical-complexity"><a class="docs-heading-anchor" href="#Statistical-complexity">Statistical complexity</a><a id="Statistical-complexity-1"></a><a class="docs-heading-anchor-permalink" href="#Statistical-complexity" title="Permalink"></a></h2><article class="docstring"><header><a class="docstring-binding" id="ComplexityMeasures.StatisticalComplexity" href="#ComplexityMeasures.StatisticalComplexity"><code>ComplexityMeasures.StatisticalComplexity</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">StatisticalComplexity &lt;: ComplexityEstimator
StatisticalComplexity([x]; kwargs...)</code></pre><p>An estimator for the statistical complexity and entropy according to Rosso et al. (2007)<sup class="footnote-reference"><a id="citeref-Rosso2007" href="#footnote-Rosso2007">[Rosso2007]</a></sup>(@ref), used with <a href="#ComplexityMeasures.complexity"><code>complexity</code></a>.</p><p><strong>Keyword arguments</strong></p><ul><li><code>est::ProbabilitiesEstimator = SymbolicPermutation()</code>: which estimator to use to get the probabilities</li><li><code>dist&lt;:SemiMetric = JSDivergence()</code>: the distance measure between the estimated probability   distribution and a uniform distribution with the same maximal number of bins</li></ul><p><strong>Description</strong></p><p>Statistical complexity is defined as</p><p class="math-container">\[C_q[P] = \mathcal{H}_q\cdot \mathcal{Q}_q[P],\]</p><p>where <span>$Q_q$</span> is a &quot;disequilibrium&quot; obtained from a distance-measure and <code>H_q</code> a disorder measure. In the original paper<sup class="footnote-reference"><a id="citeref-Rosso2007" href="#footnote-Rosso2007">[Rosso2007]</a></sup>, this complexity measure was defined via an ordinal pattern-based probability distribution, the Shannon entropy and the Jensen-Shannon divergence as a distance measure. This implementation allows for a generalization of the complexity measure as developed in <sup class="footnote-reference"><a id="citeref-Rosso2013" href="#footnote-Rosso2013">[Rosso2013]</a></sup>. Here, <span>$H_q$</span><code>can be the (q-order) Shannon-, Renyi or Tsallis entropy and</code><code>Q_q</code>` based either on the Euclidean, Wooters, Kullback, q-Kullback, Jensen or q-Jensen distance as</p><p class="math-container">\[Q_q[P] = Q_q^0\cdot D[P, P_e],\]</p><p>where <span>$D[P, P_e]$</span> is the distance between the obtained distribution <span>$P$</span> and a uniform distribution with the same maximum number of bins, measured by the distance measure <code>dist</code>.</p><p><strong>Usage</strong></p><p>The statistical complexity is exclusively used in combination with the related information measure (entropy). <code>complexity(c::StatisticalComplexity, x)</code> returns only the statistical complexity. The entropy can be accessed as a <code>Ref</code> value of the struct as</p><pre><code class="language-julia hljs">x = randn(100)
c = StatisticalComplexity
compl = complexity(c, x)
entr = c.entr_val[]</code></pre><p>To obtain both the entropy and the statistical complexity together as a <code>Tuple</code>, use the wrapper <a href="#ComplexityMeasures.entropy_complexity"><code>entropy_complexity</code></a>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaDynamics/ComplexityMeasures.jl/blob/6bdeaf2132c4501e675e8817979c221f1bdea337/src/complexity_measures/statistical_complexity.jl#L6-L62">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="ComplexityMeasures.entropy_complexity" href="#ComplexityMeasures.entropy_complexity"><code>ComplexityMeasures.entropy_complexity</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">entropy_complexity(c::StatisticalComplexity, x)</code></pre><p>Return both the entropy and the corresponding <a href="#ComplexityMeasures.StatisticalComplexity"><code>StatisticalComplexity</code></a>. Useful when wanting to plot data on the &quot;entropy-complexity plane&quot;. See also <a href="#ComplexityMeasures.entropy_complexity_curves"><code>entropy_complexity_curves</code></a>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaDynamics/ComplexityMeasures.jl/blob/6bdeaf2132c4501e675e8817979c221f1bdea337/src/complexity_measures/statistical_complexity.jl#L78-L84">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="ComplexityMeasures.entropy_complexity_curves" href="#ComplexityMeasures.entropy_complexity_curves"><code>ComplexityMeasures.entropy_complexity_curves</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">entropy_complexity_curves(c::StatisticalComplexity; num_max=1, num_min=1000) -&gt; (min_entropy_complexity, max_entropy_complexity)</code></pre><p>Calculate the maximum complexity-entropy curve for the statistical complexity according to <sup class="footnote-reference"><a id="citeref-Rosso2007" href="#footnote-Rosso2007">[Rosso2007]</a></sup> for <code>num_max * total_outcomes(c.est)</code> different values of the normalized information measure of choice (in case of the maximum complexity curves) and <code>num_min</code> different values of the normalized information measure of choice (in case of the minimum complexity curve).</p><p><strong>Description</strong></p><p>The way the statistical complexity is designed, there is a minimum and maximum possible complexity for data with a given permutation entropy. The calculation time of the maximum complexity curve grows as <code>O(total_outcomes(c.est)^2)</code>, and thus takes very long for high numbers of outcomes. This function is inspired by S. Sippels implementation in statcomp <sup class="footnote-reference"><a id="citeref-statcomp" href="#footnote-statcomp">[statcomp]</a></sup>.</p><p>This function will work with any <code>ProbabilitiesEstimator</code> where <code>total_outcomes</code>(@ref) is known a priori.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaDynamics/ComplexityMeasures.jl/blob/6bdeaf2132c4501e675e8817979c221f1bdea337/src/complexity_measures/statistical_complexity.jl#L113-L135">source</a></section></article><section class="footnotes is-size-7"><ul><li class="footnote" id="footnote-Pincus1991"><a class="tag is-link" href="#citeref-Pincus1991">Pincus1991</a>Pincus, S. M. (1991). Approximate entropy as a measure of system complexity. Proceedings of the National Academy of Sciences, 88(6), 2297-2301.</li><li class="footnote" id="footnote-Richman2000"><a class="tag is-link" href="#citeref-Richman2000">Richman2000</a>Richman, J. S., &amp; Moorman, J. R. (2000). Physiological time-series analysis using approximate entropy and sample entropy. American Journal of Physiology-Heart and Circulatory Physiology, 278(6), H2039-H2049.</li><li class="footnote" id="footnote-Zhou2022"><a class="tag is-link" href="#citeref-Zhou2022">Zhou2022</a>Zhou, Q., Shang, P., &amp; Zhang, B. (2022). Using missing dispersion patterns to detect determinism and nonlinearity in time series data. Nonlinear Dynamics, 1-20.</li><li class="footnote" id="footnote-Li2019"><a class="tag is-link" href="#citeref-Li2019">Li2019</a>Li, Y., Gao, X., &amp; Wang, L. (2019). Reverse dispersion entropy: a new complexity measure for sensor signal. Sensors, 19(23), 5203.</li><li class="footnote" id="footnote-Rosso2007"><a class="tag is-link" href="#citeref-Rosso2007">Rosso2007</a>Rosso, O. A., Larrondo, H. A., Martin, M. T., Plastino, A., &amp; Fuentes, M. A. (2007).         <a href="https://doi.org/10.1103/PhysRevLett.99.154102">Distinguishing noise from chaos</a>.         Physical review letters, 99(15), 154102.</li><li class="footnote" id="footnote-Rosso2013"><a class="tag is-link" href="#citeref-Rosso2013">Rosso2013</a>Rosso, O. A. (2013) Generalized Statistical Complexity: A New Tool for Dynamical Systems.</li><li class="footnote" id="footnote-Rosso2007"><a class="tag is-link" href="#citeref-Rosso2007">Rosso2007</a>Rosso, O. A., Larrondo, H. A., Martin, M. T., Plastino, A., &amp; Fuentes, M. A. (2007).         <a href="https://doi.org/10.1103/PhysRevLett.99.154102">Distinguishing noise from chaos</a>.         Physical review letters, 99(15), 154102.</li><li class="footnote" id="footnote-statcomp"><a class="tag is-link" href="#citeref-statcomp">statcomp</a>Sippel, S., Lange, H., Gans, F. (2019).         <a href="https://cran.r-project.org/web/packages/statcomp/index.html">statcomp: Statistical Complexity and Information Measures for Time Series Analysis</a></li></ul></section></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../entropies/">« Entropies</a><a class="docs-footer-nextpage" href="../convenience/">Convenience functions »</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 0.27.25 on <span class="colophon-date" title="Wednesday 26 July 2023 12:57">Wednesday 26 July 2023</span>. Using Julia version 1.9.2.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
