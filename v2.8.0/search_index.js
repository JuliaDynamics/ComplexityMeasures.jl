var documenterSearchIndex = {"docs":
[{"location":"complexity/#Complexity-measures","page":"Complexity measures","title":"Complexity measures","text":"","category":"section"},{"location":"complexity/","page":"Complexity measures","title":"Complexity measures","text":"In this page we document estimators for complexity measures that are not entropies in the strict mathematical sense. The API is almost identical to entropy and is defined by:","category":"page"},{"location":"complexity/","page":"Complexity measures","title":"Complexity measures","text":"complexity\ncomplexity_normalized\nComplexityEstimator","category":"page"},{"location":"complexity/#Complexity-measures-API","page":"Complexity measures","title":"Complexity measures API","text":"","category":"section"},{"location":"complexity/","page":"Complexity measures","title":"Complexity measures","text":"complexity\ncomplexity_normalized\nComplexityEstimator","category":"page"},{"location":"complexity/#ComplexityMeasures.complexity","page":"Complexity measures","title":"ComplexityMeasures.complexity","text":"complexity(c::ComplexityEstimator, x)\n\nEstimate a complexity measure according to c for input data x, where c can be any of the following estimators:\n\nReverseDispersion.\nApproximateEntropy.\nSampleEntropy.\nMissingDispersionPatterns.\n\n\n\n\n\n","category":"function"},{"location":"complexity/#ComplexityMeasures.complexity_normalized","page":"Complexity measures","title":"ComplexityMeasures.complexity_normalized","text":"complexity_normalized(c::ComplexityEstimator, x) → m ∈ [a, b]\n\nThe same as complexity, but the result is normalized to the interval [a, b], where [a, b] depends on c.\n\n\n\n\n\n","category":"function"},{"location":"complexity/#ComplexityMeasures.ComplexityEstimator","page":"Complexity measures","title":"ComplexityMeasures.ComplexityEstimator","text":"ComplexityEstimator\n\nSupertype for estimators for various complexity measures that are not entropies in the strict mathematical sense. See complexity for all available estimators.\n\n\n\n\n\n","category":"type"},{"location":"complexity/#Approximate-entropy","page":"Complexity measures","title":"Approximate entropy","text":"","category":"section"},{"location":"complexity/","page":"Complexity measures","title":"Complexity measures","text":"ApproximateEntropy","category":"page"},{"location":"complexity/#ComplexityMeasures.ApproximateEntropy","page":"Complexity measures","title":"ComplexityMeasures.ApproximateEntropy","text":"ApproximateEntropy <: ComplexityEstimator\nApproximateEntropy([x]; r = 0.2std(x), kwargs...)\n\nAn estimator for the approximate entropy (ApEn; Pincus, 1991)[Pincus1991] complexity measure, used with complexity.\n\nThe keyword argument r is mandatory if an input timeseries x is not provided.\n\nKeyword arguments\n\nr::Real: The radius used when querying for nearest neighbors around points. Its value   should be determined from the input data, for example as some proportion of the   standard deviation of the data.\nm::Int = 2: The embedding dimension.\nτ::Int = 1: The embedding lag.\nbase::Real = MathConstants.e: The base to use for the logarithm. Pincus (1991) uses the   natural logarithm.\n\nDescription\n\nApproximate entropy is defined as\n\nApEn(m r) = lim_N to infty left phi(x m r) - phi(x m + 1 r) right\n\nApproximate entropy is estimated for a timeseries x, by first embedding x using embedding dimension m and embedding lag τ, then searching for similar vectors within tolerance radius r, using the estimator described below, with logarithms to the given base (natural logarithm is used in Pincus, 1991).\n\nSpecifically, for a finite-length timeseries x, an estimator for ApEn(m r) is\n\nApEn(m r N) = phi(x m r N) -  phi(x m + 1 r N)\n\nwhere N = length(x) and\n\nphi(x k r N) =\ndfrac1N-(k-1)tau sum_i=1^N - (k-1)tau\nlogleft(\n    sum_j = 1^N-(k-1)tau dfractheta(d(bf x_i^m bf x_j^m) leq r)N-(k-1)tau\n    right)\n\nHere, theta(cdot) returns 1 if the argument is true and 0 otherwise,  d(bf x_i bf x_j) returns the Chebyshev distance between vectors  bf x_i and bf x_j, and the k-dimensional embedding vectors are constructed from the input timeseries x(t) as\n\nbf x_i^k = (x(i) x(i+τ) x(i+2τ) ldots x(i+(k-1)tau))\n\nnote: Flexible embedding lag\nIn the original paper, they fix τ = 1. In our implementation, the normalization constant is modified to account for embeddings with τ != 1.\n\n[Pincus1991]: Pincus, S. M. (1991). Approximate entropy as a measure of system complexity. Proceedings of the National Academy of Sciences, 88(6), 2297-2301.\n\n\n\n\n\n","category":"type"},{"location":"complexity/#Sample-entropy","page":"Complexity measures","title":"Sample entropy","text":"","category":"section"},{"location":"complexity/","page":"Complexity measures","title":"Complexity measures","text":"SampleEntropy","category":"page"},{"location":"complexity/#ComplexityMeasures.SampleEntropy","page":"Complexity measures","title":"ComplexityMeasures.SampleEntropy","text":"SampleEntropy([x]; r = 0.2std(x), kwargs...) <: ComplexityEstimator\n\nAn estimator for the sample entropy complexity measure (Richman & Moorman, 2000)[Richman2000], used with complexity and complexity_normalized.\n\nThe keyword argument r is mandatory if an input timeseries x is not provided.\n\nKeyword arguments\n\nr::Real: The radius used when querying for nearest neighbors around points. Its value   should be determined from the input data, for example as some proportion of the   standard deviation of the data.\nm::Int = 1: The embedding dimension.\nτ::Int = 1: The embedding lag.\n\nDescription\n\nAn estimator for sample entropy using radius r, embedding dimension m, and embedding lag τ is\n\nSampEn(mr N) = -lndfracA(r N)B(r N)\n\nHere,\n\nbeginaligned\nB(r m N) = sum_i = 1^N-mtau sum_j = 1 j neq i^N-mtau theta(d(bf x_i^m bf x_j^m) leq r) \nA(r m N) = sum_i = 1^N-mtau sum_j = 1 j neq i^N-mtau theta(d(bf x_i^m+1 bf x_j^m+1) leq r) \nendaligned\n\nwhere theta(cdot) returns 1 if the argument is true and 0 otherwise, and d(x y) computes the Chebyshev distance between x and y, and  bf x_i^m and bf x_i^m+1 are m-dimensional and m+1-dimensional embedding vectors, where k-dimensional embedding vectors are constructed from the input timeseries x(t) as\n\nbf x_i^k = (x(i) x(i+τ) x(i+2τ) ldots x(i+(k-1)tau))\n\nQuoting Richman & Moorman (2002): \"SampEn(m,r,N) will be defined except when B = 0, in which case no regularity has been detected, or when A = 0, which corresponds to a conditional probability of 0 and an infinite value of SampEn(m,r,N)\". In these cases, NaN is returned.\n\nIf computing the normalized measure, then the resulting sample entropy is on [0, 1].\n\nnote: Flexible embedding lag\nThe original algorithm fixes τ = 1. All formulas here are modified to account for any τ.\n\nSee also: entropy_sample.\n\n[Richman2000]: Richman, J. S., & Moorman, J. R. (2000). Physiological time-series analysis using approximate entropy and sample entropy. American Journal of Physiology-Heart and Circulatory Physiology, 278(6), H2039-H2049.\n\n\n\n\n\n","category":"type"},{"location":"complexity/#Missing-dispersion-patterns","page":"Complexity measures","title":"Missing dispersion patterns","text":"","category":"section"},{"location":"complexity/","page":"Complexity measures","title":"Complexity measures","text":"MissingDispersionPatterns","category":"page"},{"location":"complexity/#ComplexityMeasures.MissingDispersionPatterns","page":"Complexity measures","title":"ComplexityMeasures.MissingDispersionPatterns","text":"MissingDispersionPatterns <: ComplexityEstimator\nMissingDispersionPatterns(est = Dispersion())\n\nAn estimator for the number of missing dispersion patterns (N_MDP), a complexity measure which can be used to detect nonlinearity in time series (Zhou et al., 2022)[Zhou2022].\n\nUsed with complexity or complexity_normalized, whose implementation uses missing_outcomes.\n\nDescription\n\nIf used with complexity, N_MDP is computed by first symbolising each xᵢ ∈ x, then embedding the resulting symbol sequence using the dispersion pattern estimator est, and computing the quantity\n\nN_MDP = L - N_ODP\n\nwhere L = total_outcomes(est) (i.e. the total number of possible dispersion patterns), and N_ODP is defined as the number of occurring dispersion patterns.\n\nIf used with complexity_normalized, then N_MDP^N = (L - N_ODP)L is computed. The authors recommend that total_outcomes(est.symbolization)^est.m << length(x) - est.m*est.τ + 1 to avoid undersampling.\n\nnote: Encoding\nDispersion's linear mapping from CDFs to integers is based on equidistant partitioning of the interval [0, 1]. This is slightly different from Zhou et al. (2022), which uses the linear mapping s_i = textround(y + 05).\n\nUsage\n\nIn Zhou et al. (2022), MissingDispersionPatterns is used to detect nonlinearity in time series by comparing the N_MDP for a time series x to N_MDP values for an ensemble of surrogates of x. If N_MDP  q_MDP^WIAAFT, where q_MDP^WIAAFT is some q-th quantile of the surrogate ensemble, then it is taken as evidence for nonlinearity.\n\nSee also: Dispersion, ReverseDispersion, total_outcomes.\n\n[Zhou2022]: Zhou, Q., Shang, P., & Zhang, B. (2022). Using missing dispersion patterns to detect determinism and nonlinearity in time series data. Nonlinear Dynamics, 1-20.\n\n\n\n\n\n","category":"type"},{"location":"complexity/#Reverse-dispersion-entropy","page":"Complexity measures","title":"Reverse dispersion entropy","text":"","category":"section"},{"location":"complexity/","page":"Complexity measures","title":"Complexity measures","text":"ReverseDispersion","category":"page"},{"location":"complexity/#ComplexityMeasures.ReverseDispersion","page":"Complexity measures","title":"ComplexityMeasures.ReverseDispersion","text":"ReverseDispersion <: ComplexityEstimator\nReverseDispersion(; c = 3, m = 2, τ = 1, check_unique = true)\n\nEstimator for the reverse dispersion entropy complexity measure (Li et al., 2019)[Li2019].\n\nDescription\n\nLi et al. (2021)[Li2019] defines the reverse dispersion entropy as\n\nH_rde = sum_i = 1^c^m left(p_i - dfrac1c^m right)^2 =\nleft( sum_i=1^c^m p_i^2 right) - dfrac1c^m\n\nwhere the probabilities p_i are obtained precisely as for the Dispersion probability estimator. Relative frequencies of dispersion patterns are computed using the given encoding scheme , which defaults to encoding using the normal cumulative distribution function (NCDF), as implemented by GaussianCDFEncoding, using embedding dimension m and embedding delay τ. Recommended parameter values[Li2018] are m ∈ [2, 3], τ = 1 for the embedding, and c ∈ [3, 4, …, 8] categories for the Gaussian mapping.\n\nIf normalizing, then the reverse dispersion entropy is normalized to [0, 1].\n\nThe minimum value of H_rde is zero and occurs precisely when the dispersion pattern distribution is flat, which occurs when all p_is are equal to 1c^m. Because H_rde geq 0, H_rde can therefore be said to be a measure of how far the dispersion pattern probability distribution is from white noise.\n\nData requirements\n\nThe input must have more than one unique element for the default GaussianEncoding to be well-defined. Li et al. (2018) recommends that x has at least 1000 data points.\n\nIf check_unique == true (default), then it is checked that the input has more than one unique value. If check_unique == false and the input only has one unique element, then a InexactError is thrown when trying to compute probabilities.\n\n[Li2019]: Li, Y., Gao, X., & Wang, L. (2019). Reverse dispersion entropy: a new complexity measure for sensor signal. Sensors, 19(23), 5203.\n\n\n\n\n\n","category":"type"},{"location":"complexity/#Statistical-complexity","page":"Complexity measures","title":"Statistical complexity","text":"","category":"section"},{"location":"complexity/","page":"Complexity measures","title":"Complexity measures","text":"StatisticalComplexity\nentropy_complexity\nentropy_complexity_curves","category":"page"},{"location":"complexity/#ComplexityMeasures.StatisticalComplexity","page":"Complexity measures","title":"ComplexityMeasures.StatisticalComplexity","text":"StatisticalComplexity <: ComplexityEstimator\nStatisticalComplexity([x]; kwargs...)\n\nAn estimator for the statistical complexity and entropy according to Rosso et al. (2007)[Rosso2007](@ref), used with complexity.\n\nKeyword arguments\n\nest::ProbabilitiesEstimator = SymbolicPermutation(): which estimator to use to get the probabilities\ndist<:SemiMetric = JSDivergence(): the distance measure between the estimated probability   distribution and a uniform distribution with the same maximal number of bins\n\nDescription\n\nStatistical complexity is defined as\n\nC_qP = mathcalH_qcdot mathcalQ_qP\n\nwhere Q_q is a \"disequilibrium\" obtained from a distance-measure and H_q a disorder measure. In the original paper[Rosso2007], this complexity measure was defined via an ordinal pattern-based probability distribution, the Shannon entropy and the Jensen-Shannon divergence as a distance measure. This implementation allows for a generalization of the complexity measure as developed in [Rosso2013]. Here, H_qcan be the (q-order) Shannon-, Renyi or Tsallis entropy andQ_q` based either on the Euclidean, Wooters, Kullback, q-Kullback, Jensen or q-Jensen distance as\n\nQ_qP = Q_q^0cdot DP P_e\n\nwhere DP P_e is the distance between the obtained distribution P and a uniform distribution with the same maximum number of bins, measured by the distance measure dist.\n\nUsage\n\nThe statistical complexity is exclusively used in combination with the related information measure (entropy). complexity(c::StatisticalComplexity, x) returns only the statistical complexity. The entropy can be accessed as a Ref value of the struct as\n\nx = randn(100)\nc = StatisticalComplexity\ncompl = complexity(c, x)\nentr = c.entr_val[]\n\nTo obtain both the entropy and the statistical complexity together as a Tuple, use the wrapper entropy_complexity.\n\n[Rosso2007]: Rosso, O. A., Larrondo, H. A., Martin, M. T., Plastino, A., & Fuentes, M. A. (2007).         Distinguishing noise from chaos.         Physical review letters, 99(15), 154102.\n\n[Rosso2013]: Rosso, O. A. (2013) Generalized Statistical Complexity: A New Tool for Dynamical Systems.\n\n\n\n\n\n","category":"type"},{"location":"complexity/#ComplexityMeasures.entropy_complexity","page":"Complexity measures","title":"ComplexityMeasures.entropy_complexity","text":"entropy_complexity(c::StatisticalComplexity, x)\n\nReturn both the entropy and the corresponding StatisticalComplexity. Useful when wanting to plot data on the \"entropy-complexity plane\". See also entropy_complexity_curves.\n\n\n\n\n\n","category":"function"},{"location":"complexity/#ComplexityMeasures.entropy_complexity_curves","page":"Complexity measures","title":"ComplexityMeasures.entropy_complexity_curves","text":"entropy_complexity_curves(c::StatisticalComplexity; num_max=1, num_min=1000) -> (min_entropy_complexity, max_entropy_complexity)\n\nCalculate the maximum complexity-entropy curve for the statistical complexity according to [Rosso2007] for num_max * total_outcomes(c.est) different values of the normalized information measure of choice (in case of the maximum complexity curves) and num_min different values of the normalized information measure of choice (in case of the minimum complexity curve).\n\nDescription\n\nThe way the statistical complexity is designed, there is a minimum and maximum possible complexity for data with a given permutation entropy. The calculation time of the maximum complexity curve grows as O(total_outcomes(c.est)^2), and thus takes very long for high numbers of outcomes. This function is inspired by S. Sippels implementation in statcomp [statcomp].\n\nThis function will work with any ProbabilitiesEstimator where total_outcomes(@ref) is known a priori.\n\n[Rosso2007]: Rosso, O. A., Larrondo, H. A., Martin, M. T., Plastino, A., & Fuentes, M. A. (2007).         Distinguishing noise from chaos.         Physical review letters, 99(15), 154102.\n\n[statcomp]: Sippel, S., Lange, H., Gans, F. (2019).         statcomp: Statistical Complexity and Information Measures for Time Series Analysis\n\n\n\n\n\n","category":"function"},{"location":"entropies/#entropies","page":"Entropies","title":"Entropies","text":"","category":"section"},{"location":"entropies/#Entropies-API","page":"Entropies","title":"Entropies API","text":"","category":"section"},{"location":"entropies/","page":"Entropies","title":"Entropies","text":"The entropies API is defined by","category":"page"},{"location":"entropies/","page":"Entropies","title":"Entropies","text":"EntropyDefinition\nentropy\nDiscreteEntropyEstimator\nDifferentialEntropyEstimator","category":"page"},{"location":"entropies/","page":"Entropies","title":"Entropies","text":"Please be sure you have read the Terminology section before going through the API here, to have a good idea of the different \"flavors\" of entropies and how they all come together over the common interface of the entropy function.","category":"page"},{"location":"entropies/#Entropy-definitions","page":"Entropies","title":"Entropy definitions","text":"","category":"section"},{"location":"entropies/","page":"Entropies","title":"Entropies","text":"EntropyDefinition\nShannon\nRenyi\nTsallis\nKaniadakis\nCurado\nStretchedExponential","category":"page"},{"location":"entropies/#ComplexityMeasures.EntropyDefinition","page":"Entropies","title":"ComplexityMeasures.EntropyDefinition","text":"EntropyDefinition\n\nEntropyDefinition is the supertype of all types that encapsulate definitions of (generalized) entropies. These also serve as estimators of discrete entropies, see description below.\n\nCurrently implemented entropy definitions are:\n\nRenyi.\nTsallis.\nShannon, which is a subcase of the above two in the limit q → 1.\nKaniadakis.\nCurado.\nStretchedExponential.\n\nThese types can be given as inputs to entropy or entropy_normalized.\n\nDescription\n\nMathematically speaking, generalized entropies are just nonnegative functions of probability distributions that verify certain (entropy-type-dependent) axioms. Amigó et al.'s[Amigó2018] summary paper gives a nice overview.\n\nHowever, for a software implementation computing entropies in practice, definitions is not really what matters; estimators matter. Because in the practical sense, one needs to estimate a definition from finite data, and different ways of estimating a quantity come with their own pros and cons.\n\nThat is why the type DiscreteEntropyEstimator exists, which is what is actually given to entropy. Some ways to estimate a discrete entropy only apply to a specific entropy definition. For estimators that can be applied to various entropy definitions, this is specified by providing an instance of EntropyDefinition to the estimator.\n\n[Amigó2018]: Amigó, J. M., Balogh, S. G., & Hernández, S. (2018). A brief review of generalized entropies. Entropy, 20(11), 813.\n\n\n\n\n\n","category":"type"},{"location":"entropies/#ComplexityMeasures.Shannon","page":"Entropies","title":"ComplexityMeasures.Shannon","text":"Shannon <: EntropyDefinition\nShannon(; base = 2)\n\nThe Shannon[Shannon1948] entropy, used with entropy to compute:\n\nH(p) = - sum_i pi log(pi)\n\nwith the log at the given base.\n\nThe maximum value of the Shannon entropy is log_base(L), which is the entropy of the uniform distribution with L the total_outcomes.\n\n[Shannon1948]: C. E. Shannon, Bell Systems Technical Journal 27, pp 379 (1948)\n\n\n\n\n\n","category":"type"},{"location":"entropies/#ComplexityMeasures.Renyi","page":"Entropies","title":"ComplexityMeasures.Renyi","text":"Renyi <: EntropyDefinition\nRenyi(q, base = 2)\nRenyi(; q = 1.0, base = 2)\n\nThe Rényi[Rényi1960] generalized order-q entropy, used with entropy to compute an entropy with units given by base (typically 2 or MathConstants.e).\n\nDescription\n\nLet p be an array of probabilities (summing to 1). Then the Rényi generalized entropy is\n\nH_q(p) = frac11-q log left(sum_i pi^qright)\n\nand generalizes other known entropies, like e.g. the information entropy (q = 1, see [Shannon1948]), the maximum entropy (q=0, also known as Hartley entropy), or the correlation entropy (q = 2, also known as collision entropy).\n\nThe maximum value of the Rényi entropy is log_base(L), which is the entropy of the uniform distribution with L the total_outcomes.\n\n[Rényi1960]: A. Rényi, Proceedings of the fourth Berkeley Symposium on Mathematics, Statistics and Probability, pp 547 (1960)\n\n[Shannon1948]: C. E. Shannon, Bell Systems Technical Journal 27, pp 379 (1948)\n\n\n\n\n\n","category":"type"},{"location":"entropies/#ComplexityMeasures.Tsallis","page":"Entropies","title":"ComplexityMeasures.Tsallis","text":"Tsallis <: EntropyDefinition\nTsallis(q; k = 1.0, base = 2)\nTsallis(; q = 1.0, k = 1.0, base = 2)\n\nThe Tsallis[Tsallis1988] generalized order-q entropy, used with entropy to compute an entropy.\n\nbase only applies in the limiting case q == 1, in which the Tsallis entropy reduces to Shannon entropy.\n\nDescription\n\nThe Tsallis entropy is a generalization of the Boltzmann-Gibbs entropy, with k standing for the Boltzmann constant. It is defined as\n\nS_q(p) = frackq - 1left(1 - sum_i pi^qright)\n\nThe maximum value of the Tsallis entropy is ``k(L^1 - q - 1)(1 - q), with L the total_outcomes.\n\n[Tsallis1988]: Tsallis, C. (1988). Possible generalization of Boltzmann-Gibbs statistics. Journal of statistical physics, 52(1), 479-487.\n\n\n\n\n\n","category":"type"},{"location":"entropies/#ComplexityMeasures.Kaniadakis","page":"Entropies","title":"ComplexityMeasures.Kaniadakis","text":"Kaniadakis <: EntropyDefinition\nKaniadakis(; κ = 1.0, base = 2.0)\n\nThe Kaniadakis entropy (Tsallis, 2009)[Tsallis2009], used with entropy to compute\n\nH_K(p) = -sum_i=1^N p_i f_kappa(p_i)\n\nf_kappa (x) = dfracx^kappa - x^-kappa2kappa\n\nwhere if kappa = 0, regular logarithm to the given base is used, and 0 probabilities are skipped.\n\n[Tsallis2009]: Tsallis, C. (2009). Introduction to nonextensive statistical mechanics: approaching a complex world. Springer, 1(1), 2-1.\n\n\n\n\n\n","category":"type"},{"location":"entropies/#ComplexityMeasures.Curado","page":"Entropies","title":"ComplexityMeasures.Curado","text":"Curado <: EntropyDefinition\nCurado(; b = 1.0)\n\nThe Curado entropy (Curado & Nobre, 2004)[Curado2004], used with entropy to compute\n\nH_C(p) = left( sum_i=1^N e^-b p_i right) + e^-b - 1\n\nwith b ∈ ℛ, b > 0, where the terms outside the sum ensures that H_C(0) = H_C(1) = 0.\n\nThe maximum entropy for Curado is L(1 - exp(-bL)) + exp(-b) - 1 with L the total_outcomes.\n\n[Curado2004]: Curado, E. M., & Nobre, F. D. (2004). On the stability of analytic entropic forms. Physica A: Statistical Mechanics and its Applications, 335(1-2), 94-106.\n\n\n\n\n\n","category":"type"},{"location":"entropies/#ComplexityMeasures.StretchedExponential","page":"Entropies","title":"ComplexityMeasures.StretchedExponential","text":"StretchedExponential <: EntropyDefinition\nStretchedExponential(; η = 2.0, base = 2)\n\nThe stretched exponential, or Anteneodo-Plastino, entropy (Anteneodo & Plastino, 1999[Anteneodo1999]), used with entropy to compute\n\nS_eta(p) = sum_i = 1^N\nGamma left( dfraceta + 1eta - log_base(p_i) right) -\np_i Gamma left( dfraceta + 1eta right)\n\nwhere eta geq 0, Gamma(cdot cdot) is the upper incomplete Gamma function, and Gamma(cdot) = Gamma(cdot 0) is the Gamma function. Reduces to Shannon entropy for η = 1.0.\n\nThe maximum entropy for StrechedExponential is a rather complicated expression involving incomplete Gamma functions (see source code).\n\n[Anteneodo1999]: Anteneodo, C., & Plastino, A. R. (1999). Maximum entropy approach to stretched exponential probability distributions. Journal of Physics A: Mathematical and General, 32(7), 1089.\n\n\n\n\n\n","category":"type"},{"location":"entropies/#Discrete-entropy","page":"Entropies","title":"Discrete entropy","text":"","category":"section"},{"location":"entropies/","page":"Entropies","title":"Entropies","text":"entropy(::EntropyDefinition, ::ProbabilitiesEstimator, ::Any)\nentropy_maximum\nentropy_normalized","category":"page"},{"location":"entropies/#ComplexityMeasures.entropy-Tuple{EntropyDefinition, ProbabilitiesEstimator, Any}","page":"Entropies","title":"ComplexityMeasures.entropy","text":"entropy([e::DiscreteEntropyEstimator,] probs::Probabilities)\nentropy([e::DiscreteEntropyEstimator,] est::ProbabilitiesEstimator, x)\n\nCompute the discrete entropy h::Real ∈ [0, ∞), using the estimator e, in one of two ways:\n\nDirectly from existing Probabilities probs.\nFrom input data x, by first estimating a probability mass function using the provided ProbabilitiesEstimator, and then computing the entropy from that mass fuction using the provided DiscreteEntropyEstimator.\n\nInstead of providing a DiscreteEntropyEstimator, an EntropyDefinition can be given directly, in which case MLEntropy is used as the estimator. If e is not provided, Shannon() is used by default.\n\nMaximum entropy and normalized entropy\n\nAll discrete entropies have a well defined maximum value for a given probability estimator. To obtain this value one only needs to call the entropy_maximum. Or, one can use entropy_normalized to obtain the normalized form of the entropy (divided by the maximum).\n\nExamples\n\nx = [rand(Bool) for _ in 1:10000] # coin toss\nps = probabilities(x) # gives about [0.5, 0.5] by definition\nh = entropy(ps) # gives 1, about 1 bit by definition\nh = entropy(Shannon(), ps) # syntactically equivalent to above\nh = entropy(Shannon(), CountOccurrences(x), x) # syntactically equivalent to above\nh = entropy(SymbolicPermutation(;m=3), x) # gives about 2, again by definition\nh = entropy(Renyi(2.0), ps) # also gives 1, order `q` doesn't matter for coin toss\n\n\n\n\n\n","category":"method"},{"location":"entropies/#ComplexityMeasures.entropy_maximum","page":"Entropies","title":"ComplexityMeasures.entropy_maximum","text":"entropy_maximum(e::EntropyDefinition, est::ProbabilitiesEstimator, x)\n\nReturn the maximum value of a discrete entropy with the given probabilities estimator and input data x. Like in outcome_space, for some estimators the concrete outcome space is known without knowledge of input x, in which case the function dispatches to entropy_maximum(e, est).\n\nentropy_maximum(e::EntropyDefinition, L::Int)\n\nSame as above, but computed directly from the number of total outcomes L.\n\n\n\n\n\n","category":"function"},{"location":"entropies/#ComplexityMeasures.entropy_normalized","page":"Entropies","title":"ComplexityMeasures.entropy_normalized","text":"entropy_normalized([e::DiscreteEntropyEstimator,] est::ProbabilitiesEstimator, x) → h̃\n\nReturn h̃ ∈ [0, 1], the normalized discrete entropy of x, i.e. the value of entropy divided by the maximum value for e, according to the given probabilities estimator.\n\nInstead of a discrete entropy estimator, an EntropyDefinition can be given as first argument. If e is not given, it defaults to Shannon().\n\nNotice that there is no method entropy_normalized(e::DiscreteEntropyEstimator, probs::Probabilities), because there is no way to know the amount of possible events (i.e., the total_outcomes) from probs.\n\n\n\n\n\n","category":"function"},{"location":"entropies/#Discrete-entropy-estimators","page":"Entropies","title":"Discrete entropy estimators","text":"","category":"section"},{"location":"entropies/","page":"Entropies","title":"Entropies","text":"DiscreteEntropyEstimator\nMLEntropy","category":"page"},{"location":"entropies/#ComplexityMeasures.DiscreteEntropyEstimator","page":"Entropies","title":"ComplexityMeasures.DiscreteEntropyEstimator","text":"DiscreteEntropyEstimator\nDiscEntropyEst # alias\n\nSupertype of all discrete entropy estimators.\n\nCurrently only the MLEntropy estimator is provided, which does not need to be used, as using an EntropyDefinition directly in entropy is possible. But in the future, more advanced estimators will be added (#237).\n\n\n\n\n\n","category":"type"},{"location":"entropies/#ComplexityMeasures.MLEntropy","page":"Entropies","title":"ComplexityMeasures.MLEntropy","text":"MLEntropy(e::EntropyDefinition) <: DiscreteEntropyEstimator\n\nStanding for \"maximum likelihood entropy\", and also called empirical/naive/plug-in, this estimator calculates the entropy exactly as defined in the given EntropyDefinition directly from a probability mass function.\n\n\n\n\n\n","category":"type"},{"location":"entropies/#Differential-entropy","page":"Entropies","title":"Differential entropy","text":"","category":"section"},{"location":"entropies/","page":"Entropies","title":"Entropies","text":"entropy(::DifferentialEntropyEstimator, ::Any)","category":"page"},{"location":"entropies/#ComplexityMeasures.entropy-Tuple{DifferentialEntropyEstimator, Any}","page":"Entropies","title":"ComplexityMeasures.entropy","text":"entropy(est::DifferentialEntropyEstimator, x) → h\n\nApproximate the differential entropy h::Real using the provided DifferentialEntropyEstimator and input data x. This method doesn't involve explicitly computing (discretized) probabilities first.\n\nThe overwhelming majority of entropy estimators estimate the Shannon entropy. If some estimator can estimate different definitions of entropy (e.g., Tsallis), this is provided as an argument to the estimator itself.\n\nSee the table of differential entropy estimators in the docs for all differential entropy estimators.\n\nExamples\n\nA standard normal distribution has a base-e differential entropy of 0.5*log(2π) + 0.5 nats.\n\nest = Kraskov(k = 5, base = ℯ) # Base `ℯ` for nats.\nh = entropy(est, randn(1_000_000))\nabs(h - 0.5*log(2π) - 0.5) # ≈ 0.001\n\n\n\n\n\n","category":"method"},{"location":"entropies/#table_diff_ent_est","page":"Entropies","title":"Table of differential entropy estimators","text":"","category":"section"},{"location":"entropies/","page":"Entropies","title":"Entropies","text":"The following estimators are differential entropy estimators, and can also be used with entropy.","category":"page"},{"location":"entropies/","page":"Entropies","title":"Entropies","text":"Each DifferentialEntropyEstimators uses a specialized technique to approximate relevant densities/integrals, and is often tailored to one or a few types of generalized entropy. For example, Kraskov estimates the Shannon entropy.","category":"page"},{"location":"entropies/","page":"Entropies","title":"Entropies","text":"Estimator Principle Input data Shannon Renyi Tsallis Kaniadakis Curado StretchedExponential\nKozachenkoLeonenko Nearest neighbors StateSpaceSet ✓ x x x x x\nKraskov Nearest neighbors StateSpaceSet ✓ x x x x x\nZhu Nearest neighbors StateSpaceSet ✓ x x x x x\nZhuSingh Nearest neighbors StateSpaceSet ✓ x x x x x\nGao Nearest neighbors StateSpaceSet ✓ x x x x x\nGoria Nearest neighbors StateSpaceSet ✓ x x x x x\nLord Nearest neighbors StateSpaceSet ✓ x x x x x\nVasicek Order statistics Vector ✓ x x x x x\nEbrahimi Order statistics Vector ✓ x x x x x\nCorrea Order statistics Vector ✓ x x x x x\nAlizadehArghami Order statistics Vector ✓ x x x x x","category":"page"},{"location":"entropies/","page":"Entropies","title":"Entropies","text":"DifferentialEntropyEstimator","category":"page"},{"location":"entropies/#ComplexityMeasures.DifferentialEntropyEstimator","page":"Entropies","title":"ComplexityMeasures.DifferentialEntropyEstimator","text":"DifferentialEntropyEstimator\nDiffEntropyEst # alias\n\nThe supertype of all differential entropy estimators. These estimators compute an entropy value in various ways that do not involve explicitly estimating a probability distribution.\n\nSee the table of differential entropy estimators in the docs for all differential entropy estimators.\n\nSee entropy for usage.\n\n\n\n\n\n","category":"type"},{"location":"entropies/","page":"Entropies","title":"Entropies","text":"Kraskov\nKozachenkoLeonenko\nZhu\nZhuSingh\nGao\nGoria\nLord\nVasicek\nAlizadehArghami\nEbrahimi\nCorrea","category":"page"},{"location":"entropies/#ComplexityMeasures.Kraskov","page":"Entropies","title":"ComplexityMeasures.Kraskov","text":"Kraskov <: DiffEntropyEst\nKraskov(; k::Int = 1, w::Int = 1, base = 2)\n\nThe Kraskov estimator computes the Shannon differential entropy of a multi-dimensional StateSpaceSet using the k-th nearest neighbor searches method from [Kraskov2004] at the given base.\n\nw is the Theiler window, which determines if temporal neighbors are excluded during neighbor searches (defaults to 0, meaning that only the point itself is excluded when searching for neighbours).\n\nDescription\n\nAssume we have samples bfx_1 bfx_2 ldots bfx_N  from a continuous random variable X in mathbbR^d with support mathcalX and density functionf  mathbbR^d to mathbbR. Kraskov estimates the Shannon differential entropy\n\nH(X) = int_mathcalX f(x) log f(x) dx = mathbbE-log(f(X))\n\nSee also: entropy, KozachenkoLeonenko, DifferentialEntropyEstimator.\n\n[Kraskov2004]: Kraskov, A., Stögbauer, H., & Grassberger, P. (2004). Estimating mutual information. Physical review E, 69(6), 066138.\n\n\n\n\n\n","category":"type"},{"location":"entropies/#ComplexityMeasures.KozachenkoLeonenko","page":"Entropies","title":"ComplexityMeasures.KozachenkoLeonenko","text":"KozachenkoLeonenko <: DiffEntropyEst\nKozachenkoLeonenko(; w::Int = 0, base = 2)\n\nThe KozachenkoLeonenko estimator computes the Shannon differential entropy of a multi-dimensional StateSpaceSet in the given base.\n\nDescription\n\nAssume we have samples bfx_1 bfx_2 ldots bfx_N  from a continuous random variable X in mathbbR^d with support mathcalX and density functionf  mathbbR^d to mathbbR. KozachenkoLeonenko estimates the Shannon differential entropy\n\nH(X) = int_mathcalX f(x) log f(x) dx = mathbbE-log(f(X))\n\nusing the nearest neighbor method from Kozachenko & Leonenko (1987)[KozachenkoLeonenko1987], as described in Charzyńska and Gambin[Charzyńska2016].\n\nw is the Theiler window, which determines if temporal neighbors are excluded during neighbor searches (defaults to 0, meaning that only the point itself is excluded when searching for neighbours).\n\nIn contrast to Kraskov, this estimator uses only the closest neighbor.\n\nSee also: entropy, Kraskov, DifferentialEntropyEstimator.\n\n[Charzyńska2016]: Charzyńska, A., & Gambin, A. (2016). Improvement of the k-NN entropy estimator with applications in systems biology. EntropyDefinition, 18(1), 13.\n\n[KozachenkoLeonenko1987]: Kozachenko, L. F., & Leonenko, N. N. (1987). Sample estimate of the entropy of a random vector. Problemy Peredachi Informatsii, 23(2), 9-16.\n\n\n\n\n\n","category":"type"},{"location":"entropies/#ComplexityMeasures.Zhu","page":"Entropies","title":"ComplexityMeasures.Zhu","text":"Zhu <: DiffEntropyEst\nZhu(; k = 1, w = 0, base = 2)\n\nThe Zhu estimator (Zhu et al., 2015)[Zhu2015] is an extension to KozachenkoLeonenko, and computes the Shannon differential entropy of a multi-dimensional StateSpaceSet in the given base.\n\nDescription\n\nAssume we have samples bfx_1 bfx_2 ldots bfx_N  from a continuous random variable X in mathbbR^d with support mathcalX and density functionf  mathbbR^d to mathbbR. Zhu estimates the Shannon differential entropy\n\nH(X) = int_mathcalX f(x) log f(x) dx = mathbbE-log(f(X))\n\nby approximating densities within hyperrectangles surrounding each point xᵢ ∈ x using using k nearest neighbor searches. w is the Theiler window, which determines if temporal neighbors are excluded during neighbor searches (defaults to 0, meaning that only the point itself is excluded when searching for neighbours).\n\nSee also: entropy, KozachenkoLeonenko, DifferentialEntropyEstimator.\n\n[Zhu2015]: Zhu, J., Bellanger, J. J., Shu, H., & Le Bouquin Jeannès, R. (2015). Contribution to transfer entropy estimation via the k-nearest-neighbors approach. EntropyDefinition, 17(6), 4173-4201.\n\n\n\n\n\n","category":"type"},{"location":"entropies/#ComplexityMeasures.ZhuSingh","page":"Entropies","title":"ComplexityMeasures.ZhuSingh","text":"ZhuSingh <: DiffEntropyEst\nZhuSingh(; k = 1, w = 0, base = 2)\n\nThe ZhuSingh estimator (Zhu et al., 2015)[Zhu2015] computes the Shannon differential entropy of a multi-dimensional StateSpaceSet in the given base.\n\nDescription\n\nAssume we have samples bfx_1 bfx_2 ldots bfx_N  from a continuous random variable X in mathbbR^d with support mathcalX and density functionf  mathbbR^d to mathbbR. ZhuSingh estimates the Shannon differential entropy\n\nH(X) = int_mathcalX f(x) log f(x) dx = mathbbE-log(f(X))\n\nLike Zhu, this estimator approximates probabilities within hyperrectangles surrounding each point xᵢ ∈ x using using k nearest neighbor searches. However, it also considers the number of neighbors falling on the borders of these hyperrectangles. This estimator is an extension to the entropy estimator in Singh et al. (2003).\n\nw is the Theiler window, which determines if temporal neighbors are excluded during neighbor searches (defaults to 0, meaning that only the point itself is excluded when searching for neighbours).\n\nSee also: entropy, DifferentialEntropyEstimator.\n\n[Zhu2015]: Zhu, J., Bellanger, J. J., Shu, H., & Le Bouquin Jeannès, R. (2015). Contribution to transfer entropy estimation via the k-nearest-neighbors approach. EntropyDefinition, 17(6), 4173-4201.\n\n[Singh2003]: Singh, H., Misra, N., Hnizdo, V., Fedorowicz, A., & Demchuk, E. (2003). Nearest neighbor estimates of entropy. American journal of mathematical and management sciences, 23(3-4), 301-321.\n\n\n\n\n\n","category":"type"},{"location":"entropies/#ComplexityMeasures.Gao","page":"Entropies","title":"ComplexityMeasures.Gao","text":"Gao <: DifferentialEntropyEstimator\nGao(; k = 1, w = 0, base = 2, corrected = true)\n\nThe Gao estimator (Gao et al., 2015) computes the Shannon differential entropy, using a k-th nearest-neighbor approach based on Singh et al. (2003)[Singh2003].\n\nw is the Theiler window, which determines if temporal neighbors are excluded during neighbor searches (defaults to 0, meaning that only the point itself is excluded when searching for neighbours).\n\nGao et al., 2015 give two variants of this estimator. If corrected == false, then the uncorrected version is used. If corrected == true, then the corrected version is used, which ensures that the estimator is asymptotically unbiased.\n\nDescription\n\nAssume we have samples bfx_1 bfx_2 ldots bfx_N  from a continuous random variable X in mathbbR^d with support mathcalX and density functionf  mathbbR^d to mathbbR. KozachenkoLeonenko estimates the Shannon differential entropy\n\nH(X) = int_mathcalX f(x) log f(x) dx = mathbbE-log(f(X))\n\n[Gao2015]: Gao, S., Ver Steeg, G., & Galstyan, A. (2015, February). Efficient estimation of mutual information for strongly dependent variables. In Artificial intelligence and     statistics (pp. 277-286). PMLR.\n\n[Singh2003]: Singh, H., Misra, N., Hnizdo, V., Fedorowicz, A., & Demchuk, E. (2003). Nearest neighbor estimates of entropy. American journal of mathematical and management sciences, 23(3-4), 301-321.\n\n\n\n\n\n","category":"type"},{"location":"entropies/#ComplexityMeasures.Goria","page":"Entropies","title":"ComplexityMeasures.Goria","text":"Goria <: DifferentialEntropyEstimator\nGoria(; k = 1, w = 0, base = 2)\n\nThe Goria estimator computes the Shannon differential entropy of a multi-dimensional StateSpaceSet in the given base.\n\nDescription\n\nAssume we have samples bfx_1 bfx_2 ldots bfx_N  from a continuous random variable X in mathbbR^d with support mathcalX and density functionf  mathbbR^d to mathbbR. Goria estimates the Shannon differential entropy\n\nH(X) = int_mathcalX f(x) log f(x) dx = mathbbE-log(f(X))\n\nSpecifically, let bfn_1 bfn_2 ldots bfn_N be the distance of the samples bfx_1 bfx_2 ldots bfx_N  to their k-th nearest neighbors. Next, let the geometric mean of the distances be\n\nhatrho_k = left( prod_i=1^N right)^dfrac1N\n\nGoria et al. (2005)[Goria2005]'s estimate of Shannon differential entropy is then\n\nhatH = mhatrho_k + log(N - 1) - psi(k) + log c_1(m)\n\nwhere c_1(m) = dfrac2pi^fracm2m Gamma(m2) and psi is the digamma function.\n\n[Goria2005]: Goria, M. N., Leonenko, N. N., Mergel, V. V., & Novi Inverardi, P. L. (2005). A new class of random vector entropy estimators and its applications in testing statistical hypotheses. Journal of Nonparametric Statistics, 17(3), 277-297.\n\n\n\n\n\n","category":"type"},{"location":"entropies/#ComplexityMeasures.Lord","page":"Entropies","title":"ComplexityMeasures.Lord","text":"Lord <: DifferentialEntropyEstimator\nLord(; k = 10, w = 0, base = 2)\n\nLord estimates the Shannon differential entropy using a nearest neighbor approach with a local nonuniformity correction (LNC).\n\nw is the Theiler window, which determines if temporal neighbors are excluded during neighbor searches (defaults to 0, meaning that only the point itself is excluded when searching for neighbours).\n\nDescription\n\nAssume we have samples barX = bfx_1 bfx_2 ldots bfx_N  from a continuous random variable X in mathbbR^d with support mathcalX and density function f  mathbbR^d to mathbbR. Lord estimates the Shannon differential entropy\n\nH(X) = int_mathcalX f(x) log f(x) dx = mathbbE-log(f(X))\n\nby using the resubstitution formula\n\nhatbarX k = -mathbbElog(f(X))\napprox sum_i = 1^N log(hatf(bfx_i))\n\nwhere hatf(bfx_i) is an estimate of the density at bfx_i constructed in a manner such that hatf(bfx_i) propto dfrack(x_i)  NV_i, where k(x_i) is the number of points in the neighborhood of bfx_i, and V_i is the volume of that neighborhood.\n\nWhile most nearest-neighbor based differential entropy estimators uses regular volume elements (e.g. hypercubes, hyperrectangles, hyperspheres) for approximating the local densities hatf(bfx_i), the Lord estimator uses hyperellopsoid volume elements. These hyperellipsoids are, for each query point xᵢ, estimated using singular value decomposition (SVD) on the k-th nearest neighbors of xᵢ. Thus, the hyperellipsoids stretch/compress in response to the local geometry around each sample point. This makes Lord a well-suited entropy estimator for a wide range of systems.\n\n[Lord2015]: Lord, W. M., Sun, J., & Bollt, E. M. (2018). Geometric k-nearest neighbor estimation of entropy and mutual information. Chaos: An Interdisciplinary Journal of Nonlinear Science, 28(3), 033114.\n\n\n\n\n\n","category":"type"},{"location":"entropies/#ComplexityMeasures.Vasicek","page":"Entropies","title":"ComplexityMeasures.Vasicek","text":"Vasicek <: DiffEntropyEst\nVasicek(; m::Int = 1, base = 2)\n\nThe Vasicek estimator computes the Shannon differential entropy (in the given base) of a timeseries using the method from Vasicek (1976)[Vasicek1976].\n\nThe Vasicek estimator belongs to a class of differential entropy estimators based on order statistics, of which Vasicek (1976) was the first. It only works for timeseries input.\n\nDescription\n\nAssume we have samples barX = x_1 x_2 ldots x_N  from a continuous random variable X in mathbbR with support mathcalX and density functionf  mathbbR to mathbbR. Vasicek estimates the Shannon differential entropy\n\nH(X) = int_mathcalX f(x) log f(x) dx = mathbbE-log(f(X))\n\nHowever, instead of estimating the above integral directly, it makes use of the equivalent integral, where F is the distribution function for X,\n\nH(X) = int_0^1 log left(dfracddpF^-1(p) right) dp\n\nThis integral is approximated by first computing the order statistics of barX (the input timeseries), i.e. x_(1) leq x_(2) leq cdots leq x_(n). The Vasicek Shannon differential entropy estimate is then\n\nhatH_V(barX m) =\ndfrac1n\nsum_i = 1^n log left dfracn2m (barX_(i+m) - barX_(i-m)) right\n\nUsage\n\nIn practice, choice of m influences how fast the entropy converges to the true value. For small value of m, convergence is slow, so we recommend to scale m according to the time series length n and use m >= n/100 (this is just a heuristic based on the tests written for this package).\n\n[Vasicek1976]: Vasicek, O. (1976). A test for normality based on sample entropy. Journal of the Royal Statistical Society: Series B (Methodological), 38(1), 54-59.\n\nSee also: entropy, Correa, AlizadehArghami, Ebrahimi, DifferentialEntropyEstimator.\n\n\n\n\n\n","category":"type"},{"location":"entropies/#ComplexityMeasures.AlizadehArghami","page":"Entropies","title":"ComplexityMeasures.AlizadehArghami","text":"AlizadehArghami <: DiffEntropyEst\nAlizadehArghami(; m::Int = 1, base = 2)\n\nThe AlizadehArghamiestimator computes the Shannon differential entropy (in the given base) of a timeseries using the method from Alizadeh & Arghami (2010)[Alizadeh2010].\n\nThe AlizadehArghami estimator belongs to a class of differential entropy estimators based on order statistics. It only works for timeseries input.\n\nDescription\n\nAssume we have samples barX = x_1 x_2 ldots x_N  from a continuous random variable X in mathbbR with support mathcalX and density functionf  mathbbR to mathbbR. AlizadehArghami estimates the Shannon differential entropy\n\nH(X) = int_mathcalX f(x) log f(x) dx = mathbbE-log(f(X))\n\nHowever, instead of estimating the above integral directly, it makes use of the equivalent integral, where F is the distribution function for X:\n\nH(X) = int_0^1 log left(dfracddpF^-1(p) right) dp\n\nThis integral is approximated by first computing the order statistics of barX (the input timeseries), i.e. x_(1) leq x_(2) leq cdots leq x_(n). The AlizadehArghami Shannon differential entropy estimate is then the the Vasicek estimate hatH_V(barX m n), plus a correction factor\n\nhatH_A(barX m n) = hatH_V(barX m n) +\ndfrac2nleft(m log(2) right)\n\n[Alizadeh2010]: Alizadeh, N. H., & Arghami, N. R. (2010). A new estimator of entropy. Journal of the Iranian Statistical Society (JIRSS).\n\nSee also: entropy, Correa, Ebrahimi, Vasicek, DifferentialEntropyEstimator.\n\n\n\n\n\n","category":"type"},{"location":"entropies/#ComplexityMeasures.Ebrahimi","page":"Entropies","title":"ComplexityMeasures.Ebrahimi","text":"Ebrahimi <: DiffEntropyEst\nEbrahimi(; m::Int = 1, base = 2)\n\nThe Ebrahimi estimator computes the Shannon entropy (in the given base) of a timeseries using the method from Ebrahimi (1994)[Ebrahimi1994].\n\nThe Ebrahimi estimator belongs to a class of differential entropy estimators based on order statistics. It only works for timeseries input.\n\nDescription\n\nAssume we have samples barX = x_1 x_2 ldots x_N  from a continuous random variable X in mathbbR with support mathcalX and density functionf  mathbbR to mathbbR. Ebrahimi estimates the Shannon differential entropy\n\nH(X) = int_mathcalX f(x) log f(x) dx = mathbbE-log(f(X))\n\nHowever, instead of estimating the above integral directly, it makes use of the equivalent integral, where F is the distribution function for X,\n\nH(X) = int_0^1 log left(dfracddpF^-1(p) right) dp\n\nThis integral is approximated by first computing the order statistics of barX (the input timeseries), i.e. x_(1) leq x_(2) leq cdots leq x_(n). The Ebrahimi Shannon differential entropy estimate is then\n\nhatH_E(barX m) =\ndfrac1n sum_i = 1^n log\nleft dfracnc_i m (barX_(i+m) - barX_(i-m)) right\n\nwhere\n\nc_i =\nbegincases\n    1 + fraci - 1m  1 geq i geq m \n    2                     m + 1 geq i geq n - m \n    1 + fracn - im  n - m + 1 geq i geq n\nendcases\n\n[Ebrahimi1994]: Ebrahimi, N., Pflughoeft, K., & Soofi, E. S. (1994). Two measures of sample entropy. Statistics & Probability Letters, 20(3), 225-234.\n\nSee also: entropy, Correa, AlizadehArghami, Vasicek, DifferentialEntropyEstimator.\n\n\n\n\n\n","category":"type"},{"location":"entropies/#ComplexityMeasures.Correa","page":"Entropies","title":"ComplexityMeasures.Correa","text":"Correa <: DiffEntropyEst\nCorrea(; m::Int = 1, base = 2)\n\nThe Correa estimator computes the Shannon differential entropy (in the given `base) of a timeseries using the method from Correa (1995)[Correa1995].\n\nThe Correa estimator belongs to a class of differential entropy estimators based on order statistics. It only works for timeseries input.\n\nDescription\n\nAssume we have samples barX = x_1 x_2 ldots x_N  from a continuous random variable X in mathbbR with support mathcalX and density functionf  mathbbR to mathbbR. Correa estimates the Shannon differential entropy\n\nH(X) = int_mathcalX f(x) log f(x) dx = mathbbE-log(f(X))\n\nHowever, instead of estimating the above integral directly, Correa makes use of the equivalent integral, where F is the distribution function for X,\n\nH(X) = int_0^1 log left(dfracddpF^-1(p) right) dp\n\nThis integral is approximated by first computing the order statistics of barX (the input timeseries), i.e. x_(1) leq x_(2) leq cdots leq x_(n), ensuring that end points are included. The Correa estimate of Shannon differential entropy is then\n\nH_C(barX m n) =\ndfrac1n sum_i = 1^n log\nleft dfrac sum_j=i-m^i+m(barX_(j) -\ntildeX_(i))(j - i)n sum_j=i-m^i+m (barX_(j) - tildeX_(i))^2\nright\n\nwhere\n\ntildeX_(i) = dfrac12m + 1 sum_j = i - m^i + m X_(j)\n\n[Correa1995]: Correa, J. C. (1995). A new estimator of entropy. Communications in Statistics-Theory and Methods, 24(10), 2439-2449.\n\nSee also: entropy, AlizadehArghami, Ebrahimi, Vasicek, DifferentialEntropyEstimator.\n\n\n\n\n\n","category":"type"},{"location":"probabilities/#Probabilities","page":"Probabilities","title":"Probabilities","text":"","category":"section"},{"location":"probabilities/#Probabilities-API","page":"Probabilities","title":"Probabilities API","text":"","category":"section"},{"location":"probabilities/","page":"Probabilities","title":"Probabilities","text":"The probabilities API is defined by","category":"page"},{"location":"probabilities/","page":"Probabilities","title":"Probabilities","text":"ProbabilitiesEstimator\nprobabilities\nprobabilities_and_outcomes","category":"page"},{"location":"probabilities/","page":"Probabilities","title":"Probabilities","text":"and related functions that you will find in the following documentation blocks:","category":"page"},{"location":"probabilities/#Probabilitities","page":"Probabilities","title":"Probabilitities","text":"","category":"section"},{"location":"probabilities/","page":"Probabilities","title":"Probabilities","text":"ProbabilitiesEstimator\nprobabilities\nprobabilities!\nProbabilities","category":"page"},{"location":"probabilities/#ComplexityMeasures.ProbabilitiesEstimator","page":"Probabilities","title":"ComplexityMeasures.ProbabilitiesEstimator","text":"ProbabilitiesEstimator\n\nThe supertype for all probabilities estimators.\n\nIn ComplexityMeasures.jl, probability mass functions are estimated from data by defining a set of possible outcomes Omega = omega_1 omega_2 ldots omega_L , and assigning to each outcome omega_i a probability p(omega_i), such that sum_i=1^N p(omega_i) = 1. It is the role of a ProbabilitiesEstimator to\n\nDefine Omega, the \"outcome space\", which is the set of all possible outcomes over  which probabilities are estimated.\nDefine how probabilities p_i = p(omega_i) are assigned to outcomes omega_i given input data.\n\nIn practice, probability estimation is done by calling probabilities with some input data and one of the implemented probabilities estimators. The result is a Probabilities p (Vector-like), where each element p[i] is the probability of the outcome ω[i]. Use probabilities_and_outcomes if you need both the probabilities and the outcomes, and use outcome_space to obtain Omega alone.  The cardinality of Omega can be obtained using total_outcomes.\n\nThe element type of Omega varies between estimators, but it is guaranteed to be hashable and sortable. This allows for conveniently tracking the probability of a specific event across experimental realizations, by using the outcome as a dictionary key and the probability as the value for that key (or, alternatively, the key remains the outcome and one has a vector of probabilities, one for each experimental realization).\n\nSome estimators can deduce Omega without knowledge of the input, such as SymbolicPermutation. For others, knowledge of input is necessary for concretely specifying Omega, such as ValueHistogram with RectangularBinning. This only matters for the functions outcome_space and total_outcomes.\n\nAll currently implemented probability estimators are listed in a nice table in the probabilities estimators section of the online documentation.\n\n\n\n\n\n","category":"type"},{"location":"probabilities/#ComplexityMeasures.probabilities","page":"Probabilities","title":"ComplexityMeasures.probabilities","text":"probabilities(est::ProbabilitiesEstimator, x::Array_or_Dataset) → p::Probabilities\n\nCompute a probability distribution over the set of possible outcomes defined by the probabilities estimator est, given input data x, which is typically an Array or a StateSpaceSet; see Input data for ComplexityMeasures.jl. Configuration options are always given as arguments to the chosen estimator.\n\nTo obtain the outcomes corresponding to these probabilities, use outcomes.\n\nDue to performance optimizations, whether the returned probablities contain 0s as entries or not depends on the estimator. E.g., in ValueHistogram 0s are skipped, while in PowerSpectrum 0 are not, because we get them for free. Use the function allprobabilities for a version with all 0 entries that ensures that given an est, the indices of p will be independent of the input data x.\n\nprobabilities(x::Vector_or_Dataset) → p::Probabilities\n\nEstimate probabilities by directly counting the elements of x, assuming that Ω = sort(unique(x)), i.e. that the outcome space is the unique elements of x. This is mostly useful when x contains categorical data.\n\nSee also: Probabilities, ProbabilitiesEstimator.\n\n\n\n\n\n","category":"function"},{"location":"probabilities/#ComplexityMeasures.probabilities!","page":"Probabilities","title":"ComplexityMeasures.probabilities!","text":"probabilities!(s, args...)\n\nSimilar to probabilities(args...), but allows pre-allocation of temporarily used containers s.\n\nOnly works for certain estimators. See for example SymbolicPermutation.\n\n\n\n\n\n","category":"function"},{"location":"probabilities/#ComplexityMeasures.Probabilities","page":"Probabilities","title":"ComplexityMeasures.Probabilities","text":"Probabilities <: AbstractArray\nProbabilities(x) → p\n\nProbabilities is a simple wrapper around x::AbstractArray{<:Real, N} that ensures its values sum to 1, so that p can be interpreted as N-dimensional probability mass function. In most use cases, p will be a vector. p behaves exactly like its contained data x with respect to indexing and iteration.\n\n\n\n\n\n","category":"type"},{"location":"probabilities/#Outcomes","page":"Probabilities","title":"Outcomes","text":"","category":"section"},{"location":"probabilities/","page":"Probabilities","title":"Probabilities","text":"probabilities_and_outcomes\noutcomes\noutcome_space\ntotal_outcomes\nmissing_outcomes","category":"page"},{"location":"probabilities/#ComplexityMeasures.probabilities_and_outcomes","page":"Probabilities","title":"ComplexityMeasures.probabilities_and_outcomes","text":"probabilities_and_outcomes(est, x)\n\nReturn probs, outs, where probs = probabilities(est, x) and outs[i] is the outcome with probability probs[i]. The element type of outs depends on the estimator. outs is a subset of the outcome_space of est.\n\nSee also outcomes, total_outcomes.\n\n\n\n\n\n","category":"function"},{"location":"probabilities/#ComplexityMeasures.outcomes","page":"Probabilities","title":"ComplexityMeasures.outcomes","text":"outcomes(est::ProbabilitiesEstimator, x)\n\nReturn all (unique) outcomes contained in x according to the given estimator. Equivalent with probabilities_and_outcomes(x, est)[2], but for some estimators it may be explicitly extended for better performance.\n\n\n\n\n\n","category":"function"},{"location":"probabilities/#ComplexityMeasures.outcome_space","page":"Probabilities","title":"ComplexityMeasures.outcome_space","text":"outcome_space(est::ProbabilitiesEstimator, x) → Ω\n\nReturn a sorted container containing all possible outcomes of est for input x.\n\nFor some estimators the concrete outcome space is known without knowledge of input x, in which case the function dispatches to outcome_space(est). In general it is recommended to use the 2-argument version irrespectively of estimator.\n\n\n\n\n\n","category":"function"},{"location":"probabilities/#ComplexityMeasures.total_outcomes","page":"Probabilities","title":"ComplexityMeasures.total_outcomes","text":"total_outcomes(est::ProbabilitiesEstimator, x)\n\nReturn the length (cardinality) of the outcome space Omega of est.\n\nFor some estimators the concrete outcome space is known without knowledge of input x, in which case the function dispatches to total_outcomes(est). In general it is recommended to use the 2-argument version irrespectively of estimator.\n\n\n\n\n\n","category":"function"},{"location":"probabilities/#ComplexityMeasures.missing_outcomes","page":"Probabilities","title":"ComplexityMeasures.missing_outcomes","text":"missing_outcomes(est::ProbabilitiesEstimator, x) → n_missing::Int\n\nEstimate a probability distribution for x using the given estimator, then count the number of missing (i.e. zero-probability) outcomes.\n\nSee also: MissingDispersionPatterns.\n\n\n\n\n\n","category":"function"},{"location":"probabilities/#probabilities_estimators","page":"Probabilities","title":"Overview of probabilities estimators","text":"","category":"section"},{"location":"probabilities/","page":"Probabilities","title":"Probabilities","text":"Any of the following estimators can be used with probabilities (in the column \"input data\"  it is assumed that the eltype of the input is <: Real).","category":"page"},{"location":"probabilities/","page":"Probabilities","title":"Probabilities","text":"Estimator Principle Input data\nCountOccurrences Count of unique elements Any\nValueHistogram Binning (histogram) Vector, StateSpaceSet\nTransferOperator Binning (transfer operator) Vector, StateSpaceSet\nNaiveKernel Kernel density estimation StateSpaceSet\nSymbolicPermutation Ordinal patterns Vector, StateSpaceSet\nSymbolicWeightedPermutation Ordinal patterns Vector, StateSpaceSet\nSymbolicAmplitudeAwarePermutation Ordinal patterns Vector, StateSpaceSet\nSpatialSymbolicPermutation Ordinal patterns in space Array\nDispersion Dispersion patterns Vector\nSpatialDispersion Dispersion patterns in space Array\nDiversity Cosine similarity Vector\nWaveletOverlap Wavelet transform Vector\nPowerSpectrum Fourier transform Vector","category":"page"},{"location":"probabilities/#Count-occurrences","page":"Probabilities","title":"Count occurrences","text":"","category":"section"},{"location":"probabilities/","page":"Probabilities","title":"Probabilities","text":"CountOccurrences","category":"page"},{"location":"probabilities/#ComplexityMeasures.CountOccurrences","page":"Probabilities","title":"ComplexityMeasures.CountOccurrences","text":"CountOccurrences()\n\nA probabilities/entropy estimator based on straight-forward counting of distinct elements in a univariate time series or multivariate dataset. This is the same as giving no estimator to probabilities.\n\nOutcome space\n\nThe outcome space is the unique sorted values of the input. Hence, input x is needed for a well-defined outcome_space.\n\n\n\n\n\n","category":"type"},{"location":"probabilities/#Histograms","page":"Probabilities","title":"Histograms","text":"","category":"section"},{"location":"probabilities/","page":"Probabilities","title":"Probabilities","text":"ValueHistogram\nAbstractBinning\nRectangularBinning\nFixedRectangularBinning","category":"page"},{"location":"probabilities/#ComplexityMeasures.ValueHistogram","page":"Probabilities","title":"ComplexityMeasures.ValueHistogram","text":"ValueHistogram(b::AbstractBinning) <: ProbabilitiesEstimator\n\nA probability estimator based on binning the values of the data as dictated by the binning scheme b and formally computing their histogram, i.e., the frequencies of points in the bins. An alias to this is VisitationFrequency. Available binnings are subtypes of AbstractBinning.\n\nThe ValueHistogram estimator has a linearithmic time complexity (n log(n) for n = length(x)) and a linear space complexity (l for l = dimension(x)). This allows computation of probabilities (histograms) of high-dimensional datasets and with small box sizes ε without memory overflow and with maximum performance. For performance reasons, the probabilities returned never contain 0s and are arbitrarily ordered.\n\nValueHistogram(ϵ::Union{Real,Vector})\n\nA convenience method that accepts same input as RectangularBinning and initializes this binning directly.\n\nOutcomes\n\nThe outcome space for ValueHistogram is the unique bins constructed from b. Each bin is identified by its left (lowest-value) corner, because bins are always left-closed-right-open intervals [a, b). The bins are in data units, not integer (cartesian indices units), and are returned as SVectors, i.e., same type as input data.\n\nFor convenience, outcome_space returns the outcomes in the same array format as the underlying binning (e.g., Matrix for 2D input).\n\nFor FixedRectangularBinning the outcome_space is well-defined from the binning, but for RectangularBinning input x is needed as well.\n\n\n\n\n\n","category":"type"},{"location":"probabilities/#ComplexityMeasures.AbstractBinning","page":"Probabilities","title":"ComplexityMeasures.AbstractBinning","text":"AbstractBinning\n\nSupertype encompassing RectangularBinning and FixedRectangualrBinning.\n\n\n\n\n\n","category":"type"},{"location":"probabilities/#ComplexityMeasures.RectangularBinning","page":"Probabilities","title":"ComplexityMeasures.RectangularBinning","text":"RectangularBinning(ϵ, precise = false) <: AbstractBinning\n\nRectangular box partition of state space using the scheme ϵ, deducing the histogram extent and bin width from the input data.\n\nRectangularBinning is a convenience struct. It is re-cast into FixedRectangularBinning once the data are provided, so see that docstring for info on the bin calculation and the meaning of precise.\n\nBinning instructions are deduced from the type of ϵ as follows:\n\nϵ::Int divides each coordinate axis into ϵ equal-length intervals  that cover all data.\nϵ::Float64 divides each coordinate axis into intervals of fixed size ϵ, starting  from the axis minima until the data is completely covered by boxes.\nϵ::Vector{Int} divides the i-th coordinate axis into ϵ[i] equal-length  intervals that cover all data.\nϵ::Vector{Float64} divides the i-th coordinate axis into intervals of fixed size  ϵ[i], starting from the axis minima until the data is completely covered by boxes.\n\nRectangularBinning ensures all input data are covered by extending the created ranges if need be.\n\n\n\n\n\n","category":"type"},{"location":"probabilities/#ComplexityMeasures.FixedRectangularBinning","page":"Probabilities","title":"ComplexityMeasures.FixedRectangularBinning","text":"FixedRectangularBinning <: AbstractBinning\nFixedRectangularBinning(ranges::Tuple{<:AbstractRange...}, precise = false)\n\nRectangular box partition of state space where the partition along each dimension is explicitly given by each range ranges, which is a tuple of AbstractRange subtypes. Typically, each range is the output of the range Base function, e.g., ranges = (0:0.1:1, range(0, 1; length = 101), range(2.1, 3.2; step = 0.33)). All ranges must be sorted.\n\nThe optional second argument precise dictates whether Julia Base's TwicePrecision is used for when searching where a point falls into the range. Useful for edge cases of points being almost exactly on the bin edges, but it is exactly four times as slow, so by default it is false.\n\nPoints falling outside the partition do not contribute to probabilities. Bins are always left-closed-right-open: [a, b). This means that the last value of each of the ranges dictates the last right-closing value. This value does not belong to the histogram! E.g., if given a range r = range(0, 1; length = 11), with r[end] = 1, the value 1 is outside the partition and would not attribute any increase of the probability corresponding to the last bin (here [0.9, 1))!\n\nEquivalently, the size of the histogram is histsize = map(r -> length(r)-1, ranges)!\n\nFixedRectangularBinning leads to a well-defined outcome space without knowledge of input data, see ValueHistogram.\n\n\n\n\n\n","category":"type"},{"location":"probabilities/#Symbolic-permutations","page":"Probabilities","title":"Symbolic permutations","text":"","category":"section"},{"location":"probabilities/","page":"Probabilities","title":"Probabilities","text":"SymbolicPermutation\nSymbolicWeightedPermutation\nSymbolicAmplitudeAwarePermutation","category":"page"},{"location":"probabilities/#ComplexityMeasures.SymbolicPermutation","page":"Probabilities","title":"ComplexityMeasures.SymbolicPermutation","text":"SymbolicPermutation <: ProbabilitiesEstimator\nSymbolicPermutation(; m = 3, τ = 1, lt::Function = ComplexityMeasures.isless_rand)\n\nA probabilities estimator based on ordinal permutation patterns.\n\nWhen passed to probabilities the output depends on the input data type:\n\nUnivariate data. If applied to a univariate timeseries (AbstractVector), then the timeseries   is first embedded using embedding delay τ and dimension m, resulting in embedding   vectors  bfx_i _i=1^N-(m-1)tau. Then, for each bfx_i,   we find its permutation pattern pi_i. Probabilities are then   estimated as the frequencies of the encoded permutation symbols   by using CountOccurrences. When giving the resulting probabilities to   entropy, the original permutation entropy is computed [BandtPompe2002].\nMultivariate data. If applied to a an D-dimensional StateSpaceSet,   then no embedding is constructed, m must be equal to D and τ is ignored.   Each vector bfx_i of the dataset is mapped   directly to its permutation pattern pi_i by comparing the   relative magnitudes of the elements of bfx_i.   Like above, probabilities are estimated as the frequencies of the permutation symbols.   The resulting probabilities can be used to compute multivariate permutation   entropy[He2016], although here we don't perform any further subdivision   of the permutation patterns (as in Figure 3 of[He2016]).\n\nInternally, SymbolicPermutation uses the OrdinalPatternEncoding to represent ordinal patterns as integers for efficient computations.\n\nSee SymbolicWeightedPermutation and SymbolicAmplitudeAwarePermutation for estimators that not only consider ordinal (sorting) patterns, but also incorporate information about within-state-vector amplitudes. For a version of this estimator that can be used on spatial data, see SpatialSymbolicPermutation.\n\nnote: Handling equal values in ordinal patterns\nIn Bandt & Pompe (2002), equal values are ordered after their order of appearance, but this can lead to erroneous temporal correlations, especially for data with low amplitude resolution [Zunino2017]. Here, by default, if two values are equal, then one of the is randomly assigned as \"the largest\", using lt = ComplexityMeasures.isless_rand. To get the behaviour from Bandt and Pompe (2002), use lt = Base.isless.\n\nOutcome space\n\nThe outcome space Ω for SymbolicPermutation is the set of length-m ordinal patterns (i.e. permutations) that can be formed by the integers 1, 2, …, m. There are factorial(m) such patterns.\n\nFor example, the outcome [2, 3, 1] corresponds to the ordinal pattern of having the smallest value in the second position, the next smallest value in the third position, and the next smallest, i.e. the largest value in the first position. See also [OrdinalPatternEncoding(@ref).\n\nIn-place symbolization\n\nSymbolicPermutation also implements the in-place probabilities! for StateSpaceSet input (or embedded vector input) for reducing allocations in looping scenarios. The length of the pre-allocated symbol vector must be the length of the dataset. For example\n\nusing ComplexityMeasures\nm, N = 2, 100\nest = SymbolicPermutation(; m, τ)\nx = StateSpaceSet(rand(N, m)) # some input dataset\nπs_ts = zeros(Int, N) # length must match length of `x`\np = probabilities!(πs_ts, est, x)\n\n[BandtPompe2002]: Bandt, Christoph, and Bernd Pompe. \"Permutation entropy: a natural complexity measure for timeseries.\" Physical review letters 88.17 (2002): 174102.\n\n[Zunino2017]: Zunino, L., Olivares, F., Scholkmann, F., & Rosso, O. A. (2017). Permutation entropy based timeseries analysis: Equalities in the input signal can lead to false conclusions. Physics Letters A, 381(22), 1883-1892.\n\n[He2016]: He, S., Sun, K., & Wang, H. (2016). Multivariate permutation entropy and its application for complexity analysis of chaotic systems. Physica A: Statistical Mechanics and its Applications, 461, 812-823.\n\n\n\n\n\n","category":"type"},{"location":"probabilities/#ComplexityMeasures.SymbolicWeightedPermutation","page":"Probabilities","title":"ComplexityMeasures.SymbolicWeightedPermutation","text":"SymbolicWeightedPermutation <: ProbabilitiesEstimator\nSymbolicWeightedPermutation(; τ = 1, m = 3, lt::Function = ComplexityMeasures.isless_rand)\n\nA variant of SymbolicPermutation that also incorporates amplitude information, based on the weighted permutation entropy[Fadlallah2013]. The outcome space and keywords are the same as in SymbolicPermutation.\n\nDescription\n\nFor each ordinal pattern extracted from each state (or delay) vector, a weight is attached to it which is the variance of the vector. Probabilities are then estimated by summing the weights corresponding to the same pattern, instead of just counting the occurrence of the same pattern.\n\nnote: An implementation note\nNote: in equation 7, section III, of the original paper, the authors writew_j = dfrac1msum_k=1^m (x_j-(k-1)tau - mathbfhatx_j^m tau)^2*But given the formula they give for the arithmetic mean, this is not the variance of the delay vector mathbfx_i, because the indices are mixed: x_j+(k-1)tau in the weights formula, vs. x_j+(k+1)tau in the arithmetic mean formula. Here, delay embedding and computation of the patterns and their weights are completely separated processes, ensuring that we compute the arithmetic mean correctly for each vector of the input dataset (which may be a delay-embedded timeseries).\n\n[Fadlallah2013]: Fadlallah, et al. \"Weighted-permutation entropy: A complexity measure for time series incorporating amplitude information.\" Physical Review E 87.2 (2013): 022911.\n\n\n\n\n\n","category":"type"},{"location":"probabilities/#ComplexityMeasures.SymbolicAmplitudeAwarePermutation","page":"Probabilities","title":"ComplexityMeasures.SymbolicAmplitudeAwarePermutation","text":"SymbolicAmplitudeAwarePermutation <: ProbabilitiesEstimator\nSymbolicAmplitudeAwarePermutation(; τ = 1, m = 3, A = 0.5, lt = ComplexityMeasures.isless_rand)\n\nA variant of SymbolicPermutation that also incorporates amplitude information, based on the amplitude-aware permutation entropy[Azami2016]. The outcome space and keywords are the same as in SymbolicPermutation.\n\nDescription\n\nSimilarly to SymbolicWeightedPermutation, a weight w_i is attached to each ordinal pattern extracted from each state (or delay) vector mathbfx_i = (x_1^i x_2^i ldots x_m^i) as\n\nw_i = dfracAm sum_k=1^m x_k^i  + dfrac1-Ad-1\nsum_k=2^d x_k^i - x_k-1^i\n\nwith 0 leq A leq 1. When A=0 , only internal differences between the elements of mathbfx_i are weighted. Only mean amplitude of the state vector elements are weighted when A=1. With, 0A1, a combined weighting is used.\n\n[Azami2016]: Azami, H., & Escudero, J. (2016). Amplitude-aware permutation entropy: Illustration in spike detection and signal segmentation. Computer methods and programs in biomedicine, 128, 40-51.\n\n\n\n\n\n","category":"type"},{"location":"probabilities/#Dispersion-patterns","page":"Probabilities","title":"Dispersion patterns","text":"","category":"section"},{"location":"probabilities/","page":"Probabilities","title":"Probabilities","text":"Dispersion","category":"page"},{"location":"probabilities/#ComplexityMeasures.Dispersion","page":"Probabilities","title":"ComplexityMeasures.Dispersion","text":"Dispersion(; c = 5, m = 2, τ = 1, check_unique = true)\n\nA probability estimator based on dispersion patterns, originally used by Rostaghi & Azami, 2016[Rostaghi2016] to compute the \"dispersion entropy\", which characterizes the complexity and irregularity of a time series.\n\nRecommended parameter values[Li2018] are m ∈ [2, 3], τ = 1 for the embedding, and c ∈ [3, 4, …, 8] categories for the Gaussian symbol mapping.\n\nDescription\n\nAssume we have a univariate time series X = x_i_i=1^N. First, this time series is encoded into a symbol timeseries S using the Gaussian encoding GaussianCDFEncoding with empirical mean μ and empirical standard deviation σ (both determined from X), and c as given to Dispersion.\n\nThen, S is embedded into an m-dimensional time series, using an embedding lag of tau, which yields a total of N - (m - 1)tau delay vectors z_i, or \"dispersion patterns\". Since each element of z_i can take on c different values, and each delay vector has m entries, there are c^m possible dispersion patterns. This number is used for normalization when computing dispersion entropy.\n\nThe returned probabilities are simply the frequencies of the unique dispersion patterns present in S (i.e., the CountOccurences of S).\n\nOutcome space\n\nThe outcome space for Dispersion is the unique delay vectors whose elements are the the symbols (integers) encoded by the Gaussian CDF, i.e., the unique elements of S.\n\nData requirements and parameters\n\nThe input must have more than one unique element for the Gaussian mapping to be well-defined. Li et al. (2018) recommends that x has at least 1000 data points.\n\nIf check_unique == true (default), then it is checked that the input has more than one unique value. If check_unique == false and the input only has one unique element, then a InexactError is thrown when trying to compute probabilities.\n\nnote: Why 'dispersion patterns'?\nEach embedding vector is called a \"dispersion pattern\". Why? Let's consider the case when m = 5 and c = 3, and use some very imprecise terminology for illustration:When c = 3, values clustering far below mean are in one group, values clustered around the mean are in one group, and values clustering far above the mean are in a third group. Then the embedding vector 2 2 2 2 2 consists of values that are close together (close to the mean), so it represents a set of numbers that are not very spread out (less dispersed). The embedding vector 1 1 2 3 3, however, represents numbers that are much more spread out (more dispersed), because the categories representing \"outliers\" both above and below the mean are represented, not only values close to the mean.\n\nFor a version of this estimator that can be used on high-dimensional arrays, see SpatialDispersion.\n\n[Rostaghi2016]: Rostaghi, M., & Azami, H. (2016). Dispersion entropy: A measure for time-series analysis. IEEE Signal Processing Letters, 23(5), 610-614.\n\n[Li2018]: Li, G., Guan, Q., & Yang, H. (2018). Noise reduction method of underwater acoustic signals based on CEEMDAN, effort-to-compress complexity, refined composite multiscale dispersion entropy and wavelet threshold denoising. EntropyDefinition, 21(1), 11.\n\n\n\n\n\n","category":"type"},{"location":"probabilities/#Transfer-operator","page":"Probabilities","title":"Transfer operator","text":"","category":"section"},{"location":"probabilities/","page":"Probabilities","title":"Probabilities","text":"TransferOperator","category":"page"},{"location":"probabilities/#ComplexityMeasures.TransferOperator","page":"Probabilities","title":"ComplexityMeasures.TransferOperator","text":"TransferOperator <: ProbabilitiesEstimator\nTransferOperator(b::AbstractBinning)\n\nA probability estimator based on binning data into rectangular boxes dictated by the given binning scheme b, then approximating the transfer (Perron-Frobenius) operator over the bins, then taking the invariant measure associated with that transfer operator as the bin probabilities. Assumes that the input data are sequential (time-ordered).\n\nThis implementation follows the grid estimator approach in Diego et al. (2019)[Diego2019].\n\nOutcome space\n\nThe outcome space for TransferOperator is the set of unique bins constructed from b. Bins are identified by their left (lowest-value) corners, are given in data units, and are returned as SVectors.\n\nBin ordering\n\nBins returned by probabilities_and_outcomes are ordered according to first appearance (i.e. the first time the input (multivariate) timeseries visits the bin). Thus, if\n\nb = RectangularBinning(4)\nest = TransferOperator(b)\nprobs, outcomes = probabilities_and_outcomes(x, est) # x is some timeseries\n\nthen probs[i] is the invariant measure (probability) of the bin outcomes[i], which is the i-th bin visited by the timeseries with nonzero measure.\n\nDescription\n\nThe transfer operator P^Nis computed as an N-by-N matrix of transition probabilities between the states defined by the partition elements, where N is the number of boxes in the partition that is visited by the orbit/points.\n\nIf  x_t^(D) _n=1^L are the L different D-dimensional points over which the transfer operator is approximated,  C_k=1^N  are the N different partition elements (as dictated by ϵ) that gets visited by the points, and  phi(x_t) = x_t+1, then\n\nP_ij = dfrac\n x_n  phi(x_n) in C_j cap x_n in C_i \n x_m  x_m in C_i \n\nwhere  denotes the cardinal. The element P_ij thus indicates how many points that are initially in box C_i end up in box C_j when the points in C_i are projected one step forward in time. Thus, the row P_ik^N where k in 1 2 ldots N  gives the probability of jumping from the state defined by box C_i to any of the other N states. It follows that sum_k=1^N P_ik = 1 for all i. Thus, P^N is a row/right stochastic matrix.\n\nInvariant measure estimation from transfer operator\n\nThe left invariant distribution mathbfrho^N is a row vector, where mathbfrho^N P^N = mathbfrho^N. Hence, mathbfrho^N is a row eigenvector of the transfer matrix P^N associated with eigenvalue 1. The distribution mathbfrho^N approximates the invariant density of the system subject to binning, and can be taken as a probability distribution over the partition elements.\n\nIn practice, the invariant measure mathbfrho^N is computed using invariantmeasure, which also approximates the transfer matrix. The invariant distribution is initialized as a length-N random distribution which is then applied to P^N. The resulting length-N distribution is then applied to P^N again. This process repeats until the difference between the distributions over consecutive iterations is below some threshold.\n\nSee also: RectangularBinning, invariantmeasure.\n\n[Diego2019]: Diego, D., Haaga, K. A., & Hannisdal, B. (2019). Transfer entropy computation using the Perron-Frobenius operator. Physical Review E, 99(4), 042212.\n\n\n\n\n\n","category":"type"},{"location":"probabilities/#Utility-methods/types","page":"Probabilities","title":"Utility methods/types","text":"","category":"section"},{"location":"probabilities/","page":"Probabilities","title":"Probabilities","text":"InvariantMeasure\ninvariantmeasure\ntransfermatrix","category":"page"},{"location":"probabilities/#ComplexityMeasures.InvariantMeasure","page":"Probabilities","title":"ComplexityMeasures.InvariantMeasure","text":"InvariantMeasure(to, ρ)\n\nMinimal return struct for invariantmeasure that contains the estimated invariant measure ρ, as well as the transfer operator to from which it is computed (including bin information).\n\nSee also: invariantmeasure.\n\n\n\n\n\n","category":"type"},{"location":"probabilities/#ComplexityMeasures.invariantmeasure","page":"Probabilities","title":"ComplexityMeasures.invariantmeasure","text":"invariantmeasure(x::AbstractStateSpaceSet, binning::RectangularBinning) → iv::InvariantMeasure\n\nEstimate an invariant measure over the points in x based on binning the data into rectangular boxes dictated by the binning, then approximate the transfer (Perron-Frobenius) operator over the bins. From the approximation to the transfer operator, compute an invariant distribution over the bins. Assumes that the input data are sequential.\n\nDetails on the estimation procedure is found the TransferOperator docstring.\n\nExample\n\nusing DynamicalSystems\nhenon_rule(x, p, n) = SVector{2}(1.0 - p[1]*x[1]^2 + x[2], p[2]*x[1])\nhenon = DeterministicIteratedMap(henon_rule, zeros(2), [1.4, 0.3])\norbit, t = trajectory(ds, 20_000; Ttr = 10)\n\n# Estimate the invariant measure over some coarse graining of the orbit.\niv = invariantmeasure(orbit, RectangularBinning(15))\n\n# Get the probabilities and bins\ninvariantmeasure(iv)\n\nProbabilities and bin information\n\ninvariantmeasure(iv::InvariantMeasure) → (ρ::Probabilities, bins::Vector{<:SVector})\n\nFrom a pre-computed invariant measure, return the probabilities and associated bins. The element ρ[i] is the probability of visitation to the box bins[i]. Analogous to binhist.\n\nhint: Transfer operator approach vs. naive histogram approach\nWhy bother with the transfer operator instead of using regular histograms to obtain probabilities?In fact, the naive histogram approach and the transfer operator approach are equivalent in the limit of long enough time series (as n to intfy), which is guaranteed by the ergodic theorem. There is a crucial difference, however:The naive histogram approach only gives the long-term probabilities that orbits visit a certain region of the state space. The transfer operator encodes that information too, but comes with the added benefit of knowing the transition probabilities between states (see transfermatrix).\n\nSee also: InvariantMeasure.\n\n\n\n\n\n","category":"function"},{"location":"probabilities/#ComplexityMeasures.transfermatrix","page":"Probabilities","title":"ComplexityMeasures.transfermatrix","text":"transfermatrix(iv::InvariantMeasure) → (M::AbstractArray{<:Real, 2}, bins::Vector{<:SVector})\n\nReturn the transfer matrix/operator and corresponding bins. Here, bins[i] corresponds to the i-th row/column of the transfer matrix. Thus, the entry M[i, j] is the probability of jumping from the state defined by bins[i] to the state defined by bins[j].\n\nSee also: TransferOperator.\n\n\n\n\n\n","category":"function"},{"location":"probabilities/#Kernel-density","page":"Probabilities","title":"Kernel density","text":"","category":"section"},{"location":"probabilities/","page":"Probabilities","title":"Probabilities","text":"NaiveKernel","category":"page"},{"location":"probabilities/#ComplexityMeasures.NaiveKernel","page":"Probabilities","title":"ComplexityMeasures.NaiveKernel","text":"NaiveKernel(ϵ::Real; method = KDTree, w = 0, metric = Euclidean()) <: ProbabilitiesEstimator\n\nEstimate probabilities/entropy using a \"naive\" kernel density estimation approach (KDE), as discussed in Prichard and Theiler (1995) [PrichardTheiler1995].\n\nProbabilities P(mathbfx epsilon) are assigned to every point mathbfx by counting how many other points occupy the space spanned by a hypersphere of radius ϵ around mathbfx, according to:\n\nP_i( X epsilon) approx dfrac1N sum_s B(X_i - X_j  epsilon)\n\nwhere B gives 1 if the argument is true. Probabilities are then normalized.\n\nKeyword arguments\n\nmethod = KDTree: the search structure supported by Neighborhood.jl. Specifically, use KDTree to use a tree-based neighbor search, or BruteForce for the direct distances between all points. KDTrees heavily outperform direct distances when the dimensionality of the data is much smaller than the data length.\nw = 0: the Theiler window, which excludes indices s that are within i - s  w from the given point x_i.\nmetric = Euclidean(): the distance metric.\n\nOutcome space\n\nThe outcome space Ω for NaiveKernel are the indices of the input data, eachindex(x). Hence, input x is needed for a well-defined outcome_space. The reason to not return the data points themselves is because duplicate data points may not get assigned same probabilities (due to having different neighbors).\n\n[PrichardTheiler1995]: Prichard, D., & Theiler, J. (1995). Generalized redundancies for time series analysis. Physica D: Nonlinear Phenomena, 84(3-4), 476-493.\n\n\n\n\n\n","category":"type"},{"location":"probabilities/#Timescales","page":"Probabilities","title":"Timescales","text":"","category":"section"},{"location":"probabilities/","page":"Probabilities","title":"Probabilities","text":"WaveletOverlap\nPowerSpectrum","category":"page"},{"location":"probabilities/#ComplexityMeasures.WaveletOverlap","page":"Probabilities","title":"ComplexityMeasures.WaveletOverlap","text":"WaveletOverlap([wavelet]) <: ProbabilitiesEstimator\n\nApply the maximal overlap discrete wavelet transform (MODWT) to a signal, then compute probabilities as the (normalized) energies at different wavelet scales. These probabilities are used to compute the wavelet entropy, according to Rosso et al. (2001)[Rosso2001]. Input timeseries x is needed for a well-defined outcome space.\n\nBy default the wavelet Wavelets.WT.Daubechies{12}() is used. Otherwise, you may choose a wavelet from the Wavelets package (it must subtype OrthoWaveletClass).\n\nOutcome space\n\nThe outcome space for WaveletOverlap are the integers 1, 2, …, N enumerating the wavelet scales. To obtain a better understanding of what these mean, we prepared a notebook you can view online. As such, this estimator only works for timeseries input and input x is needed for a well-defined outcome_space.\n\n[Rosso2001]: Rosso et al. (2001). Wavelet entropy: a new tool for analysis of short duration brain electrical signals. Journal of neuroscience methods, 105(1), 65-75.\n\n\n\n\n\n","category":"type"},{"location":"probabilities/#ComplexityMeasures.PowerSpectrum","page":"Probabilities","title":"ComplexityMeasures.PowerSpectrum","text":"PowerSpectrum() <: ProbabilitiesEstimator\n\nCalculate the power spectrum of a timeseries (amplitude square of its Fourier transform), and return the spectrum normalized to sum = 1 as probabilities. The Shannon entropy of these probabilities is typically referred in the literature as spectral entropy, e.g. [Llanos2016],[Tian2017].\n\nThe closer the spectrum is to flat, i.e., white noise, the higher the entropy. However, you can't compare entropies of timeseries with different length, because the binning in spectral space depends on the length of the input.\n\nOutcome space\n\nThe outcome space Ω for PowerSpectrum is the set of frequencies in Fourier space. They should be multiplied with the sampling rate of the signal, which is assumed to be 1. Input x is needed for a well-defined outcome_space.\n\n[Llanos2016]: Llanos et al., Power spectral entropy as an information-theoretic correlate of manner of articulation in American English, The Journal of the Acoustical Society of America 141, EL127 (2017)\n\n[Tian2017]: Tian et al, Spectral EntropyDefinition Can Predict Changes of Working Memory Performance Reduced by Short-Time Training in the Delayed-Match-to-Sample Task, Front. Hum. Neurosci.\n\n\n\n\n\n","category":"type"},{"location":"probabilities/#Diversity","page":"Probabilities","title":"Diversity","text":"","category":"section"},{"location":"probabilities/","page":"Probabilities","title":"Probabilities","text":"Diversity","category":"page"},{"location":"probabilities/#ComplexityMeasures.Diversity","page":"Probabilities","title":"ComplexityMeasures.Diversity","text":"Diversity(; m::Int, τ::Int, nbins::Int)\n\nA ProbabilitiesEstimator based on the cosine similarity. It can be used with entropy to compute the diversity entropy of an input timeseries[Wang2020].\n\nThe implementation here allows for τ != 1, which was not considered in the original paper.\n\nDescription\n\nDiversity probabilities are computed as follows.\n\nFrom the input time series x, using embedding lag τ and embedding dimension m,  construct the embedding  Y = bf x_i  = (x_i x_i+tau x_i+2tau ldots x_i+mtau - 1_i = 1^N-mτ.\nCompute D = d(bf x_t bf x_t+1) _t=1^N-mτ-1,  where d(cdot cdot) is the cosine similarity between two m-dimensional  vectors in the embedding.\nDivide the interval [-1, 1] into nbins equally sized subintervals (including the value +1).\nConstruct a histogram of cosine similarities d in D over those subintervals.\nSum-normalize the histogram to obtain probabilities.\n\nOutcome space\n\nThe outcome space for Diversity is the bins of the [-1, 1] interval, and the return configuration is the same as in ValueHistogram (left bin edge).\n\n[Wang2020]: Wang, X., Si, S., & Li, Y. (2020). Multiscale diversity entropy: A novel dynamical measure for fault diagnosis of rotating machinery. IEEE Transactions on Industrial Informatics, 17(8), 5419-5429.\n\n\n\n\n\n","category":"type"},{"location":"probabilities/#Spatial-estimators","page":"Probabilities","title":"Spatial estimators","text":"","category":"section"},{"location":"probabilities/","page":"Probabilities","title":"Probabilities","text":"SpatialSymbolicPermutation\nSpatialDispersion","category":"page"},{"location":"probabilities/#ComplexityMeasures.SpatialSymbolicPermutation","page":"Probabilities","title":"ComplexityMeasures.SpatialSymbolicPermutation","text":"SpatialSymbolicPermutation <: ProbabilitiesEstimator\nSpatialSymbolicPermutation(stencil, x; periodic = true)\n\nA symbolic, permutation-based probabilities estimator for spatiotemporal systems that generalises SymbolicPermutation to high-dimensional arrays. The order m of the permutation pattern is extracted from the stencil, see below.\n\nSpatialSymbolicPermutation is based on the 2D and 3D spatiotemporal permutation entropy estimators by by Ribeiro et al. (2012)[Ribeiro2012] and Schlemmer et al. (2018)[Schlemmer2018]), respectively, but is here implemented as a pure probabilities probabilities estimator that is generalized for D-dimensional input array x, with arbitrary regions (stencils) to get patterns form and (possibly) periodic boundary conditions.\n\nSee below for ways to specify the stencil. If periodic = true, then the stencil wraps around at the ends of the array. If false, then collected regions with indices which exceed the array bounds are skipped.\n\nIn combination with entropy and entropy_normalized, this probabilities estimator can be used to compute generalized spatiotemporal permutation EntropyDefinition of any type.\n\nOutcome space\n\nThe outcome space Ω for SpatialSymbolicPermutation is the set of length-m ordinal patterns (i.e. permutations) that can be formed by the integers 1, 2, …, m, ordered lexicographically. There are factorial(m) such patterns. Here m refers to the number of points included in stencil.\n\nStencils\n\nThe stencil defines what local area to use to group hypervoxels. Each grouping of hypervoxels is mapped to an order-m permutation pattern, which is then mapped to an integer as in SymbolicPermutation. The stencil is moved around the input array, in a sense \"scanning\" the input array, to collect all possible groupings allowed by the boundary condition (periodic or not).\n\nStencils are passed in one of the following three ways:\n\nAs vectors of CartesianIndex which encode the offset of indices to include in the  stencil, with respect to the current array index when scanning over the array.  For example stencil = CartesianIndex.([(0,0), (0,1), (1,1), (1,0)]).  Don't forget to include the zero offset index if you want to include the hypervoxel  itself, which is almost always the case.  Here the stencil creates a 2x2 square extending to the bottom and right of the pixel  (directions here correspond to the way Julia prints matrices by default).  When passing a stencil as a vector of CartesianIndex, m = length(stencil).\nAs a D-dimensional array (where D matches the dimensionality of the input data)  containing 0s and 1s, where if stencil[index] == 1, the corresponding pixel is  included, and if stencil[index] == 0, it is not included.  To generate the same estimator as in 1., use stencil = [1 1; 1 1].  When passing a stencil as a D-dimensional array, m = sum(stencil)\nAs a Tuple containing two Tuples, both of length D, for D-dimensional data.  The first tuple specifies the extent of the stencil, where extent[i]  dictates the number of hypervoxels to be included along the ith axis and lag[i]  the separation of hypervoxels along the same axis.  This method can only generate (hyper)rectangular stencils. To create the same estimator as  in the previous examples, use here stencil = ((2, 2), (1, 1)).  When passing a stencil using extent and lag, m = prod(extent).\n\n[Ribeiro2012]: Ribeiro et al. (2012). Complexity-entropy causality plane as a complexity measure for two-dimensional patterns. https://doi.org/10.1371/journal.pone.0040689\n\n[Schlemmer2018]: Schlemmer et al. (2018). Spatiotemporal Permutation EntropyDefinition as a Measure for Complexity of Cardiac Arrhythmia. https://doi.org/10.3389/fphy.2018.00039\n\n\n\n\n\n","category":"type"},{"location":"probabilities/#ComplexityMeasures.SpatialDispersion","page":"Probabilities","title":"ComplexityMeasures.SpatialDispersion","text":"SpatialDispersion <: ProbabilitiesEstimator\nSpatialDispersion(stencil, x::AbstractArray;\n    periodic = true,\n    c = 5,\n    skip_encoding = false,\n    L = nothing,\n)\n\nA dispersion-based probabilities estimator that generalises Dispersion for input data that are high-dimensional arrays.\n\nSpatialDispersion is based on Azami et al. (2019)[Azami2019]'s 2D square dispersion (Shannon) entropy estimator, but is here implemented as a pure probabilities probabilities estimator that is generalized for N-dimensional input data x, with arbitrary neighborhood regions (stencils) and (optionally) periodic boundary conditions.\n\nIn combination with entropy and entropy_normalized, this probabilities estimator can be used to compute (normalized) generalized spatiotemporal dispersion EntropyDefinition of any type.\n\nArguments\n\nstencil. Defines what local area (hyperrectangle), or which points within this area,   to include around each hypervoxel (i.e. pixel in 2D). The examples below demonstrate   different ways of specifying stencils. For details, see   SpatialSymbolicPermutation. See SpatialSymbolicPermutation for   more information about stencils.\nx::AbstractArray. The input data. Must be provided because we need to know its size   for optimization and bound checking.\n\nKeyword arguments\n\nperiodic::Bool. If periodic == true, then the stencil should wrap around at the   end of the array. If periodic = false, then pixels whose stencil exceeds the array   bounds are skipped.\nc::Int. Determines how many discrete categories to use for the Gaussian encoding.\nskip_encoding. If skip_encoding == true, encoding is ignored, and dispersion   patterns are computed directly from x, under the assumption that L is the alphabet   length for x (useful for categorical or integer data). Thus, if   skip_encoding == true, then L must also be specified. This is useful for   categorical or integer-valued data.\nL. If L == nothing (default), then the number of total outcomes is inferred from   stencil and encoding. If L is set to an integer, then the data is considered   pre-encoded and the number of total outcomes is set to L.\n\nOutcome space\n\nThe outcome space for SpatialDispersion is the unique delay vectors whose elements are the the symbols (integers) encoded by the Gaussian CDF. Hence, the outcome space is all m-dimensional delay vectors whose elements are all possible values in 1:c. There are c^m such vectors.\n\nDescription\n\nEstimating probabilities/entropies from higher-dimensional data is conceptually simple.\n\nDiscretize each value (hypervoxel) in x relative to all other values xᵢ ∈ x using the  provided encoding scheme.\nUse stencil to extract relevant (discretized) points around each hypervoxel.\nConstruct a symbol these points.\nTake the sum-normalized histogram of the symbol as a probability distribution.\nOptionally, compute entropy or entropy_normalized from this  probability distribution.\n\nUsage\n\nHere's how to compute spatial dispersion entropy using the three different ways of specifying stencils.\n\nx = rand(50, 50) # first \"time slice\" of a spatial system evolution\n\n# Cartesian stencil\nstencil_cartesian = CartesianIndex.([(0,0), (1,0), (1,1), (0,1)])\nest = SpatialDispersion(stencil_cartesian, x)\nentropy_normalized(est, x)\n\n# Extent/lag stencil\nextent = (2, 2); lag = (1, 1); stencil_ext_lag = (extent, lag)\nest = SpatialDispersion(stencil_ext_lag, x)\nentropy_normalized(est, x)\n\n# Matrix stencil\nstencil_matrix = [1 1; 1 1]\nest = SpatialDispersion(stencil_matrix, x)\nentropy_normalized(est, x)\n\nTo apply this to timeseries of spatial data, simply loop over the call (broadcast), e.g.:\n\nimgs = [rand(50, 50) for i = 1:100]; # one image per second over 100 seconds\nstencil = ((2, 2), (1, 1)) # a 2x2 stencil (i.e. dispersion patterns of length 4)\nest = SpatialDispersion(stencil, first(imgs))\nh_vs_t = entropy_normalized.(Ref(est), imgs)\n\nComputing generalized spatiotemporal dispersion entropy is trivial, e.g. with Renyi:\n\nx = reshape(repeat(1:5, 500) .+ 0.1*rand(500*5), 50, 50)\nest = SpatialDispersion(stencil, x)\nentropy(Renyi(q = 2), est, x)\n\nSee also: SpatialSymbolicPermutation, GaussianCDFEncoding, symbolize.\n\n[Azami2019]: Azami, H., da Silva, L. E. V., Omoto, A. C. M., & Humeau-Heurtier, A. (2019). Two-dimensional dispersion entropy: An information-theoretic method for irregularity analysis of images. Signal Processing: Image Communication, 75, 178-187.\n\n\n\n\n\n","category":"type"},{"location":"dev/multiscale/#Complexity:-multiscale","page":"-","title":"Complexity: multiscale","text":"","category":"section"},{"location":"dev/multiscale/","page":"-","title":"-","text":"using ComplexityMeasures\nusing CairoMakie\n\nN, a = 2000, 20\nt = LinRange(0, 2*a*π, N)\n\nx = repeat([-5:5 |> collect; 4:-1:-4 |> collect], N ÷ 20);\ny = sin.(t .+ cos.(t/0.5)) .+ 0.2 .* x\nmaxscale = 10\nhs = ComplexityMeasures.multiscale_normalized(Regular(), SampleEntropy(y), y; maxscale)\n\nfig = Figure()\nax1 = Axis(fig[1,1]; ylabel = \"y\")\nlines!(ax1, t, y; color = Cycled(1));\nax2 = Axis(fig[2, 1]; ylabel = \"Sample entropy (h)\", xlabel = \"Scale\")\nscatterlines!(ax2, 1:maxscale |> collect, hs; color = Cycled(1));\nfig","category":"page"},{"location":"convenience/#Convenience-functions","page":"Convenience functions","title":"Convenience functions","text":"","category":"section"},{"location":"convenience/","page":"Convenience functions","title":"Convenience functions","text":"We provide a few convenience functions for widely used names for entropy or \"entropy-like\" quantities. Other arbitrary specialized convenience functions can easily be defined in a couple lines of code.","category":"page"},{"location":"convenience/","page":"Convenience functions","title":"Convenience functions","text":"entropy_permutation\nentropy_wavelet\nentropy_dispersion\nentropy_approx\nentropy_sample","category":"page"},{"location":"convenience/#ComplexityMeasures.entropy_permutation","page":"Convenience functions","title":"ComplexityMeasures.entropy_permutation","text":"entropy_permutation(x; τ = 1, m = 3, base = 2)\n\nCompute the permutation entropy of x of order m with delay/lag τ. This function is just a convenience call to:\n\nest = SymbolicPermutation(; m, τ)\nentropy(Shannon(base), est, x)\n\nSee SymbolicPermutation for more info. Similarly, one can use SymbolicWeightedPermutation or SymbolicAmplitudeAwarePermutation for the weighted/amplitude-aware versions.\n\n\n\n\n\n","category":"function"},{"location":"convenience/#ComplexityMeasures.entropy_wavelet","page":"Convenience functions","title":"ComplexityMeasures.entropy_wavelet","text":"entropy_wavelet(x; wavelet = Wavelets.WT.Daubechies{12}(), base = 2)\n\nCompute the wavelet entropy. This function is just a convenience call to:\n\nest = WaveletOverlap(wavelet)\nentropy(Shannon(base), est, x)\n\nSee WaveletOverlap for more info.\n\n\n\n\n\n","category":"function"},{"location":"convenience/#ComplexityMeasures.entropy_dispersion","page":"Convenience functions","title":"ComplexityMeasures.entropy_dispersion","text":"entropy_dispersion(x; base = 2, kwargs...)\n\nCompute the dispersion entropy. This function is just a convenience call to:\n\nest = Dispersion(kwargs...)\nentropy(Shannon(base), est, x)\n\nSee Dispersion for more info.\n\n\n\n\n\n","category":"function"},{"location":"convenience/#ComplexityMeasures.entropy_approx","page":"Convenience functions","title":"ComplexityMeasures.entropy_approx","text":"entropy_approx(x; m = 2, τ = 1, r = 0.2 * Statistics.std(x), base = MathConstants.e)\n\nConvenience syntax for computing the approximate entropy (Pincus, 1991) for timeseries x.\n\nThis is just a wrapper for complexity(ApproximateEntropy(; m, τ, r, base), x) (see also ApproximateEntropy).\n\n\n\n\n\n","category":"function"},{"location":"convenience/#ComplexityMeasures.entropy_sample","page":"Convenience functions","title":"ComplexityMeasures.entropy_sample","text":"entropy_sample(x; r = 0.2std(x), m = 2, τ = 1, normalize = true)\n\nConvenience syntax for estimating the (normalized) sample entropy (Richman & Moorman, 2000) of timeseries x.\n\nThis is just a wrapper for complexity(SampleEntropy(; r, m, τ, base), x).\n\nSee also: SampleEntropy, complexity, complexity_normalized).\n\n\n\n\n\n","category":"function"},{"location":"examples/#ComplexityMeasures.jl-Examples","page":"ComplexityMeasures.jl Examples","title":"ComplexityMeasures.jl Examples","text":"","category":"section"},{"location":"examples/#Probabilities:-kernel-density","page":"ComplexityMeasures.jl Examples","title":"Probabilities: kernel density","text":"","category":"section"},{"location":"examples/","page":"ComplexityMeasures.jl Examples","title":"ComplexityMeasures.jl Examples","text":"Here, we draw some random points from a 2D normal distribution. Then, we use kernel density estimation to associate a probability to each point p, measured by how many points are within radius 1.5 of p. Plotting the actual points, along with their associated probabilities estimated by the KDE procedure, we get the following surface plot.","category":"page"},{"location":"examples/","page":"ComplexityMeasures.jl Examples","title":"ComplexityMeasures.jl Examples","text":"using ComplexityMeasures\nusing CairoMakie\nusing Distributions: MvNormal\n\n𝒩 = MvNormal([1, -4], 2)\nN = 500\nD = StateSpaceSet(sort([rand(𝒩) for i = 1:N]))\nx, y = columns(D)\np = probabilities(NaiveKernel(1.5), D)\nfig, ax = scatter(D[:, 1], D[:, 2], zeros(N);\n    markersize=8, axis=(type = Axis3,)\n)\nsurface!(ax, x, y, p.p)\nax.zlabel = \"P\"\nax.zticklabelsvisible = false\nfig","category":"page"},{"location":"examples/#Probabilities:-KL-divergence-of-histograms","page":"ComplexityMeasures.jl Examples","title":"Probabilities: KL-divergence of histograms","text":"","category":"section"},{"location":"examples/","page":"ComplexityMeasures.jl Examples","title":"ComplexityMeasures.jl Examples","text":"In this example we show how simple it is to compute the KL-divergence (or any other distance function for probability distributions) using ComplexityMeasures.jl. For simplicity, we will compute the KL-divergence between the ValueHistograms of two timeseries.","category":"page"},{"location":"examples/","page":"ComplexityMeasures.jl Examples","title":"ComplexityMeasures.jl Examples","text":"Note that it is crucial to use allprobabilities instead of probabilities.","category":"page"},{"location":"examples/","page":"ComplexityMeasures.jl Examples","title":"ComplexityMeasures.jl Examples","text":"using ComplexityMeasures\n\nN = 1000\nt = range(0, 20π; length=N)\nx = @. clamp(sin(t), -0.5, 1)\ny = @. sin(t + cos(2t))\n\nr = -1:0.1:1\nest = ValueHistogram(FixedRectangularBinning(r))\npx = allprobabilities(est, x)\npy = allprobabilities(est, y)\n\n# Visualize\nusing CairoMakie\nbins = r[1:end-1] .+ step(r)/2\nfig, ax = barplot(bins, px; label = L\"p_x\")\nbarplot!(ax, bins, py; label = L\"p_y\")\naxislegend(ax; labelsize = 30)\nfig","category":"page"},{"location":"examples/","page":"ComplexityMeasures.jl Examples","title":"ComplexityMeasures.jl Examples","text":"using StatsBase: kldivergence\n\nkldivergence(px, py)","category":"page"},{"location":"examples/","page":"ComplexityMeasures.jl Examples","title":"ComplexityMeasures.jl Examples","text":"kldivergence(py, px)","category":"page"},{"location":"examples/","page":"ComplexityMeasures.jl Examples","title":"ComplexityMeasures.jl Examples","text":"(Inf because there are events with 0 probability in px)","category":"page"},{"location":"examples/#Differential-entropy:-estimator-comparison","page":"ComplexityMeasures.jl Examples","title":"Differential entropy: estimator comparison","text":"","category":"section"},{"location":"examples/","page":"ComplexityMeasures.jl Examples","title":"ComplexityMeasures.jl Examples","text":"Here, we compare how the nearest neighbor differential entropy estimators (Kraskov, KozachenkoLeonenko, Zhu and ZhuSingh) converge towards the true entropy value for increasing time series length.","category":"page"},{"location":"examples/","page":"ComplexityMeasures.jl Examples","title":"ComplexityMeasures.jl Examples","text":"ComplexityMeasures.jl also provides entropy estimators based on order statistics. These estimators are only defined for scalar-valued vectors, in this example, so we compute these estimates separately, and add these estimators (Vasicek, Ebrahimi, AlizadehArghami and Correa) to the comparison.","category":"page"},{"location":"examples/","page":"ComplexityMeasures.jl Examples","title":"ComplexityMeasures.jl Examples","text":"Input data are from a normal 1D distribution mathcalN(0 1), for which the true entropy is 0.5*log(2π) + 0.5 nats when using natural logarithms.","category":"page"},{"location":"examples/","page":"ComplexityMeasures.jl Examples","title":"ComplexityMeasures.jl Examples","text":"using ComplexityMeasures\nusing CairoMakie, Statistics\nnreps = 30\nNs = [100:100:500; 1000:1000:5000]\ne = Shannon(; base = MathConstants.e)\n\n# --------------------------\n# kNN estimators\n# --------------------------\nw = 0 # Theiler window of 0 (only exclude the point itself during neighbor searches)\nknn_estimators = [\n    # with k = 1, Kraskov is virtually identical to\n    # Kozachenko-Leonenko, so pick a higher number of neighbors for Kraskov\n    Kraskov(; k = 3, base = ℯ, w),\n    KozachenkoLeonenko(; base = ℯ, w),\n    Zhu(; k = 3, base = ℯ, w),\n    ZhuSingh(; k = 3, base = ℯ, w),\n    Gao(; k = 3, base = ℯ, corrected = false, w),\n    Gao(; k = 3, base = ℯ, corrected = true, w),\n    Goria(; k = 3, w, base = ℯ),\n    Lord(; k = 20, w, base = ℯ) # more neighbors for accurate ellipsoid estimation\n]\n\n# Test each estimator `nreps` times over time series of varying length.\nHs_uniform_knn = [[zeros(nreps) for N in Ns] for e in knn_estimators]\nfor (i, est) in enumerate(knn_estimators)\n    for j = 1:nreps\n        pts = randn(maximum(Ns)) |> StateSpaceSet\n        for (k, N) in enumerate(Ns)\n            Hs_uniform_knn[i][k][j] = entropy(est, pts[1:N])\n        end\n    end\nend\n\n# --------------------------\n# Order statistic estimators\n# --------------------------\n\n# Just provide types here, they are instantiated inside the loop\nestimators_os = [Vasicek, Ebrahimi, AlizadehArghami, Correa]\nHs_uniform_os = [[zeros(nreps) for N in Ns] for e in estimators_os]\nfor (i, est_os) in enumerate(estimators_os)\n    for j = 1:nreps\n        pts = randn(maximum(Ns)) # raw timeseries, not a `StateSpaceSet`\n        for (k, N) in enumerate(Ns)\n            m = floor(Int, N / 100) # Scale `m` to timeseries length\n            est = est_os(; m, base = ℯ) # Instantiate estimator with current `m`\n            Hs_uniform_os[i][k][j] = entropy(est, pts[1:N])\n        end\n    end\nend\n\n# -------------\n# Plot results\n# -------------\nfig = Figure(resolution = (700, 11 * 200))\nlabels_knn = [\"KozachenkoLeonenko\", \"Kraskov\", \"Zhu\", \"ZhuSingh\", \"Gao (not corrected)\",\n    \"Gao (corrected)\", \"Goria\", \"Lord\"]\nlabels_os = [\"Vasicek\", \"Ebrahimi\", \"AlizadehArghami\", \"Correa\"]\n\nfor (i, e) in enumerate(knn_estimators)\n    Hs = Hs_uniform_knn[i]\n    ax = Axis(fig[i,1]; ylabel = \"h (nats)\")\n    lines!(ax, Ns, mean.(Hs); color = Cycled(i), label = labels_knn[i])\n    band!(ax, Ns, mean.(Hs) .+ std.(Hs), mean.(Hs) .- std.(Hs); alpha = 0.5,\n        color = (Main.COLORS[i], 0.5))\n    hlines!(ax, [(0.5*log(2π) + 0.5)], color = :black, lw = 5, linestyle = :dash)\n\n    ylims!(1.2, 1.6)\n    axislegend()\nend\n\nfor (i, e) in enumerate(estimators_os)\n    Hs = Hs_uniform_os[i]\n    ax = Axis(fig[i + length(knn_estimators),1]; ylabel = \"h (nats)\")\n    lines!(ax, Ns, mean.(Hs); color = Cycled(i), label = labels_os[i])\n    band!(ax, Ns, mean.(Hs) .+ std.(Hs), mean.(Hs) .- std.(Hs), alpha = 0.5,\n        color = (Main.COLORS[i], 0.5))\n    hlines!(ax, [(0.5*log(2π) + 0.5)], color = :black, lw = 5, linestyle = :dash)\n    ylims!(1.2, 1.6)\n    axislegend()\nend\n\nfig","category":"page"},{"location":"examples/","page":"ComplexityMeasures.jl Examples","title":"ComplexityMeasures.jl Examples","text":"All estimators approach the true differential entropy, but those based on order statistics are negatively biased for small sample sizes.","category":"page"},{"location":"examples/#Discrete-entropy:-permutation-entropy","page":"ComplexityMeasures.jl Examples","title":"Discrete entropy: permutation entropy","text":"","category":"section"},{"location":"examples/","page":"ComplexityMeasures.jl Examples","title":"ComplexityMeasures.jl Examples","text":"This example plots permutation entropy for time series of the chaotic logistic map. Entropy estimates using SymbolicWeightedPermutation and SymbolicAmplitudeAwarePermutation are added here for comparison. The entropy behaviour can be parallelized with the ChaosTools.lyapunov of the map.","category":"page"},{"location":"examples/","page":"ComplexityMeasures.jl Examples","title":"ComplexityMeasures.jl Examples","text":"using DynamicalSystemsBase, CairoMakie\n\nlogistic_rule(x, p, n) = @inbounds SVector(p[1]*x[1]*(1-x[1]))\nds = DeterministicIteratedMap(logistic_rule, [0.4], [4.0])\nrs = 3.4:0.001:4\nN_lyap, N_ent = 100000, 10000\nm, τ = 6, 1 # Symbol size/dimension and embedding lag\n\n# Generate one time series for each value of the logistic parameter r\nhs_perm, hs_wtperm, hs_ampperm = [zeros(length(rs)) for _ in 1:4]\n\nfor (i, r) in enumerate(rs)\n    ds.p[1] = r\n\n    x, t = trajectory(ds, N_ent)\n    ## `x` is a 1D dataset, need to recast into a timeseries\n    x = columns(x)[1]\n    hs_perm[i] = entropy(SymbolicPermutation(; m, τ), x)\n    hs_wtperm[i] = entropy(SymbolicWeightedPermutation(; m, τ), x)\n    hs_ampperm[i] = entropy(SymbolicAmplitudeAwarePermutation(; m, τ), x)\nend\n\nfig = Figure()\na1 = Axis(fig[1,1]; ylabel = L\"h_6 (SP)\")\nlines!(a1, rs, hs_perm; color = Cycled(2))\na2 = Axis(fig[2,1]; ylabel = L\"h_6 (WT)\")\nlines!(a2, rs, hs_wtperm; color = Cycled(3))\na3 = Axis(fig[3,1]; ylabel = L\"h_6 (SAAP)\", xlabel = L\"r\")\nlines!(a3, rs, hs_ampperm; color = Cycled(4))\n\nfor a in (a1,a2,a3)\n    hidexdecorations!(a, grid = false)\nend\nfig","category":"page"},{"location":"examples/#Discrete-entropy:-wavelet-entropy","page":"ComplexityMeasures.jl Examples","title":"Discrete entropy: wavelet entropy","text":"","category":"section"},{"location":"examples/","page":"ComplexityMeasures.jl Examples","title":"ComplexityMeasures.jl Examples","text":"The scale-resolved wavelet entropy should be lower for very regular signals (most of the energy is contained at one scale) and higher for very irregular signals (energy spread more out across scales).","category":"page"},{"location":"examples/","page":"ComplexityMeasures.jl Examples","title":"ComplexityMeasures.jl Examples","text":"using CairoMakie\nN, a = 1000, 10\nt = LinRange(0, 2*a*π, N)\n\nx = sin.(t);\ny = sin.(t .+ cos.(t/0.5));\nz = sin.(rand(1:15, N) ./ rand(1:10, N))\n\nh_x = entropy_wavelet(x)\nh_y = entropy_wavelet(y)\nh_z = entropy_wavelet(z)\n\nfig = Figure()\nax = Axis(fig[1,1]; ylabel = \"x\")\nlines!(ax, t, x; color = Cycled(1), label = \"h=$(h=round(h_x, sigdigits = 5))\");\nay = Axis(fig[2,1]; ylabel = \"y\")\nlines!(ay, t, y; color = Cycled(2), label = \"h=$(h=round(h_y, sigdigits = 5))\");\naz = Axis(fig[3,1]; ylabel = \"z\", xlabel = \"time\")\nlines!(az, t, z; color = Cycled(3), label = \"h=$(h=round(h_z, sigdigits = 5))\");\nfor a in (ax, ay, az); axislegend(a); end\nfor a in (ax, ay); hidexdecorations!(a; grid=false); end\nfig","category":"page"},{"location":"examples/#Discrete-entropies:-properties","page":"ComplexityMeasures.jl Examples","title":"Discrete entropies: properties","text":"","category":"section"},{"location":"examples/","page":"ComplexityMeasures.jl Examples","title":"ComplexityMeasures.jl Examples","text":"Here, we show the sensitivity of the various entropies to variations in their parameters.","category":"page"},{"location":"examples/#Curado-entropy","page":"ComplexityMeasures.jl Examples","title":"Curado entropy","text":"","category":"section"},{"location":"examples/","page":"ComplexityMeasures.jl Examples","title":"ComplexityMeasures.jl Examples","text":"Here, we reproduce Figure 2 from Curado & Nobre (2004)[Curado2004], showing how the Curado entropy changes as function of the parameter a for a range of two-element probability distributions given by Probabilities([p, 1 - p] for p in 1:0.0:0.01:1.0).","category":"page"},{"location":"examples/","page":"ComplexityMeasures.jl Examples","title":"ComplexityMeasures.jl Examples","text":"using ComplexityMeasures, CairoMakie\nbs = [1.0, 1.5, 2.0, 3.0, 4.0, 10.0]\nps = [Probabilities([p, 1 - p]) for p = 0.0:0.01:1.0]\nhs = [[entropy(Curado(; b = b), p) for p in ps] for b in bs]\nfig = Figure()\nax = Axis(fig[1,1]; xlabel = \"p\", ylabel = \"H(p)\")\npp = [p[1] for p in ps]\nfor (i, b) in enumerate(bs)\n    lines!(ax, pp, hs[i], label = \"b=$b\", color = Cycled(i))\nend\naxislegend(ax)\nfig","category":"page"},{"location":"examples/","page":"ComplexityMeasures.jl Examples","title":"ComplexityMeasures.jl Examples","text":"[Curado2004]: Curado, E. M., & Nobre, F. D. (2004). On the stability of analytic entropic forms. Physica A: Statistical Mechanics and its Applications, 335(1-2), 94-106.","category":"page"},{"location":"examples/#Kaniadakis-entropy","page":"ComplexityMeasures.jl Examples","title":"Kaniadakis entropy","text":"","category":"section"},{"location":"examples/","page":"ComplexityMeasures.jl Examples","title":"ComplexityMeasures.jl Examples","text":"Here, we show how Kaniadakis entropy changes as function of the parameter a for a range of two-element probability distributions given by Probabilities([p, 1 - p] for p in 1:0.0:0.01:1.0).","category":"page"},{"location":"examples/","page":"ComplexityMeasures.jl Examples","title":"ComplexityMeasures.jl Examples","text":"using ComplexityMeasures\nusing CairoMakie\n\nprobs = [Probabilities([p, 1-p]) for p in 0.0:0.01:1.0]\nps = collect(0.0:0.01:1.0);\nκs = [-0.99, -0.66, -0.33, 0, 0.33, 0.66, 0.99];\nHs = [[entropy(Kaniadakis(κ = κ), p) for p in probs] for κ in κs];\n\nfig = Figure()\nax = Axis(fig[1, 1], xlabel = \"p\", ylabel = \"H(p)\")\n\nfor (i, H) in enumerate(Hs)\n    lines!(ax, ps, H, label = \"$(κs[i])\")\nend\n\naxislegend()\n\nfig","category":"page"},{"location":"examples/#Stretched-exponential-entropy","page":"ComplexityMeasures.jl Examples","title":"Stretched exponential entropy","text":"","category":"section"},{"location":"examples/","page":"ComplexityMeasures.jl Examples","title":"ComplexityMeasures.jl Examples","text":"Here, we reproduce the example from Anteneodo & Plastino (1999)[Anteneodo1999], showing how the stretched exponential entropy changes as function of the parameter η for a range of two-element probability distributions given by Probabilities([p, 1 - p] for p in 1:0.0:0.01:1.0).","category":"page"},{"location":"examples/","page":"ComplexityMeasures.jl Examples","title":"ComplexityMeasures.jl Examples","text":"using ComplexityMeasures, SpecialFunctions, CairoMakie\nηs = [0.01, 0.2, 0.3, 0.5, 0.7, 1.0, 1.5, 3.0]\nps = [Probabilities([p, 1 - p]) for p = 0.0:0.01:1.0]\n\nhs_norm = [[entropy(StretchedExponential( η = η), p) / gamma((η + 1)/η) for p in ps] for η in ηs]\nfig = Figure()\nax = Axis(fig[1,1]; xlabel = \"p\", ylabel = \"H(p)\")\npp = [p[1] for p in ps]\n\nfor (i, η) in enumerate(ηs)\n    lines!(ax, pp, hs_norm[i], label = \"η=$η\")\nend\naxislegend(ax)\nfig","category":"page"},{"location":"examples/","page":"ComplexityMeasures.jl Examples","title":"ComplexityMeasures.jl Examples","text":"[Anteneodo1999]: Anteneodo, C., & Plastino, A. R. (1999). Maximum entropy approach to stretched exponential probability distributions. Journal of Physics A: Mathematical and General, 32(7), 1089.","category":"page"},{"location":"examples/#dispersion_example","page":"ComplexityMeasures.jl Examples","title":"Discrete entropy: dispersion entropy","text":"","category":"section"},{"location":"examples/","page":"ComplexityMeasures.jl Examples","title":"ComplexityMeasures.jl Examples","text":"Here we compute dispersion entropy (Rostaghi et al. 2016)[Rostaghi2016], using the use the Dispersion probabilities estimator, for a time series consisting of normally distributed noise with a single spike in the middle of the signal. We compute the entropies over a range subsets of the data, using a sliding window consisting of 70 data points, stepping the window 10 time steps at a time. This example is adapted from Li et al. (2021)[Li2019].","category":"page"},{"location":"examples/","page":"ComplexityMeasures.jl Examples","title":"ComplexityMeasures.jl Examples","text":"using ComplexityMeasures\nusing Random\nusing CairoMakie\nusing Distributions: Normal\n\nn = 1000\nts = 1:n\nx = [i == n ÷ 2 ? 50.0 : 0.0 for i in ts]\nrng = Random.default_rng()\ns = rand(rng, Normal(0, 1), n)\ny = x .+ s\n\nws = 70\nwindows = [t:t+ws for t in 1:10:n-ws]\nrdes = zeros(length(windows))\ndes = zeros(length(windows))\npes = zeros(length(windows))\n\nm, c = 2, 6\nest_de = Dispersion(c = c, m = m, τ = 1)\nfor (i, window) in enumerate(windows)\n    des[i] = entropy_normalized(Renyi(), est_de, y[window])\nend\n\nfig = Figure()\na1 = Axis(fig[1,1]; xlabel = \"Time step\", ylabel = \"Value\")\nlines!(a1, ts, y)\ndisplay(fig)\na2 = Axis(fig[2, 1]; xlabel = \"Time step\", ylabel = \"Value\")\np_de = scatterlines!([first(w) for w in windows], des,\n    label = \"Dispersion entropy\",\n    color = :red,\n    markercolor = :red, marker = '●', markersize = 20)\n\naxislegend(position = :rc)\nylims!(0, max(maximum(pes), 1))\nfig","category":"page"},{"location":"examples/","page":"ComplexityMeasures.jl Examples","title":"ComplexityMeasures.jl Examples","text":"[Rostaghi2016]: Rostaghi, M., & Azami, H. (2016). Dispersion entropy: A measure for time-series analysis. IEEE Signal Processing Letters, 23(5), 610-614.","category":"page"},{"location":"examples/","page":"ComplexityMeasures.jl Examples","title":"ComplexityMeasures.jl Examples","text":"[Li2019]: Li, Y., Gao, X., & Wang, L. (2019). Reverse dispersion entropy: a new complexity measure for sensor signal. Sensors, 19(23), 5203.","category":"page"},{"location":"examples/#Discrete-entropy:-normalized-entropy-for-comparing-different-signals","page":"ComplexityMeasures.jl Examples","title":"Discrete entropy: normalized entropy for comparing different signals","text":"","category":"section"},{"location":"examples/","page":"ComplexityMeasures.jl Examples","title":"ComplexityMeasures.jl Examples","text":"When comparing different signals or signals that have different length, it is best to normalize entropies so that the \"complexity\" or \"disorder\" quantification is directly comparable between signals. Here is an example based on the wavelet entropy example where we use the spectral entropy instead of the wavelet entropy:","category":"page"},{"location":"examples/","page":"ComplexityMeasures.jl Examples","title":"ComplexityMeasures.jl Examples","text":"using ComplexityMeasures\nN1, N2, a = 101, 10001, 10\n\nfor N in (N1, N2)\n    local t = LinRange(0, 2*a*π, N)\n    local x = sin.(t) # periodic\n    local y = sin.(t .+ cos.(t/0.5)) # periodic, complex spectrum\n    local z = sin.(rand(1:15, N) ./ rand(1:10, N)) # random\n\n    for q in (x, y, z)\n        h = entropy(PowerSpectrum(), q)\n        n = entropy_normalized(PowerSpectrum(), q)\n        println(\"entropy: $(h), normalized: $(n).\")\n    end\nend","category":"page"},{"location":"examples/","page":"ComplexityMeasures.jl Examples","title":"ComplexityMeasures.jl Examples","text":"You see that while the direct entropy values of noisy signal changes strongly with N but they are almost the same for the normalized version. For the regular signals, the entropy decreases nevertheless because the noise contribution of the Fourier computation becomes less significant.","category":"page"},{"location":"examples/#Spatiotemporal-permutation-entropy","page":"ComplexityMeasures.jl Examples","title":"Spatiotemporal permutation entropy","text":"","category":"section"},{"location":"examples/","page":"ComplexityMeasures.jl Examples","title":"ComplexityMeasures.jl Examples","text":"Usage of a SpatialSymbolicPermutation estimator is straightforward. Here we get the spatial permutation entropy of a 2D array (e.g., an image):","category":"page"},{"location":"examples/","page":"ComplexityMeasures.jl Examples","title":"ComplexityMeasures.jl Examples","text":"using ComplexityMeasures\nx = rand(50, 50) # some image\nstencil = [1 1; 0 1] # or one of the other ways of specifying stencils\nest = SpatialSymbolicPermutation(stencil, x)\nh = entropy(est, x)","category":"page"},{"location":"examples/","page":"ComplexityMeasures.jl Examples","title":"ComplexityMeasures.jl Examples","text":"To apply this to timeseries of spatial data, simply loop over the call, e.g.:","category":"page"},{"location":"examples/","page":"ComplexityMeasures.jl Examples","title":"ComplexityMeasures.jl Examples","text":"data = [rand(50, 50) for i in 1:10] # e.g., evolution of a 2D field of a PDE\nest = SpatialSymbolicPermutation(stencil, first(data))\nh_vs_t = map(d -> entropy(est, d), data)","category":"page"},{"location":"examples/","page":"ComplexityMeasures.jl Examples","title":"ComplexityMeasures.jl Examples","text":"Computing any other generalized spatiotemporal permutation entropy is trivial, e.g. with Renyi:","category":"page"},{"location":"examples/","page":"ComplexityMeasures.jl Examples","title":"ComplexityMeasures.jl Examples","text":"x = reshape(repeat(1:5, 500) .+ 0.1*rand(500*5), 50, 50)\nest = SpatialSymbolicPermutation(stencil, x)\nentropy(Renyi(q = 2), est, x)","category":"page"},{"location":"examples/#Spatial-discrete-entropy:-Fabio","page":"ComplexityMeasures.jl Examples","title":"Spatial discrete entropy: Fabio","text":"","category":"section"},{"location":"examples/","page":"ComplexityMeasures.jl Examples","title":"ComplexityMeasures.jl Examples","text":"Let's see how the normalized permutation and dispersion entropies increase for an image that gets progressively more noise added to it.","category":"page"},{"location":"examples/","page":"ComplexityMeasures.jl Examples","title":"ComplexityMeasures.jl Examples","text":"using ComplexityMeasures\nusing Distributions: Uniform\nusing CairoMakie\nusing Statistics\nusing TestImages, ImageTransformations, CoordinateTransformations, Rotations\n\nimg = testimage(\"fabio_grey_256\")\nrot = warp(img, recenter(RotMatrix(-3pi/2), center(img));)\noriginal = Float32.(rot)\nnoise_levels = collect(0.0:0.25:1.0) .* std(original) * 5 # % of 1 standard deviation\n\nnoisy_imgs = [i == 1 ? original : original .+ rand(Uniform(0, nL), size(original))\n    for (i, nL) in enumerate(noise_levels)]\n\n# a 2x2 stencil (i.e. dispersion/permutation patterns of length 4)\nstencil = ((2, 2), (1, 1))\n\nest_disp = SpatialDispersion(stencil, original; c = 5, periodic = false)\nest_perm = SpatialSymbolicPermutation(stencil, original; periodic = false)\nhs_disp = [entropy_normalized(est_disp, img) for img in noisy_imgs]\nhs_perm = [entropy_normalized(est_perm, img) for img in noisy_imgs]\n\n# Plot the results\nfig = Figure(size = (800, 1000))\nax = Axis(fig[1, 1:length(noise_levels)],\n    xlabel = \"Noise level\",\n    ylabel = \"Normalized entropy\")\nscatterlines!(ax, noise_levels, hs_disp, label = \"Dispersion\")\nscatterlines!(ax, noise_levels, hs_perm, label = \"Permutation\")\nylims!(ax, 0, 1.05)\naxislegend(position = :rb)\nfor (i, nl) in enumerate(noise_levels)\n    ax_i = Axis(fig[2, i])\n    image!(ax_i, Float32.(noisy_imgs[i]), label = \"$nl\")\n    hidedecorations!(ax_i)  # hides ticks, grid and lables\n    hidespines!(ax_i)  # hide the frame\nend\nfig","category":"page"},{"location":"examples/","page":"ComplexityMeasures.jl Examples","title":"ComplexityMeasures.jl Examples","text":"While the normalized SpatialSymbolicPermutation entropy quickly approaches its maximum value, the normalized SpatialDispersion entropy much better resolves the increase in entropy as the image gets noiser. This can probably be explained by the fact that the number of possible states (or total_outcomes) for any given stencil is larger for SpatialDispersion than for SpatialSymbolicPermutation, so the dispersion approach is much less sensitive to noise addition (i.e. noise saturation over the possible states is slower for SpatialDispersion).","category":"page"},{"location":"examples/#Complexity:-reverse-dispersion-entropy","page":"ComplexityMeasures.jl Examples","title":"Complexity: reverse dispersion entropy","text":"","category":"section"},{"location":"examples/","page":"ComplexityMeasures.jl Examples","title":"ComplexityMeasures.jl Examples","text":"Here, we compare regular dispersion entropy (Rostaghi et al., 2016)[Rostaghi2016], and reverse dispersion entropy Li et al. (2021)[Li2019] for a time series consisting of normally distributed noise with a single spike in the middle of the signal. We compute the entropies over a range subsets of the data, using a sliding window consisting of 70 data points, stepping the window 10 time steps at a time. This example reproduces parts of figure 3 in Li et al. (2021), but results here are not exactly the same as in the original paper, because their examples are based on randomly generated numbers and do not provide code that specify random number seeds.","category":"page"},{"location":"examples/","page":"ComplexityMeasures.jl Examples","title":"ComplexityMeasures.jl Examples","text":"using ComplexityMeasures\nusing Random\nusing CairoMakie\nusing Distributions: Normal\n\nn = 1000\nts = 1:n\nx = [i == n ÷ 2 ? 50.0 : 0.0 for i in ts]\nrng = Random.default_rng()\ns = rand(rng, Normal(0, 1), n)\ny = x .+ s\n\nws = 70\nwindows = [t:t+ws for t in 1:10:n-ws]\nrdes = zeros(length(windows))\ndes = zeros(length(windows))\npes = zeros(length(windows))\n\nm, c = 2, 6\nest_rd = ReverseDispersion(; c, m, τ = 1)\nest_de = Dispersion(; c, m, τ = 1)\n\nfor (i, window) in enumerate(windows)\n    rdes[i] = complexity_normalized(est_rd, y[window])\n    des[i] = entropy_normalized(Renyi(), est_de, y[window])\nend\n\nfig = Figure()\n\na1 = Axis(fig[1,1]; xlabel = \"Time step\", ylabel = \"Value\")\nlines!(a1, ts, y)\ndisplay(fig)\n\na2 = Axis(fig[2, 1]; xlabel = \"Time step\", ylabel = \"Value\")\np_rde = scatterlines!([first(w) for w in windows], rdes,\n    label = \"Reverse dispersion entropy\",\n    color = :black,\n    markercolor = :black, marker = '●')\np_de = scatterlines!([first(w) for w in windows], des,\n    label = \"Dispersion entropy\",\n    color = :red,\n    markercolor = :red, marker = 'x', markersize = 20)\n\naxislegend(position = :rc)\nylims!(0, max(maximum(pes), 1))\nfig","category":"page"},{"location":"examples/","page":"ComplexityMeasures.jl Examples","title":"ComplexityMeasures.jl Examples","text":"[Rostaghi2016]: Rostaghi, M., & Azami, H. (2016). Dispersion entropy: A measure for time-series analysis. IEEE Signal Processing Letters, 23(5), 610-614.","category":"page"},{"location":"examples/","page":"ComplexityMeasures.jl Examples","title":"ComplexityMeasures.jl Examples","text":"[Li2019]: Li, Y., Gao, X., & Wang, L. (2019). Reverse dispersion entropy: a new complexity measure for sensor signal. Sensors, 19(23), 5203.","category":"page"},{"location":"examples/#Complexity:-missing-dispersion-patterns","page":"ComplexityMeasures.jl Examples","title":"Complexity: missing dispersion patterns","text":"","category":"section"},{"location":"examples/","page":"ComplexityMeasures.jl Examples","title":"ComplexityMeasures.jl Examples","text":"using ComplexityMeasures\nusing CairoMakie\nusing DynamicalSystemsBase\nusing TimeseriesSurrogates\n\nest = MissingDispersionPatterns(Dispersion(m = 3, c = 7))\nlogistic_rule(x, p, n) = @inbounds SVector(p[1]*x[1]*(1-x[1]))\nsys = DeterministicIteratedMap(logistic_rule, [0.6], [4.0])\nLs = collect(100:100:1000)\nnL = length(Ls)\nnreps = 30 # should be higher for real applications\nmethod = WLS(IAAFT(), rescale = true)\n\nr_det, r_noise = zeros(length(Ls)), zeros(length(Ls))\nr_det_surr, r_noise_surr = [zeros(nreps) for L in Ls], [zeros(nreps) for L in Ls]\ny = rand(maximum(Ls))\n\nfor (i, L) in enumerate(Ls)\n    # Deterministic time series\n    x, t = trajectory(sys, L - 1, Ttr = 5000)\n    x = columns(x)[1] # remember to make it `Vector{<:Real}\n    sx = surrogenerator(x, method)\n    r_det[i] = complexity_normalized(est, x)\n    r_det_surr[i][:] = [complexity_normalized(est, sx()) for j = 1:nreps]\n\n    # Random time series\n    r_noise[i] = complexity_normalized(est, y[1:L])\n    sy = surrogenerator(y[1:L], method)\n    r_noise_surr[i][:] = [complexity_normalized(est, sy()) for j = 1:nreps]\nend\n\nfig = Figure()\nax = Axis(fig[1, 1],\n    xlabel = \"Time series length (L)\",\n    ylabel = \"# missing dispersion patterns (normalized)\"\n)\n\nlines!(ax, Ls, r_det, label = \"logistic(x0 = 0.6; r = 4.0)\", color = :black)\nlines!(ax, Ls, r_noise, label = \"Uniform noise\", color = :red)\nfor i = 1:nL\n    if i == 1\n        boxplot!(ax, fill(Ls[i], nL), r_det_surr[i]; width = 50, color = :black,\n            label = \"WIAAFT surrogates (logistic)\")\n         boxplot!(ax, fill(Ls[i], nL), r_noise_surr[i]; width = 50, color = :red,\n            label = \"WIAAFT surrogates (noise)\")\n    else\n        boxplot!(ax, fill(Ls[i], nL), r_det_surr[i]; width = 50, color = :black)\n        boxplot!(ax, fill(Ls[i], nL), r_noise_surr[i]; width = 50, color = :red)\n    end\nend\naxislegend(position = :rc)\nylims!(0, 1.1)\n\nfig","category":"page"},{"location":"examples/","page":"ComplexityMeasures.jl Examples","title":"ComplexityMeasures.jl Examples","text":"We don't need to actually to compute the quantiles here to see that for the logistic map, across all time series lengths, the N_MDP values are above the extremal values of the N_MDP values for the surrogate ensembles. Thus, we conclude that the logistic map time series has nonlinearity (well, of course).","category":"page"},{"location":"examples/","page":"ComplexityMeasures.jl Examples","title":"ComplexityMeasures.jl Examples","text":"For the univariate noise time series, there is considerable overlap between N_MDP for the surrogate distributions and the original signal, so we can't claim nonlinearity for this signal.","category":"page"},{"location":"examples/","page":"ComplexityMeasures.jl Examples","title":"ComplexityMeasures.jl Examples","text":"Of course, to robustly reject the null hypothesis, we'd need to generate a sufficient number of surrogate realizations, and actually compute quantiles to compare with.","category":"page"},{"location":"examples/","page":"ComplexityMeasures.jl Examples","title":"ComplexityMeasures.jl Examples","text":"[Zhou2022]: Zhou, Q., Shang, P., & Zhang, B. (2022). Using missing dispersion patterns to detect determinism and nonlinearity in time series data. Nonlinear Dynamics, 1-20.","category":"page"},{"location":"examples/#Complexity:-approximate-entropy","page":"ComplexityMeasures.jl Examples","title":"Complexity: approximate entropy","text":"","category":"section"},{"location":"examples/","page":"ComplexityMeasures.jl Examples","title":"ComplexityMeasures.jl Examples","text":"Here, we reproduce the Henon map example with R=08 from Pincus (1991), comparing our values with relevant values from table 1 in Pincus (1991).","category":"page"},{"location":"examples/","page":"ComplexityMeasures.jl Examples","title":"ComplexityMeasures.jl Examples","text":"We use DiscreteDynamicalSystem from DynamicalSystemsBase to represent the map, and use the trajectory function from the same package to iterate the map for different initial conditions, for multiple time series lengths.","category":"page"},{"location":"examples/","page":"ComplexityMeasures.jl Examples","title":"ComplexityMeasures.jl Examples","text":"Finally, we summarize our results in box plots and compare the values to those obtained by Pincus (1991).","category":"page"},{"location":"examples/","page":"ComplexityMeasures.jl Examples","title":"ComplexityMeasures.jl Examples","text":"using ComplexityMeasures\nusing DynamicalSystemsBase\nusing DelayEmbeddings\nusing CairoMakie\n\n# Equation 13 in Pincus (1991)\nfunction henon_rule(u, p, n)\n    R = p[1]\n    x, y = u\n    dx = R*y + 1 - 1.4*x^2\n    dy = 0.3*R*x\n    return SVector(dx, dy)\nend\n\nfunction henon(; u₀ = rand(2), R = 0.8)\n    DeterministicIteratedMap(henon_rule, u₀, [R])\nend\n\nts_lengths = [300, 1000, 2000, 3000]\nnreps = 100\napens_08 = [zeros(nreps) for i = 1:length(ts_lengths)]\n\n# For some initial conditions, the Henon map as specified here blows up,\n# so we need to check for infinite values.\ncontainsinf(x) = any(isinf.(x))\n\nc = ApproximateEntropy(r = 0.05, m = 2)\n\nfor (i, L) in enumerate(ts_lengths)\n    k = 1\n    while k <= nreps\n        sys = henon(u₀ = rand(2), R = 0.8)\n        t = trajectory(sys, L; Ttr = 5000)[1]\n\n        if !any([containsinf(tᵢ) for tᵢ in t])\n            x, y = columns(t)\n            apens_08[i][k] = complexity(c, x)\n            k += 1\n        end\n    end\nend\n\nfig = Figure()\n\n# Example time series\na1 = Axis(fig[1,1]; xlabel = \"Time (t)\", ylabel = \"Value\")\nsys = henon(u₀ = [0.5, 0.1], R = 0.8)\nx, y = columns(trajectory(sys, 100, Ttr = 500))\nlines!(a1, 1:length(x), x, label = \"x\")\nlines!(a1, 1:length(y), y, label = \"y\")\n\n# Approximate entropy values, compared to those of the original paper (black dots).\na2 = Axis(fig[2, 1];\n    xlabel = \"Time series length (L)\",\n    ylabel = \"ApEn(m = 2, r = 0.05)\")\n\n# hacky boxplot, but this seems to be how it's done in Makie at the moment\nn = length(ts_lengths)\nfor i = 1:n\n    boxplot!(a2, fill(ts_lengths[i], n), apens_08[i];\n        width = 200)\nend\n\nscatter!(a2, ts_lengths, [0.337, 0.385, NaN, 0.394];\n    label = \"Pincus (1991)\", color = :black)\nfig","category":"page"},{"location":"examples/#Complexity:-sample-entropy","page":"ComplexityMeasures.jl Examples","title":"Complexity: sample entropy","text":"","category":"section"},{"location":"examples/","page":"ComplexityMeasures.jl Examples","title":"ComplexityMeasures.jl Examples","text":"Completely regular signals should have sample entropy approaching zero, while less regular signals should have higher sample entropy.","category":"page"},{"location":"examples/","page":"ComplexityMeasures.jl Examples","title":"ComplexityMeasures.jl Examples","text":"using ComplexityMeasures\nusing CairoMakie\nN, a = 2000, 10\nt = LinRange(0, 2*a*π, N)\n\nx = repeat([-5:5 |> collect; 4:-1:-4 |> collect], N ÷ 20);\ny = sin.(t .+ cos.(t/0.5));\nz = rand(N)\n\nh_x, h_y, h_z = map(t -> complexity(SampleEntropy(t), t), (x, y, z))\n\nfig = Figure()\nax = Axis(fig[1,1]; ylabel = \"x\")\nlines!(ax, t, x; color = Cycled(1), label = \"h=$(h=round(h_x, sigdigits = 5))\");\nay = Axis(fig[2,1]; ylabel = \"y\")\nlines!(ay, t, y; color = Cycled(2), label = \"h=$(h=round(h_y, sigdigits = 5))\");\naz = Axis(fig[3,1]; ylabel = \"z\", xlabel = \"time\")\nlines!(az, t, z; color = Cycled(3), label = \"h=$(h=round(h_z, sigdigits = 5))\");\nfor a in (ax, ay, az); axislegend(a); end\nfor a in (ax, ay); hidexdecorations!(a; grid=false); end\nfig","category":"page"},{"location":"examples/","page":"ComplexityMeasures.jl Examples","title":"ComplexityMeasures.jl Examples","text":"Next, we compare the sample entropy obtained for different values of the radius r for uniform noise, normally distributed noise, and a periodic signal.","category":"page"},{"location":"examples/","page":"ComplexityMeasures.jl Examples","title":"ComplexityMeasures.jl Examples","text":"using ComplexityMeasures\nusing CairoMakie\nusing Statistics\nusing Distributions: Normal\nN = 2000\nx_U = rand(N)\nx_N = rand(Normal(0, 3), N)\nx_periodic = repeat(rand(20), N ÷ 20)\n\nx_U .= (x_U .- mean(x_U)) ./ std(x_U)\nx_N .= (x_N .- mean(x_N)) ./ std(x_N)\nx_periodic .= (x_periodic .- mean(x_periodic)) ./ std(x_periodic)\n\nrs = 10 .^ range(-1, 0, length = 30)\nbase = 2\nm = 2\nhs_U = [complexity_normalized(SampleEntropy(m = m, r = r), x_U) for r in rs]\nhs_N = [complexity_normalized(SampleEntropy(m = m, r = r), x_N) for r in rs]\nhs_periodic = [complexity_normalized(SampleEntropy(m = m, r = r), x_periodic) for r in rs]\n\nfig = Figure()\n# Time series\na1 = Axis(fig[1,1]; xlabel = \"r\", ylabel = \"Sample entropy\")\nlines!(a1, rs, hs_U, label = \"Uniform noise, U(0, 1)\")\nlines!(a1, rs, hs_N, label = \"Gaussian noise, N(0, 1)\")\nlines!(a1, rs, hs_periodic, label = \"Periodic signal\")\naxislegend()\nfig","category":"page"},{"location":"examples/#Statistical-complexity-of-iterated-maps","page":"ComplexityMeasures.jl Examples","title":"Statistical complexity of iterated maps","text":"","category":"section"},{"location":"examples/","page":"ComplexityMeasures.jl Examples","title":"ComplexityMeasures.jl Examples","text":"In this example, we reproduce parts of Fig. 1 in Rosso et al. (2007): We compute the statistical complexity of the Henon, logistic and Schuster map, as well as that of k-noise.","category":"page"},{"location":"examples/","page":"ComplexityMeasures.jl Examples","title":"ComplexityMeasures.jl Examples","text":"using ComplexityMeasures\nusing Distances\nusing DynamicalSystemsBase\nusing CairoMakie\nusing FFTW\nusing Statistics\n\nN = 2^15\n\nfunction logistic(x0=0.4; r = 4.0)\n    return DeterministicIteratedMap(logistic_rule, SVector(x0), [r])\nend\nlogistic_rule(x, p, n) = @inbounds SVector(p[1]*x[1]*(1 - x[1]))\nlogistic_jacob(x, p, n) = @inbounds SMatrix{1,1}(p[1]*(1 - 2x[1]))\n\nfunction henon(u0=zeros(2); a = 1.4, b = 0.3)\n    return DeterministicIteratedMap(henon_rule, u0, [a,b])\nend\nhenon_rule(x, p, n) = SVector{2}(1.0 - p[1]*x[1]^2 + x[2], p[2]*x[1])\nhenon_jacob(x, p, n) = SMatrix{2,2}(-2*p[1]*x[1], p[2], 1.0, 0.0)\n\nfunction schuster(x0=0.5, z=3.0/2)\n    return DeterministicIteratedMap(schuster_rule, SVector(x0), [z])\nend\nschuster_rule(x, p, n) = @inbounds SVector((x[1]+x[1]^p[1]) % 1)\n\n# generate noise with power spectrum that falls like 1/f^k\nfunction k_noise(k=3)\n    function f(N)\n        x = rand(Float64, N)\n        # generate power spectrum of random numbers and multiply by f^(-k/2)\n        x_hat = fft(x) .* abs.(vec(fftfreq(length(x)))) .^ (-k/2)\n        # set to zero for frequency zero\n        x_hat[1] = 0\n        return real.(ifft(x_hat))\n    end\n    return f\nend\n\nfig = Figure()\nax = Axis(fig[1, 1]; xlabel=L\"H_S\", ylabel=L\"C_{JS}\")\n\nm, τ = 6, 1\nm_kwargs = (\n        (color=:transparent,\n        strokecolor=:red,\n        marker=:utriangle,\n        strokewidth=2),\n        (color=:transparent,\n        strokecolor=:blue,\n        marker=:rect,\n        strokewidth=2),\n        (color=:magenta,\n        marker=:circle),\n        (color=:blue,\n        marker=:rect)\n    )\n\nn = 100\n\nc = StatisticalComplexity(\n    dist=JSDivergence(),\n    est=SymbolicPermutation(; m, τ),\n    entr=Renyi()\n)\nfor (j, (ds_gen, sym, ds_name)) in enumerate(zip(\n        (logistic, henon, schuster, k_noise),\n        (:utriangle, :rect, :dtriangle, :diamond),\n        (\"Logistic map\", \"Henon map\", \"Schuster map\", \"k-noise (k=3)\"),\n    ))\n\n    if j < 4\n        dim = dimension(ds_gen())\n        hs, cs = zeros(n), zeros(n)\n        for k in 1:n\n            ic = rand(dim) * 0.3\n            ds = ds_gen(SVector{dim}(ic))\n            x, t = trajectory(ds, N, Ttr=100)\n            hs[k], cs[k] = entropy_complexity(c, x[:, 1])\n        end\n        scatter!(ax, mean(hs), mean(cs); label=\"$ds_name\", markersize=25, m_kwargs[j]...)\n    else\n        ds = ds_gen()\n        hs, cs = zeros(n), zeros(n)\n        for k in 1:n\n            x = ds(N)\n            hs[k], cs[k] = entropy_complexity(c, x[:, 1])\n        end\n        scatter!(ax, mean(hs), mean(cs); label=\"$ds_name\", markersize=25, m_kwargs[j]...)\n    end\nend\n\nmin_curve, max_curve = entropy_complexity_curves(c)\nlines!(ax, min_curve; color=:black)\nlines!(ax, max_curve; color=:black)\naxislegend(; position=:lt)\nfig","category":"page"},{"location":"multiscale/#Multiscale","page":"Multiscale","title":"Multiscale","text":"","category":"section"},{"location":"multiscale/#Multiscale-API","page":"Multiscale","title":"Multiscale API","text":"","category":"section"},{"location":"multiscale/","page":"Multiscale","title":"Multiscale","text":"The multiscale API is defined by the functions","category":"page"},{"location":"multiscale/","page":"Multiscale","title":"Multiscale","text":"multiscale\nmultiscale_normalized\ndownsample","category":"page"},{"location":"multiscale/","page":"Multiscale","title":"Multiscale","text":"which dispatch any of the MultiScaleAlgorithms listed below.","category":"page"},{"location":"multiscale/","page":"Multiscale","title":"Multiscale","text":"MultiScaleAlgorithm\nRegular\nComposite\ndownsample\nmultiscale\nmultiscale_normalized","category":"page"},{"location":"multiscale/#ComplexityMeasures.Regular","page":"Multiscale","title":"ComplexityMeasures.Regular","text":"Regular <: MultiScaleAlgorithm\nRegular(; f::Function = Statistics.mean)\n\nThe original multi-scale algorithm for multiscale entropy analysis (Costa et al., 2022)[Costa2002], which yields a single downsampled time series per scale s.\n\nDescription\n\nGiven a scalar-valued input time series x, the Regular multiscale algorithm downsamples and coarse-grains x by splitting it into non-overlapping windows of length s, and then constructing a new downsampled time series D_t(s f) by applying the function f to each of the resulting length-s windows.\n\nThe downsampled time series D_t(s) with t ∈ [1, 2, …, L], where L = floor(N / s), is given by:\n\n D_t(s f)  _t = 1^L = left f left( bf x_t right) right_t = 1^L =\nleft\n    fleft( (x_i)_i = (t - 1)s + 1^ts right)\nright_t = 1^L\n\nwhere f is some summary statistic applied to the length-ts-((t - 1)s + 1) tuples xₖ. Different choices of f have yield different multiscale methods appearing in the literature. For example:\n\nf == Statistics.mean yields the original first-moment multiscale sample entropy (Costa   et al., 2002)[Costa2002].\nf == Statistics.var yields the generalized multiscale sample entropy (Costa &   Goldberger, 2015)[Costa2015], which uses the second-moment (variance) instead of the   mean.\n\nSee also: Composite.\n\n[Costa2002]: Costa, M., Goldberger, A. L., & Peng, C. K. (2002). Multiscale entropy analysis of complex physiologic time series. Physical review letters, 89(6), 068102.\n\n[Costa2015]: Costa, M. D., & Goldberger, A. L. (2015). Generalized multiscale entropy analysis: Application to quantifying the complex volatility of human heartbeat time series. EntropyDefinition, 17(3), 1197-1203.\n\n\n\n\n\n","category":"type"},{"location":"multiscale/#ComplexityMeasures.Composite","page":"Multiscale","title":"ComplexityMeasures.Composite","text":"Composite <: MultiScaleAlgorithm\nComposite(; f::Function = Statistics.mean)\n\nComposite multi-scale algorithm for multiscale entropy analysis (Wu et al., 2013)[Wu2013], used, with multiscale to compute, for example, composite multiscale entropy (CMSE).\n\nDescription\n\nGiven a scalar-valued input time series x, the composite multiscale algorithm, like Regular, downsamples and coarse-grains x by splitting it into non-overlapping windows of length s, and then constructing downsampled time series by applying the function f to each of the resulting length-s windows.\n\nHowever, Wu et al. (2013)[Wu2013] realized that for each scale s, there are actually s different ways of selecting windows, depending on where indexing starts/ends. These s different downsampled time series D_t(s, f) at each scale s are constructed as follows:\n\n D_k(s)  =  D_t k(s) _t = 1^L =  f left( bf x_t k right)  =\nleft\n    fleft( (x_i)_i = (t - 1)s + k^ts + k - 1 right)\nright_t = 1^L\n\nwhere L = floor((N - s + 1) / s) and 1 ≤ k ≤ s, such that D_i k(s) is the i-th element of the k-th downsampled time series at scale s.\n\nFinally, compute dfrac1s sum_k = 1^s g(D_k(s)), where g is some summary function, for example entropy or complexity.\n\nnote: Relation to Regular\nThe downsampled time series D_t 1(s) constructed using the composite multiscale method is equivalent to the downsampled time series D_t(s) constructed using the Regular method, for which k == 1 is fixed, such that only a single time series is returned.\n\nSee also: Regular.\n\n[Wu2013]: Wu, S. D., Wu, C. W., Lin, S. G., Wang, C. C., & Lee, K. Y. (2013). Time series analysis using composite multiscale entropy. Entropy, 15(3), 1069-1084.\n\n\n\n\n\n","category":"type"},{"location":"multiscale/#Available-literature-methods","page":"Multiscale","title":"Available literature methods","text":"","category":"section"},{"location":"multiscale/","page":"Multiscale","title":"Multiscale","text":"A non-exhaustive list of literature methods, and the syntax to compute them, are listed below. Please open an issue or make a pull-request to ComplexityMeasures.jl if you find a literature method missing from this list, or if you publish a paper based on some new multiscale combination.","category":"page"},{"location":"multiscale/","page":"Multiscale","title":"Multiscale","text":"Method Syntax Reference\nRefined composite multiscale dispersion entropy multiscale(Composite(), Dispersion(), est, x, normalized = true) Azami et al. (2017)[Azami2017]\nMultiscale sample entropy (first moment) multiscale(Regular(f = mean), SampleEntropy(), x) Costa et al. (2002)[Costa2002]\nGeneralized multiscale sample entropy (second moment) multiscale(Regular(f = std), SampleEntropy(),  x) Costa et al. (2015)[Costa2015]","category":"page"},{"location":"multiscale/","page":"Multiscale","title":"Multiscale","text":"[Azami2017]: Azami, H., Rostaghi, M., Abásolo, D., & Escudero, J. (2017). Refined composite multiscale dispersion entropy and its application to biomedical signals. IEEE Transactions on Biomedical Engineering, 64(12), 2872-2879.","category":"page"},{"location":"multiscale/","page":"Multiscale","title":"Multiscale","text":"[Costa2002]: Costa, M., Goldberger, A. L., & Peng, C. K. (2002). Multiscale entropy analysis of complex physiologic time series. Physical review letters, 89(6), 068102.","category":"page"},{"location":"multiscale/","page":"Multiscale","title":"Multiscale","text":"[Costa2015]: Costa, M. D., & Goldberger, A. L. (2015). Generalized multiscale entropy analysis: Application to quantifying the complex volatility of human heartbeat time series. EntropyDefinition, 17(3), 1197-1203.","category":"page"},{"location":"encodings/#Encodings","page":"Encodings","title":"Encodings","text":"","category":"section"},{"location":"encodings/#Encodings-API","page":"Encodings","title":"Encodings API","text":"","category":"section"},{"location":"encodings/","page":"Encodings","title":"Encodings","text":"Some probability estimators first \"encode\" input data into an intermediate representation indexed by the positive integers. This intermediate representation is called an \"encoding\".","category":"page"},{"location":"encodings/","page":"Encodings","title":"Encodings","text":"The encodings API is defined by:","category":"page"},{"location":"encodings/","page":"Encodings","title":"Encodings","text":"Encoding\nencode\ndecode","category":"page"},{"location":"encodings/","page":"Encodings","title":"Encodings","text":"Encoding\nencode\ndecode","category":"page"},{"location":"encodings/#ComplexityMeasures.Encoding","page":"Encodings","title":"ComplexityMeasures.Encoding","text":"Encoding\n\nThe supertype for all encoding schemes. Encodings always encode elements of input data into the positive integers. The encoding API is defined by the functions encode and decode. Some probability estimators utilize encodings internally.\n\nCurrent available encodings are:\n\nOrdinalPatternEncoding.\nGaussianCDFEncoding.\nRectangularBinEncoding.\n\n\n\n\n\n","category":"type"},{"location":"encodings/#ComplexityMeasures.encode","page":"Encodings","title":"ComplexityMeasures.encode","text":"encode(c::Encoding, χ) -> i::Int\n\nEncode an element χ ∈ x of input data x (those given to probabilities) using encoding c.\n\nThe special value of -1 is reserved as a return value for inappropriate elements χ that cannot be encoded according to c.\n\n\n\n\n\n","category":"function"},{"location":"encodings/#ComplexityMeasures.decode","page":"Encodings","title":"ComplexityMeasures.decode","text":"decode(c::Encoding, i::Int) -> ω\n\nDecode an encoded element i into the outcome ω ∈ Ω it corresponds to.\n\nΩ is the outcome_space of a probabilities estimator that uses encoding c.\n\n\n\n\n\n","category":"function"},{"location":"encodings/#Available-encodings","page":"Encodings","title":"Available encodings","text":"","category":"section"},{"location":"encodings/","page":"Encodings","title":"Encodings","text":"OrdinalPatternEncoding\nGaussianCDFEncoding\nRectangularBinEncoding","category":"page"},{"location":"encodings/#ComplexityMeasures.OrdinalPatternEncoding","page":"Encodings","title":"ComplexityMeasures.OrdinalPatternEncoding","text":"OrdinalPatternEncoding <: Encoding\nOrdinalPatternEncoding(m::Int, lt = ComplexityMeasures.isless_rand)\n\nAn encoding scheme that encodes length-m vectors into their permutation/ordinal patterns and then into the integers based on the Lehmer code. It is used by SymbolicPermutation and similar estimators, see that for a description of the outcome space.\n\nThe ordinal/permutation pattern of a vector χ is simply sortperm(χ), which gives the indices that would sort χ in ascending order.\n\nDescription\n\nThe Lehmer code, as implemented here, is a bijection between the set of factorial(m) possible permutations for a length-m sequence, and the integers 1, 2, …, factorial(m). The encoding step uses algorithm 1 in Berger et al. (2019)[Berger2019], which is highly optimized. The decoding step is much slower due to missing optimizations (pull requests welcomed!).\n\nExample\n\njulia> using ComplexityMeasures\n\njulia> χ = [4.0, 1.0, 9.0];\n\njulia> c = OrdinalPatternEncoding(3);\n\njulia> i = encode(c, χ)\n3\n\njulia> decode(c, i)\n3-element SVector{3, Int64} with indices SOneTo(3):\n 2\n 1\n 3\n\nIf you want to encode something that is already a permutation pattern, then you can use the non-exported permutation_to_integer function.\n\n[Berger2019]: Berger et al. \"Teaching Ordinal Patterns to a Computer: Efficient Encoding Algorithms Based on the Lehmer Code.\" Entropy 21.10 (2019): 1023.\n\n\n\n\n\n","category":"type"},{"location":"encodings/#ComplexityMeasures.GaussianCDFEncoding","page":"Encodings","title":"ComplexityMeasures.GaussianCDFEncoding","text":"GaussianCDFEncoding <: Encoding\nGaussianCDFEncoding(; μ, σ, c::Int = 3)\n\nAn encoding scheme that encodes a scalar value into one of the integers sᵢ ∈ [1, 2, …, c] based on the normal cumulative distribution function (NCDF), and decodes the sᵢ into subintervals of [0, 1] (with some loss of information).\n\nNotice that the decoding step does not yield an element of any outcome space of the estimators that use GaussianCDFEncoding internally, such as Dispersion. That is because these estimators additionally delay embed the encoded data.\n\nDescription\n\nGaussianCDFEncoding first maps an input point x  (scalar) to a new real number y_ in 0 1 by using the normal cumulative distribution function (CDF) with the given mean μ and standard deviation σ, according to the map\n\nx to y  y = dfrac1 sigma\n    sqrt2 pi int_-infty^x e^(-(x - mu)^2)(2 sigma^2) dx\n\nNext, the interval [0, 1] is equidistantly binned and enumerated 1 2 ldots c,  and y is linearly mapped to one of these integers using the linear map  y to z  z = textfloor(y(c-1)) + 1.\n\nBecause of the floor operation, some information is lost, so when used with decode, each decoded sᵢ is mapped to a subinterval of [0, 1].\n\nExamples\n\njulia> using ComplexityMeasures, Statistics\n\njulia> x = [0.1, 0.4, 0.7, -2.1, 8.0];\n\njulia> μ, σ = mean(x), std(x); encoding = GaussianCDFEncoding(; μ, σ, c = 5)\n\njulia> es = encode.(Ref(encoding), x)\n5-element Vector{Int64}:\n 2\n 2\n 3\n 1\n 5\n\njulia> decode(encoding, 3)\n2-element SVector{2, Float64} with indices SOneTo(2):\n 0.4\n 0.6\n\n\n\n\n\n","category":"type"},{"location":"encodings/#ComplexityMeasures.RectangularBinEncoding","page":"Encodings","title":"ComplexityMeasures.RectangularBinEncoding","text":"RectangularBinEncoding <: Encoding\nRectangularBinEncoding(binning::RectangularBinning, x)\nRectangularBinEncoding(binning::FixedRectangularBinning)\n\nAn encoding scheme that encodes points χ ∈ x into their histogram bins.\n\nThe first call signature simply initializes a FixedRectangularBinning and then calls the second call signature.\n\nSee FixedRectangularBinning for info on mapping points to bins.\n\n\n\n\n\n","category":"type"},{"location":"#ComplexityMeasures.jl","page":"ComplexityMeasures.jl","title":"ComplexityMeasures.jl","text":"","category":"section"},{"location":"","page":"ComplexityMeasures.jl","title":"ComplexityMeasures.jl","text":"ComplexityMeasures","category":"page"},{"location":"#ComplexityMeasures","page":"ComplexityMeasures.jl","title":"ComplexityMeasures","text":"ComplexityMeasures.jl\n\n(Image: Docs) (Image: CI) (Image: codecov) (Image: Package Downloads) (Image: Package Downloads) (Image: DOI)\n\nA Julia package that provides estimators for probabilities, entropies, and other complexity measures, in the context of nonlinear dynamics, nonlinear timeseries analysis, and complex systems. It can be used as a standalone package, or as part of other projects in the JuliaDynamics organization, such as DynamicalSystems.jl or CausalityTools.jl.\n\nTo install it, run import Pkg; Pkg.add(\"ComplexityMeasures\").\n\nAll further information is provided in the documentation, which you can either find online or build locally by running the docs/make.jl file.\n\nPreviously, this package was called Entropies.jl.\n\n\n\n\n\n","category":"module"},{"location":"#terminology","page":"ComplexityMeasures.jl","title":"Content and terminology","text":"","category":"section"},{"location":"","page":"ComplexityMeasures.jl","title":"ComplexityMeasures.jl","text":"note: Note\nThe documentation here follows (loosely) chapter 5 of Nonlinear Dynamics, Datseris & Parlitz, Springer 2022.","category":"page"},{"location":"","page":"ComplexityMeasures.jl","title":"ComplexityMeasures.jl","text":"Before exploring the features of ComplexityMeasures.jl, it is useful to read through this terminology section. Here, we briefly review important complexity-related concepts and names from the scientific literature, and outline how we've structured ComplexityMeasures.jl around these concepts.","category":"page"},{"location":"","page":"ComplexityMeasures.jl","title":"ComplexityMeasures.jl","text":"In these scientific literature, words like probabilities, entropies, and other complexity measures are used (and abused) in multiple contexts, and are often used interchangeably to describe similar concepts. The API and documentation of ComplexityMeasures.jl aim to clarify the meaning and usage of these words, and to provide simple ways to obtain probabilities, entropies, or other complexity measures from input data.","category":"page"},{"location":"","page":"ComplexityMeasures.jl","title":"ComplexityMeasures.jl","text":"For ComplexityMeasures.jl entropies are also complexity measures, while sometimes a distinction is made so that \"complexity measures\" means anything beyond entropy. However we believe the general nonlinear dynamics community agrees with our take, as most papers that introduce different entropy flavors, call them complexity measures. Example: \"Permutation Entropy: A Natural Complexity Measure for Time Series\" from Brandt and Pompe 2002.","category":"page"},{"location":"#Probabilities","page":"ComplexityMeasures.jl","title":"Probabilities","text":"","category":"section"},{"location":"","page":"ComplexityMeasures.jl","title":"ComplexityMeasures.jl","text":"Entropies and other complexity measures are typically computed based on probability distributions, which we simply refer to as \"probabilities\". Probabilities can be obtained from input data in a plethora of different ways. The central API function that returns a probability distribution is probabilities, which takes in a subtype of ProbabilitiesEstimator to specify how the probabilities are computed. All available estimators can be found in the estimators page.","category":"page"},{"location":"#Entropies","page":"ComplexityMeasures.jl","title":"Entropies","text":"","category":"section"},{"location":"","page":"ComplexityMeasures.jl","title":"ComplexityMeasures.jl","text":"Entropy is an established concept in statistics, information theory, and nonlinear dynamics. However, it is also an umbrella term that may mean several computationally, and sometimes even fundamentally, different quantities. In ComplexityMeasures.jl, we provide the generic function entropy that tries to both clarify disparate entropy concepts, while unifying them under a common interface that highlights the modular nature of the word \"entropy\".","category":"page"},{"location":"","page":"ComplexityMeasures.jl","title":"ComplexityMeasures.jl","text":"On the highest level, there are two main types of entropy.","category":"page"},{"location":"","page":"ComplexityMeasures.jl","title":"ComplexityMeasures.jl","text":"Discrete entropies are functions of probability mass functions. Computing a discrete entropy boils   down to two simple steps: first estimating a probability distribution, then plugging   the estimated probabilities into an estimator of a so-called \"generalized entropy\" definition.   Internally, this is literally just a few lines of code where we first apply some   ProbabilitiesEstimator to the input data, and feed the resulting   probabilities to entropy with some DiscreteEntropyEstimator.\nDifferential/continuous entropies are functions of   probability density functions,   which are integrals. Computing differential entropies therefore rely on estimating   some density functional. For this task, we provide DifferentialEntropyEstimators,   which compute entropies via alternate means, without explicitly computing some   probability distribution. For example, the Correa estimator computes the   Shannon differential entropy using order statistics.","category":"page"},{"location":"","page":"ComplexityMeasures.jl","title":"ComplexityMeasures.jl","text":"Crucially, many quantities in the nonlinear dynamics literature that are named as entropies, such as \"permutation entropy\" (entropy_permutation) and \"wavelet entropy\" (entropy_wavelet), are not really new entropies. They are the good old discrete Shannon entropy (Shannon), but calculated with new probabilities estimators.","category":"page"},{"location":"","page":"ComplexityMeasures.jl","title":"ComplexityMeasures.jl","text":"Even though the names of these methods (e.g. \"wavelet entropy\") sound like names for new entropies, what they actually do is to devise novel ways of calculating probabilities from data, and then plug those probabilities into formal discrete entropy formulas such as the Shannon entropy. These probabilities estimators are of course smartly created so that they elegantly highlight important complexity-related aspects of the data.","category":"page"},{"location":"","page":"ComplexityMeasures.jl","title":"ComplexityMeasures.jl","text":"Names for methods such as \"permutation entropy\" are commonplace, so in ComplexityMeasures.jl we provide convenience functions like entropy_permutation. However, we emphasize that these functions really aren't anything more than 2-lines-of-code wrappers that call entropy with the appropriate ProbabilitiesEstimator.","category":"page"},{"location":"","page":"ComplexityMeasures.jl","title":"ComplexityMeasures.jl","text":"What are genuinely different entropies are different definitions of entropy. And there are a lot of them! Examples are Shannon (the classic), Renyi or Tsallis entropy. These different definitions can be found in EntropyDefinition.","category":"page"},{"location":"#Other-complexity-measures","page":"ComplexityMeasures.jl","title":"Other complexity measures","text":"","category":"section"},{"location":"","page":"ComplexityMeasures.jl","title":"ComplexityMeasures.jl","text":"Other complexity measures, which strictly speaking don't compute entropies, and may or may not explicitly compute probability distributions, are found in Complexity measures page. This includes measures like sample entropy and approximate entropy.","category":"page"},{"location":"#input_data","page":"ComplexityMeasures.jl","title":"Input data for ComplexityMeasures.jl","text":"","category":"section"},{"location":"","page":"ComplexityMeasures.jl","title":"ComplexityMeasures.jl","text":"The input data type typically depend on the probability estimator chosen. In general though, the standard DynamicalSystems.jl approach is taken and as such we have three types of input data:","category":"page"},{"location":"","page":"ComplexityMeasures.jl","title":"ComplexityMeasures.jl","text":"Timeseries, which are AbstractVector{<:Real}, used in e.g. with WaveletOverlap.\nMulti-variate timeseries, or datasets, or state space sets, which are StateSpaceSets, used e.g. with NaiveKernel.\nSpatial data, which are higher dimensional standard Arrays, used e.g. with  SpatialSymbolicPermutation.","category":"page"},{"location":"","page":"ComplexityMeasures.jl","title":"ComplexityMeasures.jl","text":"StateSpaceSet","category":"page"},{"location":"#StateSpaceSets.StateSpaceSet","page":"ComplexityMeasures.jl","title":"StateSpaceSets.StateSpaceSet","text":"StateSpaceSet{D, T} <: AbstractStateSpaceSet{D,T}\n\nA dedicated interface for sets in a state space. It is an ordered container of equally-sized points of length D. Each point is represented by SVector{D, T}. The data are a standard Julia Vector{SVector}, and can be obtained with vec(ssset::StateSpaceSet). Typically the order of points in the set is the time direction, but it doesn't have to be.\n\nWhen indexed with 1 index, StateSpaceSet is like a vector of points. When indexed with 2 indices it behaves like a matrix that has each of the columns be the timeseries of each of the variables. When iterated over, it iterates over its contained points. See description of indexing below for more.\n\nStateSpaceSet also supports almost all sensible vector operations like append!, push!, hcat, eachrow, among others.\n\nDescription of indexing\n\nIn the following let i, j be integers, typeof(X) <: AbstractStateSpaceSet and v1, v2 be <: AbstractVector{Int} (v1, v2 could also be ranges, and for performance benefits make v2 an SVector{Int}).\n\nX[i] == X[i, :] gives the ith point (returns an SVector)\nX[v1] == X[v1, :], returns a StateSpaceSet with the points in those indices.\nX[:, j] gives the jth variable timeseries (or collection), as Vector\nX[v1, v2], X[:, v2] returns a StateSpaceSet with the appropriate entries (first indices being \"time\"/point index, while second being variables)\nX[i, j] value of the jth variable, at the ith timepoint\n\nUse Matrix(ssset) or StateSpaceSet(matrix) to convert. It is assumed that each column of the matrix is one variable. If you have various timeseries vectors x, y, z, ... pass them like StateSpaceSet(x, y, z, ...). You can use columns(dataset) to obtain the reverse, i.e. all columns of the dataset in a tuple.\n\n\n\n\n\n","category":"type"},{"location":"devdocs/#ComplexityMeasures.jl-Dev-Docs","page":"ComplexityMeasures.jl Dev Docs","title":"ComplexityMeasures.jl Dev Docs","text":"","category":"section"},{"location":"devdocs/","page":"ComplexityMeasures.jl Dev Docs","title":"ComplexityMeasures.jl Dev Docs","text":"Good practices in developing a code base apply in every Pull Request. The Good Scientific Code Workshop is worth checking out for this.","category":"page"},{"location":"devdocs/","page":"ComplexityMeasures.jl Dev Docs","title":"ComplexityMeasures.jl Dev Docs","text":"All PRs contributing new functionality must be well tested and well documented. You only need to add tests for methods that you explicitly extended.","category":"page"},{"location":"devdocs/#Adding-a-new-ProbabilitiesEstimator","page":"ComplexityMeasures.jl Dev Docs","title":"Adding a new ProbabilitiesEstimator","text":"","category":"section"},{"location":"devdocs/#Mandatory-steps","page":"ComplexityMeasures.jl Dev Docs","title":"Mandatory steps","text":"","category":"section"},{"location":"devdocs/","page":"ComplexityMeasures.jl Dev Docs","title":"ComplexityMeasures.jl Dev Docs","text":"Decide on the outcome space and how the estimator will map probabilities to outcomes.\nDefine your type and make it subtype ProbabilitiesEstimator.\nAdd a docstring to your type following the style of the docstrings of other estimators.\nIf suitable, the estimator may be able to operate based on Encodings. If so, it is preferred to implement an Encoding subtype and extend the methods encode and decode. This will allow your probabilities estimator to be used with a larger span of entropy and complexity methods without additional effort. Have a look at the file defining SymbolicPermutation for an idea of how this works.\nImplement dispatch for probabilities_and_outcomes and your probabilities estimator type.\nImplement dispatch for outcome_space and your probabilities estimator type. The return value of outcome_space must be sorted (as in the default behavior of sort, in ascending order).\nAdd your probabilities estimator type to the table list in the documentation page of probabilities. If you made an encoding, also add it to corresponding table in the encodings section.","category":"page"},{"location":"devdocs/#Optional-steps","page":"ComplexityMeasures.jl Dev Docs","title":"Optional steps","text":"","category":"section"},{"location":"devdocs/","page":"ComplexityMeasures.jl Dev Docs","title":"ComplexityMeasures.jl Dev Docs","text":"You may extend any of the following functions if there are potential performance benefits in doing so:","category":"page"},{"location":"devdocs/","page":"ComplexityMeasures.jl Dev Docs","title":"ComplexityMeasures.jl Dev Docs","text":"probabilities. By default it calls probabilities_and_outcomes and returns the first value.\noutcomes. By default calls probabilities_and_outcomes and returns the second value.\ntotal_outcomes. By default it returns the length of outcome_space. This is the function that most typically has performance benefits if implemented explicitly, so most existing estimators extend it by default.","category":"page"},{"location":"devdocs/#Adding-a-new-DifferentialEntropyEstimator","page":"ComplexityMeasures.jl Dev Docs","title":"Adding a new DifferentialEntropyEstimator","text":"","category":"section"},{"location":"devdocs/#Mandatory-steps-2","page":"ComplexityMeasures.jl Dev Docs","title":"Mandatory steps","text":"","category":"section"},{"location":"devdocs/","page":"ComplexityMeasures.jl Dev Docs","title":"ComplexityMeasures.jl Dev Docs","text":"Define your type and make it subtype DifferentialEntropyEstimator.\nAdd a docstring to your type following the style of the docstrings of other estimators.  This docstring should contain the formula(s)/integral(s) which it estimates, and a  reference to relevant EntropyDefinition(s).\nImplement dispatch for entropy with the relevant EntropyDefinition.  If your estimator works for multiple entropies, implement one method for  entropy for each of them.","category":"page"},{"location":"devdocs/#Tests","page":"ComplexityMeasures.jl Dev Docs","title":"Tests","text":"","category":"section"},{"location":"devdocs/","page":"ComplexityMeasures.jl Dev Docs","title":"ComplexityMeasures.jl Dev Docs","text":"You need to add tests verifying that your estimator actually convergences, within some reasonable tolerance (that you define), to the true entropy of data from some known distribution. Have a look in the tests for existing estimators for inspiration (you can just copy-paste one of the existing tests, or make them more elaborate if you want to).","category":"page"},{"location":"devdocs/#Adding-a-new-EntropyDefinition","page":"ComplexityMeasures.jl Dev Docs","title":"Adding a new EntropyDefinition","text":"","category":"section"},{"location":"devdocs/#Mandatory-steps-3","page":"ComplexityMeasures.jl Dev Docs","title":"Mandatory steps","text":"","category":"section"},{"location":"devdocs/","page":"ComplexityMeasures.jl Dev Docs","title":"ComplexityMeasures.jl Dev Docs","text":"Define your entropy definition type and make it subtype EntropyDefinition.\nImplement dispatch for entropy(def::YourType, p::Probabilities)\nAdd a docstring to your type following the style of the docstrings of other entropy  definitions, and should include the mathematical definition of the entropy.\nAdd your entropy definition type to the list of definitions in the  docs/src/entropies.md documentation page.\nAdd a reference to your entropy definition in the docstring for  EntropyDefinition.","category":"page"},{"location":"devdocs/#Optional-steps-2","page":"ComplexityMeasures.jl Dev Docs","title":"Optional steps","text":"","category":"section"},{"location":"devdocs/","page":"ComplexityMeasures.jl Dev Docs","title":"ComplexityMeasures.jl Dev Docs","text":"If the maximum value of your entropy type is analytically computable for a probability  distribution with a known number of elements, implementing dispatch for  entropy_maximum automatically enables entropy_normalized for your  type.","category":"page"}]
}
