@book{Datseris2022,
  title={Nonlinear dynamics: a concise introduction interlaced with code},
  author={Datseris, George and Parlitz, Ulrich},
  year={2022},
  publisher={Springer Nature},
  url={https://link.springer.com/book/10.1007/978-3-030-91032-7},
  doi={https://doi.org/10.1007/978-3-030-91032-7},
}

@article{Li2019,
  title={Reverse dispersion entropy: a new complexity measure for sensor signal},
  author={Li, Yuxing and Gao, Xiang and Wang, Long},
  journal={Sensors},
  volume={19},
  number={23},
  pages={5203},
  year={2019},
  publisher={MDPI}
}

@article{Zhou2023,
  title={Using missing dispersion patterns to detect determinism and nonlinearity in time series data},
  author={Zhou, Qin and Shang, Pengjian and Zhang, Boyi},
  journal={Nonlinear Dynamics},
  volume={111},
  number={1},
  pages={439--458},
  year={2023},
  publisher={Springer},
  doi={https://doi.org/10.1007/s11071-022-07835-3}
}

@article{Curado2004,
  title = {On the stability of analytic entropic forms},
  journal = {Physica A: Statistical Mechanics and its Applications},
  volume = {335},
  number = {1},
  pages = {94-106},
  year = {2004},
  issn = {0378-4371},
  doi = {https://doi.org/10.1016/j.physa.2003.12.026},
  url = {https://www.sciencedirect.com/science/article/pii/S0378437103011889},
  author = {Evaldo M.F. Curado and Fernando D. Nobre},
  keywords = {Entropy stability, Nonextensive statistical mechanics},
  abstract = {The stability against small perturbations on the probability distributions (also called experimental robustness) of analytic entropic forms is analyzed. Entropies S[p], associated with a given set of probabilities {pi}, that can be written in the simple form S[p]=∑i=1Wr(pi), are shown to be robust, if r(pi) is an analytic function of the pi's. The same property holds for entropies Σ(S[p]) that are monotonic and analytic functions of S[p]. The Tsallis entropy Sq[p] falls in the first class of entropies, whenever the entropic index q is an integer greater than 1. A new kind of entropy, that follows such requirements, is discussed.},
}
@article{Anteneodo1999,
doi = {https://doi.org/10.1088/0305-4470/32/7/002},
url = {https://dx.doi.org/10.1088/0305-4470/32/7/002},
year = {1999},
month = {feb},
publisher = {},
volume = {32},
number = {7},
pages = {1089},
author = {C Anteneodo and  A R Plastino},
title = {Maximum entropy approach to stretched exponential probability distributions},
journal = {Journal of Physics A: Mathematical and General},
abstract = {We introduce a nonextensive entropy functional  whose optimization under simple constraints (mean values of some standard quantities) yields stretched exponential probability distributions, which occur in many complex systems. The new entropy functional is characterized by a parameter  (the stretching exponent) such that for  the standard logarithmic entropy is recovered. We study its mathematical properties, showing that the basic requirements for a well-behaved entropy functional are verified, i.e.  possesses the usual properties of positivity, equiprobability, concavity and irreversibility and verifies Khinchin axioms except the one related to additivity since  is nonextensive. The entropy  is shown to be superadditive for  and subadditive for .}
}

@article{Rostaghi2016,
  title={Dispersion entropy: A measure for time-series analysis},
  author={Rostaghi, Mostafa and Azami, Hamed},
  journal={IEEE Signal Processing Letters},
  volume={23},
  number={5},
  pages={610--614},
  year={2016},
  publisher={IEEE},
  doi={https://doi.org/10.1109/LSP.2016.2542881},
}

@article{Li2018,
  AUTHOR = {Li, Guohui and Guan, Qianru and Yang, Hong},
  TITLE = {Noise Reduction Method of Underwater Acoustic Signals Based on CEEMDAN, Effort-To-Compress Complexity, Refined Composite Multiscale Dispersion Entropy and Wavelet Threshold Denoising},
  JOURNAL = {Entropy},
  VOLUME = {21},
  YEAR = {2019},
  NUMBER = {1},
  ARTICLE-NUMBER = {11},
  URL = {https://www.mdpi.com/1099-4300/21/1/11},
  PubMedID = {33266727},
  ISSN = {1099-4300},
  ABSTRACT = {Owing to the problems that imperfect decomposition process of empirical mode decomposition (EMD) denoising algorithm and poor self-adaptability, it will be extremely difficult to reduce the noise of signal. In this paper, a noise reduction method of underwater acoustic signal denoising based on complete ensemble empirical mode decomposition with adaptive noise (CEEMDAN), effort-to-compress complexity (ETC), refined composite multiscale dispersion entropy (RCMDE) and wavelet threshold denoising is proposed. Firstly, the original signal is decomposed into several IMFs by CEEMDAN and noise IMFs can be identified according to the ETC of IMFs. Then, calculating the RCMDE of remaining IMFs, these IMFs are divided into three kinds of IMFs by RCMDE, namely noise-dominant IMFs, real signal-dominant IMFs, real IMFs. Finally, noise IMFs are removed, wavelet soft threshold denoising is applied to noise-dominant IMFs and real signal-dominant IMFs. The denoised signal can be obtained by combining the real IMFs with the denoised IMFs after wavelet soft threshold denoising. Chaotic signals with different signal-to-noise ratio (SNR) are used for denoising experiments by comparing with EMD_MSE_WSTD and EEMD_DE_WSTD, it shows that the proposed algorithm has higher SNR and smaller root mean square error (RMSE). In order to further verify the effectiveness of the proposed method, which is applied to noise reduction of real underwater acoustic signals. The results show that the denoised underwater acoustic signals not only eliminate noise interference also restore the topological structure of the chaotic attractors more clearly, which lays a foundation for the further processing of underwater acoustic signals.},
  DOI = {10.3390/e21010011},
}

@article{Li2019,
  AUTHOR = {Li, Yuxing and Gao, Xiang and Wang, Long},
  TITLE = {Reverse Dispersion Entropy: A New Complexity Measure for Sensor Signal},
  JOURNAL = {Sensors},
  VOLUME = {19},
  YEAR = {2019},
  NUMBER = {23},
  ARTICLE-NUMBER = {5203},
  URL = {https://www.mdpi.com/1424-8220/19/23/5203},
  PubMedID = {31783659},
  ISSN = {1424-8220},
  ABSTRACT = {Permutation entropy (PE), as one of the powerful complexity measures for analyzing time series, has advantages of easy implementation and high efficiency. In order to improve the performance of PE, some improved PE methods have been proposed through introducing amplitude information and distance information in recent years. Weighted-permutation entropy (W-PE) weight each arrangement pattern by using variance information, which has good robustness and stability in the case of high noise level and can extract complexity information from data with spike feature or abrupt amplitude change. Dispersion entropy (DE) introduces amplitude information by using the normal cumulative distribution function (NCDF); it not only can detect the change of simultaneous frequency and amplitude, but also is superior to the PE method in distinguishing different data sets. Reverse permutation entropy (RPE) is defined as the distance to white noise in the opposite trend with PE and W-PE, which has high stability for time series with varying lengths. To further improve the performance of PE, we propose a new complexity measure for analyzing time series, and term it as reverse dispersion entropy (RDE). RDE takes PE as its theoretical basis and combines the advantages of DE and RPE by introducing amplitude information and distance information. Simulation experiments were carried out on simulated and sensor signals, including mutation signal detection under different parameters, noise robustness testing, stability testing under different signal-to-noise ratios (SNRs), and distinguishing real data for different kinds of ships and faults. The experimental results show, compared with PE, W-PE, RPE, and DE, that RDE has better performance in detecting abrupt signal and noise robustness testing, and has better stability for simulated and sensor signal. Moreover, it also shows higher distinguishing ability than the other four kinds of PE for sensor signals.},
  DOI = {10.3390/s19235203},
}

@article{Azami2017,
  author={Azami, Hamed and Rostaghi, Mostafa and Abásolo, Daniel and Escudero, Javier},
  journal={IEEE Transactions on Biomedical Engineering}, 
  title={Refined Composite Multiscale Dispersion Entropy and its Application to Biomedical Signals}, 
  year={2017},
  volume={64},
  number={12},
  pages={2872-2879},
  doi={https://doi.org/10.1109/TBME.2017.2679136},
}

@article{Costa2002,
  title = {Multiscale Entropy Analysis of Complex Physiologic Time Series},
  author = {Costa, Madalena and Goldberger, Ary L. and Peng, C.-K.},
  journal = {Phys. Rev. Lett.},
  volume = {89},
  issue = {6},
  pages = {068102},
  numpages = {4},
  year = {2002},
  month = {Jul},
  publisher = {American Physical Society},
  doi = {https://doi.org/10.1103/PhysRevLett.89.068102},
  url = {https://link.aps.org/doi/10.1103/PhysRevLett.89.068102},
}

@article{Costa2015,
  AUTHOR = {Costa, Madalena D. and Goldberger, Ary L.},
  TITLE = {Generalized Multiscale Entropy Analysis: Application to Quantifying the Complex Volatility of Human Heartbeat Time Series},
  JOURNAL = {Entropy},
  VOLUME = {17},
  YEAR = {2015},
  NUMBER = {3},
  PAGES = {1197--1203},
  URL = {https://www.mdpi.com/1099-4300/17/3/1197},
  PubMedID = {27099455},
  ISSN = {1099-4300},
  ABSTRACT = {We introduce a generalization of multiscale entropy (MSE) analysis. The method is termed MSEn, where the subscript denotes the moment used to coarse-grain a time series. MSEμ, described previously, uses the mean value (first moment). Here, we focus on MSEσ2 , which uses the second moment, i.e., the variance. MSEσ2 quantifies the dynamics of the volatility (variance) of a signal over multiple time scales. We use the method to analyze the structure of heartbeat time series. We find that the dynamics of the volatility of heartbeat time series obtained from healthy young subjects is highly complex. Furthermore, we find that the multiscale complexity of the volatility, not only the multiscale complexity of the mean heart rate, degrades with aging and pathology. The “bursty” behavior of the dynamics may be related to intermittency in energy and information flows, as part of multiscale cycles of activation and recovery. Generalized MSE may also be useful in quantifying the dynamical properties of other physiologic and of non-physiologic time series.},
  DOI = {10.3390/e17031197}
}

@article{Pincus1991,
  title={Approximate entropy as a measure of system complexity.},
  author={Pincus, Steven M},
  journal={Proceedings of the National Academy of Sciences},
  volume={88},
  number={6},
  pages={2297--2301},
  year={1991},
  publisher={National Acad Sciences},
  doi={https://doi.org/10.1073/pnas.88.6.2297},
}

@article{LempelZiv1976,
  author={Lempel, A. and Ziv, J.},
  journal={IEEE Transactions on Information Theory}, 
  title={On the Complexity of Finite Sequences}, 
  year={1976},
  volume={22},
  number={1},
  pages={75-81},
  doi={https://doi.org/10.1109/TIT.1976.1055501},
}

@article{Amigó2004,
author = {Amigó, José M. and Szczepański, Janusz and Wajnryb, Elek and Sanchez-Vives, Maria V.},
  title = {Estimating the Entropy Rate of Spike Trains via Lempel-Ziv Complexity},
  journal = {Neural Computation},
  volume = {16},
  number = {4},
  pages = {717-736},
  year = {2004},
  month = {04},
  abstract = "{Normalized Lempel-Ziv complexity, which measures the generation rate of new patterns along a digital sequence, is closely related to such important source properties as entropy and compression ratio, but, in contrast to these, it is a property of individual sequences. In this article, we propose to exploit this concept to estimate (or, at least, to bound from below) the entropy of neural discharges (spike trains). The main advantages of this method include fast convergence of the estimator (as supported by numerical simulation) and the fact that there is no need to know the probability law of the process generating the signal. Furthermore, we present numerical and experimental comparisons of the new method against the standard method based on word frequencies, providing evidence that this new approach is an alternative entropy estimator for binned spike trains.}",
  issn = {0899-7667},
  doi = {https://doi.org/10.1162/089976604322860677},
  url = {https://doi.org/10.1162/089976604322860677},
  eprint = {https://direct.mit.edu/neco/article-pdf/16/4/717/815838/089976604322860677.pdf},
}

@article{Richman2000,
  title={Physiological time-series analysis using approximate entropy and sample entropy},
  author={Richman, Joshua S and Moorman, J Randall},
  journal={American journal of physiology-heart and circulatory physiology},
  volume={278},
  number={6},
  pages={H2039--H2049},
  year={2000},
  publisher={American Physiological Society Bethesda, MD},
  doi={https://doi.org/10.1152/ajpheart.2000.278.6.H2039},
}

@article{Rosso2007,
  title={Distinguishing noise from chaos},
  author={Rosso, Osvaldo A and Larrondo, HA and Martin, María Teresa and Plastino, A and Fuentes, Miguel A},
  journal={Physical review letters},
  volume={99},
  number={15},
  pages={154102},
  year={2007},
  publisher={APS},
  doi={https://doi.org/10.1103/PhysRevLett.99.154102},
}

@article{Rosso2013,
  title={Generalized statistical complexity: A new tool for dynamical systems},
  author={Rosso, Osvaldo A and Martín, MT and Larrondo, Hilda A and Kowalski, AM and Plastino, A},
  journal={Concepts and recent advances in generalized information measures and statistics},
  pages={169--215},
  year={2013},
  publisher={Bentham Science Publishers Emirate of Sharjah, United Arab Emirates},
  url={https://www.researchgate.net/profile/Osvaldo-Rosso-3/publication/260145202_Generalized_Statistical_Complexity_a_new_tool_for_dynamical_systems/links/02e7e52fbe7122d461000000/Generalized-Statistical-Complexity-a-new-tool-for-dynamical-systems.pdf}
}

@article{Sippel2016,
  title={statcomp: Statistical Complexity and Information measures for time series analysis},
  author={Sippel, S and Lange, H and Gans, F},
  journal={R package version},
  year={2016},
  url={https://cran.r-project.org/web/packages/statcomp/index.html},
}

@inproceedings{Gao2015,
  title = {Efficient Estimation of Mutual Information for Strongly Dependent Variables},
  author = {Gao, Shuyang and Ver Steeg, Greg and Galstyan, Aram},
  booktitle = {Proceedings of the Eighteenth International Conference on Artificial Intelligence and Statistics},
  pages = {277--286},
  year = {2015},
  editor = {Lebanon, Guy and Vishwanathan, S. V. N.},
  volume = {38},
  series = {Proceedings of Machine Learning Research},
  address = {San Diego, California, USA},
  month = {09--12 May},
  publisher =  {PMLR},
  pdf = {http://proceedings.mlr.press/v38/gao15.pdf},
  url = {https://proceedings.mlr.press/v38/gao15.html},
  abstract = {We demonstrate that a popular class of non-parametric mutual information (MI) estimators based on k-nearest-neighbor graphs requires number of samples that scales exponentially with the true MI. Consequently, accurate estimation of MI between two strongly dependent variables is possible only for prohibitively large sample size. This important yet overlooked shortcoming of the existing estimators is due to their implicit reliance on  local uniformity of the underlying joint distribution. We introduce a new  estimator that is robust to local non-uniformity, works well with limited data, and is able to capture relationship strengths over many orders of magnitude. We demonstrate the superior performance of the proposed estimator on both synthetic and real-world data.}
}


@article{Singh2003,
  title={Nearest neighbor estimates of entropy},
  author={Singh, Harshinder and Misra, Neeraj and Hnizdo, Vladimir and Fedorowicz, Adam and Demchuk, Eugene},
  journal={American journal of mathematical and management sciences},
  volume={23},
  number={3-4},
  pages={301--321},
  year={2003},
  publisher={Taylor \& Francis},
  doi={https://doi.org/10.1080/01966324.2003.10737616},
}

@article{Goria2005,
  author = { M. N. Goria  and  N. N. Leonenko  and  V. V. Mergel and P. L. Novi Inverardi},
  title = {A new class of random vector entropy estimators and its applications in testing statistical hypotheses},
  journal = {Journal of Nonparametric Statistics},
  volume = {17},
  number = {3},
  pages = {277-297},
  year  = {2005},
  publisher = {Taylor & Francis},
  doi = {https://doi.org/10.1080/104852504200026815},
  URL = {https://doi.org/10.1080/104852504200026815},
  eprint = {https://doi.org/10.1080/104852504200026815},
}

@article{KozachenkoLeonenko1987,
  title={Sample estimate of the entropy of a random vector},
  author={Kozachenko, Lyudmyla F and Leonenko, Nikolai N},
  journal={Problemy Peredachi Informatsii},
  volume={23},
  number={2},
  pages={9--16},
  year={1987},
  publisher={Russian Academy of Sciences, Branch of Informatics, Computer Equipment and~…},
  url={https://www.mathnet.ru/php/archive.phtml?wshow=paper&jrnid=ppi&paperid=797&option_lang=eng}
}

@article{Charzyńska2015,
  AUTHOR = {Charzyńska, Agata and Gambin, Anna},
  TITLE = {Improvement of the k-nn Entropy Estimator with Applications in Systems Biology},
  JOURNAL = {Entropy},
  VOLUME = {18},
  YEAR = {2016},
  NUMBER = {1},
  ARTICLE-NUMBER = {13},
  URL = {https://www.mdpi.com/1099-4300/18/1/13},
  ISSN = {1099-4300},
  ABSTRACT = {In this paper, we investigate efficient estimation of differential entropy for multivariate random variables. We propose bias correction for the nearest neighbor estimator, which yields more accurate results in higher dimensions. In order to demonstrate the accuracy of the improvement, we calculated the corrected estimator for several families of random variables. For multivariate distributions, we considered the case of independent marginals and the dependence structure between the marginal distributions described by Gaussian copula. The presented solution may be particularly useful for high dimensional data, like those analyzed in the systems biology field. To illustrate such an application, we exploit differential entropy to define the robustness of biochemical kinetic models.},
  DOI = {10.3390/e18010013}
}

@article{Kraskov2004,
  title = {Estimating mutual information},
  author = {Kraskov, Alexander and Stögbauer, Harald and Grassberger, Peter},
  journal = {Phys. Rev. E},
  volume = {69},
  issue = {6},
  pages = {066138},
  numpages = {16},
  year = {2004},
  month = {Jun},
  publisher = {American Physical Society},
  doi = {https://doi.org/10.1103/PhysRevE.69.066138},
  url = {https://link.aps.org/doi/10.1103/PhysRevE.69.066138},
}

@article{Lord2018,
  title={Geometric k-nearest neighbor estimation of entropy and mutual information},
  author={Lord, Warren M and Sun, Jie and Bollt, Erik M},
  journal={Chaos: An Interdisciplinary Journal of Nonlinear Science},
  volume={28},
  number={3},
  year={2018},
  publisher={AIP Publishing},
  doi = {https://doi.org/10.1063/1.5011683},
  url = {https://pubs.aip.org/aip/cha/article/28/3/033114/685022},
}

@article{Zhu2015,
  title={Contribution to transfer entropy estimation via the k-nearest-neighbors approach},
  author={Zhu, Jie and Bellanger, Jean-Jacques and Shu, Huazhong and Le Bouquin Jeannès, Régine},
  journal={Entropy},
  volume={17},
  number={6},
  pages={4173--4201},
  year={2015},
  publisher={MDPI},
  doi={https://doi.org/10.3390/e17064173}
}

@article{Alizadeh2010,
  title={A new estimator of entropy},
  author={Alizadeh, Noughabi Hadi and Arghami, Naser Reza},
  year={2010},
  journal={Journal of the Iranian Statistical Society (JIRSS)},
  publisher={Journal of the Iranian Statistical Society (JIRSS)},
  url={http://jirss.irstat.ir/article-1-81-en.pdf},
}

@article{Correa1995,
  author = {Correa, Juan C.},
  title = {A new estimator of entropy},
  journal = {Communications in Statistics - Theory and Methods},
  volume = {24},
  number = {10},
  pages = {2439-2449},
  year  = {1995},
  publisher = {Taylor & Francis},
  doi = {https://doi.org/10.1080/03610929508831626},
  URL = {https://doi.org/10.1080/03610929508831626},
  eprint = {https://doi.org/10.1080/03610929508831626}
}

@article{Ebrahimi1994,
  title = {Two measures of sample entropy},
  journal = {Statistics & Probability Letters},
  volume = {20},
  number = {3},
  pages = {225-234},
  year = {1994},
  issn = {0167-7152},
  doi = {https://doi.org/10.1016/0167-7152(94)90046-9},
  url = {https://www.sciencedirect.com/science/article/pii/0167715294900469},
  author = {Nader Ebrahimi and Kurt Pflughoeft and Ehsan S. Soofi},
  keywords = {Information theory, Entropy estimator, Exponential, Normal, Uniform},
  abstract = {In many statistical studies the entropy of a distribution function is of prime interest. This paper proposes two estimators of the entropy. Both estimators are obtained by modifying the estimator proposed by Vasicek (1976). Consistency of both estimators is proved, and comparisons have been made with Vasicek's estimator and its generalization proposed by Dudewicz and Van der Meulen (1987). The results indicate that the proposed estimators have less bias and have less mean squared error than Vasicek's estimator and its generalization}
}

@article{Vasicek1976,
  title={A test for normality based on sample entropy},
  author={Vasicek, Oldrich},
  journal={Journal of the Royal Statistical Society Series B: Statistical Methodology},
  volume={38},
  number={1},
  pages={54--59},
  year={1976},
  publisher={Oxford University Press},
  doi={https://doi.org/10.1111/j.2517-6161.1976.tb01566.x}
}

@article{Chao2003,
	abstract = {A biological community usually has a large number of species with relatively small abundances. When a random sample of individuals is selected and each individual is classified according to species identity, some rare species may not be discovered. This paper is concerned with the estimation of Shannon's index of diversity when the number of species and the species abundances are unknown. The traditional estimator that ignores the missing species underestimates when there is a non-negligible number of unseen species. We provide a different approach based on unequal probability sampling theory because species have different probabilities of being discovered in the sample. No parametric forms are assumed for the species abundances. The proposed estimation procedure combines the Horvitz--Thompson (1952) adjustment for missing species and the concept of sample coverage, which is used to properly estimate the relative abundances of species discovered in the sample. Simulation results show that the proposed estimator works well under various abundance models even when a relatively large fraction of the species is missing. Three real data sets, two from biology and the other one from numismatics, are given for illustration.},
	author = {Chao, Anne and Shen, Tsung-Jen},
	date = {2003/12/01},
	doi = {https://doi.org/10.1023/A:1026096204727},
	id = {Chao2003},
	isbn = {1573-3009},
	journal = {Environmental and Ecological Statistics},
	number = {4},
	pages = {429--443},
	title = {Nonparametric estimation of Shannon's index of diversity when there are unseen species in sample},
	url = {https://doi.org/10.1023/A:1026096204727},
	volume = {10},
	year = {2003},
}

@article{Arora2022,
  title={Estimating the Entropy of Linguistic Distributions}, 
  author={Aryaman Arora and Clara Meister and Ryan Cotterell},
  year={2022},
  eprint={2204.01469},
  archivePrefix={arXiv},
  primaryClass={cs.CL},
  url={https://arxiv.org/abs/2204.01469},
}

@article{Horvitz1952,
  author = { D. G.   Horvitz  and  D. J.   Thompson },
  title = {A Generalization of Sampling Without Replacement from a Finite Universe},
  journal = {Journal of the American Statistical Association},
  volume = {47},
  number = {260},
  pages = {663-685},
  year  = {1952},
  publisher = {Taylor & Francis},
  doi = {https://doi.org/10.1080/01621459.1952.10483446},
  URL = {https://www.tandfonline.com/doi/abs/10.1080/01621459.1952.10483446},
  eprint = {https://www.tandfonline.com/doi/pdf/10.1080/01621459.1952.10483446},
}

@article{Zahl1977,
  title={Jackknifing an index of diversity},
  author={Zahl, Samuel},
  journal={Ecology},
  volume={58},
  number={4},
  pages={907--913},
  year={1977},
  publisher={Wiley Online Library},
  doi={https://doi.org/10.2307/1936227}
}

@article{Miller1955,
  title={Note on the bias of information estimates},
  author={Miller, George},
  journal={Information theory in psychology: Problems and methods},
  year={1955},
  publisher={Free Press}
}

@article{Paninski2003,
  title={Estimation of entropy and mutual information},
  author={Paninski, Liam},
  journal={Neural computation},
  volume={15},
  number={6},
  pages={1191--1253},
  year={2003},
  publisher={MIT Press},
  doi={https://doi.org/10.1162/089976603321780272},
  url={https://ieeexplore.ieee.org/abstract/document/6790247},
}

@article{Grassberger2022,
  AUTHOR = {Grassberger, Peter},
  TITLE = {On Generalized Schürmann Entropy Estimators},
  JOURNAL = {Entropy},
  VOLUME = {24},
  YEAR = {2022},
  NUMBER = {5},
  ARTICLE-NUMBER = {680},
  URL = {https://www.mdpi.com/1099-4300/24/5/680},
  PubMedID = {35626564},
  ISSN = {1099-4300},
  ABSTRACT = {We present a new class of estimators of Shannon entropy for severely undersampled discrete distributions. It is based on a generalization of an estimator proposed by T. Schürmann, which itself is a generalization of an estimator proposed by myself. For a special set of parameters, they are completely free of bias and have a finite variance, something which is widely believed to be impossible. We present also detailed numerical tests, where we compare them with other recent estimators and with exact results, and point out a clash with Bayesian estimators for mutual information.},
  DOI = {10.3390/e24050680}
}

@article{Berger2019,
  publisher={MDPI},
  author = {Berger, Sebastian and Kravtsiv, Andrii and Schneider, Gerhard and Jordan, Denis},
  title = {Teaching Ordinal Patterns to a Computer: Efficient Encoding Algorithms Based on the Lehmer Code},
  journal = {Entropy},
  volume = {21},
  year = {2019},
  number = {10},
  article-number = {1023},
  url = {https://www.mdpi.com/1099-4300/21/10/1023},
  issn = {1099-4300},
  abstract = {Ordinal patterns are the common basis of various techniques used in the study of dynamical systems and nonlinear time series analysis. The present article focusses on the computational problem of turning time series into sequences of ordinal patterns. In a first step, a numerical encoding scheme for ordinal patterns is proposed. Utilising the classical Lehmer code, it enumerates ordinal patterns by consecutive non-negative integers, starting from zero. This compact representation considerably simplifies working with ordinal patterns in the digital domain. Subsequently, three algorithms for the efficient extraction of ordinal patterns from time series are discussed, including previously published approaches that can be adapted to the Lehmer code. The respective strengths and weaknesses of those algorithms are discussed, and further substantiated by benchmark results. One of the algorithms stands out in terms of scalability: its run-time increases linearly with both the pattern order and the sequence length, while its memory footprint is practically negligible. These properties enable the study of high-dimensional pattern spaces at low computational cost. In summary, the tools described herein may improve the efficiency of virtually any ordinal pattern-based analysis method, among them quantitative measures like permutation entropy and symbolic transfer entropy, but also techniques like forbidden pattern identification. Moreover, the concepts presented may allow for putting ideas into practice that up to now had been hindered by computational burden. To enable smooth evaluation, a function library written in the C programming language, as well as language bindings and native implementations for various numerical computation environments are provided in the supplements.},
  doi = {https://doi.org/10.3390/e21101023}
}

@article{Lad2015,
  author = {Frank Lad and Giuseppe Sanfilippo and Gianna Agrò},
  title = {Extropy: Complementary Dual of Entropy},
  volume = {30},
  journal = {Statistical Science},
  number = {1},
  publisher = {Institute of Mathematical Statistics},
  pages = {40 -- 58},
  keywords = {Bregman divergence, Differential and relative entropy/extropy, Duality, Gini index of heterogeneity, Kullback–Leibler divergence, proper scoring rules, repeat rate},
  year = {2015},
  doi = {https://doi.org/10.1214/14-STS430},
  url = {https://doi.org/10.1214/14-STS430},
}


@book{Ahlswede2006,
  title={General theory of information transfer and combinatorics},
  author={Ahlswede, Rudolf and Bäumer, Lars and Cai, Ning and Aydinian, Harout and Blinovsky, Vladimir and Deppe, Christian and Mashurian, Haik},
  volume={68},
  year={2006},
  publisher={Springer}
}

@book{Ahlswede2021,
  title={Identification and Other Probabilistic Models},
  author={Ahlswede, Rudolf and Ahlswede, Alexander and Althöfer, Ingo and Deppe, Christian and Tamm, Ulrich},
  year={2021},
  publisher={Springer}
}

@book{Tsallis2009,
  title={Introduction to nonextensive statistical mechanics: approaching a complex world},
  author={Tsallis, Constantino},
  volume={1},
  number={1},
  year={2009},
  publisher={Springer},
  url={https://link.springer.com/book/10.1007/978-0-387-85359-8}
}

@article{Liu2023,
  author = {Jiali Liu and Fuyuan Xiao},
  title = {Renyi extropy},
  journal = {Communications in Statistics, Theory and Methods},
  volume = {52},
  number = {16},
  pages = {5836-5847},
  year  = {2023},
  publisher = {Taylor & Francis},
  doi = {https://doi.org/10.1080/03610926.2021.2020843},
}

@article{Shannon1948,
  title={A mathematical theory of communication},
  author={Shannon, Claude Elwood},
  journal={The Bell system technical journal},
  volume={27},
  number={3},
  pages={379--423},
  year={1948},
  publisher={Nokia Bell Labs},
  doi={https://doi.org/10.1002/j.1538-7305.1948.tb01338.x},
}

@inproceedings{Rényi1961,
  title={On measures of entropy and information},
  author={Rényi, Alfréd},
  booktitle={Proceedings of the Fourth Berkeley Symposium on Mathematical Statistics and Probability, Volume 1: Contributions to the Theory of Statistics},
  volume={4},
  pages={547--562},
  year={1961},
  organization={University of California Press},
  url={https://projecteuclid.org/ebook/Download?urlid=bsmsp/1200512181&isFullBook=false}
}

@article{Xue2023,
  title={Tsallis extropy},
  author={Xue, Yige and Deng, Yong},
  journal={Communications in Statistics-Theory and Methods},
  volume={52},
  number={3},
  pages={751--762},
  year={2023},
  publisher={Taylor \& Francis},
  doi={https://doi.org/10.1080/03610926.2021.1921804},
}

@article{Tsallis1988,
  title={Possible generalization of Boltzmann-Gibbs statistics},
  author={Tsallis, Constantino},
  journal={Journal of statistical physics},
  volume={52},
  pages={479--487},
  year={1988},
  publisher={Springer},
  doi={https://doi.org/10.1007/BF01016429},
}

@article{Wu2013,
  title={Time series analysis using composite multiscale entropy},
  author={Wu, Shuen-De and Wu, Chiu-Wen and Lin, Shiou-Gwo and Wang, Chun-Chieh and Lee, Kung-Yen},
  journal={Entropy},
  volume={15},
  number={3},
  pages={1069--1084},
  year={2013},
  publisher={MDPI},
  doi={https://doi.org/10.3390/e15031069}
}

@article{Wang2020,
  title={Multiscale diversity entropy: A novel dynamical measure for fault diagnosis of rotating machinery},
  author={Wang, Xianzhi and Si, Shubin and Li, Yongbo},
  journal={IEEE Transactions on Industrial Informatics},
  volume={17},
  number={8},
  pages={5419--5429},
  year={2020},
  publisher={IEEE},
  doi={https://doi.org/10.1109/TII.2020.3022369},
}

@article{PrichardTheiler1995,
  title={Generalized redundancies for time series analysis},
  author={Prichard, Dean and Theiler, James},
  journal={Physica D: Nonlinear Phenomena},
  volume={84},
  number={3-4},
  pages={476--493},
  year={1995},
  publisher={Elsevier},
  doi={https://doi.org/10.1016/0167-2789(95)00041-2},
}

@article{Llanos2017,
  title={Power spectral entropy as an information-theoretic correlate of manner of articulation in American English},
  author={Llanos, Fernando and Alexander, Joshua M and Stilp, Christian E and Kluender, Keith R},
  journal={The Journal of the Acoustical Society of America},
  volume={141},
  number={2},
  pages={EL127--EL133},
  year={2017},
  publisher={AIP Publishing},
  doi={https://doi.org/10.1121/1.4976109},
  url={https://pubmed.ncbi.nlm.nih.gov/28253693/}
}

@article{Tian2017,
  title={Spectral entropy can predict changes of working memory performance reduced by short-time training in the delayed-match-to-sample task},
  author={Tian, Yin and Zhang, Huiling and Xu, Wei and Zhang, Haiyong and Yang, Li and Zheng, Shuxing and Shi, Yupan},
  journal={Frontiers in human neuroscience},
  volume={11},
  pages={437},
  year={2017},
  publisher={Frontiers Media SA},
  doi={https://doi.org/10.3389/fnhum.2017.00437},
}

@article{BandtPompe2002,
  title = {Permutation Entropy: A Natural Complexity Measure for Time Series},
  author = {Bandt, Christoph and Pompe, Bernd},
  journal = {Phys. Rev. Lett.},
  volume = {88},
  issue = {17},
  pages = {174102},
  numpages = {4},
  year = {2002},
  month = {Apr},
  publisher = {American Physical Society},
  doi = {https://doi.org/10.1103/PhysRevLett.88.174102},
  url = {https://link.aps.org/doi/10.1103/PhysRevLett.88.174102},
}

@article{Zunino2017,
  title={Permutation entropy based time series analysis: Equalities in the input signal can lead to false conclusions},
  author={Zunino, Luciano and Olivares, Felipe and Scholkmann, Felix and Rosso, Osvaldo A},
  journal={Physics Letters A},
  volume={381},
  number={22},
  pages={1883--1892},
  year={2017},
  publisher={Elsevier},
  doi={https://doi.org/10.1016/j.physleta.2017.03.052}
}

@article{He2016,
  title = {Multivariate permutation entropy and its application for complexity analysis of chaotic systems},
  journal = {Physica A: Statistical Mechanics and its Applications},
  volume = {461},
  pages = {812-823},
  year = {2016},
  issn = {0378-4371},
  doi = {https://doi.org/10.1016/j.physa.2016.06.012},
  url = {https://www.sciencedirect.com/science/article/pii/S0378437116302801},
  author = {Shaobo He and Kehui Sun and Huihai Wang},
  keywords = {Permutation entropy, Multivariate complexity, Simplified Lorenz system, Financial chaotic system},
  abstract = {To measure the complexity of multivariate systems, the multivariate permutation entropy (MvPE) algorithm is proposed. It is employed to measure complexity of multivariate system in the phase space. As an application, MvPE is applied to analyze the complexity of chaotic systems, including hyperchaotic Hénon map, fractional-order simplified Lorenz system and financial chaotic system. Results show that MvPE algorithm is effective for analyzing the complexity of the multivariate systems. It also shows that fractional-order system does not become more complex with derivative order varying. Compared with PE, MvPE has better robustness for noise and sampling interval, and the results are not affected by different normalization methods.}
}

@article{Fadlallah2013,
  title = {Weighted-permutation entropy: A complexity measure for time series incorporating amplitude information},
  author = {Fadlallah, Bilal and Chen, Badong and Keil, Andreas and Pr\'{\i}ncipe, Jos\'e},
  journal = {Phys. Rev. E},
  volume = {87},
  issue = {2},
  pages = {022911},
  numpages = {7},
  year = {2013},
  month = {Feb},
  publisher = {American Physical Society},
  doi = {https://doi.org/10.1103/PhysRevE.87.022911},
  url = {https://link.aps.org/doi/10.1103/PhysRevE.87.022911},
}

@article{Azami2016,
title = {Amplitude-aware permutation entropy: Illustration in spike detection and signal segmentation},
journal = {Computer Methods and Programs in Biomedicine},
volume = {128},
pages = {40-51},
year = {2016},
issn = {0169-2607},
doi = {https://doi.org/10.1016/j.cmpb.2016.02.008},
url = {https://www.sciencedirect.com/science/article/pii/S0169260715301152},
author = {Hamed Azami and Javier Escudero},
keywords = {Signal irregularity, Amplitude-aware permutation entropy, Spike detection, Signal segmentation, Electroencephalogram, Extracellular neuronal data},
abstract = {Background and objective
Signal segmentation and spike detection are two important biomedical signal processing applications. Often, non-stationary signals must be segmented into piece-wise stationary epochs or spikes need to be found among a background of noise before being further analyzed. Permutation entropy (PE) has been proposed to evaluate the irregularity of a time series. PE is conceptually simple, structurally robust to artifacts, and computationally fast. It has been extensively used in many applications, but it has two key shortcomings. First, when a signal is symbolized using the Bandt–Pompe procedure, only the order of the amplitude values is considered and information regarding the amplitudes is discarded. Second, in the PE, the effect of equal amplitude values in each embedded vector is not addressed. To address these issues, we propose a new entropy measure based on PE: the amplitude-aware permutation entropy (AAPE).
Methods
AAPE is sensitive to the changes in the amplitude, in addition to the frequency, of the signals thanks to it being more flexible than the classical PE in the quantification of the signal motifs. To demonstrate how the AAPE method can enhance the quality of the signal segmentation and spike detection, a set of synthetic and realistic synthetic neuronal signals, electroencephalograms and neuronal data are processed. We compare the performance of AAPE in these problems against state-of-the-art approaches and evaluate the significance of the differences with a repeated ANOVA with post hoc Tukey's test.
Results
In signal segmentation, the accuracy of AAPE-based method is higher than conventional segmentation methods. AAPE also leads to more robust results in the presence of noise. The spike detection results show that AAPE can detect spikes well, even when presented with single-sample spikes, unlike PE. For multi-sample spikes, the changes in AAPE are larger than in PE.
Conclusion
We introduce a new entropy metric, AAPE, that enables us to consider amplitude information in the formulation of PE. The AAPE algorithm can be used in almost every irregularity-based application in various signal and image processing fields. We also made freely available the Matlab code of the AAPE.}
}

@article{Rosso2001,
  title = {Wavelet entropy: a new tool for analysis of short duration brain electrical signals},
  journal = {Journal of Neuroscience Methods},
  volume = {105},
  number = {1},
  pages = {65-75},
  year = {2001},
  issn = {0165-0270},
  doi = {https://doi.org/10.1016/S0165-0270(00)00356-3},
  url = {https://www.sciencedirect.com/science/article/pii/S0165027000003563},
  author = {Osvaldo A. Rosso and Susana Blanco and Juliana Yordanova and Vasil Kolev and Alejandra Figliola and Martin Schürmann and Erol Başar},
  keywords = {EEG, event-related potentials (ERP), Visual evoked potential, Time–frequency signal analysis, Wavelet analysis, Signal entropy},
  abstract = {Since traditional electrical brain signal analysis is mostly qualitative, the development of new quantitative methods is crucial for restricting the subjectivity in the study of brain signals. These methods are particularly fruitful when they are strongly correlated with intuitive physical concepts that allow a better understanding of brain dynamics. Here, new method based on orthogonal discrete wavelet transform (ODWT) is applied. It takes as a basic element the ODWT of the EEG signal, and defines the relative wavelet energy, the wavelet entropy (WE) and the relative wavelet entropy (RWE). The relative wavelet energy provides information about the relative energy associated with different frequency bands present in the EEG and their corresponding degree of importance. The WE carries information about the degree of order/disorder associated with a multi-frequency signal response, and the RWE measures the degree of similarity between different segments of the signal. In addition, the time evolution of the WE is calculated to give information about the dynamics in the EEG records. Within this framework, the major objective of the present work was to characterize in a quantitative way functional dynamics of order/disorder microstates in short duration EEG signals. For that aim, spontaneous EEG signals under different physiological conditions were analyzed. Further, specific quantifiers were derived to characterize how stimulus affects electrical events in terms of frequency synchronization (tuning) in the event related potentials.},
}

@article{Azami2019,
title = {Two-dimensional dispersion entropy: An information-theoretic method for irregularity analysis of images},
journal = {Signal Processing: Image Communication},
volume = {75},
pages = {178-187},
year = {2019},
issn = {0923-5965},
doi = {https://doi.org/10.1016/j.image.2019.04.013},
url = {https://www.sciencedirect.com/science/article/pii/S0923596519300682},
author = {Azami, Hamed and da Silva, Luiz Eduardo Virgilio and Omoto, Ana Carolina Mieko and Humeau-Heurtier, Anne},
keywords = {Biomedical image processing, Texture analysis, Irregularity, Two-dimensional dispersion entropy, Two-dimensional sample entropy},
abstract = {Two-dimensional sample entropy (SampEn2D) is a recently developed method in the field of information theory for evaluating the regularity or predictability of images. SampEn2D, though powerful, has two key limitations: (1) SampEn2D values are undefined for small-sized images; and (2) SampEn2D is computationally expensive for several real-world applications. To overcome these drawbacks, we introduce the two-dimensional dispersion entropy (DispEn2D) measure. To evaluate the ability of DispEn2D, in comparison with SampEn2D, we use various synthetic and real datasets. The results demonstrate that DispEn2D distinguishes different amounts of white Gaussian and salt and pepper noise. The periodic images, compared with their corresponding synthesized ones, have lower DispEn2D values. The results for Kylberg texture dataset show the ability of DispEn2D to differentiate various textures. Although the results based on DispEn2D and SampEn2D for both the synthetic and real datasets are consistent in that they lead to similar findings about the irregularity of images, DispEn2D has three main advantages over SampEn2D: (1) DispEn2D, unlike SampEn2D, does not lead to undefined values; (2) DispEn2D is noticeably quicker; and (3) The coefficient of variations and Mann–Whitney U test-based p-values for DispEn2D are considerably smaller, showing the more stability of the DispEn2D results. Overall, thanks to its successful performance and low computational time, DispEn2D opens up a new way to analyze the uncertainty of images.}
}

@article{Ribeiro2012,
  doi = {https://doi.org/10.1371/journal.pone.0040689},
  author = {Ribeiro, Haroldo V. and Zunino, Luciano and Lenzi, Ervin K. and Santoro, Perseu A. and Mendes, Renio S.},
  journal = {PLOS ONE},
  publisher = {Public Library of Science},
  title = {Complexity-Entropy Causality Plane as a Complexity Measure for Two-Dimensional Patterns},
  year = {2012},
  month = {08},
  volume = {7},
  url = {https://doi.org/10.1371/journal.pone.0040689},
  pages = {1-9},
  abstract = {Complexity measures are essential to understand complex systems and there are numerous definitions to analyze one-dimensional data. However, extensions of these approaches to two or higher-dimensional data, such as images, are much less common. Here, we reduce this gap by applying the ideas of the permutation entropy combined with a relative entropic index. We build up a numerical procedure that can be easily implemented to evaluate the complexity of two or higher-dimensional patterns. We work out this method in different scenarios where numerical experiments and empirical data were taken into account. Specifically, we have applied the method to  fractal landscapes generated numerically where we compare our measures with the Hurst exponent;  liquid crystal textures where nematic-isotropic-nematic phase transitions were properly identified;  12 characteristic textures of liquid crystals where the different values show that the method can distinguish different phases;  and Ising surfaces where our method identified the critical temperature and also proved to be stable.},
  number = {8},

}

@article{Schlemmer2018,
  title={Spatiotemporal permutation entropy as a measure for complexity of cardiac arrhythmia},
  author={Schlemmer, Alexander and Berg, Sebastian and Lilienkamp, Thomas and Luther, Stefan and Parlitz, Ulrich},
  journal={Frontiers in Physics},
  volume={6},
  pages={39},
  year={2018},
  publisher={Frontiers Media SA},
  doi={https://doi.org/10.3389/fphy.2018.00039}
}

@article{Diego2019,
 title = {Transfer entropy computation using the Perron-Frobenius operator},
  author = {Diego, David and Haaga, Kristian Agasøster and Hannisdal, Bjarte},
  journal = {Phys. Rev. E},
  volume = {99},
  issue = {4},
  pages = {042212},
  numpages = {17},
  year = {2019},
  month = {Apr},
  publisher = {American Physical Society},
  doi = {https://doi.org/10.1103/PhysRevE.99.042212},
  url = {https://link.aps.org/doi/10.1103/PhysRevE.99.042212},
}

@article{Hausser2009,
  title={Entropy inference and the James-Stein estimator, with application to nonlinear gene association networks.},
  author={Hausser, Jean and Strimmer, Korbinian},
  journal={Journal of Machine Learning Research},
  volume={10},
  number={7},
  year={2009},
  url={https://jmlr.csail.mit.edu/papers/volume10/hausser09a/hausser09a.pdf}
}

@incollection{JamesStein1992,
  title={Estimation with quadratic loss},
  author={James, William and Stein, Charles},
  booktitle={Breakthroughs in statistics: Foundations and basic theory},
  pages={443--460},
  year={1992},
  publisher={Springer},
  url={https://link.springer.com/chapter/10.1007/978-1-4612-0919-5_30},
}

@article{Schurmann2004,
  title={Bias analysis in entropy estimation},
  author={Schürmann, Thomas},
  journal={Journal of Physics A: Mathematical and General},
  volume={37},
  number={27},
  pages={L295},
  year={2004},
  publisher={IOP Publishing},
  doi={https://doi.org/10.1088/0305-4470/37/27/L02},
}

@article{Amigó2018,
  title={A brief review of generalized entropies},
  author={Amigó, José M and Balogh, Sámuel G and Hernández, Sergio},
  journal={Entropy},
  volume={20},
  number={11},
  pages={813},
  year={2018},
  publisher={MDPI}
}