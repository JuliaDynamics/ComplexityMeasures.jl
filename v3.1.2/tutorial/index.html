<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Tutorial · ComplexityMeasures.jl</title><meta name="title" content="Tutorial · ComplexityMeasures.jl"/><meta property="og:title" content="Tutorial · ComplexityMeasures.jl"/><meta property="twitter:title" content="Tutorial · ComplexityMeasures.jl"/><meta name="description" content="Documentation for ComplexityMeasures.jl."/><meta property="og:description" content="Documentation for ComplexityMeasures.jl."/><meta property="twitter:description" content="Documentation for ComplexityMeasures.jl."/><script data-outdated-warner src="../assets/warner.js"></script><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.050/juliamono.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.8/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL=".."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../assets/documenter.js"></script><script src="../search_index.js"></script><script src="../siteinfo.js"></script><script src="../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../assets/themeswap.js"></script><link href="https://fonts.googleapis.com/css?family=Montserrat|Source+Code+Pro&amp;display=swap" rel="stylesheet" type="text/css"/></head><body><div id="documenter"><nav class="docs-sidebar"><div class="docs-package-name"><span class="docs-autofit"><a href="../">ComplexityMeasures.jl</a></span></div><button class="docs-search-query input is-rounded is-small is-clickable my-2 mx-auto py-1 px-2" id="documenter-search-query">Search docs (Ctrl + /)</button><ul class="docs-menu"><li><a class="tocitem" href="../">ComplexityMeasures.jl</a></li><li class="is-active"><a class="tocitem" href>Tutorial</a><ul class="internal"><li><a class="tocitem" href="#First-things-first:-&quot;complexity-measures&quot;"><span>First things first: &quot;complexity measures&quot;</span></a></li><li><a class="tocitem" href="#The-basics:-probabilities-and-outcome-spaces"><span>The basics: probabilities and outcome spaces</span></a></li><li><a class="tocitem" href="#Beyond-the-basics:-probabilities-*estimators*"><span>Beyond the basics: probabilities <em>estimators</em></span></a></li><li><a class="tocitem" href="#Entropies"><span>Entropies</span></a></li><li><a class="tocitem" href="#Beyond-Shannon:-more-entropies"><span>Beyond Shannon: more entropies</span></a></li><li><a class="tocitem" href="#Beyond-entropies:-discrete-estimators"><span>Beyond entropies: discrete estimators</span></a></li><li><a class="tocitem" href="#Beyond-entropies:-other-information-measures"><span>Beyond entropies: other information measures</span></a></li><li><a class="tocitem" href="#Beyond-discrete:-differential-or-continuous"><span>Beyond discrete: differential or continuous</span></a></li><li><a class="tocitem" href="#Beyond-information:-other-complexity-measures"><span>Beyond <code>information</code>: other complexity measures</span></a></li></ul></li><li><a class="tocitem" href="../probabilities/">Probabilities</a></li><li><a class="tocitem" href="../information_measures/">Information measures (entropies and co.)</a></li><li><a class="tocitem" href="../complexity/">Complexity measures</a></li><li><a class="tocitem" href="../convenience/">Convenience functions</a></li><li><a class="tocitem" href="../examples/">ComplexityMeasures.jl Examples</a></li><li><a class="tocitem" href="../devdocs/">ComplexityMeasures.jl Dev Docs</a></li><li><a class="tocitem" href="../references/">References</a></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><a class="docs-sidebar-button docs-navbar-link fa-solid fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a><nav class="breadcrumb"><ul class="is-hidden-mobile"><li class="is-active"><a href>Tutorial</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>Tutorial</a></li></ul></nav><div class="docs-right"><a class="docs-navbar-link" href="https://github.com/JuliaDynamics/ComplexityMeasures.jl" title="View the repository on GitHub"><span class="docs-icon fa-brands"></span><span class="docs-label is-hidden-touch">GitHub</span></a><a class="docs-navbar-link" href="https://github.com/JuliaDynamics/ComplexityMeasures.jl/blob/main/docs/src/tutorial.jl" title="Edit source on GitHub"><span class="docs-icon fa-solid"></span></a><a class="docs-settings-button docs-navbar-link fa-solid fa-gear" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-article-toggle-button fa-solid fa-chevron-up" id="documenter-article-toggle-button" href="javascript:;" title="Collapse all docstrings"></a></div></header><article class="content" id="documenter-page"><h1 id="Tutorial"><a class="docs-heading-anchor" href="#Tutorial">Tutorial</a><a id="Tutorial-1"></a><a class="docs-heading-anchor-permalink" href="#Tutorial" title="Permalink"></a></h1><p>The goal of this tutorial is threefold:</p><ol><li>To convey the <em>terminology</em> used by ComplexityMeasures.jl: key terms, what they mean, and how they are used within the codebase.</li><li>To provide a <em>rough overview</em> of the overall features provided by ComplexityMeasures.jl.</li><li>To introduce the <em>main API functions</em> of ComplexityMeasures.jl in a single, self-contained document: how these functions connect to key terms, what are their main inputs and outputs, and how they are used in realistic scientific scripting.</li></ol><div class="admonition is-info"><header class="admonition-header">Note</header><div class="admonition-body"><p>The documentation and exposition of ComplexityMeasures.jl is inspired by chapter 5 of <a href="https://link.springer.com/book/10.1007/978-3-030-91032-7">Nonlinear Dynamics</a>, Datseris &amp; Parlitz, Springer 2022 (<a href="../references/#Datseris2022">Datseris and Parlitz, 2022</a>), and expanded to cover more content.</p></div></div><h2 id="First-things-first:-&quot;complexity-measures&quot;"><a class="docs-heading-anchor" href="#First-things-first:-&quot;complexity-measures&quot;">First things first: &quot;complexity measures&quot;</a><a id="First-things-first:-&quot;complexity-measures&quot;-1"></a><a class="docs-heading-anchor-permalink" href="#First-things-first:-&quot;complexity-measures&quot;" title="Permalink"></a></h2><p>&quot;Complexity measure&quot; is a generic, umbrella term, used extensively in the nonlinear timeseries analysis (NLTS) literature. Roughly speaking, a complexity measure is a quantity extracted from input data that quantifies some dynamical property in the data (often, complexity measures are entropy variants). These complexity measures can highlight some aspects of the dynamics more than others, or distinguish one type of dynamics from another, or classify timeseries into classes with different dynamics, among other things. Typically, more &quot;complex&quot; data have higher complexity measure value. ComplexityMeasures.jl implements hundreds such measures, and hence it is named as such. To enable this, ComplexityMeasures.jl is more than a collection &quot;dynamic statistics&quot;: it is also a framework for rigorously defining probability spaces and estimating probabilities from input data.</p><p>Within the codebase of ComplexityMeasures.jl we make a separation with the functions <a href="../information_measures/#ComplexityMeasures.information-Tuple{InformationMeasure, OutcomeSpace, Any}"><code>information</code></a> (or its daughter function <a href="../information_measures/#ComplexityMeasures.entropy"><code>entropy</code></a>) and <a href="../complexity/#ComplexityMeasures.complexity"><code>complexity</code></a>. We use <code>information</code> for complexity measures that are <em>explicit functionals of probability mass or probability density functions</em>, even though these measures might not be labelled as &quot;information measures&quot; in the literature. We use <code>complexity</code> for other complexity measures that are not explicit functionals of probabilities. We stress that the separation between <code>information</code> and <code>complexity</code> is purely pragmatic, to establish a generic and extendable software interface within ComplexityMeasures.jl.</p><h2 id="The-basics:-probabilities-and-outcome-spaces"><a class="docs-heading-anchor" href="#The-basics:-probabilities-and-outcome-spaces">The basics: probabilities and outcome spaces</a><a id="The-basics:-probabilities-and-outcome-spaces-1"></a><a class="docs-heading-anchor-permalink" href="#The-basics:-probabilities-and-outcome-spaces" title="Permalink"></a></h2><p>Information measures and some other complexity measures are computed based on <strong>probabilities</strong> derived from input data. In order to derive probabilities from data, an <strong>outcome space</strong> (also called a sample space) needs to be defined: a way to transform data into elements <span>$\omega$</span> of an outcome space <span>$\omega \in \Omega$</span>, and assign probabilities to each outcome <span>$p(\omega)$</span>, such that <span>$p(\Omega)=1$</span>. <span>$\omega$</span> are called <em>outcomes</em> or <em>events</em>. In code, outcome spaces are subtypes of <a href="../probabilities/#ComplexityMeasures.OutcomeSpace"><code>OutcomeSpace</code></a>. For example, one outcome space is the <a href="../probabilities/#ComplexityMeasures.ValueBinning"><code>ValueBinning</code></a>, which is the most commonly known outcome space, and corresponds to discretizing data by putting the data values into bins of a specific size.</p><pre><code class="language-julia hljs">using ComplexityMeasures

x = randn(10_000)
ε = 0.1 # bin width
o = ValueBinning(ε)
o isa OutcomeSpace</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">true</code></pre><p>Such outcome spaces may be given to <a href="../probabilities/#ComplexityMeasures.probabilities_and_outcomes"><code>probabilities_and_outcomes</code></a> to estimate the probabilities and corresponding outcomes from input data:</p><pre><code class="language-julia hljs">probs, outs = probabilities_and_outcomes(o, x);
probs</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi"><span class="sgr33"> Probabilities{Float64,1} over 71 outcomes</span>
 <span class="sgr90">[-3.432979142018276]</span>   0.0001
 <span class="sgr90">[-3.3329791420182757]</span>  0.0001
 <span class="sgr90">[-3.2329791420182756]</span>  0.0001
 <span class="sgr90">[-3.132979142018276]</span>   0.0002
 <span class="sgr90">[-3.032979142018276]</span>   0.0008
 <span class="sgr90">[-2.932979142018276]</span>   0.0005
 <span class="sgr90">[-2.8329791420182757]</span>  0.001
 <span class="sgr90">[-2.7329791420182756]</span>  0.0018
 <span class="sgr90">[-2.632979142018276]</span>   0.0013
 <span class="sgr90">[-2.532979142018276]</span>   0.002
 ⋮                      
 <span class="sgr90">[2.7670208579817244]</span>   0.0007
 <span class="sgr90">[2.867020857981725]</span>    0.0004
 <span class="sgr90">[2.9670208579817245]</span>   0.0002
 <span class="sgr90">[3.067020857981724]</span>    0.0003
 <span class="sgr90">[3.1670208579817247]</span>   0.0002
 <span class="sgr90">[3.2670208579817244]</span>   0.0001
 <span class="sgr90">[3.367020857981725]</span>    0.0002
 <span class="sgr90">[3.4670208579817245]</span>   0.0001
 <span class="sgr90">[3.9670208579817245]</span>   0.0001</code></pre><p>In this example the probabilities are the (normalized) heights of each bin of the histogram. The bins, which are the <em>elements</em> of the outcome space, are shown in the margin, left of the probabilities. They are also returned explicitly as <code>outs</code> above.</p><p>This convenience printing syntax with outcomes and probabilities is useful for visual inspection of the probabilities data. However, don&#39;t let it worry you. Probabilities are returned as a special <a href="../probabilities/#ComplexityMeasures.Probabilities"><code>Probabilities</code></a> type that behaves identically to a standard Julia numerical <code>Vector</code>. You can obtain the maximum probability</p><pre><code class="language-julia hljs">maximum(probs)</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">0.041</code></pre><p>or iterate over the probabilities</p><pre><code class="language-julia hljs">function total(probs)
    t = 0.0
    for p in probs
        t += p
    end
    return t
end

total(probs)</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">0.9999999999999999</code></pre><p>Notice that if you use <a href="../probabilities/#ComplexityMeasures.probabilities"><code>probabilities</code></a> instead of <a href="../probabilities/#ComplexityMeasures.probabilities_and_outcomes"><code>probabilities_and_outcomes</code></a>, then outcomes are enumerated generically. This avoids computing outcomes explicitly, and can save some computation time in cases where you don&#39;t need the outcomes.</p><pre><code class="language-julia hljs">probs2 = probabilities(o, x)</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi"><span class="sgr33"> Probabilities{Float64,1} over 71 outcomes</span>
  <span class="sgr90">Outcome(1)</span>  0.0001
  <span class="sgr90">Outcome(2)</span>  0.0001
  <span class="sgr90">Outcome(3)</span>  0.0001
  <span class="sgr90">Outcome(4)</span>  0.0002
  <span class="sgr90">Outcome(5)</span>  0.0008
  <span class="sgr90">Outcome(6)</span>  0.0005
  <span class="sgr90">Outcome(7)</span>  0.001
  <span class="sgr90">Outcome(8)</span>  0.0018
  <span class="sgr90">Outcome(9)</span>  0.0013
 <span class="sgr90">Outcome(10)</span>  0.002
           ⋮  
 <span class="sgr90">Outcome(63)</span>  0.0007
 <span class="sgr90">Outcome(64)</span>  0.0004
 <span class="sgr90">Outcome(65)</span>  0.0002
 <span class="sgr90">Outcome(66)</span>  0.0003
 <span class="sgr90">Outcome(67)</span>  0.0002
 <span class="sgr90">Outcome(68)</span>  0.0001
 <span class="sgr90">Outcome(69)</span>  0.0002
 <span class="sgr90">Outcome(70)</span>  0.0001
 <span class="sgr90">Outcome(71)</span>  0.0001</code></pre><p>Of course, if the outcomes are obtained for free while estimating the probabilities, they would be included in the return value.</p><p>For the <code>ValueBinning</code> example that we use, the outcomes are the left edges of each bin. This allows us to straightforwardly visualize the results.</p><pre><code class="language-julia hljs">using CairoMakie
outs = outcomes(probs);
left_edges = only.(outs) # convert `Vector{SVector}` into `Vector{Real}`
barplot(left_edges, probs; axis = (ylabel = &quot;probability&quot;,))</code></pre><img src="8a448f3f.png" alt="Example block output"/><p>Naturally, there are other outcome spaces one may use, and one can find the list of implemented ones in <a href="../probabilities/#ComplexityMeasures.OutcomeSpace"><code>OutcomeSpace</code></a>. A prominent example used in the NLTS literature are ordinal patterns. The outcome space for it is <a href="../probabilities/#ComplexityMeasures.OrdinalPatterns"><code>OrdinalPatterns</code></a>, and can be particularly useful with timeseries that come from nonlinear dynamical systems. For example, let&#39;s simulate a logistic map timeseries.</p><pre><code class="language-julia hljs">using DynamicalSystemsBase

logistic_rule(u, r, t) = SVector(r*u[1]*(1 - u[1]))
ds = DeterministicIteratedMap(logistic_rule, [0.4], 4.0)
Y, t = trajectory(ds, 10_000; Ttr = 100)
y = Y[:, 1]
summary(y)</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">&quot;10001-element Vector{Float64}&quot;</code></pre><p>We can then estimate the probabilities corresponding to the ordinal patterns of a certain length (here we use <code>m = 3</code>).</p><pre><code class="language-julia hljs">o = OrdinalPatterns{3}()
probsy = probabilities(o, y)</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi"><span class="sgr33"> Probabilities{Float64,1} over 5 outcomes</span>
 <span class="sgr90">[1, 2, 3]</span>  0.33973397339733974
 <span class="sgr90">[1, 3, 2]</span>  0.06440644064406441
 <span class="sgr90">[2, 1, 3]</span>  0.13051305130513052
 <span class="sgr90">[2, 3, 1]</span>  0.1996199619961996
 <span class="sgr90">[3, 1, 2]</span>  0.26572657265726574</code></pre><p>Comparing these probabilities with those for the purely random timeseries <code>x</code>,</p><pre><code class="language-julia hljs">probsx = probabilities(o, x)</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi"><span class="sgr33"> Probabilities{Float64,1} over 6 outcomes</span>
 <span class="sgr90">[1, 2, 3]</span>  0.17163432686537308
 <span class="sgr90">[1, 3, 2]</span>  0.16643328665733145
 <span class="sgr90">[2, 1, 3]</span>  0.16403280656131225
 <span class="sgr90">[2, 3, 1]</span>  0.17003400680136027
 <span class="sgr90">[3, 1, 2]</span>  0.16753350670134026
 <span class="sgr90">[3, 2, 1]</span>  0.16033206641328265</code></pre><p>you will notice that the probabilities computing from <code>x</code> has six outcomes, while the probabilities computed from the timeseries <code>y</code> has five outcomes.</p><p>The reason that there are less outcomes in the <code>y</code> is because one outcome was never encountered in the <code>y</code> data. This is a common theme in ComplexityMeasures.jl: outcomes that are not in the data are skipped. This can save memory for outcome spaces with large numbers of outcomes. To explicitly obtain all outcomes, by assigning 0 probability to not encountered outcomes, use <a href="../probabilities/#ComplexityMeasures.allprobabilities_and_outcomes"><code>allprobabilities_and_outcomes</code></a>. For <a href="../probabilities/#ComplexityMeasures.OrdinalPatterns"><code>OrdinalPatterns</code></a> the outcome space does not depend on input data and is always the same. Hence, the corresponding outcomes matching to <a href="../probabilities/#ComplexityMeasures.allprobabilities_and_outcomes"><code>allprobabilities_and_outcomes</code></a>, coincide for <code>x</code> and <code>y</code>, and also coincide with the output of the function <a href="../probabilities/#ComplexityMeasures.outcome_space"><code>outcome_space</code></a>:</p><pre><code class="language-julia hljs">o = OrdinalPatterns()
probsx = allprobabilities(o, x)
probsy = allprobabilities(o, y)
outsx = outsy = outcome_space(o)
# display all quantities as parallel columns
hcat(outsx, probsx, probsy)</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">6×3 Matrix{Any}:
 [1, 2, 3]  0.171634  0.339734
 [1, 3, 2]  0.166433  0.0644064
 [2, 1, 3]  0.164033  0.130513
 [2, 3, 1]  0.170034  0.19962
 [3, 1, 2]  0.167534  0.265727
 [3, 2, 1]  0.160332  0.0</code></pre><p>The number of <em>possible</em> outcomes, i.e., the cardinality of the outcome space, can always be found using <a href="../probabilities/#ComplexityMeasures.total_outcomes"><code>total_outcomes</code></a>:</p><pre><code class="language-julia hljs">total_outcomes(o)</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">6</code></pre><h2 id="Beyond-the-basics:-probabilities-*estimators*"><a class="docs-heading-anchor" href="#Beyond-the-basics:-probabilities-*estimators*">Beyond the basics: probabilities <em>estimators</em></a><a id="Beyond-the-basics:-probabilities-*estimators*-1"></a><a class="docs-heading-anchor-permalink" href="#Beyond-the-basics:-probabilities-*estimators*" title="Permalink"></a></h2><p>So far we have been estimating probabilities by counting the amount of times each possible outcome was encountered in the data, then normalizing. This is called &quot;maximum likelihood estimation&quot; of probabilities. To get the counts themselves, use the <a href="../probabilities/#ComplexityMeasures.counts"><code>counts</code></a> or <a href="../probabilities/#ComplexityMeasures.counts_and_outcomes"><code>counts_and_outcomes</code></a> function.</p><pre><code class="language-julia hljs">countsy = counts(o, y)</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi"><span class="sgr33"> Counts{Int64,1} over 5 outcomes</span>
 <span class="sgr90">Outcome(1)</span>  3397
 <span class="sgr90">Outcome(2)</span>   644
 <span class="sgr90">Outcome(3)</span>  1305
 <span class="sgr90">Outcome(4)</span>  1996
 <span class="sgr90">Outcome(5)</span>  2657</code></pre><p>Counts are printed like <code>Probabilities</code>: they display the outcomes they match to on the left marginal, but otherwise can be used as standard Julia numerical <code>Vector</code>s. To go from outcomes to probabilities, we divide with the total:</p><pre><code class="language-julia hljs">probsy = probabilities(o, y)
outsy = outcomes(probsy)
hcat(outsy, countsy, countsy ./ sum(countsy), probsy)</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">5×4 Matrix{Any}:
 [1, 2, 3]  3397  0.339734   0.339734
 [1, 3, 2]   644  0.0644064  0.0644064
 [2, 1, 3]  1305  0.130513   0.130513
 [2, 3, 1]  1996  0.19962    0.19962
 [3, 1, 2]  2657  0.265727   0.265727</code></pre><p>By definition, columns 3 and 4 are identical. However, there are other ways to estimate probabilities that may account for biases in counting outcomes from finite data. Alternative estimators for probabilities are subtypes of <a href="../probabilities/#ComplexityMeasures.ProbabilitiesEstimator"><code>ProbabilitiesEstimator</code></a>. <code>ProbabilitiesEstimator</code>s  dictate alternative ways to estimate probabilities, given some outcome space and unput data. For example, one could use <a href="../probabilities/#ComplexityMeasures.BayesianRegularization"><code>BayesianRegularization</code></a>.</p><pre><code class="language-julia hljs">probsy_bayes = probabilities(BayesianRegularization(), o, y)

probsy_bayes .- probsy</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">5-element Vector{Float64}:
 -6.98390510782132e-5
  6.776967180924243e-5
  3.472958251440894e-5
  1.8994302469765856e-7
 -3.285014627013583e-5</code></pre><p>While the corrections of <a href="../probabilities/#ComplexityMeasures.BayesianRegularization"><code>BayesianRegularization</code></a> are small in this case, they are nevertheless measurable.</p><p>When calling <a href="../probabilities/#ComplexityMeasures.probabilities"><code>probabilities</code></a> only with an outcome space instance and some input data (skipping the <a href="../probabilities/#ComplexityMeasures.ProbabilitiesEstimator"><code>ProbabilitiesEstimator</code></a>), then by default, the <a href="../probabilities/#ComplexityMeasures.RelativeAmount"><code>RelativeAmount</code></a> probabilities estimator is used to extract the probabilities.</p><h2 id="Entropies"><a class="docs-heading-anchor" href="#Entropies">Entropies</a><a id="Entropies-1"></a><a class="docs-heading-anchor-permalink" href="#Entropies" title="Permalink"></a></h2><p>Many compexity measures are a straightforward estimation of Shannon entropy, computed over probabilities estimated from data over some particular outcome space. For example, the well known <em>permutation entropy</em> (<a href="../references/#BandtPompe2002">Bandt and Pompe, 2002</a>) is exactly the Shannon entropy of the probabilities <code>probsy</code> we computed above based on ordinal patterns. To compute it, we use the <a href="../information_measures/#ComplexityMeasures.entropy"><code>entropy</code></a> function.</p><pre><code class="language-julia hljs">perm_ent_x = entropy(OrdinalPatterns(), x)
perm_ent_y = entropy(OrdinalPatterns(), y)
(perm_ent_x, perm_ent_y)</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">(2.584598406969109, 2.139506127185978)</code></pre><p>As expected, the permutation entropy of the <code>x</code> signal is higher, because the signal is &quot;more random&quot;. Moreover, since we have estimated the probabilities already, we could have passed these to the entropy function directly instead of recomputing them as above</p><pre><code class="language-julia hljs">perm_ent_y_2 = entropy(probsy)</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">2.139506127185978</code></pre><p>We crucially realize here that many quantities in the NLTS literature that are named as entropies, such as &quot;permutation entropy&quot;, are <em>not really new entropies</em>. They are the good old Shannon entropy (<a href="../information_measures/#ComplexityMeasures.Shannon"><code>Shannon</code></a>), but calculated with <em>new outcome spaces</em> that smartly quantify some dynamic property in the data. Nevertheless, we acknowledge that names such as &quot;permutation entropy&quot; are commonplace, so in ComplexityMeasures.jl we provide convenience functions like <a href="../convenience/#ComplexityMeasures.entropy_permutation"><code>entropy_permutation</code></a>. More convenience functions can be found in the <a href="../convenience/#convenience">convenience</a> documentation page.</p><h2 id="Beyond-Shannon:-more-entropies"><a class="docs-heading-anchor" href="#Beyond-Shannon:-more-entropies">Beyond Shannon: more entropies</a><a id="Beyond-Shannon:-more-entropies-1"></a><a class="docs-heading-anchor-permalink" href="#Beyond-Shannon:-more-entropies" title="Permalink"></a></h2><p>Just like there are different outcome spaces, the same concept applies to entropy. There are many <em>fundamentally different</em> entropies. Shannon entropy is not the only one, just the one used most often. Each entropy is a subtype of <a href="../information_measures/#ComplexityMeasures.InformationMeasure"><code>InformationMeasure</code></a>. Another commonly used entropy is the <a href="../information_measures/#ComplexityMeasures.Renyi"><code>Renyi</code></a> or generalized entropy. We can use <a href="../information_measures/#ComplexityMeasures.Renyi"><code>Renyi</code></a> as an additional first argument to the <a href="../information_measures/#ComplexityMeasures.entropy"><code>entropy</code></a> function to estimate the generalized entropy:</p><pre><code class="language-julia hljs">perm_ent_y_q2 = entropy(Renyi(q = 2.0), OrdinalPatterns(), y)
(perm_ent_y_q2, perm_ent_y)</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">(2.0170680478970375, 2.139506127185978)</code></pre><p>In fact, when we called <code>entropy(OrdinalPatterns(), y)</code>, this dispatched to the default call of <code>entropy(Shannon(), OrdinalPatterns(), y)</code>.</p><h2 id="Beyond-entropies:-discrete-estimators"><a class="docs-heading-anchor" href="#Beyond-entropies:-discrete-estimators">Beyond entropies: discrete estimators</a><a id="Beyond-entropies:-discrete-estimators-1"></a><a class="docs-heading-anchor-permalink" href="#Beyond-entropies:-discrete-estimators" title="Permalink"></a></h2><p>The estimation of an entropy truly parallelizes the estimation of probabilities: in the latter, we could decide an outcome space <em>and</em> an <em>estimator</em> to estimate probabilities. The same happens for entropy: we can decide an entropy definition and an <em>estimator</em> of how to estimate the entropy. For example, instead of the default <a href="../information_measures/#ComplexityMeasures.PlugIn"><code>PlugIn</code></a> estimator that we used above implicitly, we could use the <a href="../information_measures/#ComplexityMeasures.Jackknife"><code>Jackknife</code></a> estimator.</p><pre><code class="language-julia hljs">ospace = OrdinalPatterns()
entdef = Renyi(q = 2.0)
entest = Jackknife(entdef)
perm_ent_y_q2_jack = entropy(entest, ospace, y)

(perm_ent_y_q2, perm_ent_y_q2_jack)</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">(2.0170680478970375, 1.2173088128474774)</code></pre><p>Entropy estimators always reference an entropy definition, even if they only apply to one type of entropy (typically the Shannon one). From here, it is up to the researcher to read the documentation of the plethora of estimators implemented and decide what is most suitable for their data at hand. They all can be found in <a href="../information_measures/#ComplexityMeasures.DiscreteInfoEstimator"><code>DiscreteInfoEstimator</code></a>.</p><h2 id="Beyond-entropies:-other-information-measures"><a class="docs-heading-anchor" href="#Beyond-entropies:-other-information-measures">Beyond entropies: other information measures</a><a id="Beyond-entropies:-other-information-measures-1"></a><a class="docs-heading-anchor-permalink" href="#Beyond-entropies:-other-information-measures" title="Permalink"></a></h2><p>Recall that at the very beginning of this notebook we mentioned a code separation of <a href="../information_measures/#ComplexityMeasures.information-Tuple{InformationMeasure, OutcomeSpace, Any}"><code>information</code></a> and <a href="../complexity/#ComplexityMeasures.complexity"><code>complexity</code></a>. We did this because there are other measures, besides entropy, that are explicit functionals of some probability mass function. One example is the Shannon <em>extropy</em> <a href="../information_measures/#ComplexityMeasures.ShannonExtropy"><code>ShannonExtropy</code></a>, the complementary dual of <em>entropy</em>, which could be computed as follows.</p><pre><code class="language-julia hljs">extdef = ShannonExtropy()
perm_ext_y = information(extdef, ospace, y)</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">1.2450281736700084</code></pre><p>Just like the Shannon <em>entropy</em>, the extropy could also be estimated with a different estimator such as <a href="../information_measures/#ComplexityMeasures.Jackknife"><code>Jackknife</code></a>.</p><pre><code class="language-julia hljs">perm_ext_y_jack = information(Jackknife(extdef), ospace, y)</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">1.126229249906828</code></pre><p>In truth, when we called <code>entropy(e, o, y)</code> it actually calls <code>information(e, o, y)</code>, as all &quot;information measures&quot; are part of the same function interface. And entropy is an information measure.</p><h2 id="Beyond-discrete:-differential-or-continuous"><a class="docs-heading-anchor" href="#Beyond-discrete:-differential-or-continuous">Beyond discrete: differential or continuous</a><a id="Beyond-discrete:-differential-or-continuous-1"></a><a class="docs-heading-anchor-permalink" href="#Beyond-discrete:-differential-or-continuous" title="Permalink"></a></h2><p>Discrete information measures are functions of probability mass functions. It is also possible to compute information measures of probability density functions. In ComplexityMeasures.jl, this is done by calling <a href="../information_measures/#ComplexityMeasures.entropy"><code>entropy</code></a> (or the more general <a href="../information_measures/#ComplexityMeasures.information-Tuple{InformationMeasure, OutcomeSpace, Any}"><code>information</code></a>) with a differential information estimator, a subtype of <a href="../information_measures/#ComplexityMeasures.DifferentialInfoEstimator"><code>DifferentialInfoEstimator</code></a>. These estimators are given directly to <a href="../information_measures/#ComplexityMeasures.information-Tuple{InformationMeasure, OutcomeSpace, Any}"><code>information</code></a> without assigning an outcome space, because the probability density is approximated implicitly, not explicitly. For example, the <a href="../information_measures/#ComplexityMeasures.Correa"><code>Correa</code></a> estimator approximates the differential Shannon entropy by utilizing order statistics of the timeseries data:</p><pre><code class="language-julia hljs">diffest = Correa()
diffent = entropy(diffest, x)</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">1.7905539162076674</code></pre><h2 id="Beyond-information:-other-complexity-measures"><a class="docs-heading-anchor" href="#Beyond-information:-other-complexity-measures">Beyond <code>information</code>: other complexity measures</a><a id="Beyond-information:-other-complexity-measures-1"></a><a class="docs-heading-anchor-permalink" href="#Beyond-information:-other-complexity-measures" title="Permalink"></a></h2><p>As discussed at the very beginning of this tutorial, there are some complexity measures that are not explicit functionals of probabilities, and hence cannot be straightforwardly related to an outcome space, in the sense of providing an instance of <a href="../probabilities/#ComplexityMeasures.OutcomeSpace"><code>OutcomeSpace</code></a> to the estimation function. These are estimated with the <a href="../complexity/#ComplexityMeasures.complexity"><code>complexity</code></a> function, by providing it a subtype of <a href="../complexity/#ComplexityMeasures.ComplexityEstimator"><code>ComplexityEstimator</code></a>. An example here is the well-known <em>sample entropy</em> (which isn&#39;t actually an entropy in the formal mathematical sense). It can be computed as follows.</p><pre><code class="language-julia hljs">complest = SampleEntropy(r = 0.1)
sampent = complexity(complest, y)</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">0.612355215298896</code></pre></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../">« ComplexityMeasures.jl</a><a class="docs-footer-nextpage" href="../probabilities/">Probabilities »</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option><option value="auto">Automatic (OS)</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 1.2.1 on <span class="colophon-date" title="Sunday 14 January 2024 13:21">Sunday 14 January 2024</span>. Using Julia version 1.10.0.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
