<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>References · ComplexityMeasures.jl</title><meta name="title" content="References · ComplexityMeasures.jl"/><meta property="og:title" content="References · ComplexityMeasures.jl"/><meta property="twitter:title" content="References · ComplexityMeasures.jl"/><meta name="description" content="Documentation for ComplexityMeasures.jl."/><meta property="og:description" content="Documentation for ComplexityMeasures.jl."/><meta property="twitter:description" content="Documentation for ComplexityMeasures.jl."/><script data-outdated-warner src="../assets/warner.js"></script><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.050/juliamono.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.8/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL=".."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../assets/documenter.js"></script><script src="../search_index.js"></script><script src="../siteinfo.js"></script><script src="../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../assets/themeswap.js"></script><link href="https://fonts.googleapis.com/css?family=Montserrat|Source+Code+Pro&amp;display=swap" rel="stylesheet" type="text/css"/></head><body><div id="documenter"><nav class="docs-sidebar"><div class="docs-package-name"><span class="docs-autofit"><a href="../">ComplexityMeasures.jl</a></span></div><button class="docs-search-query input is-rounded is-small is-clickable my-2 mx-auto py-1 px-2" id="documenter-search-query">Search docs (Ctrl + /)</button><ul class="docs-menu"><li><a class="tocitem" href="../">ComplexityMeasures.jl</a></li><li><a class="tocitem" href="../tutorial/">Tutorial</a></li><li><a class="tocitem" href="../probabilities/">Probabilities</a></li><li><a class="tocitem" href="../information_measures/">Information measures (entropies and co.)</a></li><li><a class="tocitem" href="../complexity/">Complexity measures</a></li><li><a class="tocitem" href="../convenience/">Convenience functions</a></li><li><a class="tocitem" href="../examples/">ComplexityMeasures.jl Examples</a></li><li><a class="tocitem" href="../devdocs/">ComplexityMeasures.jl Dev Docs</a></li><li class="is-active"><a class="tocitem" href>References</a></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><a class="docs-sidebar-button docs-navbar-link fa-solid fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a><nav class="breadcrumb"><ul class="is-hidden-mobile"><li class="is-active"><a href>References</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>References</a></li></ul></nav><div class="docs-right"><a class="docs-navbar-link" href="https://github.com/JuliaDynamics/ComplexityMeasures.jl" title="View the repository on GitHub"><span class="docs-icon fa-brands"></span><span class="docs-label is-hidden-touch">GitHub</span></a><a class="docs-navbar-link" href="https://github.com/JuliaDynamics/ComplexityMeasures.jl/blob/main/docs/src/references.md" title="Edit source on GitHub"><span class="docs-icon fa-solid"></span></a><a class="docs-settings-button docs-navbar-link fa-solid fa-gear" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-article-toggle-button fa-solid fa-chevron-up" id="documenter-article-toggle-button" href="javascript:;" title="Collapse all docstrings"></a></div></header><article class="content" id="documenter-page"><h1 id="References"><a class="docs-heading-anchor" href="#References">References</a><a id="References-1"></a><a class="docs-heading-anchor-permalink" href="#References" title="Permalink"></a></h1><div class="citation canonical"><ul><li><div id="Alizadeh2010">Alizadeh, N. H. and Arghami, N. R. (2010). <a href="http://jirss.irstat.ir/article-1-81-en.pdf"><em>A new estimator of entropy</em></a>. Journal of the Iranian Statistical Society (JIRSS).</div></li><li><div id="Amigó2018">Amigó, J. M.; Balogh, S. G. and Hernández, S. (2018). <em>A brief review of generalized entropies</em>. Entropy <strong>20</strong>, 813.</div></li><li><div id="Amigó2004">Amigó, J. M.; Szczepański, J.; Wajnryb, E. and Sanchez-Vives, M. V. (2004). <a href="https://doi.org/10.1162/089976604322860677"><em>Estimating the Entropy Rate of Spike Trains via Lempel-Ziv Complexity</em></a>. <a href="https://doi.org/10.1162/089976604322860677">Neural Computation <strong>16</strong>, 717–736</a>, <a href="https://arxiv.org/abs/https://direct.mit.edu/neco/article-pdf/16/4/717/815838/089976604322860677.pdf">arXiv:https://direct.mit.edu/neco/article-pdf/16/4/717/815838/089976604322860677.pdf</a>.</div></li><li><div id="Anteneodo1999">Anteneodo, C. and Plastino, A. R. (1999). <a href="https://dx.doi.org/10.1088/0305-4470/32/7/002"><em>Maximum entropy approach to stretched exponential probability distributions</em></a>. <a href="https://doi.org/10.1088/0305-4470/32/7/002">Journal of Physics A: Mathematical and General <strong>32</strong>, 1089</a>.</div></li><li><div id="Arora2022">Arora, A.; Meister, C. and Cotterell, R. (2022). <a href="https://arxiv.org/abs/2204.01469"><em>Estimating the Entropy of Linguistic Distributions</em></a>, arXiv, <a href="https://arxiv.org/abs/2204.01469">arXiv:2204.01469 [cs.CL]</a>.</div></li><li><div id="Azami2016">Azami, H. and Escudero, J. (2016). <a href="https://www.sciencedirect.com/science/article/pii/S0169260715301152"><em>Amplitude-aware permutation entropy: Illustration in spike detection and signal segmentation</em></a>. <a href="https://doi.org/10.1016/j.cmpb.2016.02.008">Computer Methods and Programs in Biomedicine <strong>128</strong>, 40–51</a>.</div></li><li><div id="Azami2019">Azami, H.; da Silva, L. E.; Omoto, A. C. and Humeau-Heurtier, A. (2019). <a href="https://www.sciencedirect.com/science/article/pii/S0923596519300682"><em>Two-dimensional dispersion entropy: An information-theoretic method for irregularity analysis of images</em></a>. <a href="https://doi.org/10.1016/j.image.2019.04.013">Signal Processing: Image Communication <strong>75</strong>, 178–187</a>.</div></li><li><div id="BandtPompe2002">Bandt, C. and Pompe, B. (2002). <a href="https://link.aps.org/doi/10.1103/PhysRevLett.88.174102"><em>Permutation Entropy: A Natural Complexity Measure for Time Series</em></a>. <a href="https://doi.org/10.1103/PhysRevLett.88.174102">Phys. Rev. Lett. <strong>88</strong>, 174102</a>.</div></li><li><div id="Berger2019">Berger, S.; Kravtsiv, A.; Schneider, G. and Jordan, D. (2019). <a href="https://www.mdpi.com/1099-4300/21/10/1023"><em>Teaching Ordinal Patterns to a Computer: Efficient Encoding Algorithms Based on the Lehmer Code</em></a>. <a href="https://doi.org/10.3390/e21101023">Entropy <strong>21</strong></a>.</div></li><li><div id="Chao2003">Chao, A. and Shen, T.-J. (2003). <a href="https://doi.org/10.1023/A:1026096204727"><em>Nonparametric estimation of Shannon&#39;s index of diversity when there are unseen species in sample</em></a>. <a href="https://doi.org/10.1023/A:1026096204727">Environmental and Ecological Statistics <strong>10</strong>, 429–443</a>.</div></li><li><div id="Charzyńska2015">Charzyńska, A. and Gambin, A. (2016). <a href="https://www.mdpi.com/1099-4300/18/1/13"><em>Improvement of the k-nn Entropy Estimator with Applications in Systems Biology</em></a>. <a href="https://doi.org/10.3390/e18010013">Entropy <strong>18</strong></a>.</div></li><li><div id="Correa1995">Correa, J. C. (1995). <a href="https://doi.org/10.1080/03610929508831626"><em>A new estimator of entropy</em></a>. <a href="https://doi.org/10.1080/03610929508831626">Communications in Statistics - Theory and Methods <strong>24</strong>, 2439–2449</a>, <a href="https://arxiv.org/abs/https://doi.org/10.1080/03610929508831626">arXiv:https://doi.org/10.1080/03610929508831626</a>.</div></li><li><div id="Curado2004">Curado, E. M. and Nobre, F. D. (2004). <a href="https://www.sciencedirect.com/science/article/pii/S0378437103011889"><em>On the stability of analytic entropic forms</em></a>. <a href="https://doi.org/10.1016/j.physa.2003.12.026">Physica A: Statistical Mechanics and its Applications <strong>335</strong>, 94–106</a>.</div></li><li><div id="Datseris2022">Datseris, G. and Parlitz, U. (2022). <a href="https://link.springer.com/book/10.1007/978-3-030-91032-7"><em>Nonlinear dynamics: a concise introduction interlaced with code</em></a> (<a href="https://doi.org/10.1007/978-3-030-91032-7">Springer Nature</a>).</div></li><li><div id="Diego2019">Diego, D.; Haaga, K. A. and Hannisdal, B. (2019). <a href="https://link.aps.org/doi/10.1103/PhysRevE.99.042212"><em>Transfer entropy computation using the Perron-Frobenius operator</em></a>. <a href="https://doi.org/10.1103/PhysRevE.99.042212">Phys. Rev. E <strong>99</strong>, 042212</a>.</div></li><li><div id="Ebrahimi1994">Ebrahimi, N.; Pflughoeft, K. and Soofi, E. S. (1994). <a href="https://www.sciencedirect.com/science/article/pii/0167715294900469"><em>Two measures of sample entropy</em></a>. <a href="https://doi.org/10.1016/0167-7152(94)90046-9">Statistics &amp; Probability Letters <strong>20</strong>, 225–234</a>.</div></li><li><div id="Fadlallah2013">Fadlallah, B.; Chen, B.; Keil, A. and Prı́ncipe, J. (2013). <a href="https://link.aps.org/doi/10.1103/PhysRevE.87.022911"><em>Weighted-permutation entropy: A complexity measure for time series incorporating amplitude information</em></a>. <a href="https://doi.org/10.1103/PhysRevE.87.022911">Phys. Rev. E <strong>87</strong>, 022911</a>.</div></li><li><div id="Gao2015">Gao, S.; Ver Steeg, G. and Galstyan, A. (09–12 May 2015). <a href="https://proceedings.mlr.press/v38/gao15.html"><em>Efficient Estimation of Mutual Information for Strongly Dependent Variables</em></a>. In: <em>Proceedings of the Eighteenth International Conference on Artificial Intelligence and Statistics</em>, Vol. 38 of <em>Proceedings of Machine Learning Research</em>, edited by Lebanon, G. and Vishwanathan, S. V. (PMLR, San Diego, California, USA); pp. 277–286.</div></li><li><div id="Goria2005">Goria, M. N.; Leonenko, N. N.; Mergel, V. V. and Inverardi, P. L. (2005). <a href="https://doi.org/10.1080/104852504200026815"><em>A new class of random vector entropy estimators and its applications in testing statistical hypotheses</em></a>. <a href="https://doi.org/10.1080/104852504200026815">Journal of Nonparametric Statistics <strong>17</strong>, 277–297</a>, <a href="https://arxiv.org/abs/https://doi.org/10.1080/104852504200026815">arXiv:https://doi.org/10.1080/104852504200026815</a>.</div></li><li><div id="Grassberger2022">Grassberger, P. (2022). <a href="https://www.mdpi.com/1099-4300/24/5/680"><em>On Generalized Schürmann Entropy Estimators</em></a>. <a href="https://doi.org/10.3390/e24050680">Entropy <strong>24</strong></a>.</div></li><li><div id="Hausser2009">Hausser, J. and Strimmer, K. (2009). <a href="https://jmlr.csail.mit.edu/papers/volume10/hausser09a/hausser09a.pdf"><em>Entropy inference and the James-Stein estimator, with application to nonlinear gene association networks.</em></a> Journal of Machine Learning Research <strong>10</strong>.</div></li><li><div id="He2016">He, S.; Sun, K. and Wang, H. (2016). <a href="https://www.sciencedirect.com/science/article/pii/S0378437116302801"><em>Multivariate permutation entropy and its application for complexity analysis of chaotic systems</em></a>. <a href="https://doi.org/10.1016/j.physa.2016.06.012">Physica A: Statistical Mechanics and its Applications <strong>461</strong>, 812–823</a>.</div></li><li><div id="Horvitz1952">Horvitz, D. G. and Thompson, D. J. (1952). <a href="https://www.tandfonline.com/doi/abs/10.1080/01621459.1952.10483446"><em>A Generalization of Sampling Without Replacement from a Finite Universe</em></a>. <a href="https://doi.org/10.1080/01621459.1952.10483446">Journal of the American Statistical Association <strong>47</strong>, 663–685</a>, <a href="https://arxiv.org/abs/https://www.tandfonline.com/doi/pdf/10.1080/01621459.1952.10483446">arXiv:https://www.tandfonline.com/doi/pdf/10.1080/01621459.1952.10483446</a>.</div></li><li><div id="JamesStein1992">James, W. and Stein, C. (1992). <a href="https://link.springer.com/chapter/10.1007/978-1-4612-0919-5_30"><em>Estimation with quadratic loss</em></a>. In: <em>Breakthroughs in statistics: Foundations and basic theory</em> (Springer); pp. 443–460.</div></li><li><div id="KozachenkoLeonenko1987">Kozachenko, L. F. and Leonenko, N. N. (1987). <a href="https://www.mathnet.ru/php/archive.phtml?wshow=paper&amp;jrnid=ppi&amp;paperid=797&amp;option_lang=eng"><em>Sample estimate of the entropy of a random vector</em></a>. Problemy Peredachi Informatsii <strong>23</strong>, 9–16.</div></li><li><div id="Kraskov2004">Kraskov, A.; Stögbauer, H. and Grassberger, P. (2004). <a href="https://link.aps.org/doi/10.1103/PhysRevE.69.066138"><em>Estimating mutual information</em></a>. <a href="https://doi.org/10.1103/PhysRevE.69.066138">Phys. Rev. E <strong>69</strong>, 066138</a>.</div></li><li><div id="Lad2015">Lad, F.; Sanfilippo, G. and Agrò, G. (2015). <a href="https://doi.org/10.1214/14-STS430"><em>Extropy: Complementary Dual of Entropy</em></a>. <a href="https://doi.org/10.1214/14-STS430">Statistical Science <strong>30</strong>, 40–58</a>.</div></li><li><div id="LempelZiv1976">Lempel, A. and Ziv, J. (1976). <em>On the Complexity of Finite Sequences</em>. <a href="https://doi.org/10.1109/TIT.1976.1055501">IEEE Transactions on Information Theory <strong>22</strong>, 75–81</a>.</div></li><li><div id="LeonenkoProzantoSavani2008">Leonenko, N.; Pronzato, L. and Savani, V. (2008). <a href="https://doi.org/10.1214/07-AOS539"><em>A class of Rényi information estimators for multidimensional densities</em></a>. <a href="https://doi.org/10.1214/07-AOS539">The Annals of Statistics <strong>36</strong>, 2153–2182</a>.</div></li><li><div id="Li2018">Li, G.; Guan, Q. and Yang, H. (2019). <a href="https://www.mdpi.com/1099-4300/21/1/11"><em>Noise Reduction Method of Underwater Acoustic Signals Based on CEEMDAN, Effort-To-Compress Complexity, Refined Composite Multiscale Dispersion Entropy and Wavelet Threshold Denoising</em></a>. <a href="https://doi.org/10.3390/e21010011">Entropy <strong>21</strong></a>.</div></li><li><div id="Li2019">Li, Y.; Gao, X. and Wang, L. (2019). <a href="https://www.mdpi.com/1424-8220/19/23/5203"><em>Reverse Dispersion Entropy: A New Complexity Measure for Sensor Signal</em></a>. <a href="https://doi.org/10.3390/s19235203">Sensors <strong>19</strong></a>.</div></li><li><div id="Liu2023">Liu, J. and Xiao, F. (2023). <em>Renyi extropy</em>. <a href="https://doi.org/10.1080/03610926.2021.2020843">Communications in Statistics, Theory and Methods <strong>52</strong>, 5836–5847</a>.</div></li><li><div id="Llanos2017">Llanos, F.; Alexander, J. M.; Stilp, C. E. and Kluender, K. R. (2017). <a href="https://pubmed.ncbi.nlm.nih.gov/28253693/"><em>Power spectral entropy as an information-theoretic correlate of manner of articulation in American English</em></a>. <a href="https://doi.org/10.1121/1.4976109">The Journal of the Acoustical Society of America <strong>141</strong>, EL127–EL133</a>.</div></li><li><div id="Lord2018">Lord, W. M.; Sun, J. and Bollt, E. M. (2018). <a href="https://pubs.aip.org/aip/cha/article/28/3/033114/685022"><em>Geometric k-nearest neighbor estimation of entropy and mutual information</em></a>. <a href="https://doi.org/10.1063/1.5011683">Chaos: An Interdisciplinary Journal of Nonlinear Science <strong>28</strong></a>.</div></li><li><div id="Miller1955">Miller, G. (1955). <em>Note on the bias of information estimates</em>. Information theory in psychology: Problems and methods.</div></li><li><div id="Paninski2003">Paninski, L. (2003). <a href="https://ieeexplore.ieee.org/abstract/document/6790247"><em>Estimation of entropy and mutual information</em></a>. <a href="https://doi.org/10.1162/089976603321780272">Neural computation <strong>15</strong>, 1191–1253</a>.</div></li><li><div id="Pincus1991">Pincus, S. M. (1991). <em>Approximate entropy as a measure of system complexity.</em> <a href="https://doi.org/10.1073/pnas.88.6.2297">Proceedings of the National Academy of Sciences <strong>88</strong>, 2297–2301</a>.</div></li><li><div id="PrichardTheiler1995">Prichard, D. and Theiler, J. (1995). <em>Generalized redundancies for time series analysis</em>. <a href="https://doi.org/10.1016/0167-2789(95)00041-2">Physica D: Nonlinear Phenomena <strong>84</strong>, 476–493</a>.</div></li><li><div id="Ribeiro2012">Ribeiro, H. V.; Zunino, L.; Lenzi, E. K.; Santoro, P. A. and Mendes, R. S. (2012). <a href="https://doi.org/10.1371/journal.pone.0040689"><em>Complexity-Entropy Causality Plane as a Complexity Measure for Two-Dimensional Patterns</em></a>. <a href="https://doi.org/10.1371/journal.pone.0040689">PLOS ONE <strong>7</strong>, 1–9</a>.</div></li><li><div id="Richman2000">Richman, J. S. and Moorman, J. R. (2000). <em>Physiological time-series analysis using approximate entropy and sample entropy</em>. <a href="https://doi.org/10.1152/ajpheart.2000.278.6.H2039">American journal of physiology-heart and circulatory physiology <strong>278</strong>, H2039–H2049</a>.</div></li><li><div id="Rosso2001">Rosso, O. A.; Blanco, S.; Yordanova, J.; Kolev, V.; Figliola, A.; Schürmann, M. and Başar, E. (2001). <a href="https://www.sciencedirect.com/science/article/pii/S0165027000003563"><em>Wavelet entropy: a new tool for analysis of short duration brain electrical signals</em></a>. <a href="https://doi.org/10.1016/S0165-0270(00)00356-3">Journal of Neuroscience Methods <strong>105</strong>, 65–75</a>.</div></li><li><div id="Rosso2007">Rosso, O. A.; Larrondo, H.; Martin, M. T.; Plastino, A. and Fuentes, M. A. (2007). <em>Distinguishing noise from chaos</em>. <a href="https://doi.org/10.1103/PhysRevLett.99.154102">Physical review letters <strong>99</strong>, 154102</a>.</div></li><li><div id="Rosso2013">Rosso, O. A.; Martín, M.; Larrondo, H. A.; Kowalski, A. and Plastino, A. (2013). <a href="https://www.researchgate.net/profile/Osvaldo-Rosso-3/publication/260145202_Generalized_Statistical_Complexity_a_new_tool_for_dynamical_systems/links/02e7e52fbe7122d461000000/Generalized-Statistical-Complexity-a-new-tool-for-dynamical-systems.pdf"><em>Generalized statistical complexity: A new tool for dynamical systems</em></a>. Concepts and recent advances in generalized information measures and statistics, 169–215.</div></li><li><div id="Rostaghi2016">Rostaghi, M. and Azami, H. (2016). <em>Dispersion entropy: A measure for time-series analysis</em>. <a href="https://doi.org/10.1109/LSP.2016.2542881">IEEE Signal Processing Letters <strong>23</strong>, 610–614</a>.</div></li><li><div id="Rényi1961">Rényi, A. (1961). <a href="https://projecteuclid.org/ebook/Download?urlid=bsmsp/1200512181&amp;isFullBook=false"><em>On measures of entropy and information</em></a>. In: <em>Proceedings of the Fourth Berkeley Symposium on Mathematical Statistics and Probability, Volume 1: Contributions to the Theory of Statistics</em>, Vol. 4 (University of California Press); pp. 547–562.</div></li><li><div id="Schlemmer2018">Schlemmer, A.; Berg, S.; Lilienkamp, T.; Luther, S. and Parlitz, U. (2018). <em>Spatiotemporal permutation entropy as a measure for complexity of cardiac arrhythmia</em>. <a href="https://doi.org/10.3389/fphy.2018.00039">Frontiers in Physics <strong>6</strong>, 39</a>.</div></li><li><div id="Schurmann2004">Schürmann, T. (2004). <em>Bias analysis in entropy estimation</em>. <a href="https://doi.org/10.1088/0305-4470/37/27/L02">Journal of Physics A: Mathematical and General <strong>37</strong>, L295</a>.</div></li><li><div id="Shannon1948">Shannon, C. E. (1948). <em>A mathematical theory of communication</em>. <a href="https://doi.org/10.1002/j.1538-7305.1948.tb01338.x">The Bell system technical journal <strong>27</strong>, 379–423</a>.</div></li><li><div id="Singh2003">Singh, H.; Misra, N.; Hnizdo, V.; Fedorowicz, A. and Demchuk, E. (2003). <em>Nearest neighbor estimates of entropy</em>. <a href="https://doi.org/10.1080/01966324.2003.10737616">American journal of mathematical and management sciences <strong>23</strong>, 301–321</a>.</div></li><li><div id="Sippel2016">Sippel, S.; Lange, H. and Gans, F. (2016), <a href="https://cran.r-project.org/web/packages/statcomp/index.html"><em>statcomp: Statistical Complexity and Information measures for time series analysis</em></a>. R package version.</div></li><li><div id="Tian2017">Tian, Y.; Zhang, H.; Xu, W.; Zhang, H.; Yang, L.; Zheng, S. and Shi, Y. (2017). <em>Spectral entropy can predict changes of working memory performance reduced by short-time training in the delayed-match-to-sample task</em>. <a href="https://doi.org/10.3389/fnhum.2017.00437">Frontiers in human neuroscience <strong>11</strong>, 437</a>.</div></li><li><div id="Tsallis1988">Tsallis, C. (1988). <em>Possible generalization of Boltzmann-Gibbs statistics</em>. <a href="https://doi.org/10.1007/BF01016429">Journal of statistical physics <strong>52</strong>, 479–487</a>.</div></li><li><div id="Tsallis2009">Tsallis, C. (2009). <a href="https://link.springer.com/book/10.1007/978-0-387-85359-8"><em>Introduction to nonextensive statistical mechanics: approaching a complex world</em></a>. Vol. 1 no. 1 (Springer).</div></li><li><div id="Vasicek1976">Vasicek, O. (1976). <em>A test for normality based on sample entropy</em>. <a href="https://doi.org/10.1111/j.2517-6161.1976.tb01566.x">Journal of the Royal Statistical Society Series B: Statistical Methodology <strong>38</strong>, 54–59</a>.</div></li><li><div id="Wang2020">Wang, X.; Si, S. and Li, Y. (2020). <em>Multiscale diversity entropy: A novel dynamical measure for fault diagnosis of rotating machinery</em>. <a href="https://doi.org/10.1109/TII.2020.3022369">IEEE Transactions on Industrial Informatics <strong>17</strong>, 5419–5429</a>.</div></li><li><div id="Xue2023">Xue, Y. and Deng, Y. (2023). <em>Tsallis extropy</em>. <a href="https://doi.org/10.1080/03610926.2021.1921804">Communications in Statistics-Theory and Methods <strong>52</strong>, 751–762</a>.</div></li><li><div id="Zahl1977">Zahl, S. (1977). <em>Jackknifing an index of diversity</em>. <a href="https://doi.org/10.2307/1936227">Ecology <strong>58</strong>, 907–913</a>.</div></li><li><div id="Zhou2023">Zhou, Q.; Shang, P. and Zhang, B. (2023). <em>Using missing dispersion patterns to detect determinism and nonlinearity in time series data</em>. <a href="https://doi.org/10.1007/s11071-022-07835-3">Nonlinear Dynamics <strong>111</strong>, 439–458</a>.</div></li><li><div id="Zhu2015">Zhu, J.; Bellanger, J.-J.; Shu, H. and Le Bouquin Jeannès, R. (2015). <em>Contribution to transfer entropy estimation via the k-nearest-neighbors approach</em>. <a href="https://doi.org/10.3390/e17064173">Entropy <strong>17</strong>, 4173–4201</a>.</div></li><li><div id="Zunino2017">Zunino, L.; Olivares, F.; Scholkmann, F. and Rosso, O. A. (2017). <em>Permutation entropy based time series analysis: Equalities in the input signal can lead to false conclusions</em>. <a href="https://doi.org/10.1016/j.physleta.2017.03.052">Physics Letters A <strong>381</strong>, 1883–1892</a>.</div></li></ul></div></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../devdocs/">« ComplexityMeasures.jl Dev Docs</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option><option value="auto">Automatic (OS)</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 1.2.1 on <span class="colophon-date" title="Saturday 13 January 2024 09:44">Saturday 13 January 2024</span>. Using Julia version 1.10.0.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
