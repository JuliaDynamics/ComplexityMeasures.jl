<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Entropies · Entropies.jl</title><script data-outdated-warner src="../assets/warner.js"></script><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.045/juliamono.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.24/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL=".."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../assets/documenter.js"></script><script src="../siteinfo.js"></script><script src="../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../assets/themeswap.js"></script><link href="https://fonts.googleapis.com/css?family=Montserrat|Source+Code+Pro&amp;display=swap" rel="stylesheet" type="text/css"/></head><body><div id="documenter"><nav class="docs-sidebar"><div class="docs-package-name"><span class="docs-autofit"><a href="../">Entropies.jl</a></span></div><form class="docs-search" action="../search/"><input class="docs-search-query" id="documenter-search-query" name="q" type="text" placeholder="Search docs"/></form><ul class="docs-menu"><li><a class="tocitem" href="../">Entropies.jl</a></li><li><a class="tocitem" href="../probabilities/">Probabilities</a></li><li class="is-active"><a class="tocitem" href>Entropies</a><ul class="internal"><li><a class="tocitem" href="#Rényi-(generalized)-entropy"><span>Rényi (generalized) entropy</span></a></li><li><a class="tocitem" href="#Tsallis-(generalized)-entropy"><span>Tsallis (generalized) entropy</span></a></li><li><a class="tocitem" href="#Shannon-entropy-(convenience)"><span>Shannon entropy (convenience)</span></a></li><li><a class="tocitem" href="#Normalization"><span>Normalization</span></a></li><li><a class="tocitem" href="#Indirect-entropies"><span>Indirect entropies</span></a></li><li><a class="tocitem" href="#Convenience-functions"><span>Convenience functions</span></a></li></ul></li><li><a class="tocitem" href="../complexity_measures/">Complexity measures</a></li><li><a class="tocitem" href="../examples/">Entropies.jl examples</a></li><li><a class="tocitem" href="../utils/">Utility methods</a></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><nav class="breadcrumb"><ul class="is-hidden-mobile"><li class="is-active"><a href>Entropies</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>Entropies</a></li></ul></nav><div class="docs-right"><a class="docs-edit-link" href="https://github.com/JuliaDynamics/Entropies.jl/blob/main/docs/src/entropies.md" title="Edit on GitHub"><span class="docs-icon fab"></span><span class="docs-label is-hidden-touch">Edit on GitHub</span></a><a class="docs-settings-button fas fa-cog" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-sidebar-button fa fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a></div></header><article class="content" id="documenter-page"><h1 id="Entropies"><a class="docs-heading-anchor" href="#Entropies">Entropies</a><a id="Entropies-1"></a><a class="docs-heading-anchor-permalink" href="#Entropies" title="Permalink"></a></h1><h2 id="Rényi-(generalized)-entropy"><a class="docs-heading-anchor" href="#Rényi-(generalized)-entropy">Rényi (generalized) entropy</a><a id="Rényi-(generalized)-entropy-1"></a><a class="docs-heading-anchor-permalink" href="#Rényi-(generalized)-entropy" title="Permalink"></a></h2><article class="docstring"><header><a class="docstring-binding" id="Entropies.entropy_renyi" href="#Entropies.entropy_renyi"><code>Entropies.entropy_renyi</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">entropy_renyi(p::Probabilities; q = 1.0, base = MathConstants.e)</code></pre><p>Compute the Rényi<sup class="footnote-reference"><a id="citeref-Rényi1960" href="#footnote-Rényi1960">[Rényi1960]</a></sup> generalized order-<code>q</code> entropy of some probabilities (typically returned by the <a href="../probabilities/#Entropies.probabilities"><code>probabilities</code></a> function).</p><pre><code class="nohighlight hljs">entropy_renyi(x::Array_or_Dataset, est; q = 1.0, base)</code></pre><p>A convenience syntax, which calls first <code>probabilities(x, est)</code> and then calculates the entropy of the result (and thus <code>est</code> can be anything the <a href="../probabilities/#Entropies.probabilities"><code>probabilities</code></a> function accepts).</p><p><strong>Description</strong></p><p>Let <span>$p$</span> be an array of probabilities (summing to 1). Then the Rényi generalized entropy is</p><p class="math-container">\[H_q(p) = \frac{1}{1-q} \log \left(\sum_i p[i]^q\right)\]</p><p>and generalizes other known entropies, like e.g. the information entropy (<span>$q = 1$</span>, see <sup class="footnote-reference"><a id="citeref-Shannon1948" href="#footnote-Shannon1948">[Shannon1948]</a></sup>), the maximum entropy (<span>$q=0$</span>, also known as Hartley entropy), or the correlation entropy (<span>$q = 2$</span>, also known as collision entropy).</p><p>See also: <a href="../utils/#Entropies.maxentropy_renyi"><code>maxentropy_renyi</code></a>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaDynamics/Entropies.jl/blob/7b430a87ec23f261e68dd95c01bb1f980a01c36e/src/entropies/renyi.jl#L4-L39">source</a></section></article><h2 id="Tsallis-(generalized)-entropy"><a class="docs-heading-anchor" href="#Tsallis-(generalized)-entropy">Tsallis (generalized) entropy</a><a id="Tsallis-(generalized)-entropy-1"></a><a class="docs-heading-anchor-permalink" href="#Tsallis-(generalized)-entropy" title="Permalink"></a></h2><article class="docstring"><header><a class="docstring-binding" id="Entropies.entropy_tsallis" href="#Entropies.entropy_tsallis"><code>Entropies.entropy_tsallis</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">entropy_tsallis(p::Probabilities; k = 1, q = 0, base = MathConstants.e)</code></pre><p>Compute the Tsallis entropy of <code>p</code> (Tsallis, 1998)<sup class="footnote-reference"><a id="citeref-Tsallis1988" href="#footnote-Tsallis1988">[Tsallis1988]</a></sup>.</p><p><code>base</code> only applies in the limiting case <code>q == 1</code>, in which the Tsallis entropy reduces to Shannon entropy.</p><pre><code class="nohighlight hljs">entropy_tsallis(x::Array_or_Dataset, est; k = 1, q = 0, base = MathConstants.e)</code></pre><p>A convenience syntax, which calls first <code>probabilities(x, est)</code> and then calculates the Tsallis entropy of the result (and thus <code>est</code> can be anything the <a href="../probabilities/#Entropies.probabilities"><code>probabilities</code></a> function accepts).</p><p><strong>Description</strong></p><p>The Tsallis entropy is a generalization of the Boltzmann-Gibbs entropy, with <code>k</code> standing for the Boltzmann constant. It is defined as</p><p class="math-container">\[S_q(p) = \frac{k}{q - 1}\left(1 - \sum_{i} p[i]^q\right)\]</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaDynamics/Entropies.jl/blob/7b430a87ec23f261e68dd95c01bb1f980a01c36e/src/entropies/tsallis.jl#L4-L30">source</a></section></article><h2 id="Shannon-entropy-(convenience)"><a class="docs-heading-anchor" href="#Shannon-entropy-(convenience)">Shannon entropy (convenience)</a><a id="Shannon-entropy-(convenience)-1"></a><a class="docs-heading-anchor-permalink" href="#Shannon-entropy-(convenience)" title="Permalink"></a></h2><article class="docstring"><header><a class="docstring-binding" id="Entropies.entropy_shannon" href="#Entropies.entropy_shannon"><code>Entropies.entropy_shannon</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">entropy_shannon(args...; base = MathConstants.e)</code></pre><p>Equivalent to <code>entropy_renyi(args...; base = base, q = 1)</code> and provided solely for convenience. Computes the Shannon entropy, given by</p><p class="math-container">\[H(p) = - \sum_i p[i] \log(p[i])\]</p><p>See also: <a href="../utils/#Entropies.maxentropy_shannon"><code>maxentropy_shannon</code></a>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaDynamics/Entropies.jl/blob/7b430a87ec23f261e68dd95c01bb1f980a01c36e/src/entropies/shannon.jl#L4-L14">source</a></section></article><h2 id="Normalization"><a class="docs-heading-anchor" href="#Normalization">Normalization</a><a id="Normalization-1"></a><a class="docs-heading-anchor-permalink" href="#Normalization" title="Permalink"></a></h2><p>The generic <a href="#Entropies.entropy_normalized"><code>entropy_normalized</code></a> normalizes any entropy value to the entropy of a uniform distribution. We also provide <a href="../utils/#maximum_entropy">maximum entropy</a> functions that are useful for manual normalization.</p><article class="docstring"><header><a class="docstring-binding" id="Entropies.entropy_normalized" href="#Entropies.entropy_normalized"><code>Entropies.entropy_normalized</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">entropy_normalized(f::Function, x, est::ProbabilitiesEstimator, args...; kwargs...)</code></pre><p>Convenience syntax for normalizing to the entropy of uniform probability distribution. First estimates probabilities as <code>p::Probabilities = f(x, est, args...; kwargs...</code>), then calls <code>entropy_normalized(f, p, args...; kwargs...)</code>.</p><p>Normalization is only defined for estimators for which <a href="../utils/#Entropies.alphabet_length"><code>alphabet_length</code></a> is defined, meaning that the total number of states or symbols is known beforehand.</p><pre><code class="nohighlight hljs">entropy_normalized(f::Function, p::Probabilities, est::ProbabilitiesEstimator, args...;
    kwargs...)</code></pre><p>Normalize the entropy, as returned by the entropy function <code>f</code> called with the given arguments (i.e. <code>f(p, args...; kwargs...)</code>), to the entropy of a uniform distribution, inferring <a href="../utils/#Entropies.alphabet_length"><code>alphabet_length</code></a> from <code>est</code>.</p><pre><code class="nohighlight hljs">entropy_normalized(f::Function, p::Probabilities, args...; kwargs...)</code></pre><p>The same as above, but infers alphabet length from counting how many elements are in <code>p</code> (zero probabilities are counted).</p><p><strong>Examples</strong></p><p>Computing normalized entropy from scratch:</p><pre><code class="language-julia hljs">x = rand(100)
entropy_normalized(entropy_renyi, x, Dispersion())</code></pre><p>Computing normalized entropy from pre-computed probabilities with known parameters:</p><pre><code class="language-julia hljs">x = rand(100)
est = Dispersion(m = 3, symbolization = GaussianSymbolization(c = 4))
p = probabilities(x, est)
entropy_normalized(entropy_renyi, p, est)</code></pre><p>Computing normalized entropy, assumming there are <code>N = 10</code> total states:</p><pre><code class="language-julia hljs">N = 10
p = Probabilities(rand(10))
entropy_normalized(entropy_renyi, p, est)</code></pre><div class="admonition is-info"><header class="admonition-header">Normalized output range</header><div class="admonition-body"><p>For Rényi entropy (e.g. Kumar et al., 1986), and for Tsallis entropy (Tsallis, 1998), normalizing to the uniform distribution ensures that the entropy lies in the interval <code>[0, 1]</code>. For other entropies and parameter choices, the resulting entropy is not guaranteed to lie in <code>[0, 1]</code>. It is up to the user to decide whether normalizing to a uniform distribution makes sense for their use case.</p></div></div></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaDynamics/Entropies.jl/blob/7b430a87ec23f261e68dd95c01bb1f980a01c36e/src/entropies/normalized_entropy.jl#L3-L63">source</a></section></article><h2 id="Indirect-entropies"><a class="docs-heading-anchor" href="#Indirect-entropies">Indirect entropies</a><a id="Indirect-entropies-1"></a><a class="docs-heading-anchor-permalink" href="#Indirect-entropies" title="Permalink"></a></h2><p>Here we list functions which compute Shannon entropies via alternate means, without explicitly computing some probability distributions and then using the Shannon formulat.</p><h3 id="Nearest-neighbors-entropy"><a class="docs-heading-anchor" href="#Nearest-neighbors-entropy">Nearest neighbors entropy</a><a id="Nearest-neighbors-entropy-1"></a><a class="docs-heading-anchor-permalink" href="#Nearest-neighbors-entropy" title="Permalink"></a></h3><article class="docstring"><header><a class="docstring-binding" id="Entropies.entropy_kraskov" href="#Entropies.entropy_kraskov"><code>Entropies.entropy_kraskov</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">entropy_kraskov(x::AbstractDataset{D, T}; k::Int = 1, w::Int = 0,
    base::Real = MathConstants.e) where {D, T}</code></pre><p>Estimate Shannon entropy to the given <code>base</code> using <code>k</code>-th nearest neighbor searches (Kraskov, 2004)<sup class="footnote-reference"><a id="citeref-Kraskov2004" href="#footnote-Kraskov2004">[Kraskov2004]</a></sup>.</p><p><code>w</code> is the Theiler window, which determines if temporal neighbors are excluded during neighbor searches (defaults to <code>0</code>, meaning that only the point itself is excluded when searching for neighbours).</p><p>See also: <a href="#Entropies.entropy_kozachenkoleonenko"><code>entropy_kozachenkoleonenko</code></a>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaDynamics/Entropies.jl/blob/7b430a87ec23f261e68dd95c01bb1f980a01c36e/src/entropies/direct_entropies/nearest_neighbors/Kraskov.jl#L3-L17">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Entropies.entropy_kozachenkoleonenko" href="#Entropies.entropy_kozachenkoleonenko"><code>Entropies.entropy_kozachenkoleonenko</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">entropy_kozachenkoleonenko(x::AbstractDataset{D, T}; k::Int = 1, w::Int = 0,
    base::Real = MathConstants.e) where {D, T}</code></pre><p>Estimate Shannon entropy to the given <code>base</code> using <code>k</code>-th nearest neighbor searches, using the method from Kozachenko &amp; Leonenko (1987)<sup class="footnote-reference"><a id="citeref-KozachenkoLeonenko1987" href="#footnote-KozachenkoLeonenko1987">[KozachenkoLeonenko1987]</a></sup>, as described in Charzyńska and Gambin (2016)<sup class="footnote-reference"><a id="citeref-Charzyńska2016" href="#footnote-Charzyńska2016">[Charzyńska2016]</a></sup>.</p><p><code>w</code> is the Theiler window, which determines if temporal neighbors are excluded during neighbor searches (defaults to <code>0</code>, meaning that only the point itself is excluded when searching for neighbours).</p><p>See also: <a href="#Entropies.entropy_kraskov"><code>entropy_kraskov</code></a>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaDynamics/Entropies.jl/blob/7b430a87ec23f261e68dd95c01bb1f980a01c36e/src/entropies/direct_entropies/nearest_neighbors/KozachenkoLeonenko.jl#L3-L21">source</a></section></article><h2 id="Convenience-functions"><a class="docs-heading-anchor" href="#Convenience-functions">Convenience functions</a><a id="Convenience-functions-1"></a><a class="docs-heading-anchor-permalink" href="#Convenience-functions" title="Permalink"></a></h2><p>In this subsection we expand documentation strings of &quot;entropy names&quot; that are used commonly in the literature, such as &quot;permutation entropy&quot;. As we made clear in <a href="../#API-and-terminology">API &amp; terminology</a>, these are just the existing Shannon/Rényi/Tsallis entropy with a particularly chosen probability estimator. We have only defined convenience functions for the most used names, and arbitrary more specialized convenience functions can be easily defined in a couple lines of code.</p><article class="docstring"><header><a class="docstring-binding" id="Entropies.entropy_permutation" href="#Entropies.entropy_permutation"><code>Entropies.entropy_permutation</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">entropy_permutation(x; τ = 1, m = 3, base = MathConstants.e)</code></pre><p>Compute the permutation entropy of order <code>m</code> with delay/lag <code>τ</code>. This function is just a convenience call to:</p><pre><code class="language-julia hljs">est = SymbolicPermutation(; m, τ)
entropy_shannon(x, est; base)</code></pre><p>See <a href="../probabilities/#Entropies.SymbolicPermutation"><code>SymbolicPermutation</code></a> for more info. Similarly, one can use <code>SymbolicWeightedPermutation</code> or <code>SymbolicAmplitudeAwarePermutation</code> for the weighted/amplitude-aware versions.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaDynamics/Entropies.jl/blob/7b430a87ec23f261e68dd95c01bb1f980a01c36e/src/entropies/convenience_definitions.jl#L6-L18">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Entropies.entropy_spatial_permutation" href="#Entropies.entropy_spatial_permutation"><code>Entropies.entropy_spatial_permutation</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">entropy_spatial_permutation(x, stencil, periodic = true; kwargs...)</code></pre><p>Compute the spatial permutation entropy of <code>x</code> given the <code>stencil</code>. Here <code>x</code> must be a matrix or higher dimensional <code>Array</code> containing spatial data. This function is just a convenience call to:</p><pre><code class="language-julia hljs">est = SpatialSymbolicPermutation(stencil, x, periodic)
entropy_shannon(x, est; kwargs...)</code></pre><p>See <a href="../probabilities/#Entropies.SpatialSymbolicPermutation"><code>SpatialSymbolicPermutation</code></a> for more info, or how to encode stencils.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaDynamics/Entropies.jl/blob/7b430a87ec23f261e68dd95c01bb1f980a01c36e/src/entropies/convenience_definitions.jl#L24-L35">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Entropies.entropy_wavelet" href="#Entropies.entropy_wavelet"><code>Entropies.entropy_wavelet</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">entropy_wavelet(x; wavelet = Wavelets.WT.Daubechies{12}(), base = MathConstants.e)</code></pre><p>Compute the wavelet entropy. This function is just a convenience call to:</p><pre><code class="language-julia hljs">est = WaveletOverlap(wavelet)
entropy_shannon(x, est; base)</code></pre><p>See <a href="../probabilities/#Entropies.WaveletOverlap"><code>WaveletOverlap</code></a> for more info.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaDynamics/Entropies.jl/blob/7b430a87ec23f261e68dd95c01bb1f980a01c36e/src/entropies/convenience_definitions.jl#L41-L50">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Entropies.entropy_dispersion" href="#Entropies.entropy_dispersion"><code>Entropies.entropy_dispersion</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">entropy_dispersion(x; m = 2, τ = 1, s = GaussianSymbolization(3),
    base = MathConstants.e)</code></pre><p>Compute the dispersion entropy. This function is just a convenience call to:</p><pre><code class="language-julia hljs">est = Dispersion(m = m, τ = τ, s = s)
entropy_shannon(x, est; base)</code></pre><p>See <a href="../probabilities/#Entropies.Dispersion"><code>Dispersion</code></a> for more info.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaDynamics/Entropies.jl/blob/7b430a87ec23f261e68dd95c01bb1f980a01c36e/src/entropies/convenience_definitions.jl#L56-L66">source</a></section></article><section class="footnotes is-size-7"><ul><li class="footnote" id="footnote-Rényi1960"><a class="tag is-link" href="#citeref-Rényi1960">Rényi1960</a>A. Rényi, <em>Proceedings of the fourth Berkeley Symposium on Mathematics, Statistics and Probability</em>, pp 547 (1960)</li><li class="footnote" id="footnote-Kumar1986"><a class="tag is-link" href="#citeref-Kumar1986">Kumar1986</a>Kumar, U., Kumar, V., &amp; Kapur, J. N. (1986). Normalized measures of entropy. International Journal Of General System, 12(1), 55-69.</li><li class="footnote" id="footnote-Shannon1948"><a class="tag is-link" href="#citeref-Shannon1948">Shannon1948</a>C. E. Shannon, Bell Systems Technical Journal <strong>27</strong>, pp 379 (1948)</li><li class="footnote" id="footnote-Tsallis1988"><a class="tag is-link" href="#citeref-Tsallis1988">Tsallis1988</a>Tsallis, C. (1988). Possible generalization of Boltzmann-Gibbs statistics. Journal of statistical physics, 52(1), 479-487.</li><li class="footnote" id="footnote-Kumar1986"><a class="tag is-link" href="#citeref-Kumar1986">Kumar1986</a>Kumar, U., Kumar, V., &amp; Kapur, J. N. (1986). Normalized measures of entropy. International Journal Of General System, 12(1), 55-69.</li><li class="footnote" id="footnote-Tsallis1998"><a class="tag is-link" href="#citeref-Tsallis1998">Tsallis1998</a>Tsallis, C. (1988). Possible generalization of Boltzmann-Gibbs statistics. Journal of statistical physics, 52(1), 479-487.</li><li class="footnote" id="footnote-Kraskov2004"><a class="tag is-link" href="#citeref-Kraskov2004">Kraskov2004</a>Kraskov, A., Stögbauer, H., &amp; Grassberger, P. (2004). Estimating mutual information. Physical review E, 69(6), 066138.</li><li class="footnote" id="footnote-Charzyńska2016"><a class="tag is-link" href="#citeref-Charzyńska2016">Charzyńska2016</a>Charzyńska, A., &amp; Gambin, A. (2016). Improvement of the k-NN entropy estimator with applications in systems biology. Entropy, 18(1), 13.</li><li class="footnote" id="footnote-KozachenkoLeonenko1987"><a class="tag is-link" href="#citeref-KozachenkoLeonenko1987">KozachenkoLeonenko1987</a>Kozachenko, L. F., &amp; Leonenko, N. N. (1987). Sample estimate of the entropy of a random vector. Problemy Peredachi Informatsii, 23(2), 9-16.</li></ul></section></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../probabilities/">« Probabilities</a><a class="docs-footer-nextpage" href="../complexity_measures/">Complexity measures »</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 0.27.23 on <span class="colophon-date" title="Monday 26 September 2022 14:05">Monday 26 September 2022</span>. Using Julia version 1.8.1.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
