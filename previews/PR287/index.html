<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>ComplexityMeasures.jl · ComplexityMeasures.jl</title><script data-outdated-warner src="assets/warner.js"></script><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.045/juliamono.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.24/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL="."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="assets/documenter.js"></script><script src="siteinfo.js"></script><script src="../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="assets/themeswap.js"></script><link href="https://fonts.googleapis.com/css?family=Montserrat|Source+Code+Pro&amp;display=swap" rel="stylesheet" type="text/css"/></head><body><div id="documenter"><nav class="docs-sidebar"><div class="docs-package-name"><span class="docs-autofit"><a href>ComplexityMeasures.jl</a></span></div><form class="docs-search" action="search/"><input class="docs-search-query" id="documenter-search-query" name="q" type="text" placeholder="Search docs"/></form><ul class="docs-menu"><li class="is-active"><a class="tocitem" href>ComplexityMeasures.jl</a><ul class="internal"><li><a class="tocitem" href="#terminology"><span>Content and terminology</span></a></li><li><a class="tocitem" href="#input_data"><span>Input data for ComplexityMeasures.jl</span></a></li></ul></li><li><a class="tocitem" href="probabilities/">Probabilities</a></li><li><a class="tocitem" href="information_measures/">Information measures</a></li><li><a class="tocitem" href="complexity/">Complexity measures</a></li><li><a class="tocitem" href="convenience/">Convenience functions</a></li><li><a class="tocitem" href="examples/">ComplexityMeasures.jl Examples</a></li><li><a class="tocitem" href="devdocs/">ComplexityMeasures.jl Dev Docs</a></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><nav class="breadcrumb"><ul class="is-hidden-mobile"><li class="is-active"><a href>ComplexityMeasures.jl</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>ComplexityMeasures.jl</a></li></ul></nav><div class="docs-right"><a class="docs-edit-link" href="https://github.com/JuliaDynamics/ComplexityMeasures.jl/blob/main/docs/src/index.md" title="Edit on GitHub"><span class="docs-icon fab"></span><span class="docs-label is-hidden-touch">Edit on GitHub</span></a><a class="docs-settings-button fas fa-cog" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-sidebar-button fa fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a></div></header><article class="content" id="documenter-page"><h1 id="ComplexityMeasures.jl"><a class="docs-heading-anchor" href="#ComplexityMeasures.jl">ComplexityMeasures.jl</a><a id="ComplexityMeasures.jl-1"></a><a class="docs-heading-anchor-permalink" href="#ComplexityMeasures.jl" title="Permalink"></a></h1><article class="docstring"><header><a class="docstring-binding" id="ComplexityMeasures" href="#ComplexityMeasures"><code>ComplexityMeasures</code></a> — <span class="docstring-category">Module</span></header><section><div><p><strong>ComplexityMeasures.jl</strong></p><p><a href="https://juliadynamics.github.io/ComplexityMeasures.jl/stable/"><img src="https://img.shields.io/badge/docs-stable-blue.svg" alt="Docs"/></a> <a href="https://github.com/JuliaDynamics/ComplexityMeasures.jl/actions"><img src="https://github.com/juliadynamics/ComplexityMeasures.jl/workflows/CI/badge.svg" alt="CI"/></a> <a href="https://codecov.io/gh/JuliaDynamics/ComplexityMeasures.jl"><img src="https://codecov.io/gh/JuliaDynamics/ComplexityMeasures.jl/branch/main/graph/badge.svg?token=6XlPGg5nRG" alt="codecov"/></a> <a href="https://pkgs.genieframework.com?packages=ComplexityMeasures"><img src="https://shields.io/endpoint?url=https://pkgs.genieframework.com/api/v1/badge/ComplexityMeasures" alt="Package Downloads"/></a> <a href="https://pkgs.genieframework.com?packages=Entropies"><img src="https://shields.io/endpoint?url=https://pkgs.genieframework.com/api/v1/badge/Entropies" alt="Package Downloads"/></a> <a href="https://zenodo.org/badge/latestdoi/306859984"><img src="https://zenodo.org/badge/306859984.svg" alt="DOI"/></a></p><p>A Julia package that provides:</p><ul><li>a rigorous framework for extracting probabilities from data, based on the mathematical formulation of <a href="https://en.wikipedia.org/wiki/Probability_space">probability spaces</a></li><li>several (12+) outcome spaces, i.e., ways to discretize data into probabilities</li><li>several estimators for estimating probabilities given an outcome space, which correct theoretically known estimation biases</li><li>several definitions of information measures, such as various flavours of entropies (Shannon, Tsallis, Curado...), extropies, and probability-based complexity measures, that are used in the context of nonlinear dynamics, nonlinear timeseries analysis, and complex systems</li><li>several discrete and continuous (differential) estimators for entropies, which correct theoretically known estimation biases</li><li>estimators for other complexity measures that are not estimated based on probability functions</li><li>an extendable interface and well thought out API that makes it trivial to define new outcome spaces, or new estimators for probabilities, information measures, or complexity measures</li></ul><p>ComplexityMeasures.jl can be used as a standalone package, or as part of other projects in the JuliaDynamics organization, such as <a href="https://juliadynamics.github.io/DynamicalSystems.jl/dev/">DynamicalSystems.jl</a> or <a href="https://juliadynamics.github.io/CausalityTools.jl/dev/">CausalityTools.jl</a>.</p><p>To install it, run <code>import Pkg; Pkg.add(&quot;ComplexityMeasures&quot;)</code>.</p><p>All further information is provided in the documentation, which you can either find <a href="https://juliadynamics.github.io/ComplexityMeasures.jl/dev/">online</a> or build locally by running the <code>docs/make.jl</code> file.</p><p><em>Previously, this package was called Entropies.jl.</em></p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaDynamics/ComplexityMeasures.jl/blob/b03edd0c9aead13b35b293bd453f20190ff6f2c4/src/ComplexityMeasures.jl#L4-L31">source</a></section></article><h2 id="terminology"><a class="docs-heading-anchor" href="#terminology">Content and terminology</a><a id="terminology-1"></a><a class="docs-heading-anchor-permalink" href="#terminology" title="Permalink"></a></h2><div class="admonition is-info"><header class="admonition-header">Note</header><div class="admonition-body"><p>The documentation here was initially inspired by chapter 5 of <a href="https://link.springer.com/book/10.1007/978-3-030-91032-7">Nonlinear Dynamics</a>, Datseris &amp; Parlitz, Springer 2022.</p></div></div><p>Before exploring the features of ComplexityMeasures.jl, it is useful to read through this terminology section. In the scientific literature, words like <strong>probabilities</strong>, <strong>entropy</strong>, and <strong>complexity measure</strong> are used (and abused) in multiple contexts, and are often used interchangeably to describe similar concepts. The API and documentation of ComplexityMeasures.jl aim to clarify the meaning and usage of these words, and to provide simple ways to obtain probabilities, information measures, or other complexity measures from input data.</p><p>For ComplexityMeasures.jl, we use the generic term &quot;complexity measure&quot; for any complexity measure that is not a direct functional of probabilities. &quot;Information measure&quot; is used exclusively about measures that are functionals of probability mass functions or probability density functions, for example entropies. We believe the general nonlinear dynamics community agrees with our take, as most papers that introduce different entropy flavors call them complexity measures. Example: <em>&quot;Permutation Entropy: A Natural Complexity Measure for Time Series&quot;</em> (Brandt and Pompe, 2002).</p><h3 id="Probabilities-and-Outcome-Spaces"><a class="docs-heading-anchor" href="#Probabilities-and-Outcome-Spaces">Probabilities and Outcome Spaces</a><a id="Probabilities-and-Outcome-Spaces-1"></a><a class="docs-heading-anchor-permalink" href="#Probabilities-and-Outcome-Spaces" title="Permalink"></a></h3><p>Information measures, and some other complexity measures, are are typically computed based on <strong>probability distributions</strong> derived from input data. Probabilities can be either discrete (<a href="https://en.wikipedia.org/wiki/Probability_mass_function">mass functions</a>) or continuous (<a href="https://en.wikipedia.org/wiki/Probability_density_function">density functions</a>). ComplexityMeasures.jl implements an interface for probabilities that exactly follows the mathematically rigorous formulation of <a href="https://en.wikipedia.org/wiki/Probability_space">probability spaces</a>. In order to derive probabilities from data, an <strong>outcome space</strong> needs to be defined: a way to transform data into elements <span>$\omega$</span> of an outcome space <span>$\omega \in \Omega$</span>, and assign probabilities to each outcome <span>$p(\omega)$</span>, such that <span>$p(\Omega)=1$</span>. <span>$\omega$</span> are called <em>outcomes</em> or <em>events</em>.</p><p>If <span>$\Omega$</span> is countable, this process is also called <em>discretization</em> of the input data. During discretization, each element of input data <span>$\chi \in x$</span> is <a href="probabilities/#ComplexityMeasures.encode"><code>encode</code></a>d (discretized) into an element of the outcome space <span>$\omega \in \Omega$</span>. Currently in ComplexityMeasures.jl, only countable <span>$\Omega$</span> are implemented explicitly. Quantities that are estimated from probability density functions (i.e., uncountable <span>$\Omega$</span>) also exist. However, these are estimated by a one-step processes without the intermediate estimation of probabilities.</p><p>Once an outcome space (subtype of <a href="probabilities/#ComplexityMeasures.OutcomeSpace"><code>OutcomeSpace</code></a>) is defined in ComplexityMeasures.jl, probabilities are estimated from input data using the <a href="probabilities/#ComplexityMeasures.probabilities"><code>probabilities</code></a> function. The simplest way to do this is simply to assign a probability to each outcome as its <em>relative count</em> with respect to the total outcomes (i.e., normalizing the output of the <a href="probabilities/#ComplexityMeasures.counts"><code>counts</code></a> function). This is also the default way. However, more advanced versions to estimate probabilities exist, that may account for known estimation biases. These <strong>probabilities estimators</strong> are subtypes of <a href="probabilities/#ComplexityMeasures.ProbabilitiesEstimator"><code>ProbabilitiesEstimator</code></a>.</p><h3 id="Information-measures"><a class="docs-heading-anchor" href="#Information-measures">Information measures</a><a id="Information-measures-1"></a><a class="docs-heading-anchor-permalink" href="#Information-measures" title="Permalink"></a></h3><p>Within ComplexityMeasures.jl, we have taken the pragmatic choice to label all measures that are <em>explicit functionals of probability mass or probability density functions</em> as <strong>information measures</strong>, even though they might not be labelled as information measures in the literature. This is encapsulated by the supertype <a href="information_measures/#ComplexityMeasures.InformationMeasure"><code>InformationMeasure</code></a>, whose subtypes are known (e.g., <a href="information_measures/#ComplexityMeasures.Shannon"><code>Shannon</code></a>) and less-known (e.g., <a href="information_measures/#ComplexityMeasures.Curado"><code>Curado</code></a>) definitions of information measures, the most common of which are entropies.</p><p>But even &quot;entropy&quot; is an umbrella term that may mean several computationally, and sometimes even fundamentally, different quantities. In ComplexityMeasures.jl, we provide the generic function <a href="information_measures/#ComplexityMeasures.information-Tuple{InformationMeasure, OutcomeSpace, Any}"><code>information</code></a> that tries to both clarify disparate information-related measures, while unifying them under a common interface that highlights the modular nature of the term &quot;information measure&quot;.</p><p>An information measure as defined by a subtype of <a href="information_measures/#ComplexityMeasures.InformationMeasure"><code>InformationMeasure</code></a>. However, estimating an information measure can be separated, on the highest level, into two main types:</p><ol><li><strong>Discrete</strong> information measures are functions of <a href="https://en.wikipedia.org/wiki/Probability_mass_function">probability mass functions</a>. Computing a discrete information measure boils  down to two simple steps: defining an outcome space (using <a href="probabilities/#ComplexityMeasures.OutcomeSpace"><code>OutcomeSpace</code></a>), then   estimating a probability distribution (using a <a href="probabilities/#ComplexityMeasures.ProbabilitiesEstimator"><code>ProbabilitiesEstimator</code></a>), then   plugging the estimated probabilities into a discrete estimator of the information measure definition (<a href="information_measures/#ComplexityMeasures.DiscreteInfoEstimator"><code>DiscreteInfoEstimator</code></a>).</li><li><strong>Differential or continuous</strong> information measures are functions of  <a href="https://en.wikipedia.org/wiki/Probability_density_function">probability density functions</a>,  which are <em>integrals</em>. Computing differential information measures therefore rely on estimating  some density functional. For this task, we provide <a href="information_measures/#ComplexityMeasures.DifferentialInfoEstimator"><code>DifferentialInfoEstimator</code></a>s,  which compute information measures via alternate means, without explicitly computing some  probability distribution. For example, the <a href="information_measures/#ComplexityMeasures.Correa"><code>Correa</code></a> estimator computes the  Shannon differential entropy using order statistics.</li></ol><p>Crucially, many quantities in the nonlinear dynamics literature that are named as entropies, such as &quot;permutation entropy&quot; (<a href="convenience/#ComplexityMeasures.entropy_permutation"><code>entropy_permutation</code></a>) and &quot;wavelet entropy&quot; (<a href="convenience/#ComplexityMeasures.entropy_wavelet"><code>entropy_wavelet</code></a>), are <em>not really new entropies</em>. They are the good old discrete Shannon entropy (<a href="information_measures/#ComplexityMeasures.Shannon"><code>Shannon</code></a>), but calculated with <em>new probabilities estimators</em>. In turn, <a href="information_measures/#ComplexityMeasures.Shannon"><code>Shannon</code></a> entropy is just one of many entropies and entropies are a subset of information measures. Nevertheless, we acknolwedge that names such as &quot;permutation entropy&quot; are commonplace, so in ComplexityMeasures.jl we provide convenience functions like <a href="convenience/#ComplexityMeasures.entropy_permutation"><code>entropy_permutation</code></a>. However, we emphasize that these functions really aren&#39;t anything more than 2-lines-of-code wrappers that call <a href="information_measures/#ComplexityMeasures.information-Tuple{InformationMeasure, OutcomeSpace, Any}"><code>information</code></a> with the appropriate <a href="probabilities/#ComplexityMeasures.ProbabilitiesEstimator"><code>ProbabilitiesEstimator</code></a> and <a href="information_measures/#ComplexityMeasures.InformationMeasure"><code>InformationMeasure</code></a>.</p><h3 id="Other-complexity-measures"><a class="docs-heading-anchor" href="#Other-complexity-measures">Other complexity measures</a><a id="Other-complexity-measures-1"></a><a class="docs-heading-anchor-permalink" href="#Other-complexity-measures" title="Permalink"></a></h3><p>Other complexity measures which are not functionals of probability mass or density functions, yet still output some quantity related with complexity analysis, are called <strong>complexity measures</strong> within ComplexityMeasures.jl. They can be found in <a href="complexity/#Complexity-measures">Complexity measures</a> page. This includes measures like sample entropy and approximate entropy (which, even if named entropies, are not entropies in the formal mathematical sense).</p><p>A complexity measure is encapsulated by a <a href="complexity/#ComplexityMeasures.ComplexityEstimator"><code>ComplexityEstimator</code></a> that can be given to the <a href="complexity/#ComplexityMeasures.complexity"><code>complexity</code></a> function to obtain the corresponding numerical value. We stress that the separation between <strong>information measure</strong> and <strong>complexity measure</strong> is purely pragmatic, to establish a generic and extendable software interface within ComplexityMeasures.jl. One can always argue that <em>&quot;measure X should have been complexity and/or information&quot;</em> but in most cases this separation does not matter, it is only important that a classification choice is done and we stick with it.</p><h2 id="input_data"><a class="docs-heading-anchor" href="#input_data">Input data for ComplexityMeasures.jl</a><a id="input_data-1"></a><a class="docs-heading-anchor-permalink" href="#input_data" title="Permalink"></a></h2><p>The input data type typically depend on the probability estimator chosen. In general though, the standard DynamicalSystems.jl approach is taken and as such we have three types of input data:</p><ul><li><em>Timeseries</em>, which are <code>AbstractVector{&lt;:Real}</code>, used in e.g. with <a href="probabilities/#ComplexityMeasures.WaveletOverlap"><code>WaveletOverlap</code></a>.</li><li><em>Multi-variate timeseries, or datasets, or state space sets</em>, which are <a href="#StateSpaceSets.StateSpaceSet"><code>StateSpaceSet</code></a>s, used e.g. with <a href="probabilities/#ComplexityMeasures.NaiveKernel"><code>NaiveKernel</code></a>. The short syntax <code>SSSet</code> may be used instead of <code>StateSpaceSet</code>.</li><li><em>Spatial data</em>, which are higher dimensional standard <code>Array</code>s, used e.g. with  <a href="probabilities/#ComplexityMeasures.SpatialSymbolicPermutation"><code>SpatialSymbolicPermutation</code></a>.</li></ul><article class="docstring"><header><a class="docstring-binding" id="StateSpaceSets.StateSpaceSet" href="#StateSpaceSets.StateSpaceSet"><code>StateSpaceSets.StateSpaceSet</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">StateSpaceSet{D, T} &lt;: AbstractStateSpaceSet{D,T}</code></pre><p>A dedicated interface for sets in a state space. It is an <strong>ordered container of equally-sized points</strong> of length <code>D</code>. Each point is represented by <code>SVector{D, T}</code>. The data are a standard Julia <code>Vector{SVector}</code>, and can be obtained with <code>vec(ssset::StateSpaceSet)</code>. Typically the order of points in the set is the time direction, but it doesn&#39;t have to be.</p><p>When indexed with 1 index, <code>StateSpaceSet</code> is like a vector of points. When indexed with 2 indices it behaves like a matrix that has each of the columns be the timeseries of each of the variables. When iterated over, it iterates over its contained points. See description of indexing below for more.</p><p><code>StateSpaceSet</code> also supports almost all sensible vector operations like <code>append!, push!, hcat, eachrow</code>, among others.</p><p><strong>Description of indexing</strong></p><p>In the following let <code>i, j</code> be integers, <code>typeof(X) &lt;: AbstractStateSpaceSet</code> and <code>v1, v2</code> be <code>&lt;: AbstractVector{Int}</code> (<code>v1, v2</code> could also be ranges, and for performance benefits make <code>v2</code> an <code>SVector{Int}</code>).</p><ul><li><code>X[i] == X[i, :]</code> gives the <code>i</code>th point (returns an <code>SVector</code>)</li><li><code>X[v1] == X[v1, :]</code>, returns a <code>StateSpaceSet</code> with the points in those indices.</li><li><code>X[:, j]</code> gives the <code>j</code>th variable timeseries (or collection), as <code>Vector</code></li><li><code>X[v1, v2], X[:, v2]</code> returns a <code>StateSpaceSet</code> with the appropriate entries (first indices being &quot;time&quot;/point index, while second being variables)</li><li><code>X[i, j]</code> value of the <code>j</code>th variable, at the <code>i</code>th timepoint</li></ul><p>Use <code>Matrix(ssset)</code> or <code>StateSpaceSet(matrix)</code> to convert. It is assumed that each <em>column</em> of the <code>matrix</code> is one variable. If you have various timeseries vectors <code>x, y, z, ...</code> pass them like <code>StateSpaceSet(x, y, z, ...)</code>. You can use <code>columns(dataset)</code> to obtain the reverse, i.e. all columns of the dataset in a tuple.</p></div></section></article></article><nav class="docs-footer"><a class="docs-footer-nextpage" href="probabilities/">Probabilities »</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 0.27.25 on <span class="colophon-date" title="Friday 18 August 2023 11:12">Friday 18 August 2023</span>. Using Julia version 1.9.2.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
