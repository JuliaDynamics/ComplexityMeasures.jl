<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Information measures · ComplexityMeasures.jl</title><script data-outdated-warner src="../assets/warner.js"></script><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.045/juliamono.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.24/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL=".."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../assets/documenter.js"></script><script src="../siteinfo.js"></script><script src="../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../assets/themeswap.js"></script><link href="https://fonts.googleapis.com/css?family=Montserrat|Source+Code+Pro&amp;display=swap" rel="stylesheet" type="text/css"/></head><body><div id="documenter"><nav class="docs-sidebar"><div class="docs-package-name"><span class="docs-autofit"><a href="../">ComplexityMeasures.jl</a></span></div><form class="docs-search" action="../search/"><input class="docs-search-query" id="documenter-search-query" name="q" type="text" placeholder="Search docs"/></form><ul class="docs-menu"><li><a class="tocitem" href="../">ComplexityMeasures.jl</a></li><li><a class="tocitem" href="../probabilities/">Probabilities</a></li><li class="is-active"><a class="tocitem" href>Information measures</a><ul class="internal"><li><a class="tocitem" href="#Information-measures-API"><span>Information measures API</span></a></li><li><a class="tocitem" href="#Definitions-(entropies-and-extropies)"><span>Definitions (entropies and extropies)</span></a></li><li><a class="tocitem" href="#Estimation-(discrete)"><span>Estimation (discrete)</span></a></li><li><a class="tocitem" href="#Estimation-(differential)"><span>Estimation (differential)</span></a></li></ul></li><li><a class="tocitem" href="../complexity/">Complexity measures</a></li><li><a class="tocitem" href="../convenience/">Convenience functions</a></li><li><a class="tocitem" href="../examples/">ComplexityMeasures.jl Examples</a></li><li><a class="tocitem" href="../devdocs/">ComplexityMeasures.jl Dev Docs</a></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><nav class="breadcrumb"><ul class="is-hidden-mobile"><li class="is-active"><a href>Information measures</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>Information measures</a></li></ul></nav><div class="docs-right"><a class="docs-edit-link" href="https://github.com/JuliaDynamics/ComplexityMeasures.jl/blob/main/docs/src/information_measures.md" title="Edit on GitHub"><span class="docs-icon fab"></span><span class="docs-label is-hidden-touch">Edit on GitHub</span></a><a class="docs-settings-button fas fa-cog" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-sidebar-button fa fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a></div></header><article class="content" id="documenter-page"><h1 id="information_measures"><a class="docs-heading-anchor" href="#information_measures">Information measures</a><a id="information_measures-1"></a><a class="docs-heading-anchor-permalink" href="#information_measures" title="Permalink"></a></h1><div class="admonition is-info"><header class="admonition-header">Note</header><div class="admonition-body"><p>Please be sure you have read the <a href="../#terminology">Terminology</a> section before going through the API here.</p></div></div><h2 id="Information-measures-API"><a class="docs-heading-anchor" href="#Information-measures-API">Information measures API</a><a id="Information-measures-API-1"></a><a class="docs-heading-anchor-permalink" href="#Information-measures-API" title="Permalink"></a></h2><p>The information measure API is defined by the <a href="#ComplexityMeasures.information-Tuple{InformationMeasure, OutcomeSpace, Any}"><code>information</code></a> function, which takes as an input an <a href="#ComplexityMeasures.InformationMeasure"><code>InformationMeasure</code></a>, or some specialized <a href="#ComplexityMeasures.DiscreteInfoEstimator"><code>DiscreteInfoEstimator</code></a> or <a href="#ComplexityMeasures.DifferentialInfoEstimator"><code>DifferentialInfoEstimator</code></a> for estimating the discrete or differential variant of the measure.</p><p>The functions <a href="#ComplexityMeasures.information_maximum"><code>information_maximum</code></a> and <a href="#ComplexityMeasures.information_normalized"><code>information_normalized</code></a> are also useful.</p><h2 id="Definitions-(entropies-and-extropies)"><a class="docs-heading-anchor" href="#Definitions-(entropies-and-extropies)">Definitions (entropies and extropies)</a><a id="Definitions-(entropies-and-extropies)-1"></a><a class="docs-heading-anchor-permalink" href="#Definitions-(entropies-and-extropies)" title="Permalink"></a></h2><article class="docstring"><header><a class="docstring-binding" id="ComplexityMeasures.InformationMeasure" href="#ComplexityMeasures.InformationMeasure"><code>ComplexityMeasures.InformationMeasure</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">InformationMeasure</code></pre><p><code>InformationMeasure</code> is the supertype of all information measure definitions.</p><p>In this package, we define &quot;information measures&quot; as functionals of probability mass functions (&quot;discrete&quot; measures), or of probability density functions (&quot;differential&quot; measures). Examples are (generalized) entropies such as <a href="#ComplexityMeasures.Shannon"><code>Shannon</code></a> or <a href="#ComplexityMeasures.Renyi"><code>Renyi</code></a>, or extropies like <a href="#ComplexityMeasures.ShannonExtropy"><code>ShannonExtropy</code></a>. A particular information measure may have both a discrete and a continuous/differential definition, which are estimated using a <a href="#ComplexityMeasures.DifferentialInfoEstimator"><code>DifferentialInfoEstimator</code></a> or a <a href="#ComplexityMeasures.DifferentialInfoEstimator"><code>DifferentialInfoEstimator</code></a>, respectively.</p><p><strong>Used with</strong></p><p>Any of the information measures listed below can be used with</p><ul><li><a href="#ComplexityMeasures.information-Tuple{InformationMeasure, OutcomeSpace, Any}"><code>information</code></a>, to compute a numerical value for the measure, given some input data.</li><li><a href="#ComplexityMeasures.information_maximum"><code>information_maximum</code></a>, to compute the maximum possible value for the measure.</li><li><a href="#ComplexityMeasures.information_normalized"><code>information_normalized</code></a>, to compute the normalized form of the   measure (divided by the maximum possible value).</li></ul><p>The <a href="#ComplexityMeasures.information_maximum"><code>information_maximum</code></a>/<a href="#ComplexityMeasures.information_normalized"><code>information_normalized</code></a> functions only works with the discrete version of the measure. See docstrings for the above functions for usage examples.</p><p><strong>Implementations</strong></p><ul><li><a href="#ComplexityMeasures.Renyi"><code>Renyi</code></a>.</li><li><a href="#ComplexityMeasures.Tsallis"><code>Tsallis</code></a>.</li><li><a href="#ComplexityMeasures.Shannon"><code>Shannon</code></a>, which is a subcase of the above two in the limit <code>q → 1</code>.</li><li><a href="#ComplexityMeasures.Kaniadakis"><code>Kaniadakis</code></a>.</li><li><a href="#ComplexityMeasures.Curado"><code>Curado</code></a>.</li><li><a href="#ComplexityMeasures.StretchedExponential"><code>StretchedExponential</code></a>.</li><li><a href="#ComplexityMeasures.RenyiExtropy"><code>RenyiExtropy</code></a>.</li><li><a href="#ComplexityMeasures.TsallisExtropy"><code>TsallisExtropy</code></a>.</li><li><a href="#ComplexityMeasures.ShannonExtropy"><code>ShannonExtropy</code></a>, which is a subcase of the above two in the limit <code>q → 1</code>.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaDynamics/ComplexityMeasures.jl/blob/b03edd0c9aead13b35b293bd453f20190ff6f2c4/src/core/information_measures.jl#L7-L48">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="ComplexityMeasures.Shannon" href="#ComplexityMeasures.Shannon"><code>ComplexityMeasures.Shannon</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">Shannon &lt;: InformationMeasure
Shannon(; base = 2)</code></pre><p>The Shannon<sup class="footnote-reference"><a id="citeref-Shannon1948" href="#footnote-Shannon1948">[Shannon1948]</a></sup> entropy, used with <a href="#ComplexityMeasures.information-Tuple{InformationMeasure, OutcomeSpace, Any}"><code>information</code></a> to compute:</p><p class="math-container">\[H(p) = - \sum_i p[i] \log(p[i])\]</p><p>with the <span>$\log$</span> at the given <code>base</code>.</p><p>The maximum value of the Shannon entropy is <span>$\log_{base}(L)$</span>, which is the entropy of the uniform distribution with <span>$L$</span> the <a href="../probabilities/#ComplexityMeasures.total_outcomes"><code>total_outcomes</code></a>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaDynamics/ComplexityMeasures.jl/blob/b03edd0c9aead13b35b293bd453f20190ff6f2c4/src/information_measure_definitions/shannon.jl#L3-L18">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="ComplexityMeasures.Renyi" href="#ComplexityMeasures.Renyi"><code>ComplexityMeasures.Renyi</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">Renyi &lt;: InformationMeasure
Renyi(q, base = 2)
Renyi(; q = 1.0, base = 2)</code></pre><p>The Rényi<sup class="footnote-reference"><a id="citeref-Rényi1960" href="#footnote-Rényi1960">[Rényi1960]</a></sup> generalized order-<code>q</code> entropy, used with <a href="#ComplexityMeasures.information-Tuple{InformationMeasure, OutcomeSpace, Any}"><code>information</code></a> to compute an entropy with units given by <code>base</code> (typically <code>2</code> or <code>MathConstants.e</code>).</p><p><strong>Description</strong></p><p>Let <span>$p$</span> be an array of probabilities (summing to 1). Then the Rényi generalized entropy is</p><p class="math-container">\[H_q(p) = \frac{1}{1-q} \log \left(\sum_i p[i]^q\right)\]</p><p>and generalizes other known entropies, like e.g. the information entropy (<span>$q = 1$</span>, see <sup class="footnote-reference"><a id="citeref-Shannon1948" href="#footnote-Shannon1948">[Shannon1948]</a></sup>), the maximum entropy (<span>$q=0$</span>, also known as Hartley entropy), or the correlation entropy (<span>$q = 2$</span>, also known as collision entropy).</p><p>The maximum value of the Rényi entropy is <span>$\log_{base}(L)$</span>, which is the entropy of the uniform distribution with <span>$L$</span> the <a href="../probabilities/#ComplexityMeasures.total_outcomes"><code>total_outcomes</code></a>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaDynamics/ComplexityMeasures.jl/blob/b03edd0c9aead13b35b293bd453f20190ff6f2c4/src/information_measure_definitions/renyi.jl#L3-L34">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="ComplexityMeasures.Tsallis" href="#ComplexityMeasures.Tsallis"><code>ComplexityMeasures.Tsallis</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">Tsallis &lt;: InformationMeasure
Tsallis(q; k = 1.0, base = 2)
Tsallis(; q = 1.0, k = 1.0, base = 2)</code></pre><p>The Tsallis<sup class="footnote-reference"><a id="citeref-Tsallis1988" href="#footnote-Tsallis1988">[Tsallis1988]</a></sup> generalized order-<code>q</code> entropy, used with <a href="#ComplexityMeasures.information-Tuple{InformationMeasure, OutcomeSpace, Any}"><code>information</code></a> to compute an entropy.</p><p><code>base</code> only applies in the limiting case <code>q == 1</code>, in which the Tsallis entropy reduces to Shannon entropy.</p><p><strong>Description</strong></p><p>The Tsallis entropy is a generalization of the Boltzmann-Gibbs entropy, with <code>k</code> standing for the Boltzmann constant. It is defined as</p><p class="math-container">\[S_q(p) = \frac{k}{q - 1}\left(1 - \sum_{i} p[i]^q\right)\]</p><p>The maximum value of the Tsallis entropy is ``<span>$k(L^{1 - q} - 1)/(1 - q)$</span>, with <span>$L$</span> the <a href="../probabilities/#ComplexityMeasures.total_outcomes"><code>total_outcomes</code></a>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaDynamics/ComplexityMeasures.jl/blob/b03edd0c9aead13b35b293bd453f20190ff6f2c4/src/information_measure_definitions/tsallis.jl#L3-L29">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="ComplexityMeasures.Kaniadakis" href="#ComplexityMeasures.Kaniadakis"><code>ComplexityMeasures.Kaniadakis</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">Kaniadakis &lt;: InformationMeasure
Kaniadakis(; κ = 1.0, base = 2.0)</code></pre><p>The Kaniadakis entropy (Tsallis, 2009)<sup class="footnote-reference"><a id="citeref-Tsallis2009" href="#footnote-Tsallis2009">[Tsallis2009]</a></sup>, used with <a href="#ComplexityMeasures.information-Tuple{InformationMeasure, OutcomeSpace, Any}"><code>information</code></a> to compute</p><p class="math-container">\[H_K(p) = -\sum_{i=1}^N p_i f_\kappa(p_i),\]</p><p class="math-container">\[f_\kappa (x) = \dfrac{x^\kappa - x^{-\kappa}}{2\kappa},\]</p><p>where if <span>$\kappa = 0$</span>, regular logarithm to the given <code>base</code> is used, and 0 probabilities are skipped.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaDynamics/ComplexityMeasures.jl/blob/b03edd0c9aead13b35b293bd453f20190ff6f2c4/src/information_measure_definitions/kaniadakis.jl#L3-L22">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="ComplexityMeasures.Curado" href="#ComplexityMeasures.Curado"><code>ComplexityMeasures.Curado</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">Curado &lt;: InformationMeasure
Curado(; b = 1.0)</code></pre><p>The Curado entropy (Curado &amp; Nobre, 2004)<sup class="footnote-reference"><a id="citeref-Curado2004" href="#footnote-Curado2004">[Curado2004]</a></sup>, used with <a href="#ComplexityMeasures.information-Tuple{InformationMeasure, OutcomeSpace, Any}"><code>information</code></a> to compute</p><p class="math-container">\[H_C(p) = \left( \sum_{i=1}^N e^{-b p_i} \right) + e^{-b} - 1,\]</p><p>with <code>b ∈ ℛ, b &gt; 0</code>, and the terms outside the sum ensures that <span>$H_C(0) = H_C(1) = 0$</span>.</p><p>The maximum entropy for Curado is <span>$L(1 - \exp(-b/L)) + \exp(-b) - 1$</span> with <span>$L$</span> the <a href="../probabilities/#ComplexityMeasures.total_outcomes"><code>total_outcomes</code></a>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaDynamics/ComplexityMeasures.jl/blob/b03edd0c9aead13b35b293bd453f20190ff6f2c4/src/information_measure_definitions/curado.jl#L3-L21">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="ComplexityMeasures.StretchedExponential" href="#ComplexityMeasures.StretchedExponential"><code>ComplexityMeasures.StretchedExponential</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">StretchedExponential &lt;: InformationMeasure
StretchedExponential(; η = 2.0, base = 2)</code></pre><p>The stretched exponential, or Anteneodo-Plastino, entropy (Anteneodo &amp; Plastino, 1999<sup class="footnote-reference"><a id="citeref-Anteneodo1999" href="#footnote-Anteneodo1999">[Anteneodo1999]</a></sup>), used with <a href="#ComplexityMeasures.information-Tuple{InformationMeasure, OutcomeSpace, Any}"><code>information</code></a> to compute</p><p class="math-container">\[S_{\eta}(p) = \sum_{i = 1}^N
\Gamma \left( \dfrac{\eta + 1}{\eta}, - \log_{base}(p_i) \right) -
p_i \Gamma \left( \dfrac{\eta + 1}{\eta} \right),\]</p><p>where <span>$\eta \geq 0$</span>, <span>$\Gamma(\cdot, \cdot)$</span> is the upper incomplete Gamma function, and <span>$\Gamma(\cdot) = \Gamma(\cdot, 0)$</span> is the Gamma function. Reduces to <a href="@ref">Shannon</a> entropy for <code>η = 1.0</code>.</p><p>The maximum entropy for <code>StrechedExponential</code> is a rather complicated expression involving incomplete Gamma functions (see source code).</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaDynamics/ComplexityMeasures.jl/blob/b03edd0c9aead13b35b293bd453f20190ff6f2c4/src/information_measure_definitions/streched_exponential.jl#L5-L28">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="ComplexityMeasures.ShannonExtropy" href="#ComplexityMeasures.ShannonExtropy"><code>ComplexityMeasures.ShannonExtropy</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">ShannonExtropy &lt;: InformationMeasure
ShannonExtropy(; base = 2)</code></pre><p>The Shannon extropy (Lad et al., 2015<sup class="footnote-reference"><a id="citeref-Lad2015" href="#footnote-Lad2015">[Lad2015]</a></sup>), used with <a href="@ref"><code>extropy</code></a> to compute</p><p class="math-container">\[J(x) -\sum_{i=1}^N (1 - p[i]) \log{(1 - p[i])},\]</p><p>for a probability distribution <span>$P = \{p_1, p_2, \ldots, p_N\}$</span>, with the <span>$\log$</span> at the given <code>base</code>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaDynamics/ComplexityMeasures.jl/blob/b03edd0c9aead13b35b293bd453f20190ff6f2c4/src/information_measure_definitions/shannon_extropy.jl#L3-L18">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="ComplexityMeasures.RenyiExtropy" href="#ComplexityMeasures.RenyiExtropy"><code>ComplexityMeasures.RenyiExtropy</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">RenyiExtropy &lt;: ProbabilitiesFunctional
RenyiExtropy(; q = 1.0, base = 2)</code></pre><p>The Rényi extropy (Liu &amp; Xiao, 2021<sup class="footnote-reference"><a id="citeref-Liu2021" href="#footnote-Liu2021">[Liu2021]</a></sup>).</p><p><strong>Description</strong></p><p><code>RenyiExtropy</code> is used with <a href="@ref"><code>extropy</code></a> to compute</p><p class="math-container">\[J_R(P) = \dfrac{-(n - 1) \log{(n - 1)} + (n - 1) \log{ \left( \sum_{i=1}^N {(1 - p[i])}^q \right)} }{q - 1}\]</p><p>for a probability distribution <span>$P = \{p_1, p_2, \ldots, p_N\}$</span>, with the <span>$\log$</span> at the given <code>base</code>. Alternatively, <code>RenyiExtropy</code> can be used with <a href="@ref"><code>extropy_normalized</code></a>, which ensures that the computed extropy is on the interval <span>$[0, 1]$</span> by normalizing to to the maximal Rényi extropy, given by</p><p class="math-container">\[J_R(P) = (N - 1)\log \left( \dfrac{n}{n-1} \right) .\]</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaDynamics/ComplexityMeasures.jl/blob/b03edd0c9aead13b35b293bd453f20190ff6f2c4/src/information_measure_definitions/renyi_extropy.jl#L3-L29">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="ComplexityMeasures.TsallisExtropy" href="#ComplexityMeasures.TsallisExtropy"><code>ComplexityMeasures.TsallisExtropy</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">TsallisExtropy &lt;: InformationMeasure
TsallisExtropy(; base = 2)</code></pre><p>The Tsallis extropy (Xue &amp; Deng<sup class="footnote-reference"><a id="citeref-Xue2023" href="#footnote-Xue2023">[Xue2023]</a></sup>).</p><p><strong>Description</strong></p><p><code>TsallisExtropy</code> is used with <a href="@ref"><code>extropy</code></a> to compute</p><p class="math-container">\[J_T(P) = k \dfrac{N - 1 - \sum_{i=1}^N ( 1 - p[i])^q}{q - 1}\]</p><p>for a probability distribution <span>$P = \{p_1, p_2, \ldots, p_N\}$</span>, with the <span>$\log$</span> at the given <code>base</code>. Alternatively, <code>TsallisExtropy</code> can be used with <a href="@ref"><code>extropy_normalized</code></a>, which ensures that the computed extropy is on the interval <span>$[0, 1]$</span> by normalizing to to the maximal Tsallis extropy, given by</p><p class="math-container">\[J_T(P) = \dfrac{(N - 1)N^{q - 1} - (N - 1)^q}{(q - 1)N^{q - 1}}\]</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaDynamics/ComplexityMeasures.jl/blob/b03edd0c9aead13b35b293bd453f20190ff6f2c4/src/information_measure_definitions/tsallis_extropy.jl#L3-L29">source</a></section></article><h2 id="Estimation-(discrete)"><a class="docs-heading-anchor" href="#Estimation-(discrete)">Estimation (discrete)</a><a id="Estimation-(discrete)-1"></a><a class="docs-heading-anchor-permalink" href="#Estimation-(discrete)" title="Permalink"></a></h2><article class="docstring"><header><a class="docstring-binding" id="ComplexityMeasures.information-Tuple{InformationMeasure, OutcomeSpace, Any}" href="#ComplexityMeasures.information-Tuple{InformationMeasure, OutcomeSpace, Any}"><code>ComplexityMeasures.information</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">information([e::DiscreteInfoEstimator,] est::ProbabilitiesEstimator, x) → h::Real</code></pre><p>Estimate a discrete information measure from input data <code>x</code> using the provided <a href="#ComplexityMeasures.DiscreteInfoEstimator"><code>DiscreteInfoEstimator</code></a> and <a href="../probabilities/#ComplexityMeasures.ProbabilitiesEstimator"><code>ProbabilitiesEstimator</code></a>. As an alternative, you can provide an <a href="#ComplexityMeasures.InformationMeasure"><code>InformationMeasure</code></a> for the first argument (which will default to <a href="#ComplexityMeasures.PlugIn"><code>PlugIn</code></a> estimation) or an <a href="../probabilities/#ComplexityMeasures.OutcomeSpace"><code>OutcomeSpace</code></a> for the second argument (which will default to the <a href="../probabilities/#ComplexityMeasures.MLE"><code>MLE</code></a> estimator).</p><p>All estimators compute <a href="#ComplexityMeasures.Shannon"><code>Shannon</code></a> entropy by default. To estimate other measures, give them as an argument to the estimator, e.g. <code>information(Jackknife(Renyi()), probest, x)</code>. If <code>e</code> is not provided, then the default is <code>information(PlugIn(Shannon()), probs)</code>.</p><p><strong>Examples (naive estimation)</strong></p><p>The simplest way to estimate a discrete measure is to provide the <a href="#ComplexityMeasures.InformationMeasure"><code>InformationMeasure</code></a> directly in combination with an <a href="../probabilities/#ComplexityMeasures.OutcomeSpace"><code>OutcomeSpace</code></a>. This will use the &quot;naive&quot; <a href="#ComplexityMeasures.PlugIn"><code>PlugIn</code></a> estimator for the measure, and the &quot;naive&quot; <a href="../probabilities/#ComplexityMeasures.MLE"><code>MLE</code></a> estimator for the probabilities.</p><pre><code class="language-julia hljs">x = randn(100) # some input data
o = ValueHistogram(RectangularBinning(5)) # a 5-bin histogram outcome space
h_s = information(Shannon(), o, x)</code></pre><p>Here are some more examples:</p><pre><code class="language-julia hljs">x = [rand(Bool) for _ in 1:10000] # coin toss
ps = probabilities(x) # gives about [0.5, 0.5] by definition
h = information(ps) # gives 1, about 1 bit by definition (Shannon entropy by default)
h = information(Shannon(), ps) # syntactically equivalent to the above
h = information(Shannon(), CountOccurrences(), x) # syntactically equivalent to above
h = information(Renyi(2.0), ps) # also gives 1, order `q` doesn&#39;t matter for coin toss
h = information(SymbolicPermutation(;m=3), x) # gives about 2, again by definition</code></pre><p><strong>Examples (bias-corrected estimation)</strong></p><p>It is known that both <a href="#ComplexityMeasures.PlugIn"><code>PlugIn</code></a> and <a href="../probabilities/#ComplexityMeasures.MLE"><code>MLE</code></a> estimation are biased. The scientific literature abounds with estimators that correct for this bias, both on the measure-estimation level and on the probability-estimation level. We thus provide the option to use any <a href="#ComplexityMeasures.DiscreteInfoEstimator"><code>DiscreteInfoEstimator</code></a> in combination with any <a href="../probabilities/#ComplexityMeasures.ProbabilitiesEstimator"><code>ProbabilitiesEstimator</code></a> for improved estimates. Note that custom probabilites estimators will only work with counting-compatible <a href="../probabilities/#ComplexityMeasures.OutcomeSpace"><code>OutcomeSpace</code></a>.</p><pre><code class="language-julia hljs">x = randn(100)
o = ValueHistogram(RectangularBinning(5))

# Estimate Shannon entropy estimation using various dedicated estimators
h_s = information(MillerMadow(Shannon()), MLE(o), x)
h_s = information(HorvitzThompson(Shannon()), Shrinkage(o), x)
h_s = information(Schürmann(Shannon()), Shrinkage(o), x)

# Estimate information measures using the generic `Jackknife` estimator
h_r = information(Jackknife(Renyi()), Shrinkage(o), x)
j_t = information(Jackknife(TsallisExtropy()), Bayes(o), x)
j_r = information(Jackknife(RenyiExtropy()), MLE(o), x)</code></pre><p><strong>Maximum/normalized information</strong></p><p>Most discrete <a href="#ComplexityMeasures.InformationMeasure"><code>InformationMeasure</code></a>s have a well-defined maximum for a given <a href="../probabilities/#ComplexityMeasures.OutcomeSpace"><code>OutcomeSpace</code></a>. You can use <a href="#ComplexityMeasures.information_maximum"><code>information_maximum</code></a> to get this maximum value, and <a href="#ComplexityMeasures.information_normalized"><code>information_normalized</code></a> to obtain a normalized version of the measure (divided by the maximum value).</p><pre><code class="nohighlight hljs">information([e::DiscreteInfoEstimator,] p::Probabilities) → h::Real</code></pre><p>Like above, but estimate the information measure from the pre-computed <a href="../probabilities/#ComplexityMeasures.Probabilities"><code>Probabilities</code></a> <code>p</code>.</p><p>See also: <a href="#ComplexityMeasures.information_maximum"><code>information_maximum</code></a>, <a href="#ComplexityMeasures.information_normalized"><code>information_normalized</code></a>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaDynamics/ComplexityMeasures.jl/blob/b03edd0c9aead13b35b293bd453f20190ff6f2c4/src/core/information_functions.jl#L11-L89">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="ComplexityMeasures.information_maximum" href="#ComplexityMeasures.information_maximum"><code>ComplexityMeasures.information_maximum</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">information_maximum(e::InformationMeasure, o::OutcomeSpace, x)
information_maximum(e::InformationMeasure, est::ProbabilitiesEstimator, x)</code></pre><p>Return the maximum value of the given information measure can have, given input data <code>x</code> and  the given outcome space (the <a href="../probabilities/#ComplexityMeasures.OutcomeSpace"><code>OutcomeSpace</code></a> may also be specified by a <a href="../probabilities/#ComplexityMeasures.ProbabilitiesEstimator"><code>ProbabilitiesEstimator</code></a>).</p><p>Like in <a href="../probabilities/#ComplexityMeasures.outcome_space"><code>outcome_space</code></a>, for some outcome spaces, the possible outcomes are known without knowledge of input <code>x</code>, in which case the function dispatches to <code>information_maximum(e, est)</code>.</p><pre><code class="nohighlight hljs">information_maximum(e::InformationMeasure, L::Int)</code></pre><p>The same as above, but computed directly from the number of total outcomes <code>L</code>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaDynamics/ComplexityMeasures.jl/blob/b03edd0c9aead13b35b293bd453f20190ff6f2c4/src/core/information_functions.jl#L111-L126">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="ComplexityMeasures.information_normalized" href="#ComplexityMeasures.information_normalized"><code>ComplexityMeasures.information_normalized</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">information_normalized([e::DiscreteInfoEstimator,] o::OutcomeSpace, x) → h̃
information_normalized([e::DiscreteInfoEstimator,] est::ProbabilitiesEstimator, x) → h̃</code></pre><p>Estimate <code>h̃</code>, a normalized discrete information measure, from input data <code>x</code>, using the <a href="#ComplexityMeasures.DiscreteInfoEstimator"><code>DiscreteInfoEstimator</code></a> <code>e</code>. This is just the value of <a href="#ComplexityMeasures.information-Tuple{InformationMeasure, OutcomeSpace, Any}"><code>information</code></a> divided by the maximum value for <code>e</code>, according to the given <a href="../probabilities/#ComplexityMeasures.OutcomeSpace"><code>OutcomeSpace</code></a> (which may be specified by <code>est</code> if not given directly).</p><p>Instead of a discrete information measure estimator, an <a href="#ComplexityMeasures.InformationMeasure"><code>InformationMeasure</code></a> can be given as first argument, in which case <a href="#ComplexityMeasures.PlugIn"><code>PlugIn</code></a> estimation is used.  If <code>e</code> is not given, it defaults to <code>Shannon()</code>.</p><p>Notice that there is no method <code>information_normalized(e::DiscreteInfoEstimator, probs::Probabilities)</code>, because there is no way to know the number of <em>possible</em> outcomes (i.e., the <a href="../probabilities/#ComplexityMeasures.total_outcomes"><code>total_outcomes</code></a>) from <code>probs</code>.</p><p><strong>Normalized values</strong></p><p>For the <a href="#ComplexityMeasures.PlugIn"><code>PlugIn</code></a> estimator, it is guaranteed that <code>h̃ ∈ [0, 1]</code>. For any other estimator, we can&#39;t guarantee this, since the estimator might over-correct. You should know what you&#39;re doing if using anything but <a href="#ComplexityMeasures.PlugIn"><code>PlugIn</code></a> to estimate normalized values.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaDynamics/ComplexityMeasures.jl/blob/b03edd0c9aead13b35b293bd453f20190ff6f2c4/src/core/information_functions.jl#L142-L166">source</a></section></article><h3 id="Discrete-information-estimators"><a class="docs-heading-anchor" href="#Discrete-information-estimators">Discrete information estimators</a><a id="Discrete-information-estimators-1"></a><a class="docs-heading-anchor-permalink" href="#Discrete-information-estimators" title="Permalink"></a></h3><article class="docstring"><header><a class="docstring-binding" id="ComplexityMeasures.DiscreteInfoEstimator" href="#ComplexityMeasures.DiscreteInfoEstimator"><code>ComplexityMeasures.DiscreteInfoEstimator</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">DiscreteInfoEstimator</code></pre><p>The supertype of all discrete information measure estimators, which are used in combination with a <a href="../probabilities/#ComplexityMeasures.ProbabilitiesEstimator"><code>ProbabilitiesEstimator</code></a> as input to  <a href="#ComplexityMeasures.information-Tuple{InformationMeasure, OutcomeSpace, Any}"><code>information</code></a> or related functions.</p><p>The first argument to a discrete estimator is always an <a href="#ComplexityMeasures.InformationMeasure"><code>InformationMeasure</code></a> (defaults to <a href="#ComplexityMeasures.Shannon"><code>Shannon</code></a>).</p><p><strong>Description</strong></p><p>A discrete <a href="#ComplexityMeasures.InformationMeasure"><code>InformationMeasure</code></a> is a functional of a probability mass function. To estimate such a measure from data, we must first estimate a probability mass function using a <a href="../probabilities/#ComplexityMeasures.ProbabilitiesEstimator"><code>ProbabilitiesEstimator</code></a> from the (encoded/discretized) input data, and then apply the estimator to the estimated probabilities. For example, the <a href="#ComplexityMeasures.Shannon"><code>Shannon</code></a> entropy is typically computed using the <a href="../probabilities/#ComplexityMeasures.MLE"><code>MLE</code></a> estimator to compute probabilities, which are then given to the <a href="#ComplexityMeasures.PlugIn"><code>PlugIn</code></a> estimator. Many other estimators exist, not only for <a href="#ComplexityMeasures.Shannon"><code>Shannon</code></a> entropy, but other information measures as well.</p><p>We provide a library of both generic estimators such as <a href="#ComplexityMeasures.PlugIn"><code>PlugIn</code></a> or <a href="#ComplexityMeasures.Jackknife"><code>Jackknife</code></a> (which can be applied to any measure), as well as dedicated estimators such as <a href="#ComplexityMeasures.MillerMadow"><code>MillerMadow</code></a>, which computes <a href="#ComplexityMeasures.Shannon"><code>Shannon</code></a> entropy using the Miller-Madow bias correction. The list below gives a complete overview.</p><p><strong>Implementations</strong></p><p>The following estimators are generic and can compute any <a href="#ComplexityMeasures.InformationMeasure"><code>InformationMeasure</code></a>.</p><ul><li><a href="#ComplexityMeasures.PlugIn"><code>PlugIn</code></a>. The default, generic plug-in estimator of any information measure.   It computes the measure exactly as stated in the definition, using the provided   probabilities.</li><li><a href="#ComplexityMeasures.Jackknife"><code>Jackknife</code></a>. Uses the a combination of the plug-in estimator and the jackknife   principle to estimate an <a href="#ComplexityMeasures.InformationMeasure"><code>InformationMeasure</code></a>.</li></ul><p><strong><a href="#ComplexityMeasures.Shannon"><code>Shannon</code></a> entropy estimators</strong></p><p>The following estimators are dedicated <a href="#ComplexityMeasures.Shannon"><code>Shannon</code></a> entropy estimators, which provide improvements over the naive <a href="#ComplexityMeasures.PlugIn"><code>PlugIn</code></a> estimator.</p><ul><li><a href="#ComplexityMeasures.MillerMadow"><code>MillerMadow</code></a>.</li><li><a href="#ComplexityMeasures.HorvitzThompson"><code>HorvitzThompson</code></a>.</li><li><a href="#ComplexityMeasures.Schürmann"><code>Schürmann</code></a>.</li><li><a href="#ComplexityMeasures.GeneralizedSchürmann"><code>GeneralizedSchürmann</code></a>.</li><li><a href="#ComplexityMeasures.ChaoShen"><code>ChaoShen</code></a>.</li></ul><div class="admonition is-info"><header class="admonition-header">Info</header><div class="admonition-body"><p>Any of the implemented <a href="#ComplexityMeasures.DiscreteInfoEstimator"><code>DiscreteInfoEstimator</code></a>s can be used in combination with <em>any</em> <a href="../probabilities/#ComplexityMeasures.ProbabilitiesEstimator"><code>ProbabilitiesEstimator</code></a> as input to <a href="#ComplexityMeasures.information-Tuple{InformationMeasure, OutcomeSpace, Any}"><code>information</code></a>. What this means is that every estimator actually comes in many different variants - one for each <a href="../probabilities/#ComplexityMeasures.ProbabilitiesEstimator"><code>ProbabilitiesEstimator</code></a>. For example, the <a href="#ComplexityMeasures.MillerMadow"><code>MillerMadow</code></a> estimator of <a href="#ComplexityMeasures.Shannon"><code>Shannon</code></a> entropy is typically calculated with <a href="../probabilities/#ComplexityMeasures.MLE"><code>MLE</code></a> probabilities. But here, you can use for example the <a href="../probabilities/#ComplexityMeasures.Bayes"><code>Bayes</code></a> or the <a href="../probabilities/#ComplexityMeasures.Shrinkage"><code>Shrinkage</code></a> probabilities estimators instead, i.e. <code>information(MillerMadow(), MLE(outcome_space), x)</code> and <code>information(MillerMadow(), Bayes(outcomes_space), x)</code> are distinct estimators. This holds for all <a href="#ComplexityMeasures.DiscreteInfoEstimator"><code>DiscreteInfoEstimator</code></a>s. Many of these estimators haven&#39;t been explored in the literature before, so feel free to explore, and please cite this software if you use it to explore some new estimator combination!</p></div></div><p>More estimators will be added in the future (<a href="https://github.com/JuliaDynamics/ComplexityMeasures.jl/issues/237">#237</a>).</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaDynamics/ComplexityMeasures.jl/blob/b03edd0c9aead13b35b293bd453f20190ff6f2c4/src/core/information_measures.jl#L92-L155">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="ComplexityMeasures.PlugIn" href="#ComplexityMeasures.PlugIn"><code>ComplexityMeasures.PlugIn</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">PlugIn(e::InformationMeasure) &lt;: DiscreteInfoEstimator</code></pre><p>The <code>PlugIn</code> estimator is also called the empirical/naive/&quot;maximum likelihood&quot; estimator, and is used with <a href="#ComplexityMeasures.information-Tuple{InformationMeasure, OutcomeSpace, Any}"><code>information</code></a> to any discrete <a href="#ComplexityMeasures.InformationMeasure"><code>InformationMeasure</code></a>.</p><p>It computes any quantity exactly as given by its formula. When computing an information measure, which here is defined as a probabilities functional, it computes the quantity directly from a probability mass function, which is derived from maximum-likelihood (<a href="../probabilities/#ComplexityMeasures.MLE"><code>MLE</code></a> estimates of the probabilities.</p><p><strong>Bias of plug-in estimates</strong></p><p>The plugin-estimator of <a href="#ComplexityMeasures.Shannon"><code>Shannon</code></a> entropy underestimates the true entropy, with a bias that grows with the number of distinct <a href="../probabilities/#ComplexityMeasures.outcomes"><code>outcomes</code></a> (Arora et al., 2022)<sup class="footnote-reference"><a id="citeref-Arora2022" href="#footnote-Arora2022">[Arora2022]</a></sup>:</p><p class="math-container">\[bias(H_S^{plugin}) = -\dfrac{K-1}{2N} + o(N^-1).\]</p><p>where <code>K</code> is the number of distinct outcomes, and <code>N</code> is the sample size. Many authors have tried to remedy this by proposing alternative Shannon entropy estimators. For example, the <a href="#ComplexityMeasures.MillerMadow"><code>MillerMadow</code></a> estimator is a simple correction to the plug-in estimator that adds back the bias term above. Many other estimators exist; see <a href="#ComplexityMeasures.DiscreteInfoEstimator"><code>DiscreteInfoEstimator</code></a>s for an overview.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaDynamics/ComplexityMeasures.jl/blob/b03edd0c9aead13b35b293bd453f20190ff6f2c4/src/discrete_info_estimators/plugin.jl#L4-L34">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="ComplexityMeasures.MillerMadow" href="#ComplexityMeasures.MillerMadow"><code>ComplexityMeasures.MillerMadow</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">MillerMadow &lt;: DiscreteInfoEstimator
MillerMadow(measure::Shannon = Shannon())</code></pre><p>The <code>MillerMadow</code> estimator is used with <a href="#ComplexityMeasures.information-Tuple{InformationMeasure, OutcomeSpace, Any}"><code>information</code></a> to compute the discrete <a href="#ComplexityMeasures.Shannon"><code>Shannon</code></a> entropy according to Miller (1955)<sup class="footnote-reference"><a id="citeref-Miller1955" href="#footnote-Miller1955">[Miller1955]</a></sup>.</p><p><strong>Description</strong></p><p>The Miller-Madow estimator of Shannon entropy is given by</p><p class="math-container">\[H_S^{MM} = H_S^{plugin} + \dfrac{m - 1}{2N},\]</p><p>where <span>$H_S^{plugin}$</span> is the Shannon entropy estimated using the <a href="#ComplexityMeasures.PlugIn"><code>PlugIn</code></a> estimator, <code>m</code> is the number of bins with nonzero probability (as defined in Paninski, 2003<sup class="footnote-reference"><a id="citeref-Paninski2003" href="#footnote-Paninski2003">[Paninski2003]</a></sup>), and <code>N</code> is the number of observations.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaDynamics/ComplexityMeasures.jl/blob/b03edd0c9aead13b35b293bd453f20190ff6f2c4/src/discrete_info_estimators/miller_madow.jl#L3-L28">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="ComplexityMeasures.Schürmann" href="#ComplexityMeasures.Schürmann"><code>ComplexityMeasures.Schürmann</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">Schürmann &lt;: DiscreteInfoEstimator
Schürmann(definition::Shannon; a = 1.0)</code></pre><p>The <code>Schürmann</code> estimator is used with <a href="#ComplexityMeasures.information-Tuple{InformationMeasure, OutcomeSpace, Any}"><code>information</code></a> to compute the discrete <a href="#ComplexityMeasures.Shannon"><code>Shannon</code></a> entropy with the bias-corrected estimator given in Schürmann (2004)<sup class="footnote-reference"><a id="citeref-Schürmann2004" href="#footnote-Schürmann2004">[Schürmann2004]</a></sup>.</p><p>See detailed description for <a href="#ComplexityMeasures.GeneralizedSchürmann"><code>GeneralizedSchürmann</code></a> for details.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaDynamics/ComplexityMeasures.jl/blob/b03edd0c9aead13b35b293bd453f20190ff6f2c4/src/discrete_info_estimators/schurmann.jl#L6-L19">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="ComplexityMeasures.GeneralizedSchürmann" href="#ComplexityMeasures.GeneralizedSchürmann"><code>ComplexityMeasures.GeneralizedSchürmann</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">GeneralizedSchürmann &lt;: DiscreteInfoEstimator
GeneralizedSchürmann(measure::Shannon; a::Union{&lt;:Real, Vector{&lt;:Real}} = 1.0)</code></pre><p>The <code>GeneralizedSchürmann</code> estimator is used with <a href="#ComplexityMeasures.information-Tuple{InformationMeasure, OutcomeSpace, Any}"><code>information</code></a> to compute the discrete <a href="#ComplexityMeasures.Shannon"><code>Shannon</code></a> entropy with the bias-corrected estimator given in Grassberger (2022)<sup class="footnote-reference"><a id="citeref-Grassberger2022" href="#footnote-Grassberger2022">[Grassberger2022]</a></sup>.</p><p>The &quot;generalized&quot; part of the name, as opposed to the <a href="@ref"><code>Schürmann2004</code></a> estimator, is due to the possibility of picking difference parameters <span>$a_i$</span> for different outcomes. If different parameters are assigned to the different outcomes, <code>a</code> must be a vector of parameters of length <code>length(outcomes)</code>, where the outcomes are obtained using <a href="../probabilities/#ComplexityMeasures.outcomes"><code>outcomes</code></a>. See Grassberger (2022) for more information. If <code>a</code> is a real number, then <span>$a_i = a \forall i$</span>, and the estimator reduces to the <a href="#ComplexityMeasures.Schürmann"><code>Schürmann</code></a> estimator.</p><p><strong>Description</strong></p><p>For a set of <span>$N$</span> observations over <span>$M$</span> outcomes, the estimator is given by</p><p class="math-container">\[H_S^{opt} = \varphi(N) - \dfrac{1}{N} \sum_{i=1}^M n_i G_{n_i}(a_i),\]</p><p>where <span>$n_i$</span> is the observed frequency of the i-th outcome,</p><p class="math-container">\[G_n(a) = \varphi(n) + (-1)^n \int_0^a \dfrac{x^{n - 1}}{x + 1} dx,\]</p><p><span>$G_n(1) = G_n$</span> and <span>$G_n(0) = \varphi(n)$</span>, and</p><p class="math-container">\[G_n = \varphi(n) + (-1)^n \int_0^1 \dfrac{x^{n - 1}}{x + 1} dx.\]</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaDynamics/ComplexityMeasures.jl/blob/b03edd0c9aead13b35b293bd453f20190ff6f2c4/src/discrete_info_estimators/schurmann_generalized.jl#L3-L41">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="ComplexityMeasures.Jackknife" href="#ComplexityMeasures.Jackknife"><code>ComplexityMeasures.Jackknife</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">Jackknife &lt;: DiscreteInfoEstimator
Jackknife(definition::InformationMeasure = Shannon())</code></pre><p>The <code>Jackknife</code> estimator is used with <a href="#ComplexityMeasures.information-Tuple{InformationMeasure, OutcomeSpace, Any}"><code>information</code></a> to compute any discrete <a href="#ComplexityMeasures.InformationMeasure"><code>InformationMeasure</code></a>.</p><p>The <code>Jackknife</code> estimator uses the generic jackknife principle to reduce bias. Zahl (1977)<sup class="footnote-reference"><a id="citeref-Zahl1977" href="#footnote-Zahl1977">[Zahl1977]</a></sup> was the first to apply the jaccknife technique in the context of <a href="#ComplexityMeasures.Shannon"><code>Shannon</code></a> entropy estimation. Here, we&#39;ve generalized his estimator to work with any <a href="#ComplexityMeasures.InformationMeasure"><code>InformationMeasure</code></a>.</p><p><strong>Description</strong></p><p>As an example of the jackknife technique, here is the formula for a jackknife estimate of <a href="#ComplexityMeasures.Shannon"><code>Shannon</code></a> entropy</p><p class="math-container">\[H_S^{J} = N H_S^{plugin} - \dfrac{N-1}{N} \sum_{i=1}^N {H_S^{plugin}}^{-\{i\}},\]</p><p>where <span>$N$</span> is the sample size, <span>$H_S^{plugin}$</span> is the plugin estimate of Shannon entropy, and <span>${H_S^{plugin}}^{-\{i\}}$</span> is the plugin estimate, but computed with the <span>$i$</span>-th sample left out.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaDynamics/ComplexityMeasures.jl/blob/b03edd0c9aead13b35b293bd453f20190ff6f2c4/src/discrete_info_estimators/jackknife.jl#L3-L30">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="ComplexityMeasures.HorvitzThompson" href="#ComplexityMeasures.HorvitzThompson"><code>ComplexityMeasures.HorvitzThompson</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">HorvitzThompson &lt;: DiscreteInfoEstimator
HorvitzThompson(measure::Shannon = Shannon())</code></pre><p>The <code>HorvitzThompson</code> estimator is used with <a href="#ComplexityMeasures.information-Tuple{InformationMeasure, OutcomeSpace, Any}"><code>information</code></a> to compute the discrete <a href="#ComplexityMeasures.Shannon"><code>Shannon</code></a> entropy according to Horvitz and Thompson (1952)<sup class="footnote-reference"><a id="citeref-Horvitz1952" href="#footnote-Horvitz1952">[Horvitz1952]</a></sup>.</p><p><strong>Description</strong></p><p>The Horvitz-Thompson estimator of <a href="#ComplexityMeasures.Shannon"><code>Shannon</code></a> entropy is given by</p><p class="math-container">\[H_S^{HT} = -\sum_{i=1}^M \dfrac{p_i \log(p_i) }{1 - (1 - p_i)^N},\]</p><p>where <span>$N$</span> is the sample size and <span>$M$</span> is the number of <a href="../probabilities/#ComplexityMeasures.outcomes"><code>outcomes</code></a>. Given the true probability <span>$p_i$</span> of the <span>$i$</span>-th outcome, <span>$1 - (1 - p_i)^N$</span> is the probability that the outcome appears at least once in a sample of size <span>$N$</span> (Arora et al., 2022). Dividing by this inclusion probability is a form of weighting, and compensates for situations where certain outcomes have so low probabilities that they are not often observed in a sample, for example in power-law distributions.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaDynamics/ComplexityMeasures.jl/blob/b03edd0c9aead13b35b293bd453f20190ff6f2c4/src/discrete_info_estimators/horvitz_thompson.jl#L3-L33">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="ComplexityMeasures.ChaoShen" href="#ComplexityMeasures.ChaoShen"><code>ComplexityMeasures.ChaoShen</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">ChaoShen &lt;: DiscreteInfoEstimator
ChaoShen(definition::Shannon = Shannon())</code></pre><p>The <code>ChaoShen</code> estimator is used with <a href="#ComplexityMeasures.information-Tuple{InformationMeasure, OutcomeSpace, Any}"><code>information</code></a> to compute the discrete <a href="#ComplexityMeasures.Shannon"><code>Shannon</code></a> entropy according to Chao &amp; Shen (2003)<sup class="footnote-reference"><a id="citeref-Chao2003" href="#footnote-Chao2003">[Chao2003]</a></sup>.</p><p><strong>Description</strong></p><p>This estimator is a modification of the <a href="#ComplexityMeasures.HorvitzThompson"><code>HorvitzThompson</code></a> estimator that multiplies each plugin probability estimate by an estimate of sample coverage. If <span>$f_1$</span> is the number of singletons (outcomes that occur only once) in a sample of length <span>$N$</span>, then the sample coverage is <span>$C = 1 - \dfrac{f_1}{N}$</span>. The Chao-Shen estimator of Shannon entropy is then</p><p class="math-container">\[H_S^{CS} = -\sum_{i=1}^M \left( \dfrac{C p_i \log(C p_i)}{1 - (1 - C p_i)^N} \right),\]</p><p>where <span>$N$</span> is the sample size and <span>$M$</span> is the number of <a href="../probabilities/#ComplexityMeasures.outcomes"><code>outcomes</code></a>. If <span>$f_1 = N$</span>, then <span>$f_1$</span> is set to <span>$f_1 = N - 1$</span> to ensure positive entropy (Arora et al., 2022)<sup class="footnote-reference"><a id="citeref-Arora2022" href="#footnote-Arora2022">[Arora2022]</a></sup>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaDynamics/ComplexityMeasures.jl/blob/b03edd0c9aead13b35b293bd453f20190ff6f2c4/src/discrete_info_estimators/chao_shen.jl#L3-L34">source</a></section></article><h2 id="Estimation-(differential)"><a class="docs-heading-anchor" href="#Estimation-(differential)">Estimation (differential)</a><a id="Estimation-(differential)-1"></a><a class="docs-heading-anchor-permalink" href="#Estimation-(differential)" title="Permalink"></a></h2><article class="docstring"><header><a class="docstring-binding" id="ComplexityMeasures.information-Tuple{DifferentialInfoEstimator, Any}" href="#ComplexityMeasures.information-Tuple{DifferentialInfoEstimator, Any}"><code>ComplexityMeasures.information</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">information(est::DifferentialInfoEstimator, x) → h::Real</code></pre><p>Estimate a <strong>differential information measure</strong> using the provided <a href="#ComplexityMeasures.DifferentialInfoEstimator"><code>DifferentialInfoEstimator</code></a> and input data <code>x</code>.</p><p><strong>Description</strong></p><p>The overwhelming majority of differential estimators estimate the <a href="#ComplexityMeasures.Shannon"><code>Shannon</code></a> entropy. If the same estimator can estimate different information measures (e.g. it can estimate both <a href="#ComplexityMeasures.Shannon"><code>Shannon</code></a> and <a href="#ComplexityMeasures.Tsallis"><code>Tsallis</code></a>), then the information measure is provided as an argument to the estimator itself.</p><p>See the <a href="#table_diff_ent_est">table of differential information measure estimators</a> in the docs for all differential information measure estimators.</p><p>Currently, unlike for the discrete information measures, this method doesn&#39;t involve explicitly first computing a probability density function and then passing this density to an information measure definition. But in the future, we want to establish a <code>density</code> API similar to the <a href="../probabilities/#ComplexityMeasures.probabilities"><code>probabilities</code></a> API.</p><p><strong>Examples</strong></p><p>To compute the differential version of a measure, give it as the first argument to a <a href="#ComplexityMeasures.DifferentialInfoEstimator"><code>DifferentialInfoEstimator</code></a> and pass it to <a href="#ComplexityMeasures.information-Tuple{InformationMeasure, OutcomeSpace, Any}"><code>information</code></a>.</p><pre><code class="language-julia hljs">x = randn(1000)
h_sh = information(Kraskov(Shannon()), x)
h_vc = information(Vasicek(Shannon()), x)</code></pre><p>A normal distribution has a base-e Shannon differential entropy of <code>0.5*log(2π) + 0.5</code> nats.</p><pre><code class="language-julia hljs">est = Kraskov(k = 5, base = ℯ) # Base `ℯ` for nats.
h = information(est, randn(2_000_000))
abs(h - 0.5*log(2π) - 0.5) # ≈ 0.0001</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaDynamics/ComplexityMeasures.jl/blob/b03edd0c9aead13b35b293bd453f20190ff6f2c4/src/core/information_functions.jl#L185-L225">source</a></section></article><h3 id="Differential-information-estimators"><a class="docs-heading-anchor" href="#Differential-information-estimators">Differential information estimators</a><a id="Differential-information-estimators-1"></a><a class="docs-heading-anchor-permalink" href="#Differential-information-estimators" title="Permalink"></a></h3><article class="docstring"><header><a class="docstring-binding" id="ComplexityMeasures.DifferentialInfoEstimator" href="#ComplexityMeasures.DifferentialInfoEstimator"><code>ComplexityMeasures.DifferentialInfoEstimator</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">DifferentialInfoEstimator</code></pre><p>The supertype of all differential information measure estimators. These estimators compute an information measure in various ways that do not involve explicitly estimating a probability distribution.</p><p>Each <a href="#ComplexityMeasures.DifferentialInfoEstimator"><code>DifferentialInfoEstimator</code></a>s uses a specialized technique to approximate relevant densities/integrals, and is often tailored to one or a few types of information measures. For example, <a href="#ComplexityMeasures.Kraskov"><code>Kraskov</code></a> estimates the <a href="#ComplexityMeasures.Shannon"><code>Shannon</code></a> entropy.</p><p>See <a href="#ComplexityMeasures.information-Tuple{InformationMeasure, OutcomeSpace, Any}"><code>information</code></a> for usage.</p><p><strong>Implementations</strong></p><ul><li><a href="#ComplexityMeasures.KozachenkoLeonenko"><code>KozachenkoLeonenko</code></a>.</li><li><a href="#ComplexityMeasures.Kraskov"><code>Kraskov</code></a>.</li><li><a href="#ComplexityMeasures.Goria"><code>Goria</code></a>.</li><li><a href="#ComplexityMeasures.Gao"><code>Gao</code></a>.</li><li><a href="#ComplexityMeasures.Zhu"><code>Zhu</code></a></li><li><a href="#ComplexityMeasures.ZhuSingh"><code>ZhuSingh</code></a>.</li><li><a href="#ComplexityMeasures.Lord"><code>Lord</code></a>.</li><li><a href="#ComplexityMeasures.AlizadehArghami"><code>AlizadehArghami</code></a>.</li><li><a href="#ComplexityMeasures.Correa"><code>Correa</code></a>.</li><li><a href="#ComplexityMeasures.Vasicek"><code>Vasicek</code></a>.</li><li><a href="#ComplexityMeasures.Ebrahimi"><code>Ebrahimi</code></a>.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaDynamics/ComplexityMeasures.jl/blob/b03edd0c9aead13b35b293bd453f20190ff6f2c4/src/core/information_measures.jl#L162-L188">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="ComplexityMeasures.Kraskov" href="#ComplexityMeasures.Kraskov"><code>ComplexityMeasures.Kraskov</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">Kraskov &lt;: DifferentialInfoEstimator
Kraskov(measure = Shannon(); k::Int = 1, w::Int = 0, base = 2)</code></pre><p>The <code>Kraskov</code> estimator computes the <a href="#ComplexityMeasures.Shannon"><code>Shannon</code></a> differential <a href="#ComplexityMeasures.information-Tuple{InformationMeasure, OutcomeSpace, Any}"><code>information</code></a> of a multi-dimensional <a href="../#StateSpaceSets.StateSpaceSet"><code>StateSpaceSet</code></a> using the <code>k</code>-th nearest neighbor searches method from <sup class="footnote-reference"><a id="citeref-Kraskov2004" href="#footnote-Kraskov2004">[Kraskov2004]</a></sup> at the given <code>base</code>.</p><p><code>w</code> is the Theiler window, which determines if temporal neighbors are excluded during neighbor searches (defaults to <code>0</code>, meaning that only the point itself is excluded when searching for neighbours).</p><p><strong>Description</strong></p><p>Assume we have samples <span>$\{\bf{x}_1, \bf{x}_2, \ldots, \bf{x}_N \}$</span> from a continuous random variable <span>$X \in \mathbb{R}^d$</span> with support <span>$\mathcal{X}$</span> and density function<span>$f : \mathbb{R}^d \to \mathbb{R}$</span>. <code>Kraskov</code> estimates the <a href="@ref">Shannon</a> differential entropy</p><p class="math-container">\[H(X) = \int_{\mathcal{X}} f(x) \log f(x) dx = \mathbb{E}[-\log(f(X))].\]</p><p>See also: <a href="#ComplexityMeasures.information-Tuple{InformationMeasure, OutcomeSpace, Any}"><code>information</code></a>, <a href="#ComplexityMeasures.KozachenkoLeonenko"><code>KozachenkoLeonenko</code></a>, <a href="#ComplexityMeasures.DifferentialInfoEstimator"><code>DifferentialInfoEstimator</code></a>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaDynamics/ComplexityMeasures.jl/blob/b03edd0c9aead13b35b293bd453f20190ff6f2c4/src/differential_info_estimators/nearest_neighbors/Kraskov.jl#L3-L31">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="ComplexityMeasures.KozachenkoLeonenko" href="#ComplexityMeasures.KozachenkoLeonenko"><code>ComplexityMeasures.KozachenkoLeonenko</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">KozachenkoLeonenko &lt;: DifferentialInfoEstimator
KozachenkoLeonenko(measure = Shannon(); w::Int = 0, base = 2)</code></pre><p>The <code>KozachenkoLeonenko</code> estimator computes the <a href="#ComplexityMeasures.Shannon"><code>Shannon</code></a> differential <a href="#ComplexityMeasures.information-Tuple{InformationMeasure, OutcomeSpace, Any}"><code>information</code></a> of a multi-dimensional <a href="../#StateSpaceSets.StateSpaceSet"><code>StateSpaceSet</code></a> in the given <code>base</code>.</p><p><strong>Description</strong></p><p>Assume we have samples <span>$\{\bf{x}_1, \bf{x}_2, \ldots, \bf{x}_N \}$</span> from a continuous random variable <span>$X \in \mathbb{R}^d$</span> with support <span>$\mathcal{X}$</span> and density function<span>$f : \mathbb{R}^d \to \mathbb{R}$</span>. <code>KozachenkoLeonenko</code> estimates the <a href="@ref">Shannon</a> differential entropy</p><p class="math-container">\[H(X) = \int_{\mathcal{X}} f(x) \log f(x) dx = \mathbb{E}[-\log(f(X))]\]</p><p>using the nearest neighbor method from Kozachenko &amp; Leonenko (1987)<sup class="footnote-reference"><a id="citeref-KozachenkoLeonenko1987" href="#footnote-KozachenkoLeonenko1987">[KozachenkoLeonenko1987]</a></sup>, as described in Charzyńska and Gambin<sup class="footnote-reference"><a id="citeref-Charzyńska2016" href="#footnote-Charzyńska2016">[Charzyńska2016]</a></sup>.</p><p><code>w</code> is the Theiler window, which determines if temporal neighbors are excluded during neighbor searches (defaults to <code>0</code>, meaning that only the point itself is excluded when searching for neighbours).</p><p>In contrast to <a href="#ComplexityMeasures.Kraskov"><code>Kraskov</code></a>, this estimator uses only the <em>closest</em> neighbor.</p><p>See also: <a href="#ComplexityMeasures.information-Tuple{InformationMeasure, OutcomeSpace, Any}"><code>information</code></a>, <a href="#ComplexityMeasures.Kraskov"><code>Kraskov</code></a>, <a href="#ComplexityMeasures.DifferentialInfoEstimator"><code>DifferentialInfoEstimator</code></a>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaDynamics/ComplexityMeasures.jl/blob/b03edd0c9aead13b35b293bd453f20190ff6f2c4/src/differential_info_estimators/nearest_neighbors/KozachenkoLeonenko.jl#L3-L38">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="ComplexityMeasures.Zhu" href="#ComplexityMeasures.Zhu"><code>ComplexityMeasures.Zhu</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">Zhu &lt;: DifferentialInfoEstimator
Zhu(; measure = Shannon(), k = 1, w = 0, base = 2)</code></pre><p>The <code>Zhu</code> estimator (Zhu et al., 2015)<sup class="footnote-reference"><a id="citeref-Zhu2015" href="#footnote-Zhu2015">[Zhu2015]</a></sup> is an extension to <a href="#ComplexityMeasures.KozachenkoLeonenko"><code>KozachenkoLeonenko</code></a>, and computes the <a href="#ComplexityMeasures.Shannon"><code>Shannon</code></a> differential <a href="#ComplexityMeasures.information-Tuple{InformationMeasure, OutcomeSpace, Any}"><code>information</code></a> of a multi-dimensional <a href="../#StateSpaceSets.StateSpaceSet"><code>StateSpaceSet</code></a> in the given <code>base</code>.</p><p><strong>Description</strong></p><p>Assume we have samples <span>$\{\bf{x}_1, \bf{x}_2, \ldots, \bf{x}_N \}$</span> from a continuous random variable <span>$X \in \mathbb{R}^d$</span> with support <span>$\mathcal{X}$</span> and density function<span>$f : \mathbb{R}^d \to \mathbb{R}$</span>. <code>Zhu</code> estimates the <a href="@ref">Shannon</a> differential entropy</p><p class="math-container">\[H(X) = \int_{\mathcal{X}} f(x) \log f(x) dx = \mathbb{E}[-\log(f(X))]\]</p><p>by approximating densities within hyperrectangles surrounding each point <code>xᵢ ∈ x</code> using using <code>k</code> nearest neighbor searches. <code>w</code> is the Theiler window, which determines if temporal neighbors are excluded during neighbor searches (defaults to <code>0</code>, meaning that only the point itself is excluded when searching for neighbours).</p><p>See also: <a href="#ComplexityMeasures.information-Tuple{InformationMeasure, OutcomeSpace, Any}"><code>information</code></a>, <a href="#ComplexityMeasures.KozachenkoLeonenko"><code>KozachenkoLeonenko</code></a>, <a href="#ComplexityMeasures.DifferentialInfoEstimator"><code>DifferentialInfoEstimator</code></a>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaDynamics/ComplexityMeasures.jl/blob/b03edd0c9aead13b35b293bd453f20190ff6f2c4/src/differential_info_estimators/nearest_neighbors/Zhu.jl#L3-L33">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="ComplexityMeasures.ZhuSingh" href="#ComplexityMeasures.ZhuSingh"><code>ComplexityMeasures.ZhuSingh</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">ZhuSingh &lt;: DifferentialInfoEstimator
ZhuSingh(measure = Shannon(); k = 1, w = 0, base = 2)</code></pre><p>The <code>ZhuSingh</code> estimator (Zhu et al., 2015)<sup class="footnote-reference"><a id="citeref-Zhu2015" href="#footnote-Zhu2015">[Zhu2015]</a></sup> computes the <a href="#ComplexityMeasures.Shannon"><code>Shannon</code></a> differential <a href="#ComplexityMeasures.information-Tuple{InformationMeasure, OutcomeSpace, Any}"><code>information</code></a> of a multi-dimensional <a href="../#StateSpaceSets.StateSpaceSet"><code>StateSpaceSet</code></a> in the given <code>base</code>.</p><p><strong>Description</strong></p><p>Assume we have samples <span>$\{\bf{x}_1, \bf{x}_2, \ldots, \bf{x}_N \}$</span> from a continuous random variable <span>$X \in \mathbb{R}^d$</span> with support <span>$\mathcal{X}$</span> and density function<span>$f : \mathbb{R}^d \to \mathbb{R}$</span>. <code>ZhuSingh</code> estimates the <a href="@ref">Shannon</a> differential entropy</p><p class="math-container">\[H(X) = \int_{\mathcal{X}} f(x) \log f(x) dx = \mathbb{E}[-\log(f(X))].\]</p><p>Like <a href="#ComplexityMeasures.Zhu"><code>Zhu</code></a>, this estimator approximates probabilities within hyperrectangles surrounding each point <code>xᵢ ∈ x</code> using using <code>k</code> nearest neighbor searches. However, it also considers the number of neighbors falling on the borders of these hyperrectangles. This estimator is an extension to the entropy estimator in Singh et al. (2003).</p><p><code>w</code> is the Theiler window, which determines if temporal neighbors are excluded during neighbor searches (defaults to <code>0</code>, meaning that only the point itself is excluded when searching for neighbours).</p><p>See also: <a href="#ComplexityMeasures.information-Tuple{InformationMeasure, OutcomeSpace, Any}"><code>information</code></a>, <a href="#ComplexityMeasures.DifferentialInfoEstimator"><code>DifferentialInfoEstimator</code></a>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaDynamics/ComplexityMeasures.jl/blob/b03edd0c9aead13b35b293bd453f20190ff6f2c4/src/differential_info_estimators/nearest_neighbors/ZhuSingh.jl#L8-L47">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="ComplexityMeasures.Gao" href="#ComplexityMeasures.Gao"><code>ComplexityMeasures.Gao</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">Gao &lt;: DifferentialInfoEstimator
Gao(measure = Shannon(); k = 1, w = 0, base = 2, corrected = true)</code></pre><p>The <code>Gao</code> estimator (Gao et al., 2015) computes the <a href="#ComplexityMeasures.Shannon"><code>Shannon</code></a> differential <a href="#ComplexityMeasures.information-Tuple{InformationMeasure, OutcomeSpace, Any}"><code>information</code></a>, using a <code>k</code>-th nearest-neighbor approach based on Singh et al. (2003)<sup class="footnote-reference"><a id="citeref-Singh2003" href="#footnote-Singh2003">[Singh2003]</a></sup>.</p><p><code>w</code> is the Theiler window, which determines if temporal neighbors are excluded during neighbor searches (defaults to <code>0</code>, meaning that only the point itself is excluded when searching for neighbours).</p><p>Gao et al., 2015 give two variants of this estimator. If <code>corrected == false</code>, then the uncorrected version is used. If <code>corrected == true</code>, then the corrected version is used, which ensures that the estimator is asymptotically unbiased.</p><p><strong>Description</strong></p><p>Assume we have samples <span>$\{\bf{x}_1, \bf{x}_2, \ldots, \bf{x}_N \}$</span> from a continuous random variable <span>$X \in \mathbb{R}^d$</span> with support <span>$\mathcal{X}$</span> and density function<span>$f : \mathbb{R}^d \to \mathbb{R}$</span>. <code>KozachenkoLeonenko</code> estimates the <a href="@ref">Shannon</a> differential entropy</p><p class="math-container">\[H(X) = \int_{\mathcal{X}} f(x) \log f(x) dx = \mathbb{E}[-\log(f(X))]\]</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaDynamics/ComplexityMeasures.jl/blob/b03edd0c9aead13b35b293bd453f20190ff6f2c4/src/differential_info_estimators/nearest_neighbors/Gao.jl#L8-L43">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="ComplexityMeasures.Goria" href="#ComplexityMeasures.Goria"><code>ComplexityMeasures.Goria</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">Goria &lt;: DifferentialInfoEstimator
Goria(measure = Shannon(); k = 1, w = 0, base = 2)</code></pre><p>The <code>Goria</code> estimator computes the <a href="#ComplexityMeasures.Shannon"><code>Shannon</code></a> differential <a href="#ComplexityMeasures.information-Tuple{InformationMeasure, OutcomeSpace, Any}"><code>information</code></a> of a multi-dimensional <a href="../#StateSpaceSets.StateSpaceSet"><code>StateSpaceSet</code></a> in the given <code>base</code>.</p><p><strong>Description</strong></p><p>Assume we have samples <span>$\{\bf{x}_1, \bf{x}_2, \ldots, \bf{x}_N \}$</span> from a continuous random variable <span>$X \in \mathbb{R}^d$</span> with support <span>$\mathcal{X}$</span> and density function<span>$f : \mathbb{R}^d \to \mathbb{R}$</span>. <code>Goria</code> estimates the <a href="@ref">Shannon</a> differential entropy</p><p class="math-container">\[H(X) = \int_{\mathcal{X}} f(x) \log f(x) dx = \mathbb{E}[-\log(f(X))].\]</p><p>Specifically, let <span>$\bf{n}_1, \bf{n}_2, \ldots, \bf{n}_N$</span> be the distance of the samples <span>$\{\bf{x}_1, \bf{x}_2, \ldots, \bf{x}_N \}$</span> to their <code>k</code>-th nearest neighbors. Next, let the geometric mean of the distances be</p><p class="math-container">\[\hat{\rho}_k = \left( \prod_{i=1}^N \right)^{\dfrac{1}{N}}\]</p><p>Goria et al. (2005)<sup class="footnote-reference"><a id="citeref-Goria2005" href="#footnote-Goria2005">[Goria2005]</a></sup>&#39;s estimate of Shannon differential entropy is then</p><p class="math-container">\[\hat{H} = m\hat{\rho}_k + \log(N - 1) - \psi(k) + \log c_1(m),\]</p><p>where <span>$c_1(m) = \dfrac{2\pi^\frac{m}{2}}{m \Gamma(m/2)}$</span> and <span>$\psi$</span> is the digamma function.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaDynamics/ComplexityMeasures.jl/blob/b03edd0c9aead13b35b293bd453f20190ff6f2c4/src/differential_info_estimators/nearest_neighbors/Goria.jl#L8-L47">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="ComplexityMeasures.Lord" href="#ComplexityMeasures.Lord"><code>ComplexityMeasures.Lord</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">Lord &lt;: DifferentialInfoEstimator
Lord(measure = Shannon(); k = 10, w = 0, base = 2)</code></pre><p><code>Lord</code> estimates the <a href="#ComplexityMeasures.Shannon"><code>Shannon</code></a> differential <a href="#ComplexityMeasures.information-Tuple{InformationMeasure, OutcomeSpace, Any}"><code>information</code></a> using a nearest neighbor approach with a local nonuniformity correction (LNC).</p><p><code>w</code> is the Theiler window, which determines if temporal neighbors are excluded during neighbor searches (defaults to <code>0</code>, meaning that only the point itself is excluded when searching for neighbours).</p><p><strong>Description</strong></p><p>Assume we have samples <span>$\bar{X} = \{\bf{x}_1, \bf{x}_2, \ldots, \bf{x}_N \}$</span> from a continuous random variable <span>$X \in \mathbb{R}^d$</span> with support <span>$\mathcal{X}$</span> and density function <span>$f : \mathbb{R}^d \to \mathbb{R}$</span>. <code>Lord</code> estimates the <a href="@ref">Shannon</a> differential entropy</p><p class="math-container">\[H(X) = \int_{\mathcal{X}} f(x) \log f(x) dx = \mathbb{E}[-\log(f(X))],\]</p><p>by using the resubstitution formula</p><p class="math-container">\[\hat{\bar{X}, k} = -\mathbb{E}[\log(f(X))]
\approx \sum_{i = 1}^N \log(\hat{f}(\bf{x}_i)),\]</p><p>where <span>$\hat{f}(\bf{x}_i)$</span> is an estimate of the density at <span>$\bf{x}_i$</span> constructed in a manner such that <span>$\hat{f}(\bf{x}_i) \propto \dfrac{k(x_i) / N}{V_i}$</span>, where <span>$k(x_i)$</span> is the number of points in the neighborhood of <span>$\bf{x}_i$</span>, and <span>$V_i$</span> is the volume of that neighborhood.</p><p>While most nearest-neighbor based differential entropy estimators uses regular volume elements (e.g. hypercubes, hyperrectangles, hyperspheres) for approximating the local densities <span>$\hat{f}(\bf{x}_i)$</span>, the <code>Lord</code> estimator uses hyperellopsoid volume elements. These hyperellipsoids are, for each query point <code>xᵢ</code>, estimated using singular value decomposition (SVD) on the <code>k</code>-th nearest neighbors of <code>xᵢ</code>. Thus, the hyperellipsoids stretch/compress in response to the local geometry around each sample point. This makes <code>Lord</code> a well-suited entropy estimator for a wide range of systems.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaDynamics/ComplexityMeasures.jl/blob/b03edd0c9aead13b35b293bd453f20190ff6f2c4/src/differential_info_estimators/nearest_neighbors/Lord.jl#L25-L71">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="ComplexityMeasures.Vasicek" href="#ComplexityMeasures.Vasicek"><code>ComplexityMeasures.Vasicek</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">Vasicek &lt;: DifferentialInfoEstimator
Vasicek(measure = Shannon(); m::Int = 1, base = 2)</code></pre><p>The <code>Vasicek</code> estimator computes the <a href="#ComplexityMeasures.Shannon"><code>Shannon</code></a> differential <a href="#ComplexityMeasures.information-Tuple{InformationMeasure, OutcomeSpace, Any}"><code>information</code></a> (in the given <code>base</code>) of a timeseries using the method from Vasicek (1976)<sup class="footnote-reference"><a id="citeref-Vasicek1976" href="#footnote-Vasicek1976">[Vasicek1976]</a></sup>.</p><p>The <code>Vasicek</code> estimator belongs to a class of differential entropy estimators based on <a href="https://en.wikipedia.org/wiki/Order_statistic">order statistics</a>, of which Vasicek (1976) was the first. It only works for <em>timeseries</em> input.</p><p><strong>Description</strong></p><p>Assume we have samples <span>$\bar{X} = \{x_1, x_2, \ldots, x_N \}$</span> from a continuous random variable <span>$X \in \mathbb{R}$</span> with support <span>$\mathcal{X}$</span> and density function<span>$f : \mathbb{R} \to \mathbb{R}$</span>. <code>Vasicek</code> estimates the <a href="@ref">Shannon</a> differential entropy</p><p class="math-container">\[H(X) = \int_{\mathcal{X}} f(x) \log f(x) dx = \mathbb{E}[-\log(f(X))].\]</p><p>However, instead of estimating the above integral directly, it makes use of the equivalent integral, where <span>$F$</span> is the distribution function for <span>$X$</span>,</p><p class="math-container">\[H(X) = \int_0^1 \log \left(\dfrac{d}{dp}F^{-1}(p) \right) dp\]</p><p>This integral is approximated by first computing the <a href="https://en.wikipedia.org/wiki/Order_statistic">order statistics</a> of <span>$\bar{X}$</span> (the input timeseries), i.e. <span>$x_{(1)} \leq x_{(2)} \leq \cdots \leq x_{(n)}$</span>. The <code>Vasicek</code> <a href="#ComplexityMeasures.Shannon"><code>Shannon</code></a> differential entropy estimate is then</p><p class="math-container">\[\hat{H}_V(\bar{X}, m) =
\dfrac{1}{n}
\sum_{i = 1}^n \log \left[ \dfrac{n}{2m} (\bar{X}_{(i+m)} - \bar{X}_{(i-m)}) \right]\]</p><p><strong>Usage</strong></p><p>In practice, choice of <code>m</code> influences how fast the entropy converges to the true value. For small value of <code>m</code>, convergence is slow, so we recommend to scale <code>m</code> according to the time series length <code>n</code> and use <code>m &gt;= n/100</code> (this is just a heuristic based on the tests written for this package).</p><p>See also: <a href="#ComplexityMeasures.information-Tuple{InformationMeasure, OutcomeSpace, Any}"><code>information</code></a>, <a href="#ComplexityMeasures.Correa"><code>Correa</code></a>, <a href="#ComplexityMeasures.AlizadehArghami"><code>AlizadehArghami</code></a>, <a href="#ComplexityMeasures.Ebrahimi"><code>Ebrahimi</code></a>, <a href="#ComplexityMeasures.DifferentialInfoEstimator"><code>DifferentialInfoEstimator</code></a>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaDynamics/ComplexityMeasures.jl/blob/b03edd0c9aead13b35b293bd453f20190ff6f2c4/src/differential_info_estimators/order_statistics/Vasicek.jl#L3-L57">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="ComplexityMeasures.AlizadehArghami" href="#ComplexityMeasures.AlizadehArghami"><code>ComplexityMeasures.AlizadehArghami</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">AlizadehArghami &lt;: DifferentialInfoEstimator
AlizadehArghami(measure = Shannon(); m::Int = 1, base = 2)</code></pre><p>The <code>AlizadehArghami</code>estimator computes the <a href="#ComplexityMeasures.Shannon"><code>Shannon</code></a> differential <a href="#ComplexityMeasures.information-Tuple{InformationMeasure, OutcomeSpace, Any}"><code>information</code></a> (in the given <code>base</code>) of a timeseries using the method from Alizadeh &amp; Arghami (2010)<sup class="footnote-reference"><a id="citeref-Alizadeh2010" href="#footnote-Alizadeh2010">[Alizadeh2010]</a></sup>.</p><p>The <code>AlizadehArghami</code> estimator belongs to a class of differential entropy estimators based on <a href="https://en.wikipedia.org/wiki/Order_statistic">order statistics</a>. It only works for <em>timeseries</em> input.</p><p><strong>Description</strong></p><p>Assume we have samples <span>$\bar{X} = \{x_1, x_2, \ldots, x_N \}$</span> from a continuous random variable <span>$X \in \mathbb{R}$</span> with support <span>$\mathcal{X}$</span> and density function<span>$f : \mathbb{R} \to \mathbb{R}$</span>. <code>AlizadehArghami</code> estimates the <a href="@ref">Shannon</a> differential entropy</p><p class="math-container">\[H(X) = \int_{\mathcal{X}} f(x) \log f(x) dx = \mathbb{E}[-\log(f(X))].\]</p><p>However, instead of estimating the above integral directly, it makes use of the equivalent integral, where <span>$F$</span> is the distribution function for <span>$X$</span>:</p><p class="math-container">\[H(X) = \int_0^1 \log \left(\dfrac{d}{dp}F^{-1}(p) \right) dp.\]</p><p>This integral is approximated by first computing the <a href="https://en.wikipedia.org/wiki/Order_statistic">order statistics</a> of <span>$\bar{X}$</span> (the input timeseries), i.e. <span>$x_{(1)} \leq x_{(2)} \leq \cdots \leq x_{(n)}$</span>. The <code>AlizadehArghami</code> <a href="#ComplexityMeasures.Shannon"><code>Shannon</code></a> differential entropy estimate is then the the <a href="#ComplexityMeasures.Vasicek"><code>Vasicek</code></a> estimate <span>$\hat{H}_{V}(\bar{X}, m, n)$</span>, plus a correction factor</p><p class="math-container">\[\hat{H}_{A}(\bar{X}, m, n) = \hat{H}_{V}(\bar{X}, m, n) +
\dfrac{2}{n}\left(m \log(2) \right).\]</p><p>See also: <a href="#ComplexityMeasures.information-Tuple{InformationMeasure, OutcomeSpace, Any}"><code>information</code></a>, <a href="#ComplexityMeasures.Correa"><code>Correa</code></a>, <a href="#ComplexityMeasures.Ebrahimi"><code>Ebrahimi</code></a>, <a href="#ComplexityMeasures.Vasicek"><code>Vasicek</code></a>, <a href="#ComplexityMeasures.DifferentialInfoEstimator"><code>DifferentialInfoEstimator</code></a>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaDynamics/ComplexityMeasures.jl/blob/b03edd0c9aead13b35b293bd453f20190ff6f2c4/src/differential_info_estimators/order_statistics/AlizadehArghami.jl#L3-L50">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="ComplexityMeasures.Ebrahimi" href="#ComplexityMeasures.Ebrahimi"><code>ComplexityMeasures.Ebrahimi</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">Ebrahimi &lt;: DifferentialInfoEstimator
Ebrahimi(measure = Shannon(); m::Int = 1, base = 2)</code></pre><p>The <code>Ebrahimi</code> estimator computes the <a href="#ComplexityMeasures.Shannon"><code>Shannon</code></a> <a href="#ComplexityMeasures.information-Tuple{InformationMeasure, OutcomeSpace, Any}"><code>information</code></a> (in the given <code>base</code>) of a timeseries using the method from Ebrahimi (1994)<sup class="footnote-reference"><a id="citeref-Ebrahimi1994" href="#footnote-Ebrahimi1994">[Ebrahimi1994]</a></sup>.</p><p>The <code>Ebrahimi</code> estimator belongs to a class of differential entropy estimators based on <a href="https://en.wikipedia.org/wiki/Order_statistic">order statistics</a>. It only works for <em>timeseries</em> input.</p><p><strong>Description</strong></p><p>Assume we have samples <span>$\bar{X} = \{x_1, x_2, \ldots, x_N \}$</span> from a continuous random variable <span>$X \in \mathbb{R}$</span> with support <span>$\mathcal{X}$</span> and density function<span>$f : \mathbb{R} \to \mathbb{R}$</span>. <code>Ebrahimi</code> estimates the <a href="@ref">Shannon</a> differential entropy</p><p class="math-container">\[H(X) = \int_{\mathcal{X}} f(x) \log f(x) dx = \mathbb{E}[-\log(f(X))].\]</p><p>However, instead of estimating the above integral directly, it makes use of the equivalent integral, where <span>$F$</span> is the distribution function for <span>$X$</span>,</p><p class="math-container">\[H(X) = \int_0^1 \log \left(\dfrac{d}{dp}F^{-1}(p) \right) dp\]</p><p>This integral is approximated by first computing the <a href="https://en.wikipedia.org/wiki/Order_statistic">order statistics</a> of <span>$\bar{X}$</span> (the input timeseries), i.e. <span>$x_{(1)} \leq x_{(2)} \leq \cdots \leq x_{(n)}$</span>. The <code>Ebrahimi</code> <a href="#ComplexityMeasures.Shannon"><code>Shannon</code></a> differential entropy estimate is then</p><p class="math-container">\[\hat{H}_{E}(\bar{X}, m) =
\dfrac{1}{n} \sum_{i = 1}^n \log
\left[ \dfrac{n}{c_i m} (\bar{X}_{(i+m)} - \bar{X}_{(i-m)}) \right],\]</p><p>where</p><p class="math-container">\[c_i =
\begin{cases}
    1 + \frac{i - 1}{m}, &amp; 1 \geq i \geq m \\
    2,                    &amp; m + 1 \geq i \geq n - m \\
    1 + \frac{n - i}{m} &amp; n - m + 1 \geq i \geq n
\end{cases}.\]</p><p>See also: <a href="#ComplexityMeasures.information-Tuple{InformationMeasure, OutcomeSpace, Any}"><code>information</code></a>, <a href="#ComplexityMeasures.Correa"><code>Correa</code></a>, <a href="#ComplexityMeasures.AlizadehArghami"><code>AlizadehArghami</code></a>, <a href="#ComplexityMeasures.Vasicek"><code>Vasicek</code></a>, <a href="#ComplexityMeasures.DifferentialInfoEstimator"><code>DifferentialInfoEstimator</code></a>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaDynamics/ComplexityMeasures.jl/blob/b03edd0c9aead13b35b293bd453f20190ff6f2c4/src/differential_info_estimators/order_statistics/Ebrahimi.jl#L3-L60">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="ComplexityMeasures.Correa" href="#ComplexityMeasures.Correa"><code>ComplexityMeasures.Correa</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">Correa &lt;: DifferentialInfoEstimator
Correa(measure = Shannon(); m::Int = 1, base = 2)</code></pre><p>The <code>Correa</code> estimator computes the <a href="#ComplexityMeasures.Shannon"><code>Shannon</code></a> differential <a href="#ComplexityMeasures.information-Tuple{InformationMeasure, OutcomeSpace, Any}"><code>information</code></a> (in the given `base) of a timeseries using the method from Correa (1995)<sup class="footnote-reference"><a id="citeref-Correa1995" href="#footnote-Correa1995">[Correa1995]</a></sup>.</p><p>The <code>Correa</code> estimator belongs to a class of differential entropy estimators based on <a href="https://en.wikipedia.org/wiki/Order_statistic">order statistics</a>. It only works for <em>timeseries</em> input.</p><p><strong>Description</strong></p><p>Assume we have samples <span>$\bar{X} = \{x_1, x_2, \ldots, x_N \}$</span> from a continuous random variable <span>$X \in \mathbb{R}$</span> with support <span>$\mathcal{X}$</span> and density function<span>$f : \mathbb{R} \to \mathbb{R}$</span>. <code>Correa</code> estimates the <a href="@ref">Shannon</a> differential entropy</p><p class="math-container">\[H(X) = \int_{\mathcal{X}} f(x) \log f(x) dx = \mathbb{E}[-\log(f(X))].\]</p><p>However, instead of estimating the above integral directly, <code>Correa</code> makes use of the equivalent integral, where <span>$F$</span> is the distribution function for <span>$X$</span>,</p><p class="math-container">\[H(X) = \int_0^1 \log \left(\dfrac{d}{dp}F^{-1}(p) \right) dp\]</p><p>This integral is approximated by first computing the <a href="https://en.wikipedia.org/wiki/Order_statistic">order statistics</a> of <span>$\bar{X}$</span> (the input timeseries), i.e. <span>$x_{(1)} \leq x_{(2)} \leq \cdots \leq x_{(n)}$</span>, ensuring that end points are included. The <code>Correa</code> estimate of <a href="#ComplexityMeasures.Shannon"><code>Shannon</code></a> differential entropy is then</p><p class="math-container">\[H_C(\bar{X}, m, n) =
\dfrac{1}{n} \sum_{i = 1}^n \log
\left[ \dfrac{ \sum_{j=i-m}^{i+m}(\bar{X}_{(j)} -
\tilde{X}_{(i)})(j - i)}{n \sum_{j=i-m}^{i+m} (\bar{X}_{(j)} - \tilde{X}_{(i)})^2}
\right],\]</p><p>where</p><p class="math-container">\[\tilde{X}_{(i)} = \dfrac{1}{2m + 1} \sum_{j = i - m}^{i + m} X_{(j)}.\]</p><p>See also: <a href="#ComplexityMeasures.information-Tuple{InformationMeasure, OutcomeSpace, Any}"><code>information</code></a>, <a href="#ComplexityMeasures.AlizadehArghami"><code>AlizadehArghami</code></a>, <a href="#ComplexityMeasures.Ebrahimi"><code>Ebrahimi</code></a>, <a href="#ComplexityMeasures.Vasicek"><code>Vasicek</code></a>, <a href="#ComplexityMeasures.DifferentialInfoEstimator"><code>DifferentialInfoEstimator</code></a>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaDynamics/ComplexityMeasures.jl/blob/b03edd0c9aead13b35b293bd453f20190ff6f2c4/src/differential_info_estimators/order_statistics/Correa.jl#L3-L58">source</a></section></article><h3 id="table_diff_ent_est"><a class="docs-heading-anchor" href="#table_diff_ent_est">Table of differential information measure estimators</a><a id="table_diff_ent_est-1"></a><a class="docs-heading-anchor-permalink" href="#table_diff_ent_est" title="Permalink"></a></h3><p>The following estimators are <em>differential</em> information measure estimators, and can also be used with <a href="#ComplexityMeasures.information-Tuple{InformationMeasure, OutcomeSpace, Any}"><code>information</code></a>.</p><p>Each <a href="#ComplexityMeasures.DifferentialInfoEstimator"><code>DifferentialInfoEstimator</code></a>s uses a specialized technique to approximate relevant densities/integrals, and is often tailored to one or a few types of information measures. For example, <a href="#ComplexityMeasures.Kraskov"><code>Kraskov</code></a> estimates the <a href="#ComplexityMeasures.Shannon"><code>Shannon</code></a> entropy.</p><table><tr><th style="text-align: left">Estimator</th><th style="text-align: left">Principle</th><th style="text-align: left">Input data</th><th style="text-align: center"><a href="#ComplexityMeasures.Shannon"><code>Shannon</code></a></th><th style="text-align: center"><a href="#ComplexityMeasures.Renyi"><code>Renyi</code></a></th><th style="text-align: center"><a href="#ComplexityMeasures.Tsallis"><code>Tsallis</code></a></th><th style="text-align: center"><a href="#ComplexityMeasures.Kaniadakis"><code>Kaniadakis</code></a></th><th style="text-align: center"><a href="#ComplexityMeasures.Curado"><code>Curado</code></a></th><th style="text-align: center"><a href="#ComplexityMeasures.StretchedExponential"><code>StretchedExponential</code></a></th></tr><tr><td style="text-align: left"><a href="#ComplexityMeasures.KozachenkoLeonenko"><code>KozachenkoLeonenko</code></a></td><td style="text-align: left">Nearest neighbors</td><td style="text-align: left"><code>StateSpaceSet</code></td><td style="text-align: center">✓</td><td style="text-align: center">x</td><td style="text-align: center">x</td><td style="text-align: center">x</td><td style="text-align: center">x</td><td style="text-align: center">x</td></tr><tr><td style="text-align: left"><a href="#ComplexityMeasures.Kraskov"><code>Kraskov</code></a></td><td style="text-align: left">Nearest neighbors</td><td style="text-align: left"><code>StateSpaceSet</code></td><td style="text-align: center">✓</td><td style="text-align: center">x</td><td style="text-align: center">x</td><td style="text-align: center">x</td><td style="text-align: center">x</td><td style="text-align: center">x</td></tr><tr><td style="text-align: left"><a href="#ComplexityMeasures.Zhu"><code>Zhu</code></a></td><td style="text-align: left">Nearest neighbors</td><td style="text-align: left"><code>StateSpaceSet</code></td><td style="text-align: center">✓</td><td style="text-align: center">x</td><td style="text-align: center">x</td><td style="text-align: center">x</td><td style="text-align: center">x</td><td style="text-align: center">x</td></tr><tr><td style="text-align: left"><a href="#ComplexityMeasures.ZhuSingh"><code>ZhuSingh</code></a></td><td style="text-align: left">Nearest neighbors</td><td style="text-align: left"><code>StateSpaceSet</code></td><td style="text-align: center">✓</td><td style="text-align: center">x</td><td style="text-align: center">x</td><td style="text-align: center">x</td><td style="text-align: center">x</td><td style="text-align: center">x</td></tr><tr><td style="text-align: left"><a href="#ComplexityMeasures.Gao"><code>Gao</code></a></td><td style="text-align: left">Nearest neighbors</td><td style="text-align: left"><code>StateSpaceSet</code></td><td style="text-align: center">✓</td><td style="text-align: center">x</td><td style="text-align: center">x</td><td style="text-align: center">x</td><td style="text-align: center">x</td><td style="text-align: center">x</td></tr><tr><td style="text-align: left"><a href="#ComplexityMeasures.Goria"><code>Goria</code></a></td><td style="text-align: left">Nearest neighbors</td><td style="text-align: left"><code>StateSpaceSet</code></td><td style="text-align: center">✓</td><td style="text-align: center">x</td><td style="text-align: center">x</td><td style="text-align: center">x</td><td style="text-align: center">x</td><td style="text-align: center">x</td></tr><tr><td style="text-align: left"><a href="#ComplexityMeasures.Lord"><code>Lord</code></a></td><td style="text-align: left">Nearest neighbors</td><td style="text-align: left"><code>StateSpaceSet</code></td><td style="text-align: center">✓</td><td style="text-align: center">x</td><td style="text-align: center">x</td><td style="text-align: center">x</td><td style="text-align: center">x</td><td style="text-align: center">x</td></tr><tr><td style="text-align: left"><a href="#ComplexityMeasures.Vasicek"><code>Vasicek</code></a></td><td style="text-align: left">Order statistics</td><td style="text-align: left"><code>Vector</code></td><td style="text-align: center">✓</td><td style="text-align: center">x</td><td style="text-align: center">x</td><td style="text-align: center">x</td><td style="text-align: center">x</td><td style="text-align: center">x</td></tr><tr><td style="text-align: left"><a href="#ComplexityMeasures.Ebrahimi"><code>Ebrahimi</code></a></td><td style="text-align: left">Order statistics</td><td style="text-align: left"><code>Vector</code></td><td style="text-align: center">✓</td><td style="text-align: center">x</td><td style="text-align: center">x</td><td style="text-align: center">x</td><td style="text-align: center">x</td><td style="text-align: center">x</td></tr><tr><td style="text-align: left"><a href="#ComplexityMeasures.Correa"><code>Correa</code></a></td><td style="text-align: left">Order statistics</td><td style="text-align: left"><code>Vector</code></td><td style="text-align: center">✓</td><td style="text-align: center">x</td><td style="text-align: center">x</td><td style="text-align: center">x</td><td style="text-align: center">x</td><td style="text-align: center">x</td></tr><tr><td style="text-align: left"><a href="#ComplexityMeasures.AlizadehArghami"><code>AlizadehArghami</code></a></td><td style="text-align: left">Order statistics</td><td style="text-align: left"><code>Vector</code></td><td style="text-align: center">✓</td><td style="text-align: center">x</td><td style="text-align: center">x</td><td style="text-align: center">x</td><td style="text-align: center">x</td><td style="text-align: center">x</td></tr></table><section class="footnotes is-size-7"><ul><li class="footnote" id="footnote-Amigó2018"><a class="tag is-link" href="#citeref-Amigó2018">Amigó2018</a>Amigó, J. M., Balogh, S. G., &amp; Hernández, S. (2018). A brief review of generalized entropies. <a href="https://www.mdpi.com/1099-4300/20/11/813">Entropy, 20(11), 813.</a></li><li class="footnote" id="footnote-Shannon1948"><a class="tag is-link" href="#citeref-Shannon1948">Shannon1948</a>C. E. Shannon, Bell Systems Technical Journal <strong>27</strong>, pp 379 (1948)</li><li class="footnote" id="footnote-Rényi1960"><a class="tag is-link" href="#citeref-Rényi1960">Rényi1960</a>A. Rényi, <em>Proceedings of the fourth Berkeley Symposium on Mathematics, Statistics and Probability</em>, pp 547 (1960)</li><li class="footnote" id="footnote-Shannon1948"><a class="tag is-link" href="#citeref-Shannon1948">Shannon1948</a>C. E. Shannon, Bell Systems Technical Journal <strong>27</strong>, pp 379 (1948)</li><li class="footnote" id="footnote-Tsallis1988"><a class="tag is-link" href="#citeref-Tsallis1988">Tsallis1988</a>Tsallis, C. (1988). Possible generalization of Boltzmann-Gibbs statistics. Journal of statistical physics, 52(1), 479-487.</li><li class="footnote" id="footnote-Tsallis2009"><a class="tag is-link" href="#citeref-Tsallis2009">Tsallis2009</a>Tsallis, C. (2009). Introduction to nonextensive statistical mechanics: approaching a complex world. Springer, 1(1), 2-1.</li><li class="footnote" id="footnote-Curado2004"><a class="tag is-link" href="#citeref-Curado2004">Curado2004</a>Curado, E. M., &amp; Nobre, F. D. (2004). On the stability of analytic entropic forms. Physica A: Statistical Mechanics and its Applications, 335(1-2), 94-106.</li><li class="footnote" id="footnote-Anteneodo1999"><a class="tag is-link" href="#citeref-Anteneodo1999">Anteneodo1999</a>Anteneodo, C., &amp; Plastino, A. R. (1999). Maximum entropy approach to stretched exponential probability distributions. Journal of Physics A: Mathematical and General, 32(7), 1089.</li><li class="footnote" id="footnote-Lad2015"><a class="tag is-link" href="#citeref-Lad2015">Lad2015</a>Lad, F., Sanfilippo, G., &amp; Agro, G. (2015). Extropy: Complementary dual of entropy.</li><li class="footnote" id="footnote-Liu2021"><a class="tag is-link" href="#citeref-Liu2021">Liu2021</a>Liu, J., &amp; Xiao, F. (2021). Renyi extropy. Communications in Statistics-Theory and Methods, 1-12.</li><li class="footnote" id="footnote-Xue2023"><a class="tag is-link" href="#citeref-Xue2023">Xue2023</a>Xue, Y., &amp; Deng, Y. (2023). Tsallis extropy. Communications in Statistics-Theory and Methods, 52(3), 751-762.</li><li class="footnote" id="footnote-Arora2022"><a class="tag is-link" href="#citeref-Arora2022">Arora2022</a>Arora, A., Meister, C., &amp; Cotterell, R. (2022). Estimating the entropy of linguistic distributions. arXiv preprint arXiv:2204.01469.</li><li class="footnote" id="footnote-Miller1955"><a class="tag is-link" href="#citeref-Miller1955">Miller1955</a>Miller, G. (1955). Note on the bias of information estimates. Information theory in psychology: Problems and methods.</li><li class="footnote" id="footnote-Paninski2003"><a class="tag is-link" href="#citeref-Paninski2003">Paninski2003</a>Paninski, L. (2003). Estimation of entropy and mutual information. Neural computation, 15(6), 1191-1253.</li><li class="footnote" id="footnote-Schürmann2004"><a class="tag is-link" href="#citeref-Schürmann2004">Schürmann2004</a>Schürmann, T. (2004). Bias analysis in entropy estimation. Journal of Physics A: Mathematical and General, 37(27), L295.</li><li class="footnote" id="footnote-Grassberger2022"><a class="tag is-link" href="#citeref-Grassberger2022">Grassberger2022</a><p>Grassberger, P. (2022). On generalized Schürmann entropy estimators. Entropy, 24(5),</p><ol><li></li></ol></li><li class="footnote" id="footnote-Zahl1977"><a class="tag is-link" href="#citeref-Zahl1977">Zahl1977</a>Zahl, S. (1977). Jackknifing an index of diversity. Ecology, 58(4), 907-913.</li><li class="footnote" id="footnote-Horvitz1952"><a class="tag is-link" href="#citeref-Horvitz1952">Horvitz1952</a>Horvitz, D. G., &amp; Thompson, D. J. (1952). A generalization of sampling without replacement from a finite universe. Journal of the American statistical Association, 47(260), 663-685.</li><li class="footnote" id="footnote-Arora2022"><a class="tag is-link" href="#citeref-Arora2022">Arora2022</a>Arora, A., Meister, C., &amp; Cotterell, R. (2022). Estimating the entropy of linguistic distributions. arXiv preprint arXiv:2204.01469.</li><li class="footnote" id="footnote-Chao2003"><a class="tag is-link" href="#citeref-Chao2003">Chao2003</a>Chao, A., &amp; Shen, T. J. (2003). Nonparametric estimation of Shannon’s index of diversity when there are unseen species in sample. Environmental and ecological statistics, 10(4), 429-443.</li><li class="footnote" id="footnote-Arora2022"><a class="tag is-link" href="#citeref-Arora2022">Arora2022</a>Arora, A., Meister, C., &amp; Cotterell, R. (2022). Estimating the entropy of linguistic distributions. arXiv preprint arXiv:2204.01469.</li><li class="footnote" id="footnote-Kraskov2004"><a class="tag is-link" href="#citeref-Kraskov2004">Kraskov2004</a>Kraskov, A., Stögbauer, H., &amp; Grassberger, P. (2004). Estimating mutual information. Physical review E, 69(6), 066138.</li><li class="footnote" id="footnote-Charzyńska2016"><a class="tag is-link" href="#citeref-Charzyńska2016">Charzyńska2016</a>Charzyńska, A., &amp; Gambin, A. (2016). Improvement of the k-NN entropy estimator with applications in systems biology. InformationMeasure, 18(1), 13.</li><li class="footnote" id="footnote-KozachenkoLeonenko1987"><a class="tag is-link" href="#citeref-KozachenkoLeonenko1987">KozachenkoLeonenko1987</a>Kozachenko, L. F., &amp; Leonenko, N. N. (1987). Sample estimate of the entropy of a random vector. Problemy Peredachi Informatsii, 23(2), 9-16.</li><li class="footnote" id="footnote-Zhu2015"><a class="tag is-link" href="#citeref-Zhu2015">Zhu2015</a>Zhu, J., Bellanger, J. J., Shu, H., &amp; Le Bouquin Jeannès, R. (2015). Contribution to transfer entropy estimation via the k-nearest-neighbors approach. InformationMeasure, 17(6), 4173-4201.</li><li class="footnote" id="footnote-Zhu2015"><a class="tag is-link" href="#citeref-Zhu2015">Zhu2015</a>Zhu, J., Bellanger, J. J., Shu, H., &amp; Le Bouquin Jeannès, R. (2015). Contribution to transfer entropy estimation via the k-nearest-neighbors approach. InformationMeasure, 17(6), 4173-4201.</li><li class="footnote" id="footnote-Singh2003"><a class="tag is-link" href="#citeref-Singh2003">Singh2003</a>Singh, H., Misra, N., Hnizdo, V., Fedorowicz, A., &amp; Demchuk, E. (2003). Nearest neighbor estimates of entropy. American journal of mathematical and management sciences, 23(3-4), 301-321.</li><li class="footnote" id="footnote-Gao2015"><a class="tag is-link" href="#citeref-Gao2015">Gao2015</a>Gao, S., Ver Steeg, G., &amp; Galstyan, A. (2015, February). Efficient estimation of mutual information for strongly dependent variables. In Artificial intelligence and     statistics (pp. 277-286). PPlugInR.</li><li class="footnote" id="footnote-Singh2003"><a class="tag is-link" href="#citeref-Singh2003">Singh2003</a>Singh, H., Misra, N., Hnizdo, V., Fedorowicz, A., &amp; Demchuk, E. (2003). Nearest neighbor estimates of entropy. American journal of mathematical and management sciences, 23(3-4), 301-321.</li><li class="footnote" id="footnote-Goria2005"><a class="tag is-link" href="#citeref-Goria2005">Goria2005</a>Goria, M. N., Leonenko, N. N., Mergel, V. V., &amp; Novi Inverardi, P. L. (2005). A new class of random vector entropy estimators and its applications in testing statistical hypotheses. Journal of Nonparametric Statistics, 17(3), 277-297.</li><li class="footnote" id="footnote-Lord2015"><a class="tag is-link" href="#citeref-Lord2015">Lord2015</a>Lord, W. M., Sun, J., &amp; Bollt, E. M. (2018). Geometric k-nearest neighbor estimation of entropy and mutual information. Chaos: An Interdisciplinary Journal of Nonlinear Science, 28(3), 033114.</li><li class="footnote" id="footnote-Vasicek1976"><a class="tag is-link" href="#citeref-Vasicek1976">Vasicek1976</a>Vasicek, O. (1976). A test for normality based on sample entropy. Journal of the Royal Statistical Society: Series B (Methodological), 38(1), 54-59.</li><li class="footnote" id="footnote-Alizadeh2010"><a class="tag is-link" href="#citeref-Alizadeh2010">Alizadeh2010</a>Alizadeh, N. H., &amp; Arghami, N. R. (2010). A new estimator of entropy. Journal of the Iranian Statistical Society (JIRSS).</li><li class="footnote" id="footnote-Ebrahimi1994"><a class="tag is-link" href="#citeref-Ebrahimi1994">Ebrahimi1994</a>Ebrahimi, N., Pflughoeft, K., &amp; Soofi, E. S. (1994). Two measures of sample entropy. Statistics &amp; Probability Letters, 20(3), 225-234.</li><li class="footnote" id="footnote-Correa1995"><a class="tag is-link" href="#citeref-Correa1995">Correa1995</a>Correa, J. C. (1995). A new estimator of entropy. Communications in Statistics-Theory and Methods, 24(10), 2439-2449.</li></ul></section></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../probabilities/">« Probabilities</a><a class="docs-footer-nextpage" href="../complexity/">Complexity measures »</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 0.27.25 on <span class="colophon-date" title="Friday 18 August 2023 11:12">Friday 18 August 2023</span>. Using Julia version 1.9.2.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
