<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Information measures (entropies and co.) · ComplexityMeasures.jl</title><script data-outdated-warner src="../assets/warner.js"></script><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.045/juliamono.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.24/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL=".."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../assets/documenter.js"></script><script src="../siteinfo.js"></script><script src="../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../assets/themeswap.js"></script><link href="https://fonts.googleapis.com/css?family=Montserrat|Source+Code+Pro&amp;display=swap" rel="stylesheet" type="text/css"/></head><body><div id="documenter"><nav class="docs-sidebar"><div class="docs-package-name"><span class="docs-autofit"><a href="../">ComplexityMeasures.jl</a></span></div><form class="docs-search" action="../search/"><input class="docs-search-query" id="documenter-search-query" name="q" type="text" placeholder="Search docs"/></form><ul class="docs-menu"><li><a class="tocitem" href="../">ComplexityMeasures.jl</a></li><li><a class="tocitem" href="../tutorial/">Tutorial</a></li><li><a class="tocitem" href="../probabilities/">Probabilities</a></li><li class="is-active"><a class="tocitem" href>Information measures (entropies and co.)</a><ul class="internal"><li><a class="tocitem" href="#Information-measures-API"><span>Information measures API</span></a></li><li><a class="tocitem" href="#Entropies"><span>Entropies</span></a></li><li><a class="tocitem" href="#Other-information-measures"><span>Other information measures</span></a></li><li><a class="tocitem" href="#Discrete-information-estimators"><span>Discrete information estimators</span></a></li><li><a class="tocitem" href="#Differential-information-estimators"><span>Differential information estimators</span></a></li></ul></li><li><a class="tocitem" href="../complexity/">Complexity measures</a></li><li><a class="tocitem" href="../convenience/">Convenience functions</a></li><li><a class="tocitem" href="../examples/">ComplexityMeasures.jl Examples</a></li><li><a class="tocitem" href="../devdocs/">ComplexityMeasures.jl Dev Docs</a></li><li><a class="tocitem" href="../references/">References</a></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><nav class="breadcrumb"><ul class="is-hidden-mobile"><li class="is-active"><a href>Information measures (entropies and co.)</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>Information measures (entropies and co.)</a></li></ul></nav><div class="docs-right"><a class="docs-edit-link" href="https://github.com/JuliaDynamics/ComplexityMeasures.jl/blob/main/docs/src/information_measures.md" title="Edit on GitHub"><span class="docs-icon fab"></span><span class="docs-label is-hidden-touch">Edit on GitHub</span></a><a class="docs-settings-button fas fa-cog" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-sidebar-button fa fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a></div></header><article class="content" id="documenter-page"><h1 id="information_measures"><a class="docs-heading-anchor" href="#information_measures">Information measures (entropies and co.)</a><a id="information_measures-1"></a><a class="docs-heading-anchor-permalink" href="#information_measures" title="Permalink"></a></h1><div class="admonition is-info"><header class="admonition-header">Note</header><div class="admonition-body"><p>Be sure you have gone through the <a href="../tutorial/#Tutorial">Tutorial</a> before going through the API here to have a good idea of the terminology used in ComplexityMeasures.jl.</p></div></div><h2 id="Information-measures-API"><a class="docs-heading-anchor" href="#Information-measures-API">Information measures API</a><a id="Information-measures-API-1"></a><a class="docs-heading-anchor-permalink" href="#Information-measures-API" title="Permalink"></a></h2><p>The information measure API is defined by the <a href="#ComplexityMeasures.information-Tuple{InformationMeasure, OutcomeSpace, Any}"><code>information</code></a> function, which takes as an input an <a href="#ComplexityMeasures.InformationMeasure"><code>InformationMeasure</code></a>, or some specialized <a href="#ComplexityMeasures.DiscreteInfoEstimator"><code>DiscreteInfoEstimator</code></a> or <a href="#ComplexityMeasures.DifferentialInfoEstimator"><code>DifferentialInfoEstimator</code></a> for estimating the discrete or differential variant of the measure. The functions <a href="#ComplexityMeasures.information_maximum"><code>information_maximum</code></a> and <a href="#ComplexityMeasures.information_normalized"><code>information_normalized</code></a> are also useful.</p><article class="docstring"><header><a class="docstring-binding" id="ComplexityMeasures.InformationMeasure" href="#ComplexityMeasures.InformationMeasure"><code>ComplexityMeasures.InformationMeasure</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">InformationMeasure</code></pre><p><code>InformationMeasure</code> is the supertype of all information measure <em>definitions</em>.</p><p>In this package, we define &quot;information measures&quot; as functionals of probability mass functions (&quot;discrete&quot; measures), or of probability density functions (&quot;differential&quot; measures). Examples are (generalized) entropies such as <a href="#ComplexityMeasures.Shannon"><code>Shannon</code></a> or <a href="#ComplexityMeasures.Renyi"><code>Renyi</code></a>, or extropies like <a href="#ComplexityMeasures.ShannonExtropy"><code>ShannonExtropy</code></a>. <a href="../references/#Amigó2018">Amigó <em>et al.</em> (2018)</a> provides a useful review of generalized entropies.</p><p><strong>Used with</strong></p><p>Any of the information measures listed below can be used with</p><ul><li><a href="#ComplexityMeasures.information-Tuple{InformationMeasure, OutcomeSpace, Any}"><code>information</code></a>, to compute a numerical value for the measure, given some input data.</li><li><a href="#ComplexityMeasures.information_maximum"><code>information_maximum</code></a>, to compute the maximum possible value for the measure.</li><li><a href="#ComplexityMeasures.information_normalized"><code>information_normalized</code></a>, to compute the normalized form of the   measure (divided by the maximum possible value).</li></ul><p>The <a href="#ComplexityMeasures.information_maximum"><code>information_maximum</code></a>/<a href="#ComplexityMeasures.information_normalized"><code>information_normalized</code></a> functions only works with the discrete version of the measure. See docstrings for the above functions for usage examples.</p><p><strong>Implementations</strong></p><ul><li><a href="#ComplexityMeasures.Renyi"><code>Renyi</code></a>.</li><li><a href="#ComplexityMeasures.Tsallis"><code>Tsallis</code></a>.</li><li><a href="#ComplexityMeasures.Shannon"><code>Shannon</code></a>, which is a subcase of the above two in the limit <code>q → 1</code>.</li><li><a href="#ComplexityMeasures.Kaniadakis"><code>Kaniadakis</code></a>.</li><li><a href="#ComplexityMeasures.Curado"><code>Curado</code></a>.</li><li><a href="#ComplexityMeasures.StretchedExponential"><code>StretchedExponential</code></a>.</li><li><a href="#ComplexityMeasures.RenyiExtropy"><code>RenyiExtropy</code></a>.</li><li><a href="#ComplexityMeasures.TsallisExtropy"><code>TsallisExtropy</code></a>.</li><li><a href="#ComplexityMeasures.ShannonExtropy"><code>ShannonExtropy</code></a>, which is a subcase of the above two in the limit <code>q → 1</code>.</li></ul><p><strong>Estimators</strong></p><p>A particular information measure may have both a discrete and a continuous/differential definition, which are estimated using a <a href="#ComplexityMeasures.DifferentialInfoEstimator"><code>DifferentialInfoEstimator</code></a> or a <a href="#ComplexityMeasures.DifferentialInfoEstimator"><code>DifferentialInfoEstimator</code></a>, respectively.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaDynamics/ComplexityMeasures.jl/blob/a81a14f550f1668986d91498afe4d1642f02e53b/src/core/information_measures.jl#L5-L46">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="ComplexityMeasures.information-Tuple{InformationMeasure, OutcomeSpace, Any}" href="#ComplexityMeasures.information-Tuple{InformationMeasure, OutcomeSpace, Any}"><code>ComplexityMeasures.information</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">information([e::DiscreteInfoEstimator,] est::ProbabilitiesEstimator, o::OutcomeSpace, x) → h::Real</code></pre><p>Estimate a discrete information measure from input data <code>x</code> using the provided <a href="#ComplexityMeasures.DiscreteInfoEstimator"><code>DiscreteInfoEstimator</code></a> and <a href="../probabilities/#ComplexityMeasures.ProbabilitiesEstimator"><code>ProbabilitiesEstimator</code></a> over the given <a href="../probabilities/#ComplexityMeasures.OutcomeSpace"><code>OutcomeSpace</code></a>.</p><p>As an alternative, you can provide an <a href="#ComplexityMeasures.InformationMeasure"><code>InformationMeasure</code></a> for the first argument (which will default to <a href="#ComplexityMeasures.PlugIn"><code>PlugIn</code></a> estimation) or an <a href="../probabilities/#ComplexityMeasures.OutcomeSpace"><code>OutcomeSpace</code></a> for the second argument (which will default to the <a href="../probabilities/#ComplexityMeasures.RelativeAmount"><code>RelativeAmount</code></a> probabilities estimator).</p><pre><code class="nohighlight hljs">information([e::DiscreteInfoEstimator,] p::Probabilities) → h::Real</code></pre><p>Like above, but estimate the information measure from the pre-computed <a href="../probabilities/#ComplexityMeasures.Probabilities"><code>Probabilities</code></a> <code>p</code>.</p><p>See also: <a href="#ComplexityMeasures.information_maximum"><code>information_maximum</code></a>, <a href="#ComplexityMeasures.information_normalized"><code>information_normalized</code></a> for a normalized version.</p><p><strong>Examples (naive estimation)</strong></p><p>The simplest way to estimate a discrete measure is to provide the <a href="#ComplexityMeasures.InformationMeasure"><code>InformationMeasure</code></a> directly in combination with an <a href="../probabilities/#ComplexityMeasures.OutcomeSpace"><code>OutcomeSpace</code></a>. This will use the &quot;naive&quot; <a href="#ComplexityMeasures.PlugIn"><code>PlugIn</code></a> estimator for the measure, and the &quot;naive&quot; <a href="../probabilities/#ComplexityMeasures.RelativeAmount"><code>RelativeAmount</code></a> estimator for the probabilities.</p><pre><code class="language-julia hljs">x = randn(100) # some input data
o = ValueBinning(RectangularBinning(5)) # a 5-bin histogram outcome space
h_s = information(Shannon(), o, x)</code></pre><p>Here are some more examples:</p><pre><code class="language-julia hljs">x = [rand(Bool) for _ in 1:10000] # coin toss
ps = probabilities(x) # gives about [0.5, 0.5] by definition
h = information(ps) # gives 1, about 1 bit by definition (Shannon entropy by default)
h = information(Shannon(), ps) # syntactically equivalent to the above
h = information(Shannon(), UniqueElements(), x) # syntactically equivalent to above
h = information(Renyi(2.0), ps) # also gives 1, order `q` doesn&#39;t matter for coin toss
h = information(OrdinalPatterns(;m=3), x) # gives about 2, again by definition</code></pre><p><strong>Examples (bias-corrected estimation)</strong></p><p>It is known that both <a href="#ComplexityMeasures.PlugIn"><code>PlugIn</code></a> estimation for information measures and <a href="../probabilities/#ComplexityMeasures.RelativeAmount"><code>RelativeAmount</code></a> estimation for probabilities are biased. The scientific literature abounds with estimators that correct for this bias, both on the measure-estimation level and on the probability-estimation level. We thus provide the option to use any <a href="#ComplexityMeasures.DiscreteInfoEstimator"><code>DiscreteInfoEstimator</code></a> in combination with any <a href="../probabilities/#ComplexityMeasures.ProbabilitiesEstimator"><code>ProbabilitiesEstimator</code></a> for improved estimates. Note that custom probabilites estimators will only work with counting-compatible <a href="../probabilities/#ComplexityMeasures.OutcomeSpace"><code>OutcomeSpace</code></a>.</p><pre><code class="language-julia hljs">x = randn(100)
o = ValueBinning(RectangularBinning(5))

# Estimate Shannon entropy estimation using various dedicated estimators
h_s = information(MillerMadow(Shannon()), RelativeAmount(), o, x)
h_s = information(HorvitzThompson(Shannon()), Shrinkage(), o, x)
h_s = information(Schürmann(Shannon()), Shrinkage(), o, x)

# Estimate information measures using the generic `Jackknife` estimator
h_r = information(Jackknife(Renyi()), Shrinkage(), o, x)
j_t = information(Jackknife(TsallisExtropy()), BayesianRegularization(), o, x)
j_r = information(Jackknife(RenyiExtropy()), RelativeAmount(),  x)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaDynamics/ComplexityMeasures.jl/blob/a81a14f550f1668986d91498afe4d1642f02e53b/src/core/information_functions.jl#L14-L84">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="ComplexityMeasures.information-Tuple{DifferentialInfoEstimator, Any}" href="#ComplexityMeasures.information-Tuple{DifferentialInfoEstimator, Any}"><code>ComplexityMeasures.information</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">information(est::DifferentialInfoEstimator, x) → h::Real</code></pre><p>Estimate a <strong>differential information measure</strong> using the provided <a href="#ComplexityMeasures.DifferentialInfoEstimator"><code>DifferentialInfoEstimator</code></a> and input data <code>x</code>.</p><p><strong>Description</strong></p><p>The overwhelming majority of differential estimators estimate the <a href="#ComplexityMeasures.Shannon"><code>Shannon</code></a> entropy. If the same estimator can estimate different information measures (e.g. it can estimate both <a href="#ComplexityMeasures.Shannon"><code>Shannon</code></a> and <a href="#ComplexityMeasures.Tsallis"><code>Tsallis</code></a>), then the information measure is provided as an argument to the estimator itself.</p><p>See the <a href="#table_diff_ent_est">table of differential information measure estimators</a> in the docs for all differential information measure estimators.</p><p>Currently, unlike for the discrete information measures, this method doesn&#39;t involve explicitly first computing a probability density function and then passing this density to an information measure definition. But in the future, we want to establish a <code>density</code> API similar to the <a href="../probabilities/#ComplexityMeasures.probabilities"><code>probabilities</code></a> API.</p><p><strong>Examples</strong></p><p>To compute the differential version of a measure, give it as the first argument to a <a href="#ComplexityMeasures.DifferentialInfoEstimator"><code>DifferentialInfoEstimator</code></a> and pass it to <a href="#ComplexityMeasures.information-Tuple{InformationMeasure, OutcomeSpace, Any}"><code>information</code></a>.</p><pre><code class="language-julia hljs">x = randn(1000)
h_sh = information(Kraskov(Shannon()), x)
h_vc = information(Vasicek(Shannon()), x)</code></pre><p>A normal distribution has a base-e Shannon differential entropy of <code>0.5*log(2π) + 0.5</code> nats.</p><pre><code class="language-julia hljs">est = Kraskov(k = 5, base = ℯ) # Base `ℯ` for nats.
h = information(est, randn(2_000_000))
abs(h - 0.5*log(2π) - 0.5) # ≈ 0.0001</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaDynamics/ComplexityMeasures.jl/blob/a81a14f550f1668986d91498afe4d1642f02e53b/src/core/information_functions.jl#L220-L260">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="ComplexityMeasures.information_maximum" href="#ComplexityMeasures.information_maximum"><code>ComplexityMeasures.information_maximum</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">information_maximum(e::InformationMeasure, o::OutcomeSpace, x)
information_maximum(e::InformationMeasure, est::ProbabilitiesEstimator, x)</code></pre><p>Return the maximum value of the given information measure can have, given input data <code>x</code> and  the given outcome space (the <a href="../probabilities/#ComplexityMeasures.OutcomeSpace"><code>OutcomeSpace</code></a> may also be specified by a <a href="../probabilities/#ComplexityMeasures.ProbabilitiesEstimator"><code>ProbabilitiesEstimator</code></a>).</p><p>Like in <a href="../probabilities/#ComplexityMeasures.outcome_space"><code>outcome_space</code></a>, for some outcome spaces, the possible outcomes are known without knowledge of input <code>x</code>, in which case the function dispatches to <code>information_maximum(e, est)</code>.</p><pre><code class="nohighlight hljs">information_maximum(e::InformationMeasure, L::Int)</code></pre><p>The same as above, but computed directly from the number of total outcomes <code>L</code>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaDynamics/ComplexityMeasures.jl/blob/a81a14f550f1668986d91498afe4d1642f02e53b/src/core/information_functions.jl#L144-L159">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="ComplexityMeasures.information_normalized" href="#ComplexityMeasures.information_normalized"><code>ComplexityMeasures.information_normalized</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">information_normalized([e::DiscreteInfoEstimator,] o::OutcomeSpace, x) → h̃
information_normalized([e::DiscreteInfoEstimator,] est::ProbabilitiesEstimator, x) → h̃</code></pre><p>Estimate <code>h̃</code>, a normalized discrete information measure, from input data <code>x</code>, using the <a href="#ComplexityMeasures.DiscreteInfoEstimator"><code>DiscreteInfoEstimator</code></a> <code>e</code>. This is just the value of <a href="#ComplexityMeasures.information-Tuple{InformationMeasure, OutcomeSpace, Any}"><code>information</code></a> divided by the maximum value for <code>e</code>, according to the given <a href="../probabilities/#ComplexityMeasures.OutcomeSpace"><code>OutcomeSpace</code></a> (which may be specified by <code>est</code> if not given directly).</p><p>Instead of a discrete information measure estimator, an <a href="#ComplexityMeasures.InformationMeasure"><code>InformationMeasure</code></a> can be given as first argument, in which case <a href="#ComplexityMeasures.PlugIn"><code>PlugIn</code></a> estimation is used.  If <code>e</code> is not given, it defaults to <code>Shannon()</code>.</p><p>Notice that there is no method <code>information_normalized(e::DiscreteInfoEstimator, probs::Probabilities)</code>, because there is no way to know the number of <em>possible</em> outcomes (i.e., the <a href="../probabilities/#ComplexityMeasures.total_outcomes"><code>total_outcomes</code></a>) from <code>probs</code>.</p><p><strong>Normalized values</strong></p><p>For the <a href="#ComplexityMeasures.PlugIn"><code>PlugIn</code></a> estimator, it is guaranteed that <code>h̃ ∈ [0, 1]</code>. For any other estimator, we can&#39;t guarantee this, since the estimator might over-correct. You should know what you&#39;re doing if using anything but <a href="#ComplexityMeasures.PlugIn"><code>PlugIn</code></a> to estimate normalized values.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaDynamics/ComplexityMeasures.jl/blob/a81a14f550f1668986d91498afe4d1642f02e53b/src/core/information_functions.jl#L175-L198">source</a></section></article><h2 id="Entropies"><a class="docs-heading-anchor" href="#Entropies">Entropies</a><a id="Entropies-1"></a><a class="docs-heading-anchor-permalink" href="#Entropies" title="Permalink"></a></h2><article class="docstring"><header><a class="docstring-binding" id="ComplexityMeasures.entropy" href="#ComplexityMeasures.entropy"><code>ComplexityMeasures.entropy</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">entropy([disce,] probest, x)</code></pre><p>Compute the discrete entropy of <code>x</code> according to the given <a href="../probabilities/#ComplexityMeasures.ProbabilitiesEstimator"><code>ProbabilitiesEstimator</code></a> or <a href="../probabilities/#ComplexityMeasures.OutcomeSpace"><code>OutcomeSpace</code></a> <code>probest</code>. The first optional argument can be an entropy definition (see <a href="#ComplexityMeasures.InformationMeasure"><code>InformationMeasure</code></a>) or a discrete estimator, see <a href="#ComplexityMeasures.DiscreteInfoEstimator"><code>DiscreteInfoEstimator</code></a>. If not given, <code>disce</code> defaults to <code>Shannon()</code>.</p><pre><code class="nohighlight hljs">entropy(diffe::DifferentialInfoEstimator, x)</code></pre><p>Compute the differential entropy of <code>x</code> using a <a href="#ComplexityMeasures.DifferentialInfoEstimator"><code>DifferentialInfoEstimator</code></a>.</p><p><code>entropy</code> is nothing more than a wrapper of <a href="#ComplexityMeasures.information-Tuple{InformationMeasure, OutcomeSpace, Any}"><code>information</code></a> that will simply throw an error if used with an information measure that is not an entropy.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaDynamics/ComplexityMeasures.jl/blob/a81a14f550f1668986d91498afe4d1642f02e53b/src/core/information_functions.jl#L103-L118">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="ComplexityMeasures.Shannon" href="#ComplexityMeasures.Shannon"><code>ComplexityMeasures.Shannon</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">Shannon &lt;: InformationMeasure
Shannon(; base = 2)</code></pre><p>The Shannon<a href="../references/#Shannon1948">(Shannon, 1948)</a> entropy, used with <a href="#ComplexityMeasures.information-Tuple{InformationMeasure, OutcomeSpace, Any}"><code>information</code></a> to compute:</p><p class="math-container">\[H(p) = - \sum_i p[i] \log(p[i])\]</p><p>with the <span>$\log$</span> at the given <code>base</code>.</p><p>The maximum value of the Shannon entropy is <span>$\log_{base}(L)$</span>, which is the entropy of the uniform distribution with <span>$L$</span> the <a href="../probabilities/#ComplexityMeasures.total_outcomes"><code>total_outcomes</code></a>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaDynamics/ComplexityMeasures.jl/blob/a81a14f550f1668986d91498afe4d1642f02e53b/src/information_measure_definitions/shannon.jl#L3-L16">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="ComplexityMeasures.Renyi" href="#ComplexityMeasures.Renyi"><code>ComplexityMeasures.Renyi</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">Renyi &lt;: InformationMeasure
Renyi(q, base = 2)
Renyi(; q = 1.0, base = 2)</code></pre><p>The Rényi generalized order-<code>q</code> entropy <a href="../references/#Rényi1961">(Rényi, 1961)</a>, used with <a href="#ComplexityMeasures.information-Tuple{InformationMeasure, OutcomeSpace, Any}"><code>information</code></a> to compute an entropy with units given by <code>base</code> (typically <code>2</code> or <code>MathConstants.e</code>).</p><p><strong>Description</strong></p><p>Let <span>$p$</span> be an array of probabilities (summing to 1). Then the Rényi generalized entropy is</p><p class="math-container">\[H_q(p) = \frac{1}{1-q} \log \left(\sum_i p[i]^q\right)\]</p><p>and generalizes other known entropies, like e.g. the information entropy (<span>$q = 1$</span>, see <a href="../references/#Shannon1948">Shannon (1948)</a>), the maximum entropy (<span>$q=0$</span>, also known as Hartley entropy), or the correlation entropy (<span>$q = 2$</span>, also known as collision entropy).</p><p>The maximum value of the Rényi entropy is <span>$\log_{base}(L)$</span>, which is the entropy of the uniform distribution with <span>$L$</span> the <a href="../probabilities/#ComplexityMeasures.total_outcomes"><code>total_outcomes</code></a>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaDynamics/ComplexityMeasures.jl/blob/a81a14f550f1668986d91498afe4d1642f02e53b/src/information_measure_definitions/renyi.jl#L3-L28">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="ComplexityMeasures.Tsallis" href="#ComplexityMeasures.Tsallis"><code>ComplexityMeasures.Tsallis</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">Tsallis &lt;: InformationMeasure
Tsallis(q; k = 1.0, base = 2)
Tsallis(; q = 1.0, k = 1.0, base = 2)</code></pre><p>The Tsallis generalized order-<code>q</code> entropy <a href="../references/#Tsallis1988">(Tsallis, 1988)</a>, used with <a href="#ComplexityMeasures.information-Tuple{InformationMeasure, OutcomeSpace, Any}"><code>information</code></a> to compute an entropy.</p><p><code>base</code> only applies in the limiting case <code>q == 1</code>, in which the Tsallis entropy reduces to <a href="#ComplexityMeasures.Shannon"><code>Shannon</code></a> entropy.</p><p><strong>Description</strong></p><p>The Tsallis entropy is a generalization of the Boltzmann-Gibbs entropy, with <code>k</code> standing for the Boltzmann constant. It is defined as</p><p class="math-container">\[S_q(p) = \frac{k}{q - 1}\left(1 - \sum_{i} p[i]^q\right)\]</p><p>The maximum value of the Tsallis entropy is <span>$k(L^{1 - q} - 1)/(1 - q)$</span>, with <span>$L$</span> the <a href="../probabilities/#ComplexityMeasures.total_outcomes"><code>total_outcomes</code></a>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaDynamics/ComplexityMeasures.jl/blob/a81a14f550f1668986d91498afe4d1642f02e53b/src/information_measure_definitions/tsallis.jl#L3-L25">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="ComplexityMeasures.Kaniadakis" href="#ComplexityMeasures.Kaniadakis"><code>ComplexityMeasures.Kaniadakis</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">Kaniadakis &lt;: InformationMeasure
Kaniadakis(; κ = 1.0, base = 2.0)</code></pre><p>The Kaniadakis entropy <a href="../references/#Tsallis2009">(Tsallis, 2009)</a>, used with <a href="#ComplexityMeasures.information-Tuple{InformationMeasure, OutcomeSpace, Any}"><code>information</code></a> to compute</p><p class="math-container">\[H_K(p) = -\sum_{i=1}^N p_i f_\kappa(p_i),\]</p><p class="math-container">\[f_\kappa (x) = \dfrac{x^\kappa - x^{-\kappa}}{2\kappa},\]</p><p>where if <span>$\kappa = 0$</span>, regular logarithm to the given <code>base</code> is used, and 0 probabilities are skipped.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaDynamics/ComplexityMeasures.jl/blob/a81a14f550f1668986d91498afe4d1642f02e53b/src/information_measure_definitions/kaniadakis.jl#L3-L18">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="ComplexityMeasures.Curado" href="#ComplexityMeasures.Curado"><code>ComplexityMeasures.Curado</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">Curado &lt;: InformationMeasure
Curado(; b = 1.0)</code></pre><p>The Curado entropy <a href="../references/#Curado2004">(Curado and Nobre, 2004)</a>, used with <a href="#ComplexityMeasures.information-Tuple{InformationMeasure, OutcomeSpace, Any}"><code>information</code></a> to compute</p><p class="math-container">\[H_C(p) = \left( \sum_{i=1}^N e^{-b p_i} \right) + e^{-b} - 1,\]</p><p>with <code>b ∈ ℛ, b &gt; 0</code>, and the terms outside the sum ensures that <span>$H_C(0) = H_C(1) = 0$</span>.</p><p>The maximum entropy for <code>Curado</code> is <span>$L(1 - \exp(-b/L)) + \exp(-b) - 1$</span> with <span>$L$</span> the <a href="../probabilities/#ComplexityMeasures.total_outcomes"><code>total_outcomes</code></a>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaDynamics/ComplexityMeasures.jl/blob/a81a14f550f1668986d91498afe4d1642f02e53b/src/information_measure_definitions/curado.jl#L3-L17">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="ComplexityMeasures.StretchedExponential" href="#ComplexityMeasures.StretchedExponential"><code>ComplexityMeasures.StretchedExponential</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">StretchedExponential &lt;: InformationMeasure
StretchedExponential(; η = 2.0, base = 2)</code></pre><p>The stretched exponential, or Anteneodo-Plastino, entropy <a href="../references/#Anteneodo1999">(Anteneodo and Plastino, 1999)</a>, used with <a href="#ComplexityMeasures.information-Tuple{InformationMeasure, OutcomeSpace, Any}"><code>information</code></a> to compute</p><p class="math-container">\[S_{\eta}(p) = \sum_{i = 1}^N
\Gamma \left( \dfrac{\eta + 1}{\eta}, - \log_{base}(p_i) \right) -
p_i \Gamma \left( \dfrac{\eta + 1}{\eta} \right),\]</p><p>where <span>$\eta \geq 0$</span>, <span>$\Gamma(\cdot, \cdot)$</span> is the upper incomplete Gamma function, and <span>$\Gamma(\cdot) = \Gamma(\cdot, 0)$</span> is the Gamma function. Reduces to <a href="@ref">Shannon</a> entropy for <code>η = 1.0</code>.</p><p>The maximum entropy for <code>StrechedExponential</code> is a rather complicated expression involving incomplete Gamma functions (see source code).</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaDynamics/ComplexityMeasures.jl/blob/a81a14f550f1668986d91498afe4d1642f02e53b/src/information_measure_definitions/streched_exponential.jl#L5-L24">source</a></section></article><h2 id="Other-information-measures"><a class="docs-heading-anchor" href="#Other-information-measures">Other information measures</a><a id="Other-information-measures-1"></a><a class="docs-heading-anchor-permalink" href="#Other-information-measures" title="Permalink"></a></h2><article class="docstring"><header><a class="docstring-binding" id="ComplexityMeasures.ShannonExtropy" href="#ComplexityMeasures.ShannonExtropy"><code>ComplexityMeasures.ShannonExtropy</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">ShannonExtropy &lt;: InformationMeasure
ShannonExtropy(; base = 2)</code></pre><p>The Shannon extropy <a href="../references/#Lad2015">(Lad <em>et al.</em>, 2015)</a>, used with <a href="@ref"><code>extropy</code></a> to compute</p><p class="math-container">\[J(x) = -\sum_{i=1}^N (1 - p[i]) \log{(1 - p[i])},\]</p><p>for a probability distribution <span>$P = \{p_1, p_2, \ldots, p_N\}$</span>, with the <span>$\log$</span> at the given <code>base</code>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaDynamics/ComplexityMeasures.jl/blob/a81a14f550f1668986d91498afe4d1642f02e53b/src/information_measure_definitions/shannon_extropy.jl#L3-L15">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="ComplexityMeasures.RenyiExtropy" href="#ComplexityMeasures.RenyiExtropy"><code>ComplexityMeasures.RenyiExtropy</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">RenyiExtropy &lt;: InformationMeasure
RenyiExtropy(; q = 1.0, base = 2)</code></pre><p>The Rényi extropy <a href="../references/#Liu2023">(Liu and Xiao, 2023)</a>.</p><p><strong>Description</strong></p><p><code>RenyiExtropy</code> is used with <a href="@ref"><code>extropy</code></a> to compute</p><p class="math-container">\[J_R(P) = \dfrac{-(n - 1) \log{(n - 1)} + (n - 1) \log{ \left( \sum_{i=1}^N {(1 - p[i])}^q \right)} }{q - 1}\]</p><p>for a probability distribution <span>$P = \{p_1, p_2, \ldots, p_N\}$</span>, with the <span>$\log$</span> at the given <code>base</code>. Alternatively, <code>RenyiExtropy</code> can be used with <a href="@ref"><code>extropy_normalized</code></a>, which ensures that the computed extropy is on the interval <span>$[0, 1]$</span> by normalizing to to the maximal Rényi extropy, given by</p><p class="math-container">\[J_R(P) = (N - 1)\log \left( \dfrac{n}{n-1} \right) .\]</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaDynamics/ComplexityMeasures.jl/blob/a81a14f550f1668986d91498afe4d1642f02e53b/src/information_measure_definitions/renyi_extropy.jl#L3-L25">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="ComplexityMeasures.TsallisExtropy" href="#ComplexityMeasures.TsallisExtropy"><code>ComplexityMeasures.TsallisExtropy</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">TsallisExtropy &lt;: InformationMeasure
TsallisExtropy(; base = 2)</code></pre><p>The Tsallis extropy <a href="../references/#Xue2023">(Xue and Deng, 2023)</a>.</p><p><strong>Description</strong></p><p><code>TsallisExtropy</code> is used with <a href="@ref"><code>extropy</code></a> to compute</p><p class="math-container">\[J_T(P) = k \dfrac{N - 1 - \sum_{i=1}^N ( 1 - p[i])^q}{q - 1}\]</p><p>for a probability distribution <span>$P = \{p_1, p_2, \ldots, p_N\}$</span>, with the <span>$\log$</span> at the given <code>base</code>. Alternatively, <code>TsallisExtropy</code> can be used with <a href="@ref"><code>extropy_normalized</code></a>, which ensures that the computed extropy is on the interval <span>$[0, 1]$</span> by normalizing to to the maximal Tsallis extropy, given by</p><p class="math-container">\[J_T(P) = \dfrac{(N - 1)N^{q - 1} - (N - 1)^q}{(q - 1)N^{q - 1}}\]</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaDynamics/ComplexityMeasures.jl/blob/a81a14f550f1668986d91498afe4d1642f02e53b/src/information_measure_definitions/tsallis_extropy.jl#L3-L25">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="ComplexityMeasures.ElectronicEntropy" href="#ComplexityMeasures.ElectronicEntropy"><code>ComplexityMeasures.ElectronicEntropy</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">ElectronicEntropy &lt;: InformationMeasure
ElectronicEntropy(; h = Shannon(; base = 2), j = ShannonExtropy(; base = 2))</code></pre><p>The <a href="https://en.wikipedia.org/wiki/Electronic_entropy">&quot;electronic entropy&quot;</a> measure is defined in discrete form in <a href="../references/#Lad2015">Lad <em>et al.</em> (2015)</a> as</p><p class="math-container">\[H_{EL}(p) = H_S(p) + J_S(P),\]</p><p>where <span>$H_S(p)$</span> is the <a href="#ComplexityMeasures.Shannon"><code>Shannon</code></a> entropy and <span>$J_S(p)$</span> is the <a href="#ComplexityMeasures.ShannonExtropy"><code>ShannonExtropy</code></a> extropy of the probability vector <span>$p$</span>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaDynamics/ComplexityMeasures.jl/blob/a81a14f550f1668986d91498afe4d1642f02e53b/src/information_measure_definitions/electronic.jl#L3-L16">source</a></section></article><h2 id="Discrete-information-estimators"><a class="docs-heading-anchor" href="#Discrete-information-estimators">Discrete information estimators</a><a id="Discrete-information-estimators-1"></a><a class="docs-heading-anchor-permalink" href="#Discrete-information-estimators" title="Permalink"></a></h2><article class="docstring"><header><a class="docstring-binding" id="ComplexityMeasures.DiscreteInfoEstimator" href="#ComplexityMeasures.DiscreteInfoEstimator"><code>ComplexityMeasures.DiscreteInfoEstimator</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">DiscreteInfoEstimator</code></pre><p>The supertype of all discrete information measure estimators, which are used in combination with a <a href="../probabilities/#ComplexityMeasures.ProbabilitiesEstimator"><code>ProbabilitiesEstimator</code></a> as input to  <a href="#ComplexityMeasures.information-Tuple{InformationMeasure, OutcomeSpace, Any}"><code>information</code></a> or related functions.</p><p>The first argument to a discrete estimator is always an <a href="#ComplexityMeasures.InformationMeasure"><code>InformationMeasure</code></a> (defaults to <a href="#ComplexityMeasures.Shannon"><code>Shannon</code></a>).</p><p><strong>Description</strong></p><p>A discrete <a href="#ComplexityMeasures.InformationMeasure"><code>InformationMeasure</code></a> is a functional of a probability mass function. To estimate such a measure from data, we must first estimate a probability mass function using a <a href="../probabilities/#ComplexityMeasures.ProbabilitiesEstimator"><code>ProbabilitiesEstimator</code></a> from the (encoded/discretized) input data, and then apply the estimator to the estimated probabilities. For example, the <a href="#ComplexityMeasures.Shannon"><code>Shannon</code></a> entropy is typically computed using the <a href="../probabilities/#ComplexityMeasures.RelativeAmount"><code>RelativeAmount</code></a> estimator to compute probabilities, which are then given to the <a href="#ComplexityMeasures.PlugIn"><code>PlugIn</code></a> estimator. Many other estimators exist, not only for <a href="#ComplexityMeasures.Shannon"><code>Shannon</code></a> entropy, but other information measures as well.</p><p>We provide a library of both generic estimators such as <a href="#ComplexityMeasures.PlugIn"><code>PlugIn</code></a> or <a href="#ComplexityMeasures.Jackknife"><code>Jackknife</code></a> (which can be applied to any measure), as well as dedicated estimators such as <a href="#ComplexityMeasures.MillerMadow"><code>MillerMadow</code></a>, which computes <a href="#ComplexityMeasures.Shannon"><code>Shannon</code></a> entropy using the Miller-Madow bias correction. The list below gives a complete overview.</p><p><strong>Implementations</strong></p><p>The following estimators are generic and can compute any <a href="#ComplexityMeasures.InformationMeasure"><code>InformationMeasure</code></a>.</p><ul><li><a href="#ComplexityMeasures.PlugIn"><code>PlugIn</code></a>. The default, generic plug-in estimator of any information measure.   It computes the measure exactly as stated in the definition, using the computed   probability mass function.</li><li><a href="#ComplexityMeasures.Jackknife"><code>Jackknife</code></a>. Uses the a combination of the plug-in estimator and the jackknife   principle to estimate the information measure.</li></ul><p><strong><a href="#ComplexityMeasures.Shannon"><code>Shannon</code></a> entropy estimators</strong></p><p>The following estimators are dedicated <a href="#ComplexityMeasures.Shannon"><code>Shannon</code></a> entropy estimators, which provide improvements over the naive <a href="#ComplexityMeasures.PlugIn"><code>PlugIn</code></a> estimator.</p><ul><li><a href="#ComplexityMeasures.MillerMadow"><code>MillerMadow</code></a>.</li><li><a href="#ComplexityMeasures.HorvitzThompson"><code>HorvitzThompson</code></a>.</li><li><a href="#ComplexityMeasures.Schürmann"><code>Schürmann</code></a>.</li><li><a href="#ComplexityMeasures.GeneralizedSchürmann"><code>GeneralizedSchürmann</code></a>.</li><li><a href="#ComplexityMeasures.ChaoShen"><code>ChaoShen</code></a>.</li></ul><div class="admonition is-info"><header class="admonition-header">Info</header><div class="admonition-body"><p>Any of the implemented <a href="#ComplexityMeasures.DiscreteInfoEstimator"><code>DiscreteInfoEstimator</code></a>s can be used in combination with <em>any</em> <a href="../probabilities/#ComplexityMeasures.ProbabilitiesEstimator"><code>ProbabilitiesEstimator</code></a> as input to <a href="#ComplexityMeasures.information-Tuple{InformationMeasure, OutcomeSpace, Any}"><code>information</code></a>. What this means is that every estimator actually comes in many different variants - one for each <a href="../probabilities/#ComplexityMeasures.ProbabilitiesEstimator"><code>ProbabilitiesEstimator</code></a>. For example, the <a href="#ComplexityMeasures.MillerMadow"><code>MillerMadow</code></a> estimator of <a href="#ComplexityMeasures.Shannon"><code>Shannon</code></a> entropy is typically calculated with <a href="../probabilities/#ComplexityMeasures.RelativeAmount"><code>RelativeAmount</code></a> probabilities. But here, you can use for example the <a href="../probabilities/#ComplexityMeasures.BayesianRegularization"><code>BayesianRegularization</code></a> or the <a href="../probabilities/#ComplexityMeasures.Shrinkage"><code>Shrinkage</code></a> probabilities estimators instead, i.e. <code>information(MillerMadow(), RelativeAmount(outcome_space), x)</code> and <code>information(MillerMadow(), BayesianRegularization(outcomes_space), x)</code> are distinct estimators. This holds for all <a href="#ComplexityMeasures.DiscreteInfoEstimator"><code>DiscreteInfoEstimator</code></a>s. Many of these estimators haven&#39;t been explored in the literature before, so feel free to explore, and please cite this software if you use it to explore some new estimator combination!</p></div></div></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaDynamics/ComplexityMeasures.jl/blob/a81a14f550f1668986d91498afe4d1642f02e53b/src/core/information_measures.jl#L99-L159">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="ComplexityMeasures.PlugIn" href="#ComplexityMeasures.PlugIn"><code>ComplexityMeasures.PlugIn</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">PlugIn(e::InformationMeasure) &lt;: DiscreteInfoEstimator</code></pre><p>The <code>PlugIn</code> estimator is also called the empirical/naive/&quot;maximum likelihood&quot; estimator, and is used with <a href="#ComplexityMeasures.information-Tuple{InformationMeasure, OutcomeSpace, Any}"><code>information</code></a> to any discrete <a href="#ComplexityMeasures.InformationMeasure"><code>InformationMeasure</code></a>.</p><p>It computes any quantity exactly as given by its formula. When computing an information measure, which here is defined as a probabilities functional, it computes the quantity directly from a probability mass function, which is derived from maximum-likelihood (<a href="../probabilities/#ComplexityMeasures.RelativeAmount"><code>RelativeAmount</code></a> estimates of the probabilities.</p><p><strong>Bias of plug-in estimates</strong></p><p>The plugin-estimator of <a href="#ComplexityMeasures.Shannon"><code>Shannon</code></a> entropy underestimates the true entropy, with a bias that grows with the number of distinct <a href="../probabilities/#ComplexityMeasures.outcomes"><code>outcomes</code></a> (Arora et al., 2022)<a href="../references/#Arora2022">(Arora <em>et al.</em>, 2022)</a>,</p><p class="math-container">\[bias(H_S^{plugin}) = -\dfrac{K-1}{2N} + o(N^-1).\]</p><p>where <code>K</code> is the number of distinct outcomes, and <code>N</code> is the sample size. Many authors have tried to remedy this by proposing alternative Shannon entropy estimators. For example, the <a href="#ComplexityMeasures.MillerMadow"><code>MillerMadow</code></a> estimator is a simple correction to the plug-in estimator that adds back the bias term above. Many other estimators exist; see <a href="#ComplexityMeasures.DiscreteInfoEstimator"><code>DiscreteInfoEstimator</code></a>s for an overview.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaDynamics/ComplexityMeasures.jl/blob/a81a14f550f1668986d91498afe4d1642f02e53b/src/discrete_info_estimators/plugin.jl#L4-L30">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="ComplexityMeasures.MillerMadow" href="#ComplexityMeasures.MillerMadow"><code>ComplexityMeasures.MillerMadow</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">MillerMadow &lt;: DiscreteInfoEstimator
MillerMadow(measure::Shannon = Shannon())</code></pre><p>The <code>MillerMadow</code> estimator is used with <a href="#ComplexityMeasures.information-Tuple{InformationMeasure, OutcomeSpace, Any}"><code>information</code></a> to compute the discrete <a href="#ComplexityMeasures.Shannon"><code>Shannon</code></a> entropy according to <a href="../references/#Miller1955">Miller (1955)</a>.</p><p><strong>Description</strong></p><p>The Miller-Madow estimator of Shannon entropy is given by</p><p class="math-container">\[H_S^{MM} = H_S^{plugin} + \dfrac{m - 1}{2N},\]</p><p>where <span>$H_S^{plugin}$</span> is the Shannon entropy estimated using the <a href="#ComplexityMeasures.PlugIn"><code>PlugIn</code></a> estimator, <code>m</code> is the number of bins with nonzero probability (as defined in <a href="../references/#Paninski2003">Paninski (2003)</a>), and <code>N</code> is the number of observations.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaDynamics/ComplexityMeasures.jl/blob/a81a14f550f1668986d91498afe4d1642f02e53b/src/discrete_info_estimators/miller_madow.jl#L3-L21">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="ComplexityMeasures.Schürmann" href="#ComplexityMeasures.Schürmann"><code>ComplexityMeasures.Schürmann</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">Schürmann &lt;: DiscreteInfoEstimator
Schürmann(definition::Shannon; a = 1.0)</code></pre><p>The <code>Schürmann</code> estimator is used with <a href="#ComplexityMeasures.information-Tuple{InformationMeasure, OutcomeSpace, Any}"><code>information</code></a> to compute the discrete <a href="#ComplexityMeasures.Shannon"><code>Shannon</code></a> entropy with the bias-corrected estimator given in <a href="../references/#Schurmann2004">Schürmann (2004)</a>.</p><p>See detailed description for <a href="#ComplexityMeasures.GeneralizedSchürmann"><code>GeneralizedSchürmann</code></a> for details.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaDynamics/ComplexityMeasures.jl/blob/a81a14f550f1668986d91498afe4d1642f02e53b/src/discrete_info_estimators/schurmann.jl#L6-L15">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="ComplexityMeasures.GeneralizedSchürmann" href="#ComplexityMeasures.GeneralizedSchürmann"><code>ComplexityMeasures.GeneralizedSchürmann</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">GeneralizedSchürmann &lt;: DiscreteInfoEstimator
GeneralizedSchürmann(measure::Shannon; a::Union{&lt;:Real, Vector{&lt;:Real}} = 1.0)</code></pre><p>The <code>GeneralizedSchürmann</code> estimator is used with <a href="#ComplexityMeasures.information-Tuple{InformationMeasure, OutcomeSpace, Any}"><code>information</code></a> to compute the discrete <a href="#ComplexityMeasures.Shannon"><code>Shannon</code></a> entropy with the bias-corrected estimator given in <a href="../references/#Grassberger2022">Grassberger (2022)</a>.</p><p>The &quot;generalized&quot; part of the name, as opposed to the <a href="../references/#Schurmann2004">Schürmann (2004)</a> estimator (<a href="#ComplexityMeasures.Schürmann"><code>Schürmann</code></a>), is due to the possibility of picking difference parameters <span>$a_i$</span> for different outcomes. If different parameters are assigned to the different outcomes, <code>a</code> must be a vector of parameters of length <code>length(outcomes)</code>, where the outcomes are obtained using <a href="../probabilities/#ComplexityMeasures.outcomes"><code>outcomes</code></a>. See <a href="../references/#Grassberger2022">Grassberger (2022)</a> for more information. If <code>a</code> is a real number, then <span>$a_i = a \forall i$</span>, and the estimator reduces to the <a href="#ComplexityMeasures.Schürmann"><code>Schürmann</code></a> estimator.</p><p><strong>Description</strong></p><p>For a set of <span>$N$</span> observations over <span>$M$</span> outcomes, the estimator is given by</p><p class="math-container">\[H_S^{opt} = \varphi(N) - \dfrac{1}{N} \sum_{i=1}^M n_i G_{n_i}(a_i),\]</p><p>where <span>$n_i$</span> is the observed frequency of the i-th outcome,</p><p class="math-container">\[G_n(a) = \varphi(n) + (-1)^n \int_0^a \dfrac{x^{n - 1}}{x + 1} dx,\]</p><p><span>$G_n(1) = G_n$</span> and <span>$G_n(0) = \varphi(n)$</span>, and</p><p class="math-container">\[G_n = \varphi(n) + (-1)^n \int_0^1 \dfrac{x^{n - 1}}{x + 1} dx.\]</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaDynamics/ComplexityMeasures.jl/blob/a81a14f550f1668986d91498afe4d1642f02e53b/src/discrete_info_estimators/schurmann_generalized.jl#L3-L39">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="ComplexityMeasures.Jackknife" href="#ComplexityMeasures.Jackknife"><code>ComplexityMeasures.Jackknife</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">Jackknife &lt;: DiscreteInfoEstimator
Jackknife(definition::InformationMeasure = Shannon())</code></pre><p>The <code>Jackknife</code> estimator is used with <a href="#ComplexityMeasures.information-Tuple{InformationMeasure, OutcomeSpace, Any}"><code>information</code></a> to compute any discrete <a href="#ComplexityMeasures.InformationMeasure"><code>InformationMeasure</code></a>.</p><p>The <code>Jackknife</code> estimator uses the generic jackknife principle to reduce bias. <a href="../references/#Zahl1977">Zahl (1977)</a> was the first to apply the jaccknife technique in the context of <a href="#ComplexityMeasures.Shannon"><code>Shannon</code></a> entropy estimation. Here, we&#39;ve generalized his estimator to work with any <a href="#ComplexityMeasures.InformationMeasure"><code>InformationMeasure</code></a>.</p><p><strong>Description</strong></p><p>As an example of the jackknife technique, here is the formula for a jackknife estimate of <a href="#ComplexityMeasures.Shannon"><code>Shannon</code></a> entropy</p><p class="math-container">\[H_S^{J} = N H_S^{plugin} - \dfrac{N-1}{N} \sum_{i=1}^N {H_S^{plugin}}^{-\{i\}},\]</p><p>where <span>$N$</span> is the sample size, <span>$H_S^{plugin}$</span> is the plugin estimate of Shannon entropy, and <span>${H_S^{plugin}}^{-\{i\}}$</span> is the plugin estimate, but computed with the <span>$i$</span>-th sample left out.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaDynamics/ComplexityMeasures.jl/blob/a81a14f550f1668986d91498afe4d1642f02e53b/src/discrete_info_estimators/jackknife.jl#L3-L27">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="ComplexityMeasures.HorvitzThompson" href="#ComplexityMeasures.HorvitzThompson"><code>ComplexityMeasures.HorvitzThompson</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">HorvitzThompson &lt;: DiscreteInfoEstimator
HorvitzThompson(measure::Shannon = Shannon())</code></pre><p>The <code>HorvitzThompson</code> estimator is used with <a href="#ComplexityMeasures.information-Tuple{InformationMeasure, OutcomeSpace, Any}"><code>information</code></a> to compute the discrete <a href="#ComplexityMeasures.Shannon"><code>Shannon</code></a> entropy according to <a href="../references/#Horvitz1952">Horvitz and Thompson (1952)</a>.</p><p><strong>Description</strong></p><p>The Horvitz-Thompson estimator of <a href="#ComplexityMeasures.Shannon"><code>Shannon</code></a> entropy is given by</p><p class="math-container">\[H_S^{HT} = -\sum_{i=1}^M \dfrac{p_i \log(p_i) }{1 - (1 - p_i)^N},\]</p><p>where <span>$N$</span> is the sample size and <span>$M$</span> is the number of <a href="../probabilities/#ComplexityMeasures.outcomes"><code>outcomes</code></a>. Given the true probability <span>$p_i$</span> of the <span>$i$</span>-th outcome, <span>$1 - (1 - p_i)^N$</span> is the probability that the outcome appears at least once in a sample of size <span>$N$</span> <a href="../references/#Arora2022">(Arora <em>et al.</em>, 2022)</a>. Dividing by this inclusion probability is a form of weighting, and compensates for situations where certain outcomes have so low probabilities that they are not often observed in a sample, for example in power-law distributions.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaDynamics/ComplexityMeasures.jl/blob/a81a14f550f1668986d91498afe4d1642f02e53b/src/discrete_info_estimators/horvitz_thompson.jl#L3-L25">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="ComplexityMeasures.ChaoShen" href="#ComplexityMeasures.ChaoShen"><code>ComplexityMeasures.ChaoShen</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">ChaoShen &lt;: DiscreteInfoEstimator
ChaoShen(definition::Shannon = Shannon())</code></pre><p>The <code>ChaoShen</code> estimator is used with <a href="#ComplexityMeasures.information-Tuple{InformationMeasure, OutcomeSpace, Any}"><code>information</code></a> to compute the discrete <a href="#ComplexityMeasures.Shannon"><code>Shannon</code></a> entropy according to <a href="../references/#Chao2003">Chao and Shen (2003)</a>.</p><p><strong>Description</strong></p><p>This estimator is a modification of the <a href="#ComplexityMeasures.HorvitzThompson"><code>HorvitzThompson</code></a> estimator that multiplies each plugin probability estimate by an estimate of sample coverage. If <span>$f_1$</span> is the number of singletons (outcomes that occur only once) in a sample of length <span>$N$</span>, then the sample coverage is <span>$C = 1 - \dfrac{f_1}{N}$</span>. The Chao-Shen estimator of Shannon entropy is then</p><p class="math-container">\[H_S^{CS} = -\sum_{i=1}^M \left( \dfrac{C p_i \log(C p_i)}{1 - (1 - C p_i)^N} \right),\]</p><p>where <span>$N$</span> is the sample size and <span>$M$</span> is the number of <a href="../probabilities/#ComplexityMeasures.outcomes"><code>outcomes</code></a>. If <span>$f_1 = N$</span>, then <span>$f_1$</span> is set to <span>$f_1 = N - 1$</span> to ensure positive entropy <a href="../references/#Arora2022">(Arora <em>et al.</em>, 2022)</a>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaDynamics/ComplexityMeasures.jl/blob/a81a14f550f1668986d91498afe4d1642f02e53b/src/discrete_info_estimators/chao_shen.jl#L3-L25">source</a></section></article><h2 id="Differential-information-estimators"><a class="docs-heading-anchor" href="#Differential-information-estimators">Differential information estimators</a><a id="Differential-information-estimators-1"></a><a class="docs-heading-anchor-permalink" href="#Differential-information-estimators" title="Permalink"></a></h2><article class="docstring"><header><a class="docstring-binding" id="ComplexityMeasures.DifferentialInfoEstimator" href="#ComplexityMeasures.DifferentialInfoEstimator"><code>ComplexityMeasures.DifferentialInfoEstimator</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">DifferentialInfoEstimator</code></pre><p>The supertype of all differential information measure estimators. These estimators compute an information measure in various ways that do not involve explicitly estimating a probability distribution.</p><p>Each <a href="#ComplexityMeasures.DifferentialInfoEstimator"><code>DifferentialInfoEstimator</code></a>s uses a specialized technique to approximate relevant densities/integrals, and is often tailored to one or a few types of information measures. For example, <a href="#ComplexityMeasures.Kraskov"><code>Kraskov</code></a> estimates the <a href="#ComplexityMeasures.Shannon"><code>Shannon</code></a> entropy.</p><p>See <a href="#ComplexityMeasures.information-Tuple{InformationMeasure, OutcomeSpace, Any}"><code>information</code></a> for usage.</p><p><strong>Implementations</strong></p><ul><li><a href="#ComplexityMeasures.KozachenkoLeonenko"><code>KozachenkoLeonenko</code></a>.</li><li><a href="#ComplexityMeasures.Kraskov"><code>Kraskov</code></a>.</li><li><a href="#ComplexityMeasures.Goria"><code>Goria</code></a>.</li><li><a href="#ComplexityMeasures.Gao"><code>Gao</code></a>.</li><li><a href="#ComplexityMeasures.Zhu"><code>Zhu</code></a></li><li><a href="#ComplexityMeasures.ZhuSingh"><code>ZhuSingh</code></a>.</li><li><a href="#ComplexityMeasures.Lord"><code>Lord</code></a>.</li><li><a href="#ComplexityMeasures.AlizadehArghami"><code>AlizadehArghami</code></a>.</li><li><a href="#ComplexityMeasures.Correa"><code>Correa</code></a>.</li><li><a href="#ComplexityMeasures.Vasicek"><code>Vasicek</code></a>.</li><li><a href="#ComplexityMeasures.Ebrahimi"><code>Ebrahimi</code></a>.</li><li><a href="#ComplexityMeasures.LeonenkoProzantoSavani"><code>LeonenkoProzantoSavani</code></a>.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaDynamics/ComplexityMeasures.jl/blob/a81a14f550f1668986d91498afe4d1642f02e53b/src/core/information_measures.jl#L163-L190">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="ComplexityMeasures.Kraskov" href="#ComplexityMeasures.Kraskov"><code>ComplexityMeasures.Kraskov</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">Kraskov &lt;: DifferentialInfoEstimator
Kraskov(definition = Shannon(); k::Int = 1, w::Int = 0)</code></pre><p>The <code>Kraskov</code> estimator computes the <a href="#ComplexityMeasures.Shannon"><code>Shannon</code></a> differential <a href="#ComplexityMeasures.information-Tuple{InformationMeasure, OutcomeSpace, Any}"><code>information</code></a> of a multi-dimensional <a href="../#StateSpaceSets.StateSpaceSet"><code>StateSpaceSet</code></a> using the <code>k</code>-th nearest neighbor searches method from <a href="../references/#Kraskov2004">Kraskov <em>et al.</em> (2004)</a>, with logarithms to the <code>base</code> specified in <code>definition</code>.</p><p><code>w</code> is the Theiler window, which determines if temporal neighbors are excluded during neighbor searches (defaults to <code>0</code>, meaning that only the point itself is excluded when searching for neighbours).</p><p><strong>Description</strong></p><p>Assume we have samples <span>$\{\bf{x}_1, \bf{x}_2, \ldots, \bf{x}_N \}$</span> from a continuous random variable <span>$X \in \mathbb{R}^d$</span> with support <span>$\mathcal{X}$</span> and density function<span>$f : \mathbb{R}^d \to \mathbb{R}$</span>. <code>Kraskov</code> estimates the <a href="@ref">Shannon</a> differential entropy</p><p class="math-container">\[H(X) = \int_{\mathcal{X}} f(x) \log f(x) dx = \mathbb{E}[-\log(f(X))].\]</p><p>See also: <a href="#ComplexityMeasures.information-Tuple{InformationMeasure, OutcomeSpace, Any}"><code>information</code></a>, <a href="#ComplexityMeasures.KozachenkoLeonenko"><code>KozachenkoLeonenko</code></a>, <a href="#ComplexityMeasures.DifferentialInfoEstimator"><code>DifferentialInfoEstimator</code></a>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaDynamics/ComplexityMeasures.jl/blob/a81a14f550f1668986d91498afe4d1642f02e53b/src/differential_info_estimators/nearest_neighbors/Kraskov.jl#L3-L29">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="ComplexityMeasures.KozachenkoLeonenko" href="#ComplexityMeasures.KozachenkoLeonenko"><code>ComplexityMeasures.KozachenkoLeonenko</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">KozachenkoLeonenko &lt;: DifferentialInfoEstimator
KozachenkoLeonenko(definition = Shannon(); w::Int = 0)</code></pre><p>The <code>KozachenkoLeonenko</code> estimator <a href="../references/#KozachenkoLeonenko1987">(Kozachenko and Leonenko, 1987)</a> computes the <a href="#ComplexityMeasures.Shannon"><code>Shannon</code></a> differential <a href="#ComplexityMeasures.information-Tuple{InformationMeasure, OutcomeSpace, Any}"><code>information</code></a> of a multi-dimensional <a href="../#StateSpaceSets.StateSpaceSet"><code>StateSpaceSet</code></a>, with logarithms to the <code>base</code> specified in <code>definition</code>.</p><p><strong>Description</strong></p><p>Assume we have samples <span>$\{\bf{x}_1, \bf{x}_2, \ldots, \bf{x}_N \}$</span> from a continuous random variable <span>$X \in \mathbb{R}^d$</span> with support <span>$\mathcal{X}$</span> and density function<span>$f : \mathbb{R}^d \to \mathbb{R}$</span>. <code>KozachenkoLeonenko</code> estimates the <a href="@ref">Shannon</a> differential entropy</p><p class="math-container">\[H(X) = \int_{\mathcal{X}} f(x) \log f(x) dx = \mathbb{E}[-\log(f(X))]\]</p><p>using the nearest neighbor method from <a href="../references/#KozachenkoLeonenko1987">Kozachenko and Leonenko (1987)</a>, as described in <a href="../references/#Charzyńska2015">Charzyńska and Gambin (2016)</a>.</p><p><code>w</code> is the Theiler window, which determines if temporal neighbors are excluded during neighbor searches (defaults to <code>0</code>, meaning that only the point itself is excluded when searching for neighbours).</p><p>In contrast to <a href="#ComplexityMeasures.Kraskov"><code>Kraskov</code></a>, this estimator uses only the <em>closest</em> neighbor.</p><p>See also: <a href="#ComplexityMeasures.information-Tuple{InformationMeasure, OutcomeSpace, Any}"><code>information</code></a>, <a href="#ComplexityMeasures.Kraskov"><code>Kraskov</code></a>, <a href="#ComplexityMeasures.DifferentialInfoEstimator"><code>DifferentialInfoEstimator</code></a>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaDynamics/ComplexityMeasures.jl/blob/a81a14f550f1668986d91498afe4d1642f02e53b/src/differential_info_estimators/nearest_neighbors/KozachenkoLeonenko.jl#L3-L32">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="ComplexityMeasures.Zhu" href="#ComplexityMeasures.Zhu"><code>ComplexityMeasures.Zhu</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">Zhu &lt;: DifferentialInfoEstimator
Zhu(; definition = Shannon(), k = 1, w = 0)</code></pre><p>The <code>Zhu</code> estimator <a href="../references/#Zhu2015">(Zhu <em>et al.</em>, 2015)</a> is an extension to <a href="#ComplexityMeasures.KozachenkoLeonenko"><code>KozachenkoLeonenko</code></a>, and computes the <a href="#ComplexityMeasures.Shannon"><code>Shannon</code></a> differential <a href="#ComplexityMeasures.information-Tuple{InformationMeasure, OutcomeSpace, Any}"><code>information</code></a> of a multi-dimensional <a href="../#StateSpaceSets.StateSpaceSet"><code>StateSpaceSet</code></a>, with logarithms to the <code>base</code> specified in <code>definition</code>.</p><p><strong>Description</strong></p><p>Assume we have samples <span>$\{\bf{x}_1, \bf{x}_2, \ldots, \bf{x}_N \}$</span> from a continuous random variable <span>$X \in \mathbb{R}^d$</span> with support <span>$\mathcal{X}$</span> and density function<span>$f : \mathbb{R}^d \to \mathbb{R}$</span>. <code>Zhu</code> estimates the <a href="@ref">Shannon</a> differential entropy</p><p class="math-container">\[H(X) = \int_{\mathcal{X}} f(x) \log f(x) dx = \mathbb{E}[-\log(f(X))]\]</p><p>by approximating densities within hyperrectangles surrounding each point <code>xᵢ ∈ x</code> using using <code>k</code> nearest neighbor searches. <code>w</code> is the Theiler window, which determines if temporal neighbors are excluded during neighbor searches (defaults to <code>0</code>, meaning that only the point itself is excluded when searching for neighbours).</p><p>See also: <a href="#ComplexityMeasures.information-Tuple{InformationMeasure, OutcomeSpace, Any}"><code>information</code></a>, <a href="#ComplexityMeasures.KozachenkoLeonenko"><code>KozachenkoLeonenko</code></a>, <a href="#ComplexityMeasures.DifferentialInfoEstimator"><code>DifferentialInfoEstimator</code></a>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaDynamics/ComplexityMeasures.jl/blob/a81a14f550f1668986d91498afe4d1642f02e53b/src/differential_info_estimators/nearest_neighbors/Zhu.jl#L3-L30">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="ComplexityMeasures.ZhuSingh" href="#ComplexityMeasures.ZhuSingh"><code>ComplexityMeasures.ZhuSingh</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">ZhuSingh &lt;: DifferentialInfoEstimator
ZhuSingh(definition = Shannon(); k = 1, w = 0)</code></pre><p>The <code>ZhuSingh</code> estimator <a href="../references/#Zhu2015">(Zhu <em>et al.</em>, 2015)</a> computes the <a href="#ComplexityMeasures.Shannon"><code>Shannon</code></a> differential <a href="#ComplexityMeasures.information-Tuple{InformationMeasure, OutcomeSpace, Any}"><code>information</code></a> of a multi-dimensional <a href="../#StateSpaceSets.StateSpaceSet"><code>StateSpaceSet</code></a>, with logarithms to the <code>base</code> specified in <code>definition</code>.</p><p><strong>Description</strong></p><p>Assume we have samples <span>$\{\bf{x}_1, \bf{x}_2, \ldots, \bf{x}_N \}$</span> from a continuous random variable <span>$X \in \mathbb{R}^d$</span> with support <span>$\mathcal{X}$</span> and density function<span>$f : \mathbb{R}^d \to \mathbb{R}$</span>. <code>ZhuSingh</code> estimates the <a href="@ref">Shannon</a> differential entropy</p><p class="math-container">\[H(X) = \int_{\mathcal{X}} f(x) \log f(x) dx = \mathbb{E}[-\log(f(X))].\]</p><p>Like <a href="#ComplexityMeasures.Zhu"><code>Zhu</code></a>, this estimator approximates probabilities within hyperrectangles surrounding each point <code>xᵢ ∈ x</code> using using <code>k</code> nearest neighbor searches. However, it also considers the number of neighbors falling on the borders of these hyperrectangles. This estimator is an extension to the entropy estimator in <a href="../references/#Singh2003">Singh <em>et al.</em> (2003)</a>.</p><p><code>w</code> is the Theiler window, which determines if temporal neighbors are excluded during neighbor searches (defaults to <code>0</code>, meaning that only the point itself is excluded when searching for neighbours).</p><p>See also: <a href="#ComplexityMeasures.information-Tuple{InformationMeasure, OutcomeSpace, Any}"><code>information</code></a>, <a href="#ComplexityMeasures.DifferentialInfoEstimator"><code>DifferentialInfoEstimator</code></a>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaDynamics/ComplexityMeasures.jl/blob/a81a14f550f1668986d91498afe4d1642f02e53b/src/differential_info_estimators/nearest_neighbors/ZhuSingh.jl#L8-L37">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="ComplexityMeasures.Gao" href="#ComplexityMeasures.Gao"><code>ComplexityMeasures.Gao</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">Gao &lt;: DifferentialInfoEstimator
Gao(definition = Shannon(); k = 1, w = 0, corrected = true)</code></pre><p>The <code>Gao</code> estimator <a href="../references/#Gao2015">(Gao <em>et al.</em>, 2015)</a> computes the <a href="#ComplexityMeasures.Shannon"><code>Shannon</code></a> differential <a href="#ComplexityMeasures.information-Tuple{InformationMeasure, OutcomeSpace, Any}"><code>information</code></a>, using a <code>k</code>-th nearest-neighbor approach based on <a href="../references/#Singh2003">Singh <em>et al.</em> (2003)</a>, with logarithms to the <code>base</code> specified in <code>definition</code>.</p><p><code>w</code> is the Theiler window, which determines if temporal neighbors are excluded during neighbor searches (defaults to <code>0</code>, meaning that only the point itself is excluded when searching for neighbours).</p><p><a href="../references/#Gao2015">Gao <em>et al.</em> (2015)</a> give two variants of this estimator. If <code>corrected == false</code>, then the uncorrected version is used. If <code>corrected == true</code>, then the corrected version is used, which ensures that the estimator is asymptotically unbiased.</p><p><strong>Description</strong></p><p>Assume we have samples <span>$\{\bf{x}_1, \bf{x}_2, \ldots, \bf{x}_N \}$</span> from a continuous random variable <span>$X \in \mathbb{R}^d$</span> with support <span>$\mathcal{X}$</span> and density function<span>$f : \mathbb{R}^d \to \mathbb{R}$</span>. <code>KozachenkoLeonenko</code> estimates the <a href="@ref">Shannon</a> differential entropy</p><p class="math-container">\[H(X) = \int_{\mathcal{X}} f(x) \log f(x) dx = \mathbb{E}[-\log(f(X))].\]</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaDynamics/ComplexityMeasures.jl/blob/a81a14f550f1668986d91498afe4d1642f02e53b/src/differential_info_estimators/nearest_neighbors/Gao.jl#L8-L35">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="ComplexityMeasures.Goria" href="#ComplexityMeasures.Goria"><code>ComplexityMeasures.Goria</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">Goria &lt;: DifferentialInfoEstimator
Goria(measure = Shannon(); k = 1, w = 0)</code></pre><p>The <code>Goria</code> estimator <a href="../references/#Goria2005">(Goria <em>et al.</em>, 2005)</a> computes the <a href="#ComplexityMeasures.Shannon"><code>Shannon</code></a> differential <a href="#ComplexityMeasures.information-Tuple{InformationMeasure, OutcomeSpace, Any}"><code>information</code></a> of a multi-dimensional <a href="../#StateSpaceSets.StateSpaceSet"><code>StateSpaceSet</code></a>, with logarithms to the <code>base</code> specified in <code>definition</code>.</p><p><strong>Description</strong></p><p>Assume we have samples <span>$\{\bf{x}_1, \bf{x}_2, \ldots, \bf{x}_N \}$</span> from a continuous random variable <span>$X \in \mathbb{R}^d$</span> with support <span>$\mathcal{X}$</span> and density function<span>$f : \mathbb{R}^d \to \mathbb{R}$</span>. <code>Goria</code> estimates the <a href="@ref">Shannon</a> differential entropy</p><p class="math-container">\[H(X) = \int_{\mathcal{X}} f(x) \log f(x) dx = \mathbb{E}[-\log(f(X))].\]</p><p>Specifically, let <span>$\bf{n}_1, \bf{n}_2, \ldots, \bf{n}_N$</span> be the distance of the samples <span>$\{\bf{x}_1, \bf{x}_2, \ldots, \bf{x}_N \}$</span> to their <code>k</code>-th nearest neighbors. Next, let the geometric mean of the distances be</p><p class="math-container">\[\hat{\rho}_k = \left( \prod_{i=1}^N \right)^{\dfrac{1}{N}}\]</p><p><a href="../references/#Goria2005">Goria <em>et al.</em> (2005)</a>&#39;s estimate of Shannon differential entropy is then</p><p class="math-container">\[\hat{H} = m\hat{\rho}_k + \log(N - 1) - \psi(k) + \log c_1(m),\]</p><p>where <span>$c_1(m) = \dfrac{2\pi^\frac{m}{2}}{m \Gamma(m/2)}$</span> and <span>$\psi$</span> is the digamma function.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaDynamics/ComplexityMeasures.jl/blob/a81a14f550f1668986d91498afe4d1642f02e53b/src/differential_info_estimators/nearest_neighbors/Goria.jl#L8-L43">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="ComplexityMeasures.Lord" href="#ComplexityMeasures.Lord"><code>ComplexityMeasures.Lord</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">Lord &lt;: DifferentialInfoEstimator
Lord(measure = Shannon(); k = 10, w = 0)</code></pre><p>The <code>Lord</code> estimator <a href="../references/#Lord2018">(Lord <em>et al.</em>, 2018)</a> estimates the <a href="#ComplexityMeasures.Shannon"><code>Shannon</code></a> differential <a href="#ComplexityMeasures.information-Tuple{InformationMeasure, OutcomeSpace, Any}"><code>information</code></a> using a nearest neighbor approach with a local nonuniformity correction (LNC), with logarithms to the <code>base</code> specified in <code>definition</code>.</p><p><code>w</code> is the Theiler window, which determines if temporal neighbors are excluded during neighbor searches (defaults to <code>0</code>, meaning that only the point itself is excluded when searching for neighbours).</p><p><strong>Description</strong></p><p>Assume we have samples <span>$\bar{X} = \{\bf{x}_1, \bf{x}_2, \ldots, \bf{x}_N \}$</span> from a continuous random variable <span>$X \in \mathbb{R}^d$</span> with support <span>$\mathcal{X}$</span> and density function <span>$f : \mathbb{R}^d \to \mathbb{R}$</span>. <code>Lord</code> estimates the <a href="@ref">Shannon</a> differential entropy</p><p class="math-container">\[H(X) = \int_{\mathcal{X}} f(x) \log f(x) dx = \mathbb{E}[-\log(f(X))],\]</p><p>by using the resubstitution formula</p><p class="math-container">\[\hat{\bar{X}, k} = -\mathbb{E}[\log(f(X))]
\approx \sum_{i = 1}^N \log(\hat{f}(\bf{x}_i)),\]</p><p>where <span>$\hat{f}(\bf{x}_i)$</span> is an estimate of the density at <span>$\bf{x}_i$</span> constructed in a manner such that <span>$\hat{f}(\bf{x}_i) \propto \dfrac{k(x_i) / N}{V_i}$</span>, where <span>$k(x_i)$</span> is the number of points in the neighborhood of <span>$\bf{x}_i$</span>, and <span>$V_i$</span> is the volume of that neighborhood.</p><p>While most nearest-neighbor based differential entropy estimators uses regular volume elements (e.g. hypercubes, hyperrectangles, hyperspheres) for approximating the local densities <span>$\hat{f}(\bf{x}_i)$</span>, the <code>Lord</code> estimator uses hyperellopsoid volume elements. These hyperellipsoids are, for each query point <code>xᵢ</code>, estimated using singular value decomposition (SVD) on the <code>k</code>-th nearest neighbors of <code>xᵢ</code>. Thus, the hyperellipsoids stretch/compress in response to the local geometry around each sample point. This makes <code>Lord</code> a well-suited entropy estimator for a wide range of systems.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaDynamics/ComplexityMeasures.jl/blob/a81a14f550f1668986d91498afe4d1642f02e53b/src/differential_info_estimators/nearest_neighbors/Lord.jl#L25-L67">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="ComplexityMeasures.LeonenkoProzantoSavani" href="#ComplexityMeasures.LeonenkoProzantoSavani"><code>ComplexityMeasures.LeonenkoProzantoSavani</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">LeonenkoProzantoSavani &lt;: DifferentialInfoEstimator
LeonenkoProzantoSavani(definition = Shannon(); k = 1, w = 0)</code></pre><p>The <code>LeonenkoProzantoSavani</code> estimator <a href="../references/#LeonenkoProzantoSavani2008">(Leonenko <em>et al.</em>, 2008)</a> computes the  <a href="#ComplexityMeasures.Shannon"><code>Shannon</code></a>, <a href="#ComplexityMeasures.Renyi"><code>Renyi</code></a>, or <a href="#ComplexityMeasures.Tsallis"><code>Tsallis</code></a> differential <a href="#ComplexityMeasures.information-Tuple{InformationMeasure, OutcomeSpace, Any}"><code>information</code></a> of a multi-dimensional <a href="../#StateSpaceSets.StateSpaceSet"><code>StateSpaceSet</code></a>, with logarithms to the <code>base</code> specified in <code>definition</code>.</p><p><strong>Description</strong></p><p>The estimator uses <code>k</code>-th nearest-neighbor searches.  <code>w</code> is the Theiler window, which determines if temporal neighbors are excluded during neighbor searches (defaults to <code>0</code>, meaning that only the point itself is excluded when searching for neighbours).</p><p>For details, see <a href="../references/#LeonenkoProzantoSavani2008">Leonenko <em>et al.</em> (2008)</a>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaDynamics/ComplexityMeasures.jl/blob/a81a14f550f1668986d91498afe4d1642f02e53b/src/differential_info_estimators/nearest_neighbors/LeonenkoProzantoSavani.jl#L7-L23">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="ComplexityMeasures.Vasicek" href="#ComplexityMeasures.Vasicek"><code>ComplexityMeasures.Vasicek</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">Vasicek &lt;: DifferentialInfoEstimator
Vasicek(definition = Shannon(); m::Int = 1)</code></pre><p>The <code>Vasicek</code> estimator computes the <a href="#ComplexityMeasures.Shannon"><code>Shannon</code></a> differential <a href="#ComplexityMeasures.information-Tuple{InformationMeasure, OutcomeSpace, Any}"><code>information</code></a> of a timeseries using the method from <a href="../references/#Vasicek1976">Vasicek (1976)</a>, with logarithms to the <code>base</code> specified in <code>definition</code>.</p><p>The <code>Vasicek</code> estimator belongs to a class of differential entropy estimators based on <a href="https://en.wikipedia.org/wiki/Order_statistic">order statistics</a>, of which <a href="../references/#Vasicek1976">Vasicek (1976)</a> was the first. It only works for <em>timeseries</em> input.</p><p><strong>Description</strong></p><p>Assume we have samples <span>$\bar{X} = \{x_1, x_2, \ldots, x_N \}$</span> from a continuous random variable <span>$X \in \mathbb{R}$</span> with support <span>$\mathcal{X}$</span> and density function<span>$f : \mathbb{R} \to \mathbb{R}$</span>. <code>Vasicek</code> estimates the <a href="@ref">Shannon</a> differential entropy</p><p class="math-container">\[H(X) = \int_{\mathcal{X}} f(x) \log f(x) dx = \mathbb{E}[-\log(f(X))].\]</p><p>However, instead of estimating the above integral directly, it makes use of the equivalent integral, where <span>$F$</span> is the distribution function for <span>$X$</span>,</p><p class="math-container">\[H(X) = \int_0^1 \log \left(\dfrac{d}{dp}F^{-1}(p) \right) dp\]</p><p>This integral is approximated by first computing the <a href="https://en.wikipedia.org/wiki/Order_statistic">order statistics</a> of <span>$\bar{X}$</span> (the input timeseries), i.e. <span>$x_{(1)} \leq x_{(2)} \leq \cdots \leq x_{(n)}$</span>. The <code>Vasicek</code> <a href="#ComplexityMeasures.Shannon"><code>Shannon</code></a> differential entropy estimate is then</p><p class="math-container">\[\hat{H}_V(\bar{X}, m) =
\dfrac{1}{n}
\sum_{i = 1}^n \log \left[ \dfrac{n}{2m} (\bar{X}_{(i+m)} - \bar{X}_{(i-m)}) \right]\]</p><p><strong>Usage</strong></p><p>In practice, choice of <code>m</code> influences how fast the entropy converges to the true value. For small value of <code>m</code>, convergence is slow, so we recommend to scale <code>m</code> according to the time series length <code>n</code> and use <code>m &gt;= n/100</code> (this is just a heuristic based on the tests written for this package).</p><p>See also: <a href="#ComplexityMeasures.information-Tuple{InformationMeasure, OutcomeSpace, Any}"><code>information</code></a>, <a href="#ComplexityMeasures.Correa"><code>Correa</code></a>, <a href="#ComplexityMeasures.AlizadehArghami"><code>AlizadehArghami</code></a>, <a href="#ComplexityMeasures.Ebrahimi"><code>Ebrahimi</code></a>, <a href="#ComplexityMeasures.DifferentialInfoEstimator"><code>DifferentialInfoEstimator</code></a>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaDynamics/ComplexityMeasures.jl/blob/a81a14f550f1668986d91498afe4d1642f02e53b/src/differential_info_estimators/order_statistics/Vasicek.jl#L3-L53">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="ComplexityMeasures.AlizadehArghami" href="#ComplexityMeasures.AlizadehArghami"><code>ComplexityMeasures.AlizadehArghami</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">AlizadehArghami &lt;: DifferentialInfoEstimator
AlizadehArghami(definition = Shannon(); m::Int = 1)</code></pre><p>The <code>AlizadehArghami</code> estimator computes the <a href="#ComplexityMeasures.Shannon"><code>Shannon</code></a> differential <a href="#ComplexityMeasures.information-Tuple{InformationMeasure, OutcomeSpace, Any}"><code>information</code></a> of a timeseries using the method from <a href="../references/#Alizadeh2010">Alizadeh and Arghami (2010)</a>, with logarithms to the <code>base</code> specified in <code>definition</code>.</p><p>The <code>AlizadehArghami</code> estimator belongs to a class of differential entropy estimators based on <a href="https://en.wikipedia.org/wiki/Order_statistic">order statistics</a>. It only works for <em>timeseries</em> input.</p><p><strong>Description</strong></p><p>Assume we have samples <span>$\bar{X} = \{x_1, x_2, \ldots, x_N \}$</span> from a continuous random variable <span>$X \in \mathbb{R}$</span> with support <span>$\mathcal{X}$</span> and density function<span>$f : \mathbb{R} \to \mathbb{R}$</span>. <code>AlizadehArghami</code> estimates the <a href="@ref">Shannon</a> differential entropy</p><p class="math-container">\[H(X) = \int_{\mathcal{X}} f(x) \log f(x) dx = \mathbb{E}[-\log(f(X))].\]</p><p>However, instead of estimating the above integral directly, it makes use of the equivalent integral, where <span>$F$</span> is the distribution function for <span>$X$</span>:</p><p class="math-container">\[H(X) = \int_0^1 \log \left(\dfrac{d}{dp}F^{-1}(p) \right) dp.\]</p><p>This integral is approximated by first computing the <a href="https://en.wikipedia.org/wiki/Order_statistic">order statistics</a> of <span>$\bar{X}$</span> (the input timeseries), i.e. <span>$x_{(1)} \leq x_{(2)} \leq \cdots \leq x_{(n)}$</span>. The <code>AlizadehArghami</code> <a href="#ComplexityMeasures.Shannon"><code>Shannon</code></a> differential entropy estimate is then the the <a href="#ComplexityMeasures.Vasicek"><code>Vasicek</code></a> estimate <span>$\hat{H}_{V}(\bar{X}, m, n)$</span>, plus a correction factor</p><p class="math-container">\[\hat{H}_{A}(\bar{X}, m, n) = \hat{H}_{V}(\bar{X}, m, n) +
\dfrac{2}{n}\left(m \log(2) \right).\]</p><p>See also: <a href="#ComplexityMeasures.information-Tuple{InformationMeasure, OutcomeSpace, Any}"><code>information</code></a>, <a href="#ComplexityMeasures.Correa"><code>Correa</code></a>, <a href="#ComplexityMeasures.Ebrahimi"><code>Ebrahimi</code></a>, <a href="#ComplexityMeasures.Vasicek"><code>Vasicek</code></a>, <a href="#ComplexityMeasures.DifferentialInfoEstimator"><code>DifferentialInfoEstimator</code></a>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaDynamics/ComplexityMeasures.jl/blob/a81a14f550f1668986d91498afe4d1642f02e53b/src/differential_info_estimators/order_statistics/AlizadehArghami.jl#L3-L46">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="ComplexityMeasures.Ebrahimi" href="#ComplexityMeasures.Ebrahimi"><code>ComplexityMeasures.Ebrahimi</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">Ebrahimi &lt;: DifferentialInfoEstimator
Ebrahimi(definition = Shannon(); m::Int = 1)</code></pre><p>The <code>Ebrahimi</code> estimator computes the <a href="#ComplexityMeasures.Shannon"><code>Shannon</code></a> <a href="#ComplexityMeasures.information-Tuple{InformationMeasure, OutcomeSpace, Any}"><code>information</code></a> of a timeseries using the method from <a href="../references/#Ebrahimi1994">Ebrahimi <em>et al.</em> (1994)</a>, with logarithms to the <code>base</code> specified in <code>definition</code>.</p><p>The <code>Ebrahimi</code> estimator belongs to a class of differential entropy estimators based on <a href="https://en.wikipedia.org/wiki/Order_statistic">order statistics</a>. It only works for <em>timeseries</em> input.</p><p><strong>Description</strong></p><p>Assume we have samples <span>$\bar{X} = \{x_1, x_2, \ldots, x_N \}$</span> from a continuous random variable <span>$X \in \mathbb{R}$</span> with support <span>$\mathcal{X}$</span> and density function<span>$f : \mathbb{R} \to \mathbb{R}$</span>. <code>Ebrahimi</code> estimates the <a href="@ref">Shannon</a> differential entropy</p><p class="math-container">\[H(X) = \int_{\mathcal{X}} f(x) \log f(x) dx = \mathbb{E}[-\log(f(X))].\]</p><p>However, instead of estimating the above integral directly, it makes use of the equivalent integral, where <span>$F$</span> is the distribution function for <span>$X$</span>,</p><p class="math-container">\[H(X) = \int_0^1 \log \left(\dfrac{d}{dp}F^{-1}(p) \right) dp\]</p><p>This integral is approximated by first computing the <a href="https://en.wikipedia.org/wiki/Order_statistic">order statistics</a> of <span>$\bar{X}$</span> (the input timeseries), i.e. <span>$x_{(1)} \leq x_{(2)} \leq \cdots \leq x_{(n)}$</span>. The <code>Ebrahimi</code> <a href="#ComplexityMeasures.Shannon"><code>Shannon</code></a> differential entropy estimate is then</p><p class="math-container">\[\hat{H}_{E}(\bar{X}, m) =
\dfrac{1}{n} \sum_{i = 1}^n \log
\left[ \dfrac{n}{c_i m} (\bar{X}_{(i+m)} - \bar{X}_{(i-m)}) \right],\]</p><p>where</p><p class="math-container">\[c_i =
\begin{cases}
    1 + \frac{i - 1}{m}, &amp; 1 \geq i \geq m \\
    2,                    &amp; m + 1 \geq i \geq n - m \\
    1 + \frac{n - i}{m} &amp; n - m + 1 \geq i \geq n
\end{cases}.\]</p><p>See also: <a href="#ComplexityMeasures.information-Tuple{InformationMeasure, OutcomeSpace, Any}"><code>information</code></a>, <a href="#ComplexityMeasures.Correa"><code>Correa</code></a>, <a href="#ComplexityMeasures.AlizadehArghami"><code>AlizadehArghami</code></a>, <a href="#ComplexityMeasures.Vasicek"><code>Vasicek</code></a>, <a href="#ComplexityMeasures.DifferentialInfoEstimator"><code>DifferentialInfoEstimator</code></a>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaDynamics/ComplexityMeasures.jl/blob/a81a14f550f1668986d91498afe4d1642f02e53b/src/differential_info_estimators/order_statistics/Ebrahimi.jl#L3-L57">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="ComplexityMeasures.Correa" href="#ComplexityMeasures.Correa"><code>ComplexityMeasures.Correa</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">Correa &lt;: DifferentialInfoEstimator
Correa(definition = Shannon(); m::Int = 1)</code></pre><p>The <code>Correa</code> estimator computes the <a href="#ComplexityMeasures.Shannon"><code>Shannon</code></a> differential <a href="#ComplexityMeasures.information-Tuple{InformationMeasure, OutcomeSpace, Any}"><code>information</code></a> of a timeseries using the method from <a href="../references/#Correa1995">Correa (1995)</a>, with logarithms to the <code>base</code> specified in <code>definition</code>.</p><p>The <code>Correa</code> estimator belongs to a class of differential entropy estimators based on <a href="https://en.wikipedia.org/wiki/Order_statistic">order statistics</a>. It only works for <em>timeseries</em> input.</p><p><strong>Description</strong></p><p>Assume we have samples <span>$\bar{X} = \{x_1, x_2, \ldots, x_N \}$</span> from a continuous random variable <span>$X \in \mathbb{R}$</span> with support <span>$\mathcal{X}$</span> and density function<span>$f : \mathbb{R} \to \mathbb{R}$</span>. <code>Correa</code> estimates the <a href="@ref">Shannon</a> differential entropy</p><p class="math-container">\[H(X) = \int_{\mathcal{X}} f(x) \log f(x) dx = \mathbb{E}[-\log(f(X))].\]</p><p>However, instead of estimating the above integral directly, <code>Correa</code> makes use of the equivalent integral, where <span>$F$</span> is the distribution function for <span>$X$</span>,</p><p class="math-container">\[H(X) = \int_0^1 \log \left(\dfrac{d}{dp}F^{-1}(p) \right) dp\]</p><p>This integral is approximated by first computing the <a href="https://en.wikipedia.org/wiki/Order_statistic">order statistics</a> of <span>$\bar{X}$</span> (the input timeseries), i.e. <span>$x_{(1)} \leq x_{(2)} \leq \cdots \leq x_{(n)}$</span>, ensuring that end points are included. The <code>Correa</code> estimate of <a href="#ComplexityMeasures.Shannon"><code>Shannon</code></a> differential entropy is then</p><p class="math-container">\[H_C(\bar{X}, m, n) =
\dfrac{1}{n} \sum_{i = 1}^n \log
\left[ \dfrac{ \sum_{j=i-m}^{i+m}(\bar{X}_{(j)} -
\tilde{X}_{(i)})(j - i)}{n \sum_{j=i-m}^{i+m} (\bar{X}_{(j)} - \tilde{X}_{(i)})^2}
\right],\]</p><p>where</p><p class="math-container">\[\tilde{X}_{(i)} = \dfrac{1}{2m + 1} \sum_{j = i - m}^{i + m} X_{(j)}.\]</p><p>See also: <a href="#ComplexityMeasures.information-Tuple{InformationMeasure, OutcomeSpace, Any}"><code>information</code></a>, <a href="#ComplexityMeasures.AlizadehArghami"><code>AlizadehArghami</code></a>, <a href="#ComplexityMeasures.Ebrahimi"><code>Ebrahimi</code></a>, <a href="#ComplexityMeasures.Vasicek"><code>Vasicek</code></a>, <a href="#ComplexityMeasures.DifferentialInfoEstimator"><code>DifferentialInfoEstimator</code></a>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaDynamics/ComplexityMeasures.jl/blob/a81a14f550f1668986d91498afe4d1642f02e53b/src/differential_info_estimators/order_statistics/Correa.jl#L3-L55">source</a></section></article><h3 id="table_diff_ent_est"><a class="docs-heading-anchor" href="#table_diff_ent_est">Table of differential information measure estimators</a><a id="table_diff_ent_est-1"></a><a class="docs-heading-anchor-permalink" href="#table_diff_ent_est" title="Permalink"></a></h3><p>The following estimators are <em>differential</em> information measure estimators, and can also be used with <a href="#ComplexityMeasures.information-Tuple{InformationMeasure, OutcomeSpace, Any}"><code>information</code></a>.</p><p>Each <a href="#ComplexityMeasures.DifferentialInfoEstimator"><code>DifferentialInfoEstimator</code></a>s uses a specialized technique to approximate relevant densities/integrals, and is often tailored to one or a few types of information measures. For example, <a href="#ComplexityMeasures.Kraskov"><code>Kraskov</code></a> estimates the <a href="#ComplexityMeasures.Shannon"><code>Shannon</code></a> entropy.</p><table><tr><th style="text-align: left">Estimator</th><th style="text-align: left">Principle</th><th style="text-align: left">Input data</th><th style="text-align: center"><a href="#ComplexityMeasures.Shannon"><code>Shannon</code></a></th><th style="text-align: center"><a href="#ComplexityMeasures.Renyi"><code>Renyi</code></a></th><th style="text-align: center"><a href="#ComplexityMeasures.Tsallis"><code>Tsallis</code></a></th><th style="text-align: center"><a href="#ComplexityMeasures.Kaniadakis"><code>Kaniadakis</code></a></th><th style="text-align: center"><a href="#ComplexityMeasures.Curado"><code>Curado</code></a></th><th style="text-align: center"><a href="#ComplexityMeasures.StretchedExponential"><code>StretchedExponential</code></a></th></tr><tr><td style="text-align: left"><a href="#ComplexityMeasures.KozachenkoLeonenko"><code>KozachenkoLeonenko</code></a></td><td style="text-align: left">Nearest neighbors</td><td style="text-align: left"><code>StateSpaceSet</code></td><td style="text-align: center">✓</td><td style="text-align: center">x</td><td style="text-align: center">x</td><td style="text-align: center">x</td><td style="text-align: center">x</td><td style="text-align: center">x</td></tr><tr><td style="text-align: left"><a href="#ComplexityMeasures.Kraskov"><code>Kraskov</code></a></td><td style="text-align: left">Nearest neighbors</td><td style="text-align: left"><code>StateSpaceSet</code></td><td style="text-align: center">✓</td><td style="text-align: center">x</td><td style="text-align: center">x</td><td style="text-align: center">x</td><td style="text-align: center">x</td><td style="text-align: center">x</td></tr><tr><td style="text-align: left"><a href="#ComplexityMeasures.Zhu"><code>Zhu</code></a></td><td style="text-align: left">Nearest neighbors</td><td style="text-align: left"><code>StateSpaceSet</code></td><td style="text-align: center">✓</td><td style="text-align: center">x</td><td style="text-align: center">x</td><td style="text-align: center">x</td><td style="text-align: center">x</td><td style="text-align: center">x</td></tr><tr><td style="text-align: left"><a href="#ComplexityMeasures.ZhuSingh"><code>ZhuSingh</code></a></td><td style="text-align: left">Nearest neighbors</td><td style="text-align: left"><code>StateSpaceSet</code></td><td style="text-align: center">✓</td><td style="text-align: center">x</td><td style="text-align: center">x</td><td style="text-align: center">x</td><td style="text-align: center">x</td><td style="text-align: center">x</td></tr><tr><td style="text-align: left"><a href="#ComplexityMeasures.Gao"><code>Gao</code></a></td><td style="text-align: left">Nearest neighbors</td><td style="text-align: left"><code>StateSpaceSet</code></td><td style="text-align: center">✓</td><td style="text-align: center">x</td><td style="text-align: center">x</td><td style="text-align: center">x</td><td style="text-align: center">x</td><td style="text-align: center">x</td></tr><tr><td style="text-align: left"><a href="#ComplexityMeasures.Goria"><code>Goria</code></a></td><td style="text-align: left">Nearest neighbors</td><td style="text-align: left"><code>StateSpaceSet</code></td><td style="text-align: center">✓</td><td style="text-align: center">x</td><td style="text-align: center">x</td><td style="text-align: center">x</td><td style="text-align: center">x</td><td style="text-align: center">x</td></tr><tr><td style="text-align: left"><a href="#ComplexityMeasures.Lord"><code>Lord</code></a></td><td style="text-align: left">Nearest neighbors</td><td style="text-align: left"><code>StateSpaceSet</code></td><td style="text-align: center">✓</td><td style="text-align: center">x</td><td style="text-align: center">x</td><td style="text-align: center">x</td><td style="text-align: center">x</td><td style="text-align: center">x</td></tr><tr><td style="text-align: left"><a href="#ComplexityMeasures.Vasicek"><code>Vasicek</code></a></td><td style="text-align: left">Order statistics</td><td style="text-align: left"><code>Vector</code></td><td style="text-align: center">✓</td><td style="text-align: center">x</td><td style="text-align: center">x</td><td style="text-align: center">x</td><td style="text-align: center">x</td><td style="text-align: center">x</td></tr><tr><td style="text-align: left"><a href="#ComplexityMeasures.Ebrahimi"><code>Ebrahimi</code></a></td><td style="text-align: left">Order statistics</td><td style="text-align: left"><code>Vector</code></td><td style="text-align: center">✓</td><td style="text-align: center">x</td><td style="text-align: center">x</td><td style="text-align: center">x</td><td style="text-align: center">x</td><td style="text-align: center">x</td></tr><tr><td style="text-align: left"><a href="#ComplexityMeasures.Correa"><code>Correa</code></a></td><td style="text-align: left">Order statistics</td><td style="text-align: left"><code>Vector</code></td><td style="text-align: center">✓</td><td style="text-align: center">x</td><td style="text-align: center">x</td><td style="text-align: center">x</td><td style="text-align: center">x</td><td style="text-align: center">x</td></tr><tr><td style="text-align: left"><a href="#ComplexityMeasures.AlizadehArghami"><code>AlizadehArghami</code></a></td><td style="text-align: left">Order statistics</td><td style="text-align: left"><code>Vector</code></td><td style="text-align: center">✓</td><td style="text-align: center">x</td><td style="text-align: center">x</td><td style="text-align: center">x</td><td style="text-align: center">x</td><td style="text-align: center">x</td></tr></table></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../probabilities/">« Probabilities</a><a class="docs-footer-nextpage" href="../complexity/">Complexity measures »</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 0.27.25 on <span class="colophon-date" title="Sunday 29 October 2023 09:27">Sunday 29 October 2023</span>. Using Julia version 1.9.3.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
