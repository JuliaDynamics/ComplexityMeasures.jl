<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Permutation (symbolic) · Entropies.jl</title><link href="https://fonts.googleapis.com/css?family=Lato|Roboto+Mono" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.11.2/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.11.2/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.11.2/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.11.1/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL=".."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../assets/documenter.js"></script><script src="../siteinfo.js"></script><script src="../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-dark.css" data-theme-name="documenter-dark"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../assets/themeswap.js"></script><link href="https://fonts.googleapis.com/css?family=Montserrat|Source+Code+Pro&amp;display=swap" rel="stylesheet" type="text/css"/></head><body><div id="documenter"><nav class="docs-sidebar"><div class="docs-package-name"><span class="docs-autofit">Entropies.jl</span></div><form class="docs-search" action="../search/"><input class="docs-search-query" id="documenter-search-query" name="q" type="text" placeholder="Search docs"/></form><ul class="docs-menu"><li><a class="tocitem" href="../">Documentation</a></li><li><span class="tocitem">Estimation</span><ul><li class="is-active"><a class="tocitem" href>Permutation (symbolic)</a><ul class="internal"><li><a class="tocitem" href="#Example"><span>Example</span></a></li><li><a class="tocitem" href="#Utils"><span>Utils</span></a></li></ul></li><li><a class="tocitem" href="../SymbolicWeightedPermutation/">Weighted permutation (symbolic)</a></li><li><a class="tocitem" href="../VisitationFrequency/">Visitation frequency</a></li></ul></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><nav class="breadcrumb"><ul class="is-hidden-mobile"><li><a class="is-disabled">Estimation</a></li><li class="is-active"><a href>Permutation (symbolic)</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>Permutation (symbolic)</a></li></ul></nav><div class="docs-right"><a class="docs-edit-link" href="https://github.com/kahaaga/Entropies.jl/blob/master/docs/src/SymbolicPermutation.md" title="Edit on GitHub"><span class="docs-icon fab"></span><span class="docs-label is-hidden-touch">Edit on GitHub</span></a><a class="docs-settings-button fas fa-cog" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-sidebar-button fa fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a></div></header><article class="content" id="documenter-page"><h1 id="Permutation-(symbolic)"><a class="docs-heading-anchor" href="#Permutation-(symbolic)">Permutation (symbolic)</a><a id="Permutation-(symbolic)-1"></a><a class="docs-heading-anchor-permalink" href="#Permutation-(symbolic)" title="Permalink"></a></h1><article class="docstring"><header><a class="docstring-binding" id="Entropies.SymbolicPermutation" href="#Entropies.SymbolicPermutation"><code>Entropies.SymbolicPermutation</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia">SymbolicPermutation &lt;: PermutationProbabilityEstimator</code></pre><p>A symbolic, permutation based probabilities/entropy estimator.</p><p><strong>Description</strong></p><p>Permutations of a signal preserve ordinal patterns (sorting information). The implementation  here is based on Bandt &amp; Pompe et al. (2002)<sup class="footnote-reference"><a id="citeref-BandtPompe2002" href="#footnote-BandtPompe2002">[BandtPompe2002]</a></sup>.</p><p><strong>From univariate time series</strong></p><p>Consider the <span>$n$</span>-element univariate time series <span>$\{x(t) = x_1, x_2, \ldots, x_n\}$</span>.  Let <span>$\mathbf{x_i}^{m, \tau} = \{x_j, x_{j+\tau}, \ldots, x_{j+(m-1)\tau}\}$</span>  for <span>$j = 1, 2, \ldots n - (m-1)\tau$</span> be the <span>$i$</span>-th state vector in a delay  reconstruction with embedding dimension <span>$m$</span> and reconstruction lag <span>$\tau$</span>.  There are then <span>$N = n - (m-1)\tau$</span> state vectors. </p><p>For an <span>$m$</span>-dimensional vector, there are <span>$m!$</span> possible ways of sorting it in  ascending order of magnitude. Each such possible sorting ordering is called a  <em>motif</em>. Let <span>$\pi_i^{m, \tau}$</span> denote the motif associated with the  <span>$m$</span>-dimensional state vector <span>$\mathbf{x_i}^{m, \tau}$</span>, and let <span>$R$</span>  be the number of distinct motifs that can be constructed from the <span>$N$</span> state  vectors. Then there are at most <span>$R$</span> motifs; <span>$R = N$</span> precisely when all motifs  are unique, and <span>$R = 1$</span> when all motifs are the same.</p><p>Each unique motif <span>$\pi_i^{m, \tau}$</span> can be mapped to a unique integer  symbol <span>$0 \leq s_i \leq M!-1$</span>. Let <span>$S(\pi) : \mathbb{R}^m \to \mathbb{N}_0$</span> be  the function that maps the motif <span>$\pi$</span> to its symbol <span>$s$</span>, and let <span>$\Pi$</span>  denote the set of symbols <span>$\Pi = \{ s_i \}_{i\in \{ 1, \ldots, R\}}$</span>.</p><p>The probability of a given motif is its frequency of occurrence, normalized by the total  number of motifs,</p><div>\[p(\pi_i^{m, \tau}) = \dfrac{\sum_{k=1}^N \mathbf{1}_{u:S(u) = s_i} \left(\mathbf{x}_k^{m, \tau} \right) }{\sum_{k=1}^N \mathbf{1}_{u:S(u) \in \Pi} \left(\mathbf{x}_k^{m, \tau} \right)} = \dfrac{\sum_{k=1}^N \mathbf{1}_{u:S(u) = s_i} \left(\mathbf{x}_k^{m, \tau} \right) }{N},\]</div><p>where the function <span>$\mathbf{1}_A(u)$</span> is the indicator function of a set <span>$A$</span>. That      is, <span>$\mathbf{1}_A(u) = 1$</span> if <span>$u \in A$</span>, and <span>$\mathbf{1}_A(u) = 0$</span> otherwise.</p><p>Permutation entropy can be computed over the probability distribution of symbols  as <span>$H(m, \tau) = - \sum_j^R p(\pi_j^{m, \tau}) \ln p(\pi_j^{m, \tau})$</span>.</p><p><strong>Estimation</strong></p><ul><li><p>To compute permutation entropy for a univariate signal <code>x</code>, use the signature    <code>entropy(x::AbstractVector, est::SymbolicPermutation; τ::Int = 1, m::Int = 3)</code>.</p></li><li><p>The corresponding (unordered) probability distribution of the permutation symbols for a    univariate signal <code>x</code> can be computed using <code>probabilities(x::AbstractVector, est::SymbolicPermutation; τ::Int = 1, m::Int = 3)</code>. </p></li></ul><p><em>Note: by default, embedding dimension <span>$m = 3$</span> with embedding lag <span>$1$</span> is used. You  should probably make a more informed decision about embedding parameters when computing  the permutation entropy of a real dataset. In all cases, <span>$m$</span> must be at least 2</em> (there  are no permutations of a single-element state vector, so need <span>$m \geq 2$</span>).</p><p><strong>From multivariate time series/datasets</strong></p><p>Permutation entropy can also be computed for multivariate datasets (either embedded or  consisting of multiple time series variables). Then,  just skip the delay reconstruction step, compute symbols directly from the <span>$L$</span> existing  state vectors <span>$\{\mathbf{x}_1, \mathbf{x}_2, \ldots, \mathbf{x_L}\}$</span>, symbolize  each <span>$\mathbf{x_i}$</span> precisely as above, then compute the  quantity </p><div>\[H = - \sum_j p(\pi) \ln p(\pi_j).\]</div><ul><li><p>To compute permutation entropy for a multivariate/embedded dataset <code>x</code>, use the    signature <code>entropy(x::Dataset, est::SymbolicPermutation)</code>.</p></li><li><p>To get the probability distribution for a multivariate/embedded dataset <code>x</code>, use    <code>probabilities(x::Dataset, est::SymbolicPermutation)</code>.</p></li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/kahaaga/Entropies.jl/blob/b8a25deb7b88fad0e51f46e5431bf6a513722441/src/symbolic/SymbolicPermutation.jl#L11-L90">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Entropies.entropy-Tuple{Dataset,SymbolicPermutation}" href="#Entropies.entropy-Tuple{Dataset,SymbolicPermutation}"><code>Entropies.entropy</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia">entropy(x::Dataset, est::SymbolicPermutation, α::Real = 1; m::Int = 3, τ::Int = 1, base = 2) → Real
entropy(x::AbstractVector, est::SymbolicPermutation, α::Real = 1; m::Int = 3, τ::Int = 1, base = 2) → Real

entropy!(s::Vector{Int}, x::Dataset, est::SymbolicPermutation, α::Real = 1; m::Int = 3, τ::Int = 1, base = 2) → Real
entropy!(s::Vector{Int}, x::AbstractVector, est::SymbolicPermutation, α::Real = 1; m::Int = 3, τ::Int = 1, base = 2) → Real</code></pre><p>Compute the generalized order <code>α</code> entropy over a permutation symbolization of <code>x</code>, using  symbol size/order <code>m</code>.</p><p>A pre-allocated symbol array <code>s</code> can be provided to save some memory allocations if   probabilities are to be computed for multiple data sets. If so, it is required that  <code>length(x) == length(s)</code> if <code>x</code> is a <code>Dataset</code>, or  <code>length(s) == length(x) - (m-1)τ</code>  if <code>x</code> is a univariate signal.</p><p><strong>Probability estimation</strong></p><p>An unordered symbol frequency histogram is obtained by symbolizing the points in <code>x</code>, using <a href="#Entropies.probabilities-Tuple{Dataset,SymbolicPermutation}"><code>probabilities(::Dataset, ::SymbolicPermutation)</code></a>. Sum-normalizing this histogram yields a probability distribution over the symbols.</p><p><strong>Entropy estimation</strong></p><p>After the symbolization histogram/distribution has been obtained, the order <code>α</code> generalized  entropy<sup class="footnote-reference"><a id="citeref-Rényi1960" href="#footnote-Rényi1960">[Rényi1960]</a></sup> is computed from that sum-normalized symbol distribution, using  <a href="@ref"><code>genentropy</code></a>.</p><p><strong>Notes</strong></p><p><em>Do not confuse the order of the generalized entropy (<code>α</code>) with the order <code>m</code> of the  permutation entropy (<code>m</code>, which controls the symbol size). Permutation entropy is usually  estimated with <code>α = 1</code>, but the implementation here allows the generalized entropy of any  dimension to be computed from the symbol frequency distribution.</em></p><p>See also: <a href="#Entropies.SymbolicPermutation"><code>SymbolicPermutation</code></a>, <a href="@ref"><code>genentropy</code></a>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/kahaaga/Entropies.jl/blob/b8a25deb7b88fad0e51f46e5431bf6a513722441/src/symbolic/SymbolicPermutation.jl#L203-L241">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Entropies.probabilities-Tuple{Dataset,SymbolicPermutation}" href="#Entropies.probabilities-Tuple{Dataset,SymbolicPermutation}"><code>Entropies.probabilities</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia">probabilities(x::Dataset, est::SymbolicPermutation) → Vector{&lt;:Real} 
probabilities(x::AbstractVector, est::SymbolicPermutation;  m::Int = 2, τ::Int = 1) → Vector{&lt;:Real} 

probabilities!(s::Vector{Int}, x::Dataset, est::SymbolicPermutation) → Vector{&lt;:Real} 
probabilities!(s::Vector{Int}, x::AbstractVector, est::SymbolicPermutation;  m::Int = 2, τ::Int = 1) → Vector{&lt;:Real}</code></pre><p>Compute the unordered probabilities of the occurrence of symbol sequences constructed from  the data <code>x</code>. </p><p>A pre-allocated symbol array <code>s</code> can be provided to save some memory allocations if the  probabilities are to be computed for multiple data sets. If so, it is required that  <code>length(x) == length(s)</code> if <code>x</code> is a <code>Dataset</code>, or  <code>length(s) == length(x) - (m-1)τ</code>  if <code>x</code> is a univariate signal.</p><p>See also: <a href="#Entropies.SymbolicPermutation"><code>SymbolicPermutation</code></a>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/kahaaga/Entropies.jl/blob/b8a25deb7b88fad0e51f46e5431bf6a513722441/src/symbolic/SymbolicPermutation.jl#L172-L188">source</a></section></article><h2 id="Example"><a class="docs-heading-anchor" href="#Example">Example</a><a id="Example-1"></a><a class="docs-heading-anchor-permalink" href="#Example" title="Permalink"></a></h2><p>This example reproduces the permutation entropy example on the logistic map from Bandt and Pompe (2002).</p><pre><code class="language-julia">using DynamicalSystems, PyPlot, Entropies

ds = Systems.logistic()
rs = 3.5:0.001:4
N_lyap, N_ent = 100000, 10000
m = 6 # Symbol size/dimension

# Generate one time series for each value of the logistic parameter r
lyaps, hs_entropies, hs_chaostools = Float64[], Float64[], Float64[]
hs_wtperm = Float64[]
for r in rs
    ds.p[1] = r
    push!(lyaps, lyapunov(ds, N_lyap))

    # For 1D systems `trajectory` returns a vector, so embed it using τs
    # to get the correct 6d dimension on the embedding
    x = trajectory(ds, N_ent)
    τs = ([-i for i in 0:m-1]...,) # embedding lags
    emb = genembed(x, τs)

    push!(hs_entropies, entropy(emb, SymbolicPermutation(), base = Base.MathConstants.e))
    push!(hs_wtperm, entropy(emb, SymbolicWeightedPermutation(), base = Base.MathConstants.e))

    # Old ChaosTools.jl style estimation
    push!(hs_chaostools, permentropy(x, 6))
end

f = figure(figsize = (10,6))
a1 = subplot(411)
plot(rs, lyaps); ylim(-2, log(2)); ylabel(&quot;\$\\lambda\$&quot;)
a1.axes.get_xaxis().set_ticklabels([])
xlim(rs[1], rs[end]);

a2 = subplot(412)
plot(rs, hs_chaostools; color = &quot;C1&quot;); xlim(rs[1], rs[end]);
xlabel(&quot;\$r\$&quot;); ylabel(&quot;\$h_6 (ChaosTools.jl)\$&quot;)

a3 = subplot(413)
plot(rs, hs_entropies; color = &quot;C2&quot;); xlim(rs[1], rs[end]);
xlabel(&quot;\$r\$&quot;); ylabel(&quot;\$h_6 (Entropies.jl)\$&quot;)

a4 = subplot(414)
plot(rs, hs_wtperm; color = &quot;C3&quot;); xlim(rs[1], rs[end]);
xlabel(&quot;\$r\$&quot;); ylabel(&quot;\$h_6 (Entropies.jl, wtperm)\$&quot;)
tight_layout()
savefig(&quot;permentropy.png&quot;)</code></pre><p><img src="../permentropy.png" alt/></p><h2 id="Utils"><a class="docs-heading-anchor" href="#Utils">Utils</a><a id="Utils-1"></a><a class="docs-heading-anchor-permalink" href="#Utils" title="Permalink"></a></h2><p>Some convenience functions for symbolization are provided.</p><article class="docstring"><header><a class="docstring-binding" id="Entropies.symbolize" href="#Entropies.symbolize"><code>Entropies.symbolize</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia">symbolize(x::Dataset, est::SymbolicPermutation) → Vector{Int}</code></pre><p>Symbolize the vectors in <code>x</code> using Algorithm 1 from Berger et al. (2019)<sup class="footnote-reference"><a id="citeref-Berger2019" href="#footnote-Berger2019">[Berger2019]</a></sup>.</p><p>The symbol length is automatically determined from the dimension of the input data vectors.</p><p><strong>Example</strong></p><p>Computing the order 5 permutation entropy for a 7-dimensional dataset.</p><pre><code class="language-julia">using DelayEmbeddings, Entropies
D = Dataset([rand(7) for i = 1:1000])
symbolize(D, SymbolicPermutation(5))</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/kahaaga/Entropies.jl/blob/b8a25deb7b88fad0e51f46e5431bf6a513722441/src/symbolic/SymbolicPermutation.jl#L98-L117">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Entropies.encode_motif" href="#Entropies.encode_motif"><code>Entropies.encode_motif</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia">encode_motif(x, m::Int = length(x)) → Int</code></pre><p>Encode the length-<code>m</code> motif <code>x</code> (a vector of indices that would sort some vector <code>v</code> in ascending order)  into its unique integer symbol, using Algorithm 1 in Berger et al. (2019)<sup class="footnote-reference"><a id="citeref-Berger2019" href="#footnote-Berger2019">[Berger2019]</a></sup>.</p><p>Note: no error checking is done to see if <code>length(x) == m</code>, so be sure to provide the correct motif length!</p><p><strong>Example</strong></p><pre><code class="language-julia"># Some random vector
v = rand(5)

# The indices that would sort `v` in ascending order. This is now a permutation 
# of the index permutation (1, 2, ..., 5)
x = sortperm(v)

# Encode this permutation as an integer.
encode_motif(x)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/kahaaga/Entropies.jl/blob/b8a25deb7b88fad0e51f46e5431bf6a513722441/src/symbolic/utils.jl#L3-L26">source</a></section></article><section class="footnotes is-size-7"><ul><li class="footnote" id="footnote-BandtPompe2002"><a class="tag is-link" href="#citeref-BandtPompe2002">BandtPompe2002</a>Bandt, Christoph, and Bernd Pompe. &quot;Permutation entropy: a natural  complexity measure for time series.&quot; Physical review letters 88.17 (2002): 174102.</li><li class="footnote" id="footnote-Rényi1960"><a class="tag is-link" href="#citeref-Rényi1960">Rényi1960</a>A. Rényi, <em>Proceedings of the fourth Berkeley Symposium on Mathematics, Statistics and Probability</em>, pp 547 (1960)</li><li class="footnote" id="footnote-Berger2019"><a class="tag is-link" href="#citeref-Berger2019">Berger2019</a>Berger, Sebastian, et al. &quot;Teaching Ordinal Patterns to a Computer: Efficient Encoding Algorithms Based on the Lehmer Code.&quot; Entropy 21.10 (2019): 1023.</li><li class="footnote" id="footnote-Berger2019"><a class="tag is-link" href="#citeref-Berger2019">Berger2019</a>Berger, Sebastian, et al. &quot;Teaching Ordinal Patterns to a Computer: Efficient Encoding Algorithms Based on the Lehmer Code.&quot; Entropy 21.10 (2019): 1023.</li></ul></section></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../">« Documentation</a><a class="docs-footer-nextpage" href="../SymbolicWeightedPermutation/">Weighted permutation (symbolic) »</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> on <span class="colophon-date" title="Saturday 31 October 2020 02:00">Saturday 31 October 2020</span>. Using Julia version 1.5.2.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
