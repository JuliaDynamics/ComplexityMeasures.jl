<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Entropies · Entropies.jl</title><script data-outdated-warner src="../assets/warner.js"></script><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.045/juliamono.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.24/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL=".."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../assets/documenter.js"></script><script src="../siteinfo.js"></script><script src="../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../assets/themeswap.js"></script><link href="https://fonts.googleapis.com/css?family=Montserrat|Source+Code+Pro&amp;display=swap" rel="stylesheet" type="text/css"/></head><body><div id="documenter"><nav class="docs-sidebar"><div class="docs-package-name"><span class="docs-autofit"><a href="../">Entropies.jl</a></span></div><form class="docs-search" action="../search/"><input class="docs-search-query" id="documenter-search-query" name="q" type="text" placeholder="Search docs"/></form><ul class="docs-menu"><li><a class="tocitem" href="../">Entropies.jl</a></li><li><a class="tocitem" href="../probabilities/">Probabilities</a></li><li class="is-active"><a class="tocitem" href>Entropies</a><ul class="internal"><li><a class="tocitem" href="#Generalized-entropies"><span>Generalized entropies</span></a></li><li><a class="tocitem" href="#Estimation"><span>Estimation</span></a></li><li><a class="tocitem" href="#Convenience-functions"><span>Convenience functions</span></a></li></ul></li><li><a class="tocitem" href="../complexity/">Complexity measures</a></li><li><a class="tocitem" href="../multiscale/">Multiscale</a></li><li><a class="tocitem" href="../examples/">Entropies.jl Examples</a></li><li><a class="tocitem" href="../devdocs/">Entropies.jl Dev Docs</a></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><nav class="breadcrumb"><ul class="is-hidden-mobile"><li class="is-active"><a href>Entropies</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>Entropies</a></li></ul></nav><div class="docs-right"><a class="docs-edit-link" href="https://github.com/JuliaDynamics/Entropies.jl/blob/main/docs/src/entropies.md" title="Edit on GitHub"><span class="docs-icon fab"></span><span class="docs-label is-hidden-touch">Edit on GitHub</span></a><a class="docs-settings-button fas fa-cog" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-sidebar-button fa fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a></div></header><article class="content" id="documenter-page"><h1 id="entropies"><a class="docs-heading-anchor" href="#entropies">Entropies</a><a id="entropies-1"></a><a class="docs-heading-anchor-permalink" href="#entropies" title="Permalink"></a></h1><article class="docstring"><header><a class="docstring-binding" id="Entropies.entropy" href="#Entropies.entropy"><code>Entropies.entropy</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">entropy([e::Entropy,] probs::Probabilities)
entropy([e::Entropy,] est::ProbabilitiesEstimator, x)
entropy([e::Entropy,] est::EntropyEstimator, x)</code></pre><p>Compute <code>h::Real</code>, which is a (generalized) entropy defined by <code>e</code>, in one of three ways:</p><ol><li>Directly from existing <a href="../probabilities/#Entropies.Probabilities"><code>Probabilities</code></a> <code>probs</code>.</li><li>From input data <code>x</code>, by first estimating a probability distribution using the provided <a href="../probabilities/#Entropies.ProbabilitiesEstimator"><code>ProbabilitiesEstimator</code></a>, then computing entropy from that distribution. In fact, the second method is just a 2-lines-of-code wrapper that calls <a href="../probabilities/#Entropies.probabilities"><code>probabilities</code></a> and gives the result to the first method.</li><li>From input data <code>x</code>, by using a dedicated <a href="#Entropies.EntropyEstimator"><code>EntropyEstimator</code></a> that computes entropy in a way that doesn&#39;t involve explicitly computing probabilities first.</li></ol><p>The entropy definition (first argument) is optional. When <code>est</code> is a probability estimator, <code>Shannon()</code> is used by default. When <code>est</code> is a dedicated entropy estimator, the default entropy type is inferred from the estimator (e.g. <a href="#Entropies.Kraskov"><code>Kraskov</code></a> estimates the <a href="#Entropies.Shannon"><code>Shannon</code></a> entropy).</p><p><strong>Input data</strong></p><p><code>x</code> is typically an <code>Array</code> or a <code>Dataset</code>, see <a href="@ref">Input data for Entropies.jl</a>.</p><p><strong>Maximum entropy and normalized entropy</strong></p><p>All entropies <code>e</code> have a well defined maximum value for a given probability estimator. To obtain this value one only needs to call the <a href="@ref"><code>entropy_maximum</code></a> function with the chosen entropy type and probability estimator. Or, one can use <a href="@ref"><code>entropy_normalized</code></a> to obtain the normalized form of the entropy (divided by the maximum).</p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">x = [rand(Bool) for _ in 1:10000] # coin toss
ps = probabilities(x) # gives about [0.5, 0.5] by definition
h = entropy(ps) # gives 1, about 1 bit by definition
h = entropy(Shannon(), ps) # syntactically equivalent to above
h = entropy(Shannon(), CountOccurrences(), x) # syntactically equivalent to above
h = entropy(SymbolicPermutation(;m=3), x) # gives about 2, again by definition
h = entropy(Renyi(2.0), ps) # also gives 1, order `q` doesn&#39;t matter for coin toss</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaDynamics/Entropies.jl/blob/0d5fd0da077bedddc20c2369b3b57ecbbc485675/src/entropy.jl#L59-L103">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Entropies.Entropy" href="#Entropies.Entropy"><code>Entropies.Entropy</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">Entropy</code></pre><p><code>Entropy</code> is the supertype of all (generalized) entropies, and currently implemented entropy types are:</p><ul><li><a href="#Entropies.Renyi"><code>Renyi</code></a>.</li><li><a href="#Entropies.Tsallis"><code>Tsallis</code></a>.</li><li><a href="#Entropies.Shannon"><code>Shannon</code></a>, which is a subcase of the above two in the limit <code>q → 1</code>.</li><li><a href="#Entropies.Curado"><code>Curado</code></a>.</li><li><a href="#Entropies.StretchedExponential"><code>StretchedExponential</code></a>.</li></ul><p>These entropy types are given as inputs to <a href="#Entropies.entropy"><code>entropy</code></a> and [<code>entropy_normalized</code>].</p><p><strong>Description</strong></p><p>Mathematically speaking, generalized entropies are just nonnegative functions of probability distributions that verify certain (entropy-type-dependent) axioms. Amigó et al.<sup class="footnote-reference"><a id="citeref-Amigó2018" href="#footnote-Amigó2018">[Amigó2018]</a></sup> summary paper gives a nice overview.</p><p>[Amigó2018]:     Amigó, J. M., Balogh, S. G., &amp; Hernández, S. (2018). A brief review of     generalized entropies. <a href="https://www.mdpi.com/1099-4300/20/11/813">Entropy, 20(11), 813.</a></p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaDynamics/Entropies.jl/blob/0d5fd0da077bedddc20c2369b3b57ecbbc485675/src/entropy.jl#L6-L29">source</a></section></article><h2 id="Generalized-entropies"><a class="docs-heading-anchor" href="#Generalized-entropies">Generalized entropies</a><a id="Generalized-entropies-1"></a><a class="docs-heading-anchor-permalink" href="#Generalized-entropies" title="Permalink"></a></h2><article class="docstring"><header><a class="docstring-binding" id="Entropies.Shannon" href="#Entropies.Shannon"><code>Entropies.Shannon</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">Shannon(; base = 2)</code></pre><p>The Shannon<sup class="footnote-reference"><a id="citeref-Shannon1948" href="#footnote-Shannon1948">[Shannon1948]</a></sup> entropy, used with <a href="#Entropies.entropy"><code>entropy</code></a> to compute:</p><p class="math-container">\[H(p) = - \sum_i p[i] \log(p[i])\]</p><p>with the <span>$log$</span> at the given <code>base</code>.</p><p><code>Shannon(base)</code> is syntactically equivalent to <code>Renyi(; base)</code>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaDynamics/Entropies.jl/blob/0d5fd0da077bedddc20c2369b3b57ecbbc485675/src/entropies/renyi.jl#L60-L73">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Entropies.Renyi" href="#Entropies.Renyi"><code>Entropies.Renyi</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">Renyi &lt;: Entropy
Renyi(q, base = 2)
Renyi(; q = 1.0, base = 2)</code></pre><p>The Rényi<sup class="footnote-reference"><a id="citeref-Rényi1960" href="#footnote-Rényi1960">[Rényi1960]</a></sup> generalized order-<code>q</code> entropy, used with <a href="#Entropies.entropy"><code>entropy</code></a> to compute an entropy with units given by <code>base</code> (typically <code>2</code> or <code>MathConstants.e</code>).</p><p><strong>Description</strong></p><p>Let <span>$p$</span> be an array of probabilities (summing to 1). Then the Rényi generalized entropy is</p><p class="math-container">\[H_q(p) = \frac{1}{1-q} \log \left(\sum_i p[i]^q\right)\]</p><p>and generalizes other known entropies, like e.g. the information entropy (<span>$q = 1$</span>, see <sup class="footnote-reference"><a id="citeref-Shannon1948" href="#footnote-Shannon1948">[Shannon1948]</a></sup>), the maximum entropy (<span>$q=0$</span>, also known as Hartley entropy), or the correlation entropy (<span>$q = 2$</span>, also known as collision entropy).</p><p>If the probability estimator has known alphabet length <span>$L$</span>, then the maximum value of the Rényi entropy is <span>$\log_{base}(L)$</span>, which is the entropy of the uniform distribution with given alphabet length.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaDynamics/Entropies.jl/blob/0d5fd0da077bedddc20c2369b3b57ecbbc485675/src/entropies/renyi.jl#L3-L34">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Entropies.Tsallis" href="#Entropies.Tsallis"><code>Entropies.Tsallis</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">Tsallis &lt;: Entropy
Tsallis(q; k = 1.0, base = 2)
Tsallis(; q = 1.0, k = 1.0, base = 2)</code></pre><p>The Tsallis<sup class="footnote-reference"><a id="citeref-Tsallis1988" href="#footnote-Tsallis1988">[Tsallis1988]</a></sup> generalized order-<code>q</code> entropy, used with <a href="#Entropies.entropy"><code>entropy</code></a> to compute an entropy.</p><p><code>base</code> only applies in the limiting case <code>q == 1</code>, in which the Tsallis entropy reduces to Shannon entropy.</p><p><strong>Description</strong></p><p>The Tsallis entropy is a generalization of the Boltzmann-Gibbs entropy, with <code>k</code> standing for the Boltzmann constant. It is defined as</p><p class="math-container">\[S_q(p) = \frac{k}{q - 1}\left(1 - \sum_{i} p[i]^q\right)\]</p><p>If the probability estimator has known alphabet length <span>$L$</span>, then the maximum value of the Tsallis entropy is <span>$k(L^{1 - q} - 1)/(1 - q)$</span>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaDynamics/Entropies.jl/blob/0d5fd0da077bedddc20c2369b3b57ecbbc485675/src/entropies/tsallis.jl#L3-L29">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Entropies.Curado" href="#Entropies.Curado"><code>Entropies.Curado</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">Curado &lt;: Entropy
Curado(; b = 1.0)</code></pre><p>The Curado entropy (Curado &amp; Nobre, 2004)<sup class="footnote-reference"><a id="citeref-Curado2004" href="#footnote-Curado2004">[Curado2004]</a></sup>, used with <a href="#Entropies.entropy"><code>entropy</code></a> to compute</p><p class="math-container">\[H_C(p) = \left( \sum_{i=1}^N e^{-b p_i} \right) + e^{-b} - 1,\]</p><p>with <code>b ∈ ℛ, b &gt; 0</code>, where the terms outside the sum ensures that <span>$H_C(0) = H_C(1) = 0$</span>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaDynamics/Entropies.jl/blob/0d5fd0da077bedddc20c2369b3b57ecbbc485675/src/entropies/curado.jl#L3-L18">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Entropies.StretchedExponential" href="#Entropies.StretchedExponential"><code>Entropies.StretchedExponential</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">StretchedExponential &lt;: Entropy
StretchedExponential(; η = 2.0, base = 2)</code></pre><p>The stretched exponential, or Anteneodo-Plastino, entropy (Anteneodo &amp; Plastino, 1999<sup class="footnote-reference"><a id="citeref-Anteneodo1999" href="#footnote-Anteneodo1999">[Anteneodo1999]</a></sup>), used with <a href="#Entropies.entropy"><code>entropy</code></a> to compute</p><p class="math-container">\[S_{\eta}(p) = \sum_{i = 1}^N
\Gamma \left( \dfrac{\eta + 1}{\eta}, - \log_{base}(p_i) \right) -
p_i \Gamma \left( \dfrac{\eta + 1}{\eta} \right),\]</p><p>where <span>$\eta \geq 0$</span>, <span>$\Gamma(\cdot, \cdot)$</span> is the upper incomplete Gamma function, and <span>$\Gamma(\cdot) = \Gamma(\cdot, 0)$</span> is the Gamma function. Reduces to <a href="@ref">Shannon</a> entropy for <code>η = 1.0</code>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaDynamics/Entropies.jl/blob/0d5fd0da077bedddc20c2369b3b57ecbbc485675/src/entropies/streched_exponential.jl#L5-L25">source</a></section></article><h2 id="Estimation"><a class="docs-heading-anchor" href="#Estimation">Estimation</a><a id="Estimation-1"></a><a class="docs-heading-anchor-permalink" href="#Estimation" title="Permalink"></a></h2><h3 id="Discrete-entropies"><a class="docs-heading-anchor" href="#Discrete-entropies">Discrete entropies</a><a id="Discrete-entropies-1"></a><a class="docs-heading-anchor-permalink" href="#Discrete-entropies" title="Permalink"></a></h3><p>Discrete entropies are just simple functions (sums, actually) of probability mass functions <a href="https://en.wikipedia.org/wiki/Probability_mass_function">(pmf)</a>, which you can estimate using <a href="../probabilities/#Entropies.ProbabilitiesEstimator"><code>ProbabilitiesEstimator</code></a>s.</p><p>Any <a href="../probabilities/#Entropies.ProbabilitiesEstimator"><code>ProbabilitiesEstimator</code></a> may therefore be used as a naive plug-in estimator for discrete <a href="#Entropies.entropy"><code>entropy</code></a>. No bias correction is currently applied to any of the discrete estimators.</p><p>Tables scroll sideways, so are best viewed on a large screen.</p><table><tr><th style="text-align: right">Estimator</th><th style="text-align: right">Principle</th><th style="text-align: right">Input data</th><th style="text-align: center"><a href="#Entropies.Shannon"><code>Shannon</code></a></th><th style="text-align: center"><a href="#Entropies.Renyi"><code>Renyi</code></a></th><th style="text-align: center"><a href="#Entropies.Tsallis"><code>Tsallis</code></a></th><th style="text-align: center"><a href="#Entropies.Curado"><code>Curado</code></a></th><th style="text-align: center"><a href="#Entropies.StretchedExponential"><code>StretchedExponential</code></a></th></tr><tr><td style="text-align: right"><a href="../probabilities/#Entropies.CountOccurrences"><code>CountOccurrences</code></a></td><td style="text-align: right">Frequencies</td><td style="text-align: right"><code>Vector</code>, <code>Dataset</code></td><td style="text-align: center">✅</td><td style="text-align: center">✅</td><td style="text-align: center">✅</td><td style="text-align: center">✅</td><td style="text-align: center">✅</td></tr><tr><td style="text-align: right"><a href="../probabilities/#Entropies.ValueHistogram"><code>ValueHistogram</code></a></td><td style="text-align: right">Binning (histogram)</td><td style="text-align: right"><code>Vector</code>, <code>Dataset</code></td><td style="text-align: center">✅</td><td style="text-align: center">✅</td><td style="text-align: center">✅</td><td style="text-align: center">✅</td><td style="text-align: center">✅</td></tr><tr><td style="text-align: right"><a href="../probabilities/#Entropies.TransferOperator"><code>TransferOperator</code></a></td><td style="text-align: right">Binning (transfer operator)</td><td style="text-align: right"><code>Vector</code>, <code>Dataset</code></td><td style="text-align: center">✅</td><td style="text-align: center">✅</td><td style="text-align: center">✅</td><td style="text-align: center">✅</td><td style="text-align: center">✅</td></tr><tr><td style="text-align: right"><a href="../probabilities/#Entropies.NaiveKernel"><code>NaiveKernel</code></a></td><td style="text-align: right">Kernel density estimation</td><td style="text-align: right"><code>Dataset</code></td><td style="text-align: center">✅</td><td style="text-align: center">✅</td><td style="text-align: center">✅</td><td style="text-align: center">✅</td><td style="text-align: center">✅</td></tr><tr><td style="text-align: right"><a href="@ref"><code>LocalLikelihood</code></a></td><td style="text-align: right">Local likelihood Estimation</td><td style="text-align: right"><code>Dataset</code></td><td style="text-align: center">✅</td><td style="text-align: center">✅</td><td style="text-align: center">✅</td><td style="text-align: center">✅</td><td style="text-align: center">✅</td></tr><tr><td style="text-align: right"><a href="../probabilities/#Entropies.SymbolicPermutation"><code>SymbolicPermutation</code></a></td><td style="text-align: right">Ordinal patterns</td><td style="text-align: right"><code>Vector</code></td><td style="text-align: center">✅</td><td style="text-align: center">✅</td><td style="text-align: center">✅</td><td style="text-align: center">✅</td><td style="text-align: center">✅</td></tr><tr><td style="text-align: right"><a href="../probabilities/#Entropies.SymbolicWeightedPermutation"><code>SymbolicWeightedPermutation</code></a></td><td style="text-align: right">Ordinal patterns</td><td style="text-align: right"><code>Vector</code></td><td style="text-align: center">✅</td><td style="text-align: center">✅</td><td style="text-align: center">✅</td><td style="text-align: center">✅</td><td style="text-align: center">✅</td></tr><tr><td style="text-align: right"><a href="../probabilities/#Entropies.SymbolicAmplitudeAwarePermutation"><code>SymbolicAmplitudeAwarePermutation</code></a></td><td style="text-align: right">Ordinal patterns</td><td style="text-align: right"><code>Vector</code></td><td style="text-align: center">✅</td><td style="text-align: center">✅</td><td style="text-align: center">✅</td><td style="text-align: center">✅</td><td style="text-align: center">✅</td></tr><tr><td style="text-align: right"><a href="../probabilities/#Entropies.Dispersion"><code>Dispersion</code></a></td><td style="text-align: right">Dispersion patterns</td><td style="text-align: right"><code>Vector</code></td><td style="text-align: center">✅</td><td style="text-align: center">✅</td><td style="text-align: center">✅</td><td style="text-align: center">✅</td><td style="text-align: center">✅</td></tr><tr><td style="text-align: right"><a href="../probabilities/#Entropies.Diversity"><code>Diversity</code></a></td><td style="text-align: right">Cosine similarity</td><td style="text-align: right"><code>Vector</code></td><td style="text-align: center">✅</td><td style="text-align: center">✅</td><td style="text-align: center">✅</td><td style="text-align: center">✅</td><td style="text-align: center">✅</td></tr><tr><td style="text-align: right"><a href="../probabilities/#Entropies.WaveletOverlap"><code>WaveletOverlap</code></a></td><td style="text-align: right">Wavelet transform</td><td style="text-align: right"><code>Vector</code></td><td style="text-align: center">✅</td><td style="text-align: center">✅</td><td style="text-align: center">✅</td><td style="text-align: center">✅</td><td style="text-align: center">✅</td></tr><tr><td style="text-align: right"><a href="../probabilities/#Entropies.PowerSpectrum"><code>PowerSpectrum</code></a></td><td style="text-align: right">Fourier spectra</td><td style="text-align: right"><code>Vector</code>, <code>Dataset</code></td><td style="text-align: center">✅</td><td style="text-align: center">✅</td><td style="text-align: center">✅</td><td style="text-align: center">✅</td><td style="text-align: center">✅</td></tr></table><h3 id="Continuous/differential-entropies"><a class="docs-heading-anchor" href="#Continuous/differential-entropies">Continuous/differential entropies</a><a id="Continuous/differential-entropies-1"></a><a class="docs-heading-anchor-permalink" href="#Continuous/differential-entropies" title="Permalink"></a></h3><p>The following estimators are <em>differential</em> entropy estimators, and can also be used with <a href="#Entropies.entropy"><code>entropy</code></a>. Differential) entropies are functions of <em>integrals</em>, and usually rely on estimating some density functional.</p><p>Each <a href="#Entropies.EntropyEstimator"><code>EntropyEstimator</code></a>s uses a specialized technique to approximating relevant densities/integrals, and is often tailored to one or a few types of generalized entropy. For example, <a href="#Entropies.Kraskov"><code>Kraskov</code></a> estimates the <a href="#Entropies.Shannon"><code>Shannon</code></a> entropy, while <a href="@ref"><code>LeonenkoProzantoSavani</code></a> estimates <a href="#Entropies.Shannon"><code>Shannon</code></a>, <a href="#Entropies.Renyi"><code>Renyi</code></a>, and <a href="#Entropies.Tsallis"><code>Tsallis</code></a> entropies.</p><table><tr><th style="text-align: right">Estimator</th><th style="text-align: right">Principle</th><th style="text-align: right">Input data</th><th style="text-align: center"><a href="#Entropies.Shannon"><code>Shannon</code></a></th><th style="text-align: center"><a href="#Entropies.Renyi"><code>Renyi</code></a></th><th style="text-align: center"><a href="#Entropies.Tsallis"><code>Tsallis</code></a></th><th style="text-align: center"><a href="#Entropies.Curado"><code>Curado</code></a></th><th style="text-align: center"><a href="#Entropies.StretchedExponential"><code>StretchedExponential</code></a></th></tr><tr><td style="text-align: right"><a href="#Entropies.KozachenkoLeonenko"><code>KozachenkoLeonenko</code></a></td><td style="text-align: right">Nearest neighbors</td><td style="text-align: right"><code>Dataset</code></td><td style="text-align: center">✅</td><td style="text-align: center">x</td><td style="text-align: center">x</td><td style="text-align: center">x</td><td style="text-align: center">x</td></tr><tr><td style="text-align: right"><a href="#Entropies.Kraskov"><code>Kraskov</code></a></td><td style="text-align: right">Nearest neighbors</td><td style="text-align: right"><code>Dataset</code></td><td style="text-align: center">✅</td><td style="text-align: center">x</td><td style="text-align: center">x</td><td style="text-align: center">x</td><td style="text-align: center">x</td></tr><tr><td style="text-align: right"><a href="#Entropies.Zhu"><code>Zhu</code></a></td><td style="text-align: right">Nearest neighbors</td><td style="text-align: right"><code>Dataset</code></td><td style="text-align: center">✅</td><td style="text-align: center">x</td><td style="text-align: center">x</td><td style="text-align: center">x</td><td style="text-align: center">x</td></tr><tr><td style="text-align: right"><a href="#Entropies.ZhuSingh"><code>ZhuSingh</code></a></td><td style="text-align: right">Nearest neighbors</td><td style="text-align: right"><code>Dataset</code></td><td style="text-align: center">✅</td><td style="text-align: center">x</td><td style="text-align: center">x</td><td style="text-align: center">x</td><td style="text-align: center">x</td></tr><tr><td style="text-align: right"><a href="#Entropies.Vasicek"><code>Vasicek</code></a></td><td style="text-align: right">Order statistics</td><td style="text-align: right"><code>Vector</code></td><td style="text-align: center">✅</td><td style="text-align: center">x</td><td style="text-align: center">x</td><td style="text-align: center">x</td><td style="text-align: center">x</td></tr><tr><td style="text-align: right"><a href="#Entropies.Ebrahimi"><code>Ebrahimi</code></a></td><td style="text-align: right">Order statistics</td><td style="text-align: right"><code>Vector</code></td><td style="text-align: center">✅</td><td style="text-align: center">x</td><td style="text-align: center">x</td><td style="text-align: center">x</td><td style="text-align: center">x</td></tr><tr><td style="text-align: right"><a href="#Entropies.Correa"><code>Correa</code></a></td><td style="text-align: right">Order statistics</td><td style="text-align: right"><code>Vector</code></td><td style="text-align: center">✅</td><td style="text-align: center">x</td><td style="text-align: center">x</td><td style="text-align: center">x</td><td style="text-align: center">x</td></tr><tr><td style="text-align: right"><a href="#Entropies.AlizadehArghami"><code>AlizadehArghami</code></a></td><td style="text-align: right">Order statistics</td><td style="text-align: right"><code>Vector</code></td><td style="text-align: center">✅</td><td style="text-align: center">x</td><td style="text-align: center">x</td><td style="text-align: center">x</td><td style="text-align: center">x</td></tr></table><article class="docstring"><header><a class="docstring-binding" id="Entropies.EntropyEstimator" href="#Entropies.EntropyEstimator"><code>Entropies.EntropyEstimator</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">EntropyEstimator</code></pre><p>The supertype of all entropy estimators.</p><p>These estimators compute some <a href="#Entropies.Entropy"><code>Entropy</code></a> in various ways that doesn&#39;t involve explicitly estimating a probability distribution. Currently implemented estimators are:</p><ul><li><a href="#Entropies.KozachenkoLeonenko"><code>KozachenkoLeonenko</code></a></li><li><a href="#Entropies.Kraskov"><code>Kraskov</code></a></li><li><a href="#Entropies.Zhu"><code>Zhu</code></a></li><li><a href="#Entropies.ZhuSingh"><code>ZhuSingh</code></a></li><li><a href="#Entropies.Vasicek"><code>Vasicek</code></a></li><li><a href="#Entropies.Ebrahimi"><code>Ebrahimi</code></a></li><li><a href="#Entropies.Correa"><code>Correa</code></a></li><li><a href="#Entropies.AlizadehArghami"><code>AlizadehArghami</code></a></li></ul><p>For example, <a href="#Entropies.entropy"><code>entropy</code></a><code>(Shannon(), Kraskov(), x)</code> computes the Shannon differential entropy of the input data <code>x</code> using the <a href="#Entropies.Kraskov"><code>Kraskov</code></a> <code>k</code>-th nearest neighbor estimator.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaDynamics/Entropies.jl/blob/0d5fd0da077bedddc20c2369b3b57ecbbc485675/src/entropy.jl#L32-L52">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Entropies.Kraskov" href="#Entropies.Kraskov"><code>Entropies.Kraskov</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">Kraskov &lt;: EntropyEstimator
Kraskov(; k::Int = 1, w::Int = 1, base = 2)</code></pre><p>The <code>Kraskov</code> estimator computes the <a href="#Entropies.Shannon"><code>Shannon</code></a> differential <a href="#Entropies.entropy"><code>entropy</code></a> of <code>x</code> (a multi-dimensional <code>Dataset</code>) to the given <code>base</code>, using the <code>k</code>-th nearest neighbor searches method from <sup class="footnote-reference"><a id="citeref-Kraskov2004" href="#footnote-Kraskov2004">[Kraskov2004]</a></sup>.</p><p><code>w</code> is the Theiler window, which determines if temporal neighbors are excluded during neighbor searches (defaults to <code>0</code>, meaning that only the point itself is excluded when searching for neighbours).</p><p>See also: <a href="#Entropies.entropy"><code>entropy</code></a>, <a href="#Entropies.KozachenkoLeonenko"><code>KozachenkoLeonenko</code></a>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaDynamics/Entropies.jl/blob/0d5fd0da077bedddc20c2369b3b57ecbbc485675/src/entropies/estimators/nearest_neighbors/Kraskov.jl#L3-L20">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Entropies.KozachenkoLeonenko" href="#Entropies.KozachenkoLeonenko"><code>Entropies.KozachenkoLeonenko</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">KozachenkoLeonenko &lt;: EntropyEstimator
KozachenkoLeonenko(; k::Int = 1, w::Int = 1, base = 2)</code></pre><p>The <code>KozachenkoLeonenko</code> estimator computes the <a href="#Entropies.Shannon"><code>Shannon</code></a> differential <a href="#Entropies.entropy"><code>entropy</code></a> of <code>x</code> (a multi-dimensional <code>Dataset</code>) to the given <code>base</code>, based on nearest neighbor searches using the method from Kozachenko &amp; Leonenko (1987)<sup class="footnote-reference"><a id="citeref-KozachenkoLeonenko1987" href="#footnote-KozachenkoLeonenko1987">[KozachenkoLeonenko1987]</a></sup>, as described in Charzyńska and Gambin<sup class="footnote-reference"><a id="citeref-Charzyńska2016" href="#footnote-Charzyńska2016">[Charzyńska2016]</a></sup>.</p><p><code>w</code> is the Theiler window, which determines if temporal neighbors are excluded during neighbor searches (defaults to <code>0</code>, meaning that only the point itself is excluded when searching for neighbours).</p><p>In contrast to <a href="#Entropies.Kraskov"><code>Kraskov</code></a>, this estimator uses only the <em>closest</em> neighbor.</p><p>See also: <a href="#Entropies.entropy"><code>entropy</code></a>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaDynamics/Entropies.jl/blob/0d5fd0da077bedddc20c2369b3b57ecbbc485675/src/entropies/estimators/nearest_neighbors/KozachenkoLeonenko.jl#L3-L24">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Entropies.Zhu" href="#Entropies.Zhu"><code>Entropies.Zhu</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">Zhu &lt;: EntropyEstimator
Zhu(k = 1, w = 0)</code></pre><p>The <code>Zhu</code> estimator (Zhu et al., 2015)<sup class="footnote-reference"><a id="citeref-Zhu2015" href="#footnote-Zhu2015">[Zhu2015]</a></sup> computes the <a href="#Entropies.Shannon"><code>Shannon</code></a> differential <a href="#Entropies.entropy"><code>entropy</code></a> of <code>x</code> (a multi-dimensional <code>Dataset</code>), by approximating probabilities within hyperrectangles surrounding each point <code>xᵢ ∈ x</code> using using <code>k</code> nearest neighbor searches.</p><p><code>w</code> is the Theiler window, which determines if temporal neighbors are excluded during neighbor searches (defaults to <code>0</code>, meaning that only the point itself is excluded when searching for neighbours).</p><p>This estimator is an extension to <a href="#Entropies.KozachenkoLeonenko"><code>KozachenkoLeonenko</code></a>.</p><p>See also: <a href="#Entropies.entropy"><code>entropy</code></a>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaDynamics/Entropies.jl/blob/0d5fd0da077bedddc20c2369b3b57ecbbc485675/src/entropies/estimators/nearest_neighbors/Zhu.jl#L3-L24">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Entropies.ZhuSingh" href="#Entropies.ZhuSingh"><code>Entropies.ZhuSingh</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">ZhuSingh &lt;: EntropyEstimator
ZhuSingh(k = 1, w = 0)</code></pre><p>The <code>ZhuSingh</code> estimator (Zhu et al., 2015)<sup class="footnote-reference"><a id="citeref-Zhu2015" href="#footnote-Zhu2015">[Zhu2015]</a></sup> computes the <a href="#Entropies.Shannon"><code>Shannon</code></a> differential <a href="#Entropies.entropy"><code>entropy</code></a> of <code>x</code> (a multi-dimensional <code>Dataset</code>).</p><p>Like <a href="#Entropies.Zhu"><code>Zhu</code></a>, this estimator approximates probabilities within hyperrectangles surrounding each point <code>xᵢ ∈ x</code> using using <code>k</code> nearest neighbor searches. However, it also considers the number of neighbors falling on the borders of these hyperrectangles. This estimator is an extension to the entropy estimator in Singh et al. (2003).</p><p><code>w</code> is the Theiler window, which determines if temporal neighbors are excluded during neighbor searches (defaults to <code>0</code>, meaning that only the point itself is excluded when searching for neighbours).</p><p>See also: <a href="#Entropies.entropy"><code>entropy</code></a>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaDynamics/Entropies.jl/blob/0d5fd0da077bedddc20c2369b3b57ecbbc485675/src/entropies/estimators/nearest_neighbors/ZhuSingh.jl#L8-L34">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Entropies.Vasicek" href="#Entropies.Vasicek"><code>Entropies.Vasicek</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">Vasicek &lt;: EntropyEstimator
Vasicek(; m::Int = 1, base = 2)</code></pre><p>The <code>Vasicek</code> estimator computes the <a href="#Entropies.Shannon"><code>Shannon</code></a> differential <a href="#Entropies.entropy"><code>entropy</code></a> of <code>x</code> (a multi-dimensional <code>Dataset</code>) to the given <code>base</code> using the method from Vasicek (1976)<sup class="footnote-reference"><a id="citeref-Vasicek1976" href="#footnote-Vasicek1976">[Vasicek1976]</a></sup>.</p><p><strong>Description</strong></p><p>The Vasicek entropy estimator first computes the <a href="https://en.wikipedia.org/wiki/Order_statistic">order statistics</a> <span>$X_{(1)} \leq X_{(2)} \leq \cdots \leq X_{(n)}$</span> for a random sample of length <span>$n$</span>, i.e. the input timeseries. The <a href="#Entropies.Shannon"><code>Shannon</code></a> entropy is then estimated as</p><p class="math-container">\[H_V(m) =
\dfrac{1}{n} \sum_{i = 1}^n \log \left[ \dfrac{n}{2m} (X_{(i+m)} - X_{(i-m)}) \right]\]</p><p><strong>Usage</strong></p><p>In practice, choice of <code>m</code> influences how fast the entropy converges to the true value. For small value of <code>m</code>, convergence is slow, so we recommend to scale <code>m</code> according to the time series length <code>n</code> and use <code>m &gt;= n/100</code> (this is just a heuristic based on the tests written for this package).</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaDynamics/Entropies.jl/blob/0d5fd0da077bedddc20c2369b3b57ecbbc485675/src/entropies/estimators/order_statistics/Vasicek.jl#L3-L33">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Entropies.AlizadehArghami" href="#Entropies.AlizadehArghami"><code>Entropies.AlizadehArghami</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">AlizadehArghami &lt;: EntropyEstimator
AlizadehArghami(; m::Int = 1, base = 2)</code></pre><p>The <code>AlizadehArghami</code>estimator computes the <a href="#Entropies.Shannon"><code>Shannon</code></a> differential <a href="#Entropies.entropy"><code>entropy</code></a> of <code>x</code> (a multi-dimensional <code>Dataset</code>) to the given <code>base</code> using the method from Alizadeh &amp; Arghami (2010)<sup class="footnote-reference"><a id="citeref-Alizadeh2010" href="#footnote-Alizadeh2010">[Alizadeh2010]</a></sup>.</p><p><strong>Description</strong></p><p>The estimator first computes the <a href="https://en.wikipedia.org/wiki/Order_statistic">order statistics</a> <span>$X_{(1)} \leq X_{(2)} \leq \cdots \leq X_{(n)}$</span> for a random sample of length <span>$n$</span>, i.e. the input timeseries. The entropy for the length-<code>n</code> sample is then estimated as the <a href="#Entropies.Vasicek"><code>Vasicek</code></a> entropy estimate, plus a correction factor</p><p class="math-container">\[H_{A}(m, n) = H_{V}(m, n) + \dfrac{2}{n}\left(m \log_{base}(2) \right).\]</p><p>See also: <a href="#Entropies.entropy"><code>entropy</code></a>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaDynamics/Entropies.jl/blob/0d5fd0da077bedddc20c2369b3b57ecbbc485675/src/entropies/estimators/order_statistics/AlizadehArghami.jl#L3-L28">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Entropies.Ebrahimi" href="#Entropies.Ebrahimi"><code>Entropies.Ebrahimi</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">Ebrahimi &lt;: EntropyEstimator
Ebrahimi(; m::Int = 1, base = 2)</code></pre><p>The <code>Ebrahimi</code> estimator computes the <a href="#Entropies.Shannon"><code>Shannon</code></a> <a href="#Entropies.entropy"><code>entropy</code></a> of <code>x</code> (a multi-dimensional <code>Dataset</code>) to the given <code>base</code> using the method from Ebrahimi (1994)<sup class="footnote-reference"><a id="citeref-Ebrahimi1994" href="#footnote-Ebrahimi1994">[Ebrahimi1994]</a></sup>.</p><p><strong>Description</strong></p><p>The Ebrahimi entropy estimator first computes the order statistics <span>$X_{(1)} \leq X_{(2)} \leq \cdots \leq X_{(n)}$</span> for a random sample of length <span>$n$</span>, i.e. the input timeseries. The <a href="#Entropies.Shannon"><code>Shannon</code></a> differential entropy is then estimated as</p><p class="math-container">\[H_{E}(m) =
\dfrac{1}{n} \sum_{i = 1}^n \log \left[ \dfrac{n}{c_i m} (X_{(i+m)} - X_{(i-m)}) \right],\]</p><p>where</p><p class="math-container">\[c_i =
\begin{cases}
    1 + \frac{i - 1}{m}, &amp; 1 \geq i \geq m \
    2,                    &amp; m + 1 \geq i \geq n - m \
    1 + \frac{n - i}{m} &amp; n - m + 1 \geq i \geq n
\end{cases}.\]</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaDynamics/Entropies.jl/blob/0d5fd0da077bedddc20c2369b3b57ecbbc485675/src/entropies/estimators/order_statistics/Ebrahimi.jl#L3-L37">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Entropies.Correa" href="#Entropies.Correa"><code>Entropies.Correa</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">Correa &lt;: EntropyEstimator
Correa(; m::Int = 1, base = 2)</code></pre><p>The <code>Correa</code> estimator computes the <a href="#Entropies.Shannon"><code>Shannon</code></a> differential <a href="#Entropies.entropy"><code>entropy</code></a> of <code>x</code> (a multi-dimensional <code>Dataset</code>) to the given <code>base</code> using the method from Correa (1995)<sup class="footnote-reference"><a id="citeref-Correa1995" href="#footnote-Correa1995">[Correa1995]</a></sup>.</p><p><strong>Description</strong></p><p>The Correa entropy estimator first computes the order statistics like <a href="#Entropies.Vasicek"><code>Vasicek</code></a>, ensuring that edge points are included, then estimates entropy as</p><p class="math-container">\[H_C(m, n) =
\dfrac{1}{n} \sum_{i = 1}^n \log
\left[ \dfrac{ \sum_{j=i-m}^{i+m}(X_{(j)} - \bar{X}_{(i)})(j - i)}{n \sum_{j=i-m}^{i+m} (X_{(j)} - \bar{X}_{(i)})^2} \right],\]</p><p>where</p><p class="math-container">\[\bar{X}_{(i)} = \dfrac{1}{2m + 1} \sum_{j = i - m}^{i + m} X_{(j)}.\]</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaDynamics/Entropies.jl/blob/0d5fd0da077bedddc20c2369b3b57ecbbc485675/src/entropies/estimators/order_statistics/Correa.jl#L3-L31">source</a></section></article><h2 id="Convenience-functions"><a class="docs-heading-anchor" href="#Convenience-functions">Convenience functions</a><a id="Convenience-functions-1"></a><a class="docs-heading-anchor-permalink" href="#Convenience-functions" title="Permalink"></a></h2><p>In this subsection we expand documentation strings of &quot;entropy names&quot; that are used commonly in the literature, such as &quot;permutation entropy&quot;. As we made clear in <a href="../#API-and-terminology">API &amp; terminology</a>, these are just the existing Shannon entropy with a particularly chosen probability estimator. We have only defined convenience functions for the most used names, and arbitrary more specialized convenience functions can be easily defined in a couple lines of code.</p><article class="docstring"><header><a class="docstring-binding" id="Entropies.entropy_permutation" href="#Entropies.entropy_permutation"><code>Entropies.entropy_permutation</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">entropy_permutation(x; τ = 1, m = 3, base = 2)</code></pre><p>Compute the permutation entropy of <code>x</code> of order <code>m</code> with delay/lag <code>τ</code>. This function is just a convenience call to:</p><pre><code class="language-julia hljs">est = SymbolicPermutation(; m, τ)
entropy(Shannon(base), x, est)</code></pre><p>See <a href="../probabilities/#Entropies.SymbolicPermutation"><code>SymbolicPermutation</code></a> for more info. Similarly, one can use <code>SymbolicWeightedPermutation</code> or <code>SymbolicAmplitudeAwarePermutation</code> for the weighted/amplitude-aware versions.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaDynamics/Entropies.jl/blob/0d5fd0da077bedddc20c2369b3b57ecbbc485675/src/entropies/convenience_definitions.jl#L9-L23">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Entropies.entropy_spatial_permutation" href="#Entropies.entropy_spatial_permutation"><code>Entropies.entropy_spatial_permutation</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">entropy_spatial_permutation(x, stencil; periodic = true; kwargs...)</code></pre><p>Compute the spatial permutation entropy of <code>x</code> given the <code>stencil</code>. Here <code>x</code> must be a matrix or higher dimensional <code>Array</code> containing spatial data. This function is just a convenience call to:</p><pre><code class="language-julia hljs">est = SpatialSymbolicPermutation(stencil, x, periodic)
entropy(Renyi(;kwargs...), est, x)</code></pre><p>See <a href="../probabilities/#Entropies.SpatialSymbolicPermutation"><code>SpatialSymbolicPermutation</code></a> for more info, or how to encode stencils.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaDynamics/Entropies.jl/blob/0d5fd0da077bedddc20c2369b3b57ecbbc485675/src/entropies/convenience_definitions.jl#L29-L42">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Entropies.entropy_wavelet" href="#Entropies.entropy_wavelet"><code>Entropies.entropy_wavelet</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">entropy_wavelet(x; wavelet = Wavelets.WT.Daubechies{12}(), base = 2)</code></pre><p>Compute the wavelet entropy. This function is just a convenience call to:</p><pre><code class="language-julia hljs">est = WaveletOverlap(wavelet)
entropy(Shannon(base), est, x)</code></pre><p>See <a href="../probabilities/#Entropies.WaveletOverlap"><code>WaveletOverlap</code></a> for more info.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaDynamics/Entropies.jl/blob/0d5fd0da077bedddc20c2369b3b57ecbbc485675/src/entropies/convenience_definitions.jl#L48-L59">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Entropies.entropy_dispersion" href="#Entropies.entropy_dispersion"><code>Entropies.entropy_dispersion</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">entropy_dispersion(x; base = 2, kwargs...)</code></pre><p>Compute the dispersion entropy. This function is just a convenience call to:</p><pre><code class="language-julia hljs">est = Dispersion(kwargs...)
entropy(Shannon(base), est, x)</code></pre><p>See <a href="../probabilities/#Entropies.Dispersion"><code>Dispersion</code></a> for more info.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaDynamics/Entropies.jl/blob/0d5fd0da077bedddc20c2369b3b57ecbbc485675/src/entropies/convenience_definitions.jl#L65-L76">source</a></section></article><section class="footnotes is-size-7"><ul><li class="footnote" id="footnote-Shannon1948"><a class="tag is-link" href="#citeref-Shannon1948">Shannon1948</a>C. E. Shannon, Bell Systems Technical Journal <strong>27</strong>, pp 379 (1948)</li><li class="footnote" id="footnote-Rényi1960"><a class="tag is-link" href="#citeref-Rényi1960">Rényi1960</a>A. Rényi, <em>Proceedings of the fourth Berkeley Symposium on Mathematics, Statistics and Probability</em>, pp 547 (1960)</li><li class="footnote" id="footnote-Shannon1948"><a class="tag is-link" href="#citeref-Shannon1948">Shannon1948</a>C. E. Shannon, Bell Systems Technical Journal <strong>27</strong>, pp 379 (1948)</li><li class="footnote" id="footnote-Tsallis1988"><a class="tag is-link" href="#citeref-Tsallis1988">Tsallis1988</a>Tsallis, C. (1988). Possible generalization of Boltzmann-Gibbs statistics. Journal of statistical physics, 52(1), 479-487.</li><li class="footnote" id="footnote-Curado2004"><a class="tag is-link" href="#citeref-Curado2004">Curado2004</a>Curado, E. M., &amp; Nobre, F. D. (2004). On the stability of analytic entropic forms. Physica A: Statistical Mechanics and its Applications, 335(1-2), 94-106.</li><li class="footnote" id="footnote-Anteneodo1999"><a class="tag is-link" href="#citeref-Anteneodo1999">Anteneodo1999</a>Anteneodo, C., &amp; Plastino, A. R. (1999). Maximum entropy approach to stretched exponential probability distributions. Journal of Physics A: Mathematical and General, 32(7), 1089.</li><li class="footnote" id="footnote-Kraskov2004"><a class="tag is-link" href="#citeref-Kraskov2004">Kraskov2004</a>Kraskov, A., Stögbauer, H., &amp; Grassberger, P. (2004). Estimating mutual information. Physical review E, 69(6), 066138.</li><li class="footnote" id="footnote-Charzyńska2016"><a class="tag is-link" href="#citeref-Charzyńska2016">Charzyńska2016</a>Charzyńska, A., &amp; Gambin, A. (2016). Improvement of the k-NN entropy estimator with applications in systems biology. Entropy, 18(1), 13.</li><li class="footnote" id="footnote-KozachenkoLeonenko1987"><a class="tag is-link" href="#citeref-KozachenkoLeonenko1987">KozachenkoLeonenko1987</a>Kozachenko, L. F., &amp; Leonenko, N. N. (1987). Sample estimate of the entropy of a random vector. Problemy Peredachi Informatsii, 23(2), 9-16.</li><li class="footnote" id="footnote-Zhu2015"><a class="tag is-link" href="#citeref-Zhu2015">Zhu2015</a>Zhu, J., Bellanger, J. J., Shu, H., &amp; Le Bouquin Jeannès, R. (2015). Contribution to transfer entropy estimation via the k-nearest-neighbors approach. Entropy, 17(6), 4173-4201.</li><li class="footnote" id="footnote-Zhu2015"><a class="tag is-link" href="#citeref-Zhu2015">Zhu2015</a>Zhu, J., Bellanger, J. J., Shu, H., &amp; Le Bouquin Jeannès, R. (2015). Contribution to transfer entropy estimation via the k-nearest-neighbors approach. Entropy, 17(6), 4173-4201.</li><li class="footnote" id="footnote-Singh2003"><a class="tag is-link" href="#citeref-Singh2003">Singh2003</a>Singh, H., Misra, N., Hnizdo, V., Fedorowicz, A., &amp; Demchuk, E. (2003). Nearest neighbor estimates of entropy. American journal of mathematical and management sciences, 23(3-4), 301-321.</li><li class="footnote" id="footnote-Vasicek1976"><a class="tag is-link" href="#citeref-Vasicek1976">Vasicek1976</a>Vasicek, O. (1976). A test for normality based on sample entropy. Journal of the Royal Statistical Society: Series B (Methodological), 38(1), 54-59.</li><li class="footnote" id="footnote-Alizadeh2010"><a class="tag is-link" href="#citeref-Alizadeh2010">Alizadeh2010</a>Alizadeh, N. H., &amp; Arghami, N. R. (2010). A new estimator of entropy. Journal of the Iranian Statistical Society (JIRSS).</li><li class="footnote" id="footnote-Ebrahimi1994"><a class="tag is-link" href="#citeref-Ebrahimi1994">Ebrahimi1994</a>Ebrahimi, N., Pflughoeft, K., &amp; Soofi, E. S. (1994). Two measures of sample entropy. Statistics &amp; Probability Letters, 20(3), 225-234.</li><li class="footnote" id="footnote-Correa1995"><a class="tag is-link" href="#citeref-Correa1995">Correa1995</a>Correa, J. C. (1995). A new estimator of entropy. Communications in Statistics-Theory and Methods, 24(10), 2439-2449.</li></ul></section></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../probabilities/">« Probabilities</a><a class="docs-footer-nextpage" href="../complexity/">Complexity measures »</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 0.27.23 on <span class="colophon-date" title="Wednesday 21 December 2022 09:00">Wednesday 21 December 2022</span>. Using Julia version 1.8.3.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
