<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Complexity measures · Entropies.jl</title><script data-outdated-warner src="../assets/warner.js"></script><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.045/juliamono.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.24/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL=".."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../assets/documenter.js"></script><script src="../siteinfo.js"></script><script src="../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../assets/themeswap.js"></script><link href="https://fonts.googleapis.com/css?family=Montserrat|Source+Code+Pro&amp;display=swap" rel="stylesheet" type="text/css"/></head><body><div id="documenter"><nav class="docs-sidebar"><div class="docs-package-name"><span class="docs-autofit"><a href="../">Entropies.jl</a></span></div><form class="docs-search" action="../search/"><input class="docs-search-query" id="documenter-search-query" name="q" type="text" placeholder="Search docs"/></form><ul class="docs-menu"><li><a class="tocitem" href="../">Entropies.jl</a></li><li><a class="tocitem" href="../probabilities/">Probabilities</a></li><li><a class="tocitem" href="../entropies/">Entropies</a></li><li class="is-active"><a class="tocitem" href>Complexity measures</a><ul class="internal"><li><a class="tocitem" href="#Reverse-dispersion-entropy"><span>Reverse dispersion entropy</span></a></li><li><a class="tocitem" href="#Fuzzy-entropy"><span>Fuzzy entropy</span></a></li></ul></li><li><a class="tocitem" href="../examples/">Entropies.jl examples</a></li><li><a class="tocitem" href="../utils/">Utility methods</a></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><nav class="breadcrumb"><ul class="is-hidden-mobile"><li class="is-active"><a href>Complexity measures</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>Complexity measures</a></li></ul></nav><div class="docs-right"><a class="docs-edit-link" href="https://github.com/JuliaDynamics/Entropies.jl/blob/main/docs/src/complexity_measures.md" title="Edit on GitHub"><span class="docs-icon fab"></span><span class="docs-label is-hidden-touch">Edit on GitHub</span></a><a class="docs-settings-button fas fa-cog" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-sidebar-button fa fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a></div></header><article class="content" id="documenter-page"><h1 id="complexity_measures"><a class="docs-heading-anchor" href="#complexity_measures">Complexity measures API</a><a id="complexity_measures-1"></a><a class="docs-heading-anchor-permalink" href="#complexity_measures" title="Permalink"></a></h1><article class="docstring"><header><a class="docstring-binding" id="Entropies.complexity" href="#Entropies.complexity"><code>Entropies.complexity</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">complexity(c::ComplexityMeasure, x)</code></pre><p>Estimate the complexity measure <code>c</code> for <a href="../#input_data">input data</a> <code>x</code>, where <code>c</code> can be any of the following measures:</p><ul><li><a href="#Entropies.ReverseDispersion"><code>ReverseDispersion</code></a>.</li><li><a href="#Entropies.FuzzyEntropy"><code>FuzzyEntropy</code></a>.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaDynamics/Entropies.jl/blob/7046e9ecb2b7e394e54da5bae030742f3e6d4d8f/src/complexity.jl#L12-L20">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Entropies.complexity_normalized" href="#Entropies.complexity_normalized"><code>Entropies.complexity_normalized</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">complexity_normalized(c::ComplexityMeasure, x) → m ∈ [a, b]</code></pre><p>Estimate the <a href="#Entropies.complexity"><code>complexity</code></a> measure <code>c</code> for <a href="../#input_data">input data</a> <code>x</code>, normalized to the interval <code>[a, b]</code>, where <code>[a, b]</code> depends on <code>c</code>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaDynamics/Entropies.jl/blob/7046e9ecb2b7e394e54da5bae030742f3e6d4d8f/src/complexity.jl#L23-L28">source</a></section></article><h2 id="Reverse-dispersion-entropy"><a class="docs-heading-anchor" href="#Reverse-dispersion-entropy">Reverse dispersion entropy</a><a id="Reverse-dispersion-entropy-1"></a><a class="docs-heading-anchor-permalink" href="#Reverse-dispersion-entropy" title="Permalink"></a></h2><article class="docstring"><header><a class="docstring-binding" id="Entropies.ReverseDispersion" href="#Entropies.ReverseDispersion"><code>Entropies.ReverseDispersion</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">ReverseDispersion &lt;: ComplexityMeasure
ReverseDispersion(; m = 2, τ = 1, check_unique = true,
    symbolization::SymbolizationScheme = GaussianSymbolization(c = 5)
)</code></pre><p>Estimator for the reverse dispersion entropy complexity measure (Li et al., 2019)<sup class="footnote-reference"><a id="citeref-Li2019" href="#footnote-Li2019">[Li2019]</a></sup>.</p><p><strong>Description</strong></p><p>Li et al. (2021)<sup class="footnote-reference"><a id="citeref-Li2019" href="#footnote-Li2019">[Li2019]</a></sup> defines the reverse dispersion entropy as</p><p class="math-container">\[H_{rde} = \sum_{i = 1}^{c^m} \left(p_i - \dfrac{1}{{c^m}} \right)^2 =
\left( \sum_{i=1}^{c^m} p_i^2 \right) - \dfrac{1}{c^{m}}\]</p><p>where the probabilities <span>$p_i$</span> are obtained precisely as for the <a href="../probabilities/#Entropies.Dispersion"><code>Dispersion</code></a> probability estimator. Relative frequencies of dispersion patterns are computed using the given <code>symbolization</code> scheme , which defaults to symbolization using the normal cumulative distribution function (NCDF), as implemented by <a href="../utils/#Entropies.GaussianSymbolization"><code>GaussianSymbolization</code></a>, using embedding dimension <code>m</code> and embedding delay <code>τ</code>. Recommended parameter values<sup class="footnote-reference"><a id="citeref-Li2018" href="#footnote-Li2018">[Li2018]</a></sup> are <code>m ∈ [2, 3]</code>, <code>τ = 1</code> for the embedding, and <code>c ∈ [3, 4, …, 8]</code> categories for the Gaussian mapping.</p><p>If normalizing, then the reverse dispersion entropy is normalized to <code>[0, 1]</code>.</p><p>The minimum value of <span>$H_{rde}$</span> is zero and occurs precisely when the dispersion pattern distribution is flat, which occurs when all <span>$p_i$</span>s are equal to <span>$1/c^m$</span>. Because <span>$H_{rde} \geq 0$</span>, <span>$H_{rde}$</span> can therefore be said to be a measure of how far the dispersion pattern probability distribution is from white noise.</p><p><strong>Data requirements</strong></p><p>Like for <a href="../probabilities/#Entropies.Dispersion"><code>Dispersion</code></a>, the input must have more than one unique element for the default Gaussian mapping symbolization to be well-defined. Li et al. (2018) recommends that <code>x</code> has at least 1000 data points.</p><p>If <code>check_unique == true</code> (default), then it is checked that the input has more than one unique value. If <code>check_unique == false</code> and the input only has one unique element, then a <code>InexactError</code> is thrown when trying to compute probabilities.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaDynamics/Entropies.jl/blob/7046e9ecb2b7e394e54da5bae030742f3e6d4d8f/src/complexity_measures/reverse_dispersion_entropy.jl#L4-L47">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Entropies.distance_to_whitenoise" href="#Entropies.distance_to_whitenoise"><code>Entropies.distance_to_whitenoise</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">distance_to_whitenoise(p::Probabilities, estimator::ReverseDispersion; normalize = false)</code></pre><p>Compute the distance of the probability distribution <code>p</code> from a uniform distribution, given the parameters of <code>estimator</code> (which must be known beforehand).</p><p>If <code>normalize == true</code>, then normalize the value to the interval <code>[0, 1]</code> by using the parameters of <code>estimator</code>.</p><p>Used to compute reverse dispersion entropy(<a href="#Entropies.ReverseDispersion"><code>ReverseDispersion</code></a>; Li et al., 2019<sup class="footnote-reference"><a id="citeref-Li2019" href="#footnote-Li2019">[Li2019]</a></sup>).</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaDynamics/Entropies.jl/blob/7046e9ecb2b7e394e54da5bae030742f3e6d4d8f/src/complexity_measures/reverse_dispersion_entropy.jl#L57-L71">source</a></section></article><h2 id="Fuzzy-entropy"><a class="docs-heading-anchor" href="#Fuzzy-entropy">Fuzzy entropy</a><a id="Fuzzy-entropy-1"></a><a class="docs-heading-anchor-permalink" href="#Fuzzy-entropy" title="Permalink"></a></h2><article class="docstring"><header><a class="docstring-binding" id="Entropies.FuzzyEntropy" href="#Entropies.FuzzyEntropy"><code>Entropies.FuzzyEntropy</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">FuzzyEntropy([x]; r = 0.2std(x), kwargs...)</code></pre><p>An estimator for the fuzzy entropy (Chen et al., 2007)<sup class="footnote-reference"><a id="citeref-Chen2007" href="#footnote-Chen2007">[Chen2007]</a></sup> complexity/irregularity measure, used with <a href="#Entropies.complexity"><code>complexity</code></a>.</p><p>The keyword argument <code>r</code> is mandatory if an input timeseries <code>x</code> is not provided.</p><p><strong>Keyword arguments</strong></p><ul><li><code>r::Real</code>: The radius used when querying for nearest neighbors around points. Its value   should be determined from the input data, for example as some proportion of the   standard deviation of the data.</li><li><code>m::Int = 1</code>: The embedding dimension.</li><li><code>n::Real = 2</code>: The &quot;fuzzy power&quot;. For <code>d ∈ [-1, 1]</code>, the exponential function described   below peaks at <code>d == 0</code>. For fixed <code>r</code>, larger <code>n</code> broadens the peak and smaller <code>n</code>   narrows the peak.</li><li><code>τ::Int =</code>: The embedding lag.</li><li><code>metric</code>: The metric used to compute distances.</li></ul><p><strong>Description</strong></p><p>Fuzzy entropy for an input timeseries <code>x</code> is computed as follows:</p><ul><li>Construct two <code>k</code>-dimensional embeddings, one for <code>k = m</code> and one for <code>k = m + 1</code>, as   follows:</li></ul><p class="math-container">\[{\bf x}_i^k = (x(i), x(i+τ), x(i+2τ), \ldots, x(i+(k-1)\tau)).\]</p><ul><li>Zero-mean-normalize both embeddings.</li><li>Define the function</li></ul><p class="math-container">\[\phi(x, k, r, n) =
\dfrac{1}{N - (m-1)\tau}\sum_{i = 1}^{N-(m-1)\tau}
\dfrac{1}{N - (m-1)\tau - 1}\sum_{j = 1, j \neq i}^{N-(m-1)\tau}  D_{ij}^m(n, r).\]</p><p>where <span>$D_{ij}^m(n, r)$</span> is the &quot;similarity degree&quot; between vectors <span>$\bf x_i ∈ \mathcal{R^k}$</span> and <span>$\bf y_i ∈ \mathcal{R^k}$</span>, given radius <code>r</code>, the &quot;fuzzy power&quot; <code>n</code>, and <span>$D_{ij}^m$</span>, which is the distance between <span>$\bf x_i$</span> and <span>$\bf y_i$</span> according to the given <code>metric</code> (Chen et al. (2007) uses the Chebyshev metric). The similarity degree is defined as</p><p class="math-container">\[D_{ij}^m(n, r) = exp(-(d_{ij}^m)^n/r)\]</p><ul><li>The fuzzy entropy is finally estimated as:</li></ul><p class="math-container">\[FuzzyEn(x, m, r, n) = \log(\phi(x, m, r, n)) - \log(\phi(x, m + 1, r, n))\]</p><div class="admonition is-info"><header class="admonition-header">Flexible embedding lag</header><div class="admonition-body"><p>In the original paper, they fix <code>τ = 1</code>. In our implementation, equations are modified to account for embeddings with <code>τ != 1</code>.</p></div></div></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaDynamics/Entropies.jl/blob/7046e9ecb2b7e394e54da5bae030742f3e6d4d8f/src/complexity_measures/fuzzy_entropy.jl#L8-L71">source</a></section></article><section class="footnotes is-size-7"><ul><li class="footnote" id="footnote-Li2019"><a class="tag is-link" href="#citeref-Li2019">Li2019</a>Li, Y., Gao, X., &amp; Wang, L. (2019). Reverse dispersion entropy: a new complexity measure for sensor signal. Sensors, 19(23), 5203.</li><li class="footnote" id="footnote-Li2019"><a class="tag is-link" href="#citeref-Li2019">Li2019</a>Li, Y., Gao, X., &amp; Wang, L. (2019). Reverse dispersion entropy: a new complexity measure for sensor signal. Sensors, 19(23), 5203.</li><li class="footnote" id="footnote-Chen2007"><a class="tag is-link" href="#citeref-Chen2007">Chen2007</a>Chen, W., Wang, Z., Xie, H., &amp; Yu, W. (2007). Characterization of surface EMG signal based on fuzzy entropy. IEEE Transactions on neural systems and rehabilitation engineering, 15(2), 266-272.</li></ul></section></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../entropies/">« Entropies</a><a class="docs-footer-nextpage" href="../examples/">Entropies.jl examples »</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 0.27.23 on <span class="colophon-date" title="Saturday 22 October 2022 20:35">Saturday 22 October 2022</span>. Using Julia version 1.8.2.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
