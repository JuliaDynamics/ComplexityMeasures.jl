var documenterSearchIndex = {"docs":
[{"location":"#Entropies.jl","page":"Entropies.jl","title":"Entropies.jl","text":"","category":"section"},{"location":"","page":"Entropies.jl","title":"Entropies.jl","text":"This package provides probability and entropy estimators used for entropy computations in the CausalityTools.jl and DynamicalSystems.jl packages.","category":"page"},{"location":"","page":"Entropies.jl","title":"Entropies.jl","text":"Most of the code in this package assumes that your data is represented by the Dataset-type from DelayEmbeddings.jl, where each observation is a D-dimensional data point represented by a static vector. See the DynamicalSystems.jl documentation for more info. Univariate timeseries given as AbstractVector{<:Real} also work with some estimators, but are treated differently based on which method for probability/entropy estimation is applied.","category":"page"},{"location":"#API","page":"Entropies.jl","title":"API","text":"","category":"section"},{"location":"","page":"Entropies.jl","title":"Entropies.jl","text":"The main API of this package is contained in two functions:","category":"page"},{"location":"","page":"Entropies.jl","title":"Entropies.jl","text":"probabilities which computes probability distributions of given datasets\ngenentropy which uses the output of probabilities, or a set of   pre-computed Probabilities, to calculate entropies.","category":"page"},{"location":"","page":"Entropies.jl","title":"Entropies.jl","text":"These functions dispatch on subtypes of ProbabilitiesEstimator, which are:","category":"page"},{"location":"","page":"Entropies.jl","title":"Entropies.jl","text":"using Entropies, InteractiveUtils\nsubtypes(ProbabilitiesEstimator)","category":"page"},{"location":"#Probabilities","page":"Entropies.jl","title":"Probabilities","text":"","category":"section"},{"location":"","page":"Entropies.jl","title":"Entropies.jl","text":"Probabilities\nprobabilities\nprobabilities!\nProbabilitiesEstimator","category":"page"},{"location":"#Entropies.Probabilities","page":"Entropies.jl","title":"Entropies.Probabilities","text":"Probabilities(x) â†’ p\n\nA simple wrapper type around an x::AbstractVector which ensures that p sums to 1. Behaves identically to Vector.\n\n\n\n\n\n","category":"type"},{"location":"#Entropies.probabilities","page":"Entropies.jl","title":"Entropies.probabilities","text":"probabilities(x::Vector_or_Dataset, est::ProbabilitiesEstimator) â†’ p::Probabilities\n\nCalculate probabilities representing x based on the provided estimator and return them as a Probabilities container (Vector-like). The probabilities are typically unordered and may or may not contain 0s, see the documentation of the individual estimators for more.\n\nThe configuration options are always given as arguments to the chosen estimator.\n\nprobabilities(x::Vector_or_Dataset, Îµ::AbstractFloat) â†’ p::Probabilities\n\nConvenience syntax which provides probabilities for x based on rectangular binning (i.e. performing a histogram). In short, the state space is divided into boxes of length Îµ, and formally we use est = VisitationFrequency(RectangularBinning(Îµ)) as an estimator, see VisitationFrequency.\n\nThis method has a linearithmic time complexity (n log(n) for n = length(x)) and a linear space complexity (l for l = dimension(x)). This allows computation of probabilities (histograms) of high-dimensional datasets and with small box sizes Îµ without memory overflow and with maximum performance. To obtain the bin information along with p, use binhist.\n\nprobabilities(x::Vector_or_Dataset) â†’ p::Probabilities\n\nDirectly count probabilities from the elements of x without any discretization, binning, or other processing (mostly useful when x contains categorical or integer data).\n\n\n\n\n\n","category":"function"},{"location":"#Entropies.probabilities!","page":"Entropies.jl","title":"Entropies.probabilities!","text":"probabilities!(args...)\n\nIdentical to probabilities(args...), but allows pre-allocation of temporarily used containers.\n\nOnly works for certain estimators. See for example SymbolicPermutation.\n\n\n\n\n\n","category":"function"},{"location":"#Entropies.ProbabilitiesEstimator","page":"Entropies.jl","title":"Entropies.ProbabilitiesEstimator","text":"An abstract type for probabilities estimators.\n\n\n\n\n\n","category":"type"},{"location":"#Generalized-entropy","page":"Entropies.jl","title":"Generalized entropy","text":"","category":"section"},{"location":"","page":"Entropies.jl","title":"Entropies.jl","text":"Entropies.genentropy","category":"page"},{"location":"#Entropies.genentropy","page":"Entropies.jl","title":"Entropies.genentropy","text":"genentropy(p::Probabilities; Î± = 1.0, base = Base.MathConstants.e)\n\nCompute the generalized order-Î± entropy of some probabilities returned by the probabilities function. Alternatively, compute entropy from pre-computed Probabilities.\n\nDescription\n\nLet p be an array of probabilities (summing to 1). Then the generalized (RÃ©nyi) entropy is\n\nH_alpha(p) = frac11-alpha log left(sum_i pi^alpharight)\n\nand generalizes other known entropies, like e.g. the information entropy (alpha = 1, see [Shannon1948]), the maximum entropy (alpha=0, also known as Hartley entropy), or the correlation entropy (alpha = 2, also known as collision entropy).\n\ngenentropy(x::Vector_or_Dataset, est; Î± = 1.0, base)\n\nA convenience syntax, which calls first probabilities(x, est) and then calculates the entropy of the result (and thus est can be a ProbabilitiesEstimator or simply Îµ::Real).\n\n[RÃ©nyi1960]: A. RÃ©nyi, Proceedings of the fourth Berkeley Symposium on Mathematics, Statistics and Probability, pp 547 (1960)\n\n[Shannon1948]: C. E. Shannon, Bell Systems Technical Journal 27, pp 379 (1948)\n\n\n\n\n\n","category":"function"},{"location":"#Fast-histograms","page":"Entropies.jl","title":"Fast histograms","text":"","category":"section"},{"location":"","page":"Entropies.jl","title":"Entropies.jl","text":"Entropies.binhist","category":"page"},{"location":"#Entropies.binhist","page":"Entropies.jl","title":"Entropies.binhist","text":"binhist(x::Dataset, Îµ::Real) â†’ p, bins\nbinhist(x::Dataset, Îµ::RectangularBinning) â†’ p, bins\n\nHyper-optimized histogram calculation for x with rectangular binning Îµ. Returns the probabilities p of each bin of the histogram as well as the bins. Notice that bins are the starting corners of each bin. If Îµ isa Real, then the actual bin size is Îµ across each dimension. If Îµ isa RectangularBinning, then the bin size for each dimension will depend on the binning scheme.\n\nSee also: RectangularBinning.\n\n\n\n\n\n","category":"function"},{"location":"NaiveKernel/#Kernel-density","page":"Kernel density","title":"Kernel density","text":"","category":"section"},{"location":"NaiveKernel/","page":"Kernel density","title":"Kernel density","text":"NaiveKernel","category":"page"},{"location":"NaiveKernel/#Entropies.NaiveKernel","page":"Kernel density","title":"Entropies.NaiveKernel","text":"NaiveKernel(Ïµ::Real, method::KernelEstimationMethod = TreeDistance()) <: ProbabilitiesEstimator\n\nEstimate probabilities/entropy using a \"naive\" kernel density estimation approach (KDE), as  discussed in Prichard and Theiler (1995) [PrichardTheiler1995].\n\nProbabilities P(mathbfx epsilon) are assigned to every point mathbfx by  counting how many other points occupy the space spanned by  a hypersphere of radius Ïµ around mathbfx, according to:\n\nP_i( mathbfx epsilon) approx dfrac1N sum_s neq i  Kleft( dfracmathbfx_i - mathbfx_s epsilon right)\n\nwhere K(z) = 1 if z  1 and zero otherwise. Probabilities are then normalized.\n\nMethods\n\nTree-based evaluation of distances using TreeDistance. Faster, but more   memory allocation.\nDirect evaluation of distances using DirectDistance. Slower, but less    memory allocation. Also works for complex numbers.\n\nEstimation\n\nProbabilities or entropies can be estimated from Datasets.\n\nprobabilities(x::AbstractDataset, est::NaiveKernel). Associates a probability p to    each point in x.\ngenentropy(x::AbstractDataset, est::NaiveKernel).  Associate probability p to each    point in x, then compute the generalized entropy from those probabilities.\n\nExamples\n\nusing Entropy, DelayEmbeddings\npts = Dataset([rand(5) for i = 1:10000]);\nÏµ = 0.2\nest_direct = NaiveKernel(Ïµ, DirectDistance())\nest_tree = NaiveKernel(Ïµ, TreeDistance())\n\np_direct = probabilities(pts, est_direct)\np_tree = probabilities(pts, est_tree)\n\n# Check that both methods give the same probabilities\nall(p_direct .== p_tree)\n\nSee also: DirectDistance, TreeDistance.\n\n[PrichardTheiler1995]: Prichard, D., & Theiler, J. (1995). Generalized redundancies for time series analysis. Physica D: Nonlinear Phenomena, 84(3-4), 476-493.\n\n\n\n\n\n","category":"type"},{"location":"NaiveKernel/#Distance-evaluation-methods","page":"Kernel density","title":"Distance evaluation methods","text":"","category":"section"},{"location":"NaiveKernel/","page":"Kernel density","title":"Kernel density","text":"TreeDistance\nDirectDistance","category":"page"},{"location":"NaiveKernel/#Entropies.TreeDistance","page":"Kernel density","title":"Entropies.TreeDistance","text":"TreeDistance(metric::M = Euclidean()) <: KernelEstimationMethod\n\nPairwise distances are evaluated using a tree-based approach with the provided metric.\n\n\n\n\n\n","category":"type"},{"location":"NaiveKernel/#Entropies.DirectDistance","page":"Kernel density","title":"Entropies.DirectDistance","text":"DirectDistance(metric::M = Euclidean()) <: KernelEstimationMethod\n\nPairwise distances are evaluated directly using the provided metric.\n\n\n\n\n\n","category":"type"},{"location":"NaiveKernel/#Example","page":"Kernel density","title":"Example","text":"","category":"section"},{"location":"NaiveKernel/","page":"Kernel density","title":"Kernel density","text":"Here, we draw some random points from a 2D normal distribution. Then, we use kernel  density estimation to associate a probability to each point p, measured by how many  points are within radius 1.5 of p. Plotting the actual points, along with their  associated probabilities estimated by the KDE procedure, we get the following surface  plot.","category":"page"},{"location":"NaiveKernel/","page":"Kernel density","title":"Kernel density","text":"using Distributions, PyPlot, DelayEmbeddings, Entropies\nð’© = MvNormal([1, -4], 2)\nN = 500\nD = Dataset(sort([rand(ð’©) for i = 1:N]))\nx, y = columns(D)\np = probabilities(D, NaiveKernel(1.5))\nsurf(x, y, p.p)\nxlabel(\"x\"); ylabel(\"y\")\nsavefig(\"kernel_surface.png\")","category":"page"},{"location":"NaiveKernel/","page":"Kernel density","title":"Kernel density","text":"(Image: )","category":"page"},{"location":"NearestNeighbors/#Nearest-neighbor-estimators","page":"Nearest neighbor estimators","title":"Nearest neighbor estimators","text":"","category":"section"},{"location":"NearestNeighbors/#Kraskov","page":"Nearest neighbor estimators","title":"Kraskov","text":"","category":"section"},{"location":"NearestNeighbors/","page":"Nearest neighbor estimators","title":"Nearest neighbor estimators","text":"Kraskov","category":"page"},{"location":"NearestNeighbors/#Entropies.Kraskov","page":"Nearest neighbor estimators","title":"Entropies.Kraskov","text":"k-th nearest neighbour(kNN) based\n\nKraskov(k::Int = 1, w::Int = 1) <: NearestNeighborEntropyEstimator\n\nEntropy estimator based on k-th nearest neighbor searches[Kraskov2004].\n\nw is the number of nearest neighbors to exclude when searching for neighbours  (defaults to 0, meaning that only the point itself is excluded).\n\ninfo: Info\nThis estimator is only available for entropy estimation. Probabilities  cannot be obtained directly.\n\n[Kraskov2004]: Kraskov, A., StÃ¶gbauer, H., & Grassberger, P. (2004). Estimating mutual information. Physical review E, 69(6), 066138.\n\n\n\n\n\n","category":"type"},{"location":"NearestNeighbors/#Kozachenko-Leonenko","page":"Nearest neighbor estimators","title":"Kozachenko-Leonenko","text":"","category":"section"},{"location":"NearestNeighbors/","page":"Nearest neighbor estimators","title":"Nearest neighbor estimators","text":"KozachenkoLeonenko","category":"page"},{"location":"NearestNeighbors/#Entropies.KozachenkoLeonenko","page":"Nearest neighbor estimators","title":"Entropies.KozachenkoLeonenko","text":"Nearest neighbour(NN) based\n\nKozachenkoLeonenko(; w::Int = 0) <: NearestNeighborEntropyEstimator\n\nEntropy estimator based on nearest neighbors. This implementation is based on Kozachenko  & Leonenko (1987)[KozachenkoLeonenko1987], as described in CharzyÅ„ska and Gambin (2016)[CharzyÅ„ska2016].\n\nw is the Theiler window (defaults to 0, meaning that only the point itself is excluded when searching for neighbours).\n\ninfo: Info\nThis estimator is only available for entropy estimation. Probabilities  cannot be obtained directly.\n\n[CharzyÅ„ska2016]: CharzyÅ„ska, A., & Gambin, A. (2016). Improvement of the k-NN entropy estimator with applications in systems biology. Entropy, 18(1), 13.\n\n[KozachenkoLeonenko1987]: Kozachenko, L. F., & Leonenko, N. N. (1987). Sample estimate of the entropy of a random vector. Problemy Peredachi Informatsii, 23(2), 9-16.\n\n\n\n\n\n","category":"type"},{"location":"NearestNeighbors/#Example","page":"Nearest neighbor estimators","title":"Example","text":"","category":"section"},{"location":"NearestNeighbors/","page":"Nearest neighbor estimators","title":"Nearest neighbor estimators","text":"This example reproduces Figure in CharzyÅ„ska & Gambin (2016)[CharzyÅ„ska2016]. Both  estimators nicely converge to the true entropy with increasing time series length.  For a uniform 1D distribution U(0 1), the true entropy is 0 (red line).","category":"page"},{"location":"NearestNeighbors/","page":"Nearest neighbor estimators","title":"Nearest neighbor estimators","text":"using Entropies, DelayEmbeddings, StatsBase\nimport Distributions: Uniform, Normal\n\nNs = [100:100:500; 1000:1000:10000]\nEkl = Vector{Vector{Float64}}(undef, 0)\nEkr = Vector{Vector{Float64}}(undef, 0)\n\nest_nn = KozachenkoLeonenko(w = 0)\n# with k = 1, Kraskov is virtually identical to KozachenkoLeonenko, so pick a higher \n# number of neighbors\nest_knn = Kraskov(w = 0, k = 3)\n\nnreps = 50\nfor N in Ns\n    kl = Float64[]\n    kr = Float64[]\n    for i = 1:nreps\n        pts = Dataset([rand(Uniform(0, 1), 1) for i = 1:N]);\n        push!(kl, genentropy(pts, est_nn))\n         # with k = 1 almost identical\n        push!(kr, genentropy(pts, est_knn))\n    end\n    push!(Ekl, kl)\n    push!(Ekr, kr)\nend\n\n# Plot\nusing PyPlot, StatsBase\nf = figure(figsize = (5,6))\nax = subplot(211)\npx = PyPlot.plot(Ns, mean.(Ekl); color = \"C1\", label = \"KozachenkoLeonenko\"); \nPyPlot.plot(Ns, mean.(Ekl) .+ StatsBase.std.(Ekl); color = \"C1\", label = \"\"); \nPyPlot.plot(Ns, mean.(Ekl) .- StatsBase.std.(Ekl); color = \"C1\", label = \"\"); \n\nxlabel(\"Time step\"); ylabel(\"Entropy (nats)\"); legend()\nay = subplot(212)\npy = PyPlot.plot(Ns, mean.(Ekr); color = \"C2\", label = \"Kraskov\"); \nPyPlot.plot(Ns, mean.(Ekr) .+ StatsBase.std.(Ekr); color = \"C2\", label = \"\"); \nPyPlot.plot(Ns, mean.(Ekr) .- StatsBase.std.(Ekr); color = \"C2\", label = \"\"); \n\nxlabel(\"Time step\"); ylabel(\"Entropy (nats)\"); legend()\ntight_layout()\nPyPlot.savefig(\"nn_entropy_example.png\")","category":"page"},{"location":"NearestNeighbors/","page":"Nearest neighbor estimators","title":"Nearest neighbor estimators","text":"(Image: )","category":"page"},{"location":"NearestNeighbors/","page":"Nearest neighbor estimators","title":"Nearest neighbor estimators","text":"[CharzyÅ„ska2016]: CharzyÅ„ska, A., & Gambin, A. (2016). Improvement of the k-NN entropy estimator with applications in systems biology. Entropy, 18(1), 13.","category":"page"},{"location":"SymbolicPermutation/#Permutation-(symbolic)","page":"Permutation (symbolic)","title":"Permutation (symbolic)","text":"","category":"section"},{"location":"SymbolicPermutation/","page":"Permutation (symbolic)","title":"Permutation (symbolic)","text":"SymbolicPermutation","category":"page"},{"location":"SymbolicPermutation/#Entropies.SymbolicPermutation","page":"Permutation (symbolic)","title":"Entropies.SymbolicPermutation","text":"SymbolicPermutation(; Ï„ = 1, m = 3) <: PermutationProbabilityEstimator\n\nA symbolic, permutation based probabilities/entropy estimator. \n\nProperties of original signal preserved\n\nPermutations of a signal preserve ordinal patterns (sorting information). This implementation is based on Bandt & Pompe et al. (2002)[BandtPompe2002] and  Berger et al. (2019) [Berger2019].\n\nEstimation\n\nUnivariate time series\n\nTo estimate probabilities or entropies from univariate time series, use the following methods:\n\nprobabilities(x::AbstractVector, est::SymbolicPermutation). Constructs state vectors    from x using embedding lag Ï„ and embedding dimension m. The ordinal patterns of the    state vectors are then symbolized, and probabilities are taken as the relative    frequency of symbols.\ngenentropy(x::AbstractVector, est::SymbolicPermutation; Î±=1, base = 2) computes   probabilities by calling probabilities(x::AbstractVector, est::SymbolicPermutation),   then computer the order-Î± generalized entropy to the given base.\n\nSee below for in-place versions below allow you to provide a pre-allocated symbol array s for faster repeated computations of input data of the same length.\n\ninfo: Default embedding dimension and embedding lag\nBy default, embedding dimension m = 3 with embedding lag tau = 1 is used when embedding a time series for symbolization. You should probably make a more informed decision about embedding parameters when computing the permutation entropy of a real time series. In all cases, m must be at least 2 (there are no permutations of a single-element state vector, so need m geq 2).\n\nMultivariate datasets\n\nAlthough not dealt with in the original Bandt & Pompe (2002) paper, numerically speaking,  permutation entropy can also be computed for multivariate datasets with dimension â‰¥ 2. Such datasets may be, for example, preembedded time series. Then, just skip the delay  reconstruction step, compute and symbols directly from the L existing state vectors  mathbfx_1 mathbfx_2 ldots mathbfx_L.\n\nprobabilities(x::Dataset, est::SymbolicPermutation). Compute ordinal patterns of the    state vectors of x directly (without doing any embedding), symbolize those patterns,   and compute probabilities as relative frequencies of symbols.\ngenentropy(x::Dataset, est::SymbolicPermutation). Computes probabilities from    symbol frequencies using probabilities(x::Dataset, est::SymbolicPermutation),   then computes the order-Î± generalized (permutation) entropy to the given base.\n\nwarn: Dynamical interpretation\nA dynamical interpretation of the permutation entropy does not necessarily hold if computing it on generic multivariate datasets. Method signatures for Datasets are provided for convenience, and should only be applied if you understand the relation between your input data, the numerical value for the permutation entropy, and its interpretation.\n\nSpeeding up repeated computations\n\ntip: Tip\nA pre-allocated integer symbol array s can be provided to save some memory  allocations if the probabilities are to be computed for multiple data sets.Note: it is not the array that will hold the final probabilities that is pre-allocated, but the temporary integer array containing the symbolized data points. Thus, if provided, it is required that length(x) == length(s) if x is a Dataset, or length(s) == length(x) - (m-1)Ï„ if x is a univariate signal that is to be embedded first.Use the following signatures.probabilities!(s::Vector{Int}, x::AbstractVector, est::SymbolicPermutation) â†’ ps::Probabilities\nprobabilities!(s::Vector{Int}, x::AbstractDataset, est::SymbolicPermutation) â†’ ps::Probabilities\n\nDescription\n\nEmbedding, ordinal patterns and symbolization\n\nConsider the n-element univariate time series x(t) = x_1 x_2 ldots x_n. Let mathbfx_i^m tau = x_j x_j+tau ldots x_j+(m-1)tau for j = 1 2 ldots n - (m-1)tau be the i-th state vector in a delay reconstruction with embedding dimension m and reconstruction lag tau. There are then N = n - (m-1)tau state vectors.\n\nFor an m-dimensional vector, there are m possible ways of sorting it in ascending order of magnitude. Each such possible sorting ordering is called a motif. Let pi_i^m tau denote the motif associated with the m-dimensional state vector mathbfx_i^m tau, and let R be the number of distinct motifs that can be constructed from the N state vectors. Then there are at most R motifs; R = N precisely when all motifs are unique, and R = 1 when all motifs are the same.\n\nEach unique motif pi_i^m tau can be mapped to a unique integer symbol 0 leq s_i leq M-1. Let S(pi)  mathbbR^m to mathbbN_0 be the function that maps the motif pi to its symbol s, and let Pi denote the set of symbols Pi =  s_i _iin  1 ldots R.\n\nProbability computation\n\nThe probability of a given motif is its frequency of occurrence, normalized by the total number of motifs (with notation from [Fadlallah2013]),\n\np(pi_i^m tau) = dfracsum_k=1^N mathbf1_uS(u) = s_i left(mathbfx_k^m tau right) sum_k=1^N mathbf1_uS(u) in Pi left(mathbfx_k^m tau right) = dfracsum_k=1^N mathbf1_uS(u) = s_i left(mathbfx_k^m tau right) N\n\nwhere the function mathbf1_A(u) is the indicator function of a set A. That     is, mathbf1_A(u) = 1 if u in A, and mathbf1_A(u) = 0 otherwise.\n\nEntropy computation\n\nThe generalized order-Î± Renyi entropy[RÃ©nyi1960] can be computed over the probability  distribution of symbols as  H(m tau alpha) = dfracalpha1-alpha log  left( sum_j=1^R p_j^alpha right). Permutation entropy, as described in  Bandt and Pompe (2002), is just the limiting case as Î± to1, that is H(m tau) = - sum_j^R p(pi_j^m tau) ln p(pi_j^m tau).\n\nhint: Generalized entropy order vs. permutation order\nDo not confuse the order of the generalized entropy (Î±) with the order m of the permutation entropy (m, which controls the symbol size). Permutation entropy is usually estimated with Î± = 1, but the implementation here allows the generalized entropy of any dimension to be computed from the symbol frequency distribution.\n\n[BandtPompe2002]: Bandt, Christoph, and Bernd Pompe. \"Permutation entropy: a natural complexity measure for time series.\" Physical review letters 88.17 (2002): 174102.\n\n[Berger2019]: Berger, Sebastian, et al. \"Teaching Ordinal Patterns to a Computer: Efficient Encoding Algorithms Based on the Lehmer Code.\" Entropy 21.10 (2019): 1023.\n\n[Fadlallah2013]: Fadlallah, Bilal, et al. \"Weighted-permutation entropy: A complexity measure for time series incorporating amplitude information.\" Physical Review E 87.2 (2013): 022911.\n\n[RÃ©nyi1960]: A. RÃ©nyi, Proceedings of the fourth Berkeley Symposium on Mathematics, Statistics and Probability, pp 547 (1960)\n\n\n\n\n\n","category":"type"},{"location":"SymbolicPermutation/#Example","page":"Permutation (symbolic)","title":"Example","text":"","category":"section"},{"location":"SymbolicPermutation/","page":"Permutation (symbolic)","title":"Permutation (symbolic)","text":"This example reproduces an example from Bandt and Pompe (2002), where the permutation entropy is compared with the largest Lyapunov exponents from time series of the chaotic  logistic map. Entropy estimates using SymbolicWeightedPermutation and SymbolicAmplitudeAwarePermutation are added here for comparison.","category":"page"},{"location":"SymbolicPermutation/","page":"Permutation (symbolic)","title":"Permutation (symbolic)","text":"using DynamicalSystems, PyPlot, Entropies\n\nds = Systems.logistic()\nrs = 3.4:0.001:4\nN_lyap, N_ent = 100000, 10000\nm, Ï„ = 6, 1 # Symbol size/dimension and embedding lag\n\n# Generate one time series for each value of the logistic parameter r\nlyaps = Float64[]\nhs_perm = Float64[]\nhs_wtperm = Float64[]\nhs_ampperm = Float64[]\n\nbase = Base.MathConstants.e\nfor r in rs\n    ds.p[1] = r\n    push!(lyaps, lyapunov(ds, N_lyap))\n\n    x = trajectory(ds, N_ent) # time series\n    hperm = Entropies.genentropy(x, SymbolicPermutation(m = m, Ï„ = Ï„), base = base)\n    hwtperm = Entropies.genentropy(x, SymbolicWeightedPermutation(m = m, Ï„ = Ï„), base = base)\n    hampperm = Entropies.genentropy(x, SymbolicAmplitudeAwarePermutation(m = m, Ï„ = Ï„), base = base)\n\n    push!(hs_perm, hperm); push!(hs_wtperm, hwtperm); push!(hs_ampperm, hampperm)\nend\n\nf = figure(figsize = (6, 8))\na1 = subplot(411)\nplot(rs, lyaps); ylim(-2, log(2)); ylabel(\"\\$\\\\lambda\\$\")\na1.axes.get_xaxis().set_ticklabels([])\nxlim(rs[1], rs[end]);\n\na2 = subplot(412)\nplot(rs, hs_perm; color = \"C2\"); xlim(rs[1], rs[end]);\nxlabel(\"\"); ylabel(\"\\$h_6 (SP)\\$\")\n\na3 = subplot(413)\nplot(rs, hs_wtperm; color = \"C3\"); xlim(rs[1], rs[end]);\nxlabel(\"\"); ylabel(\"\\$h_6 (SWP)\\$\")\n\na4 = subplot(414)\nplot(rs, hs_ampperm; color = \"C4\"); xlim(rs[1], rs[end]);\nxlabel(\"\\$r\\$\"); ylabel(\"\\$h_6 (SAAP)\\$\")\ntight_layout()\nsavefig(\"permentropy.png\")","category":"page"},{"location":"SymbolicPermutation/","page":"Permutation (symbolic)","title":"Permutation (symbolic)","text":"(Image: )","category":"page"},{"location":"SymbolicPermutation/#Utility-methods","page":"Permutation (symbolic)","title":"Utility methods","text":"","category":"section"},{"location":"SymbolicPermutation/","page":"Permutation (symbolic)","title":"Permutation (symbolic)","text":"Some convenience functions for symbolization are provided.","category":"page"},{"location":"SymbolicPermutation/","page":"Permutation (symbolic)","title":"Permutation (symbolic)","text":"symbolize\nencode_motif","category":"page"},{"location":"SymbolicPermutation/#Entropies.symbolize","page":"Permutation (symbolic)","title":"Entropies.symbolize","text":"symbolize(x::AbstractVector{T}, est::SymbolicPermutation) where {T} â†’ Vector{Int}\nsymbolize(x::AbstractDataset{m, T}, est::SymbolicPermutation) where {m, T} â†’ Vector{Int}\n\nIf x is an m-dimensional dataset, then symbolize x by converting each m-dimensional  state vector as a unique integer in the range 1 2 ldots m-1, using  encode_motif. \n\nIf x is a univariate time series, first x create a delay reconstruction of x using embedding lag est.Ï„ and embedding dimension est.m, then symbolizing the resulting  state vectors with encode_motif. \n\nExamples\n\nSymbolize a 7-dimensional dataset. Motif lengths (or order of the permutations) are  inferred to be 7.\n\nusing DelayEmbeddings, Entropies\nD = Dataset([rand(7) for i = 1:1000])\ns = symbolize(D, SymbolicPermutation())\n\nSymbolize a univariate time series by first embedding it in dimension 5 with embedding lag 2. Motif lengths (or order of the permutations) are therefore 5.\n\nusing DelayEmbeddings, Entropies\nn = 5000\nx = rand(n)\ns = symbolize(x, SymbolicPermutation(m = 5, Ï„ = 2))\n\nThe integer vector s now has length n-(m-1)*Ï„ = 4992, and each s[i] contains  the integer symbol for the ordinal pattern of state vector x[i].\n\n[Berger2019]: Berger, Sebastian, et al. \"Teaching Ordinal Patterns to a Computer: Efficient Encoding Algorithms Based on the Lehmer Code.\" Entropy 21.10 (2019): 1023.\n\n\n\n\n\n","category":"function"},{"location":"SymbolicPermutation/#Entropies.encode_motif","page":"Permutation (symbolic)","title":"Entropies.encode_motif","text":"encode_motif(x, m::Int = length(x)) â†’ Int\n\nEncode the length-m motif x (a vector of indices that would sort some vector v in ascending order)  into its unique integer symbol, using Algorithm 1 in Berger et al. (2019)[Berger2019].\n\nExample\n\nv = rand(5)\n\n# The indices that would sort `v` in ascending order. This is now a permutation \n# of the index permutation (1, 2, ..., 5)\nx = sortperm(v)\n\n# Encode this permutation as an integer.\nencode_motif(x)\n\n[Berger2019]: Berger, Sebastian, et al. \"Teaching Ordinal Patterns to a Computer: Efficient Encoding Algorithms Based on the Lehmer Code.\" Entropy 21.10 (2019): 1023.\n\n\n\n\n\n","category":"function"},{"location":"TimeScaleMODWT/#Time-scale-(wavelet)","page":"Time-scale (wavelet)","title":"Time-scale (wavelet)","text":"","category":"section"},{"location":"TimeScaleMODWT/","page":"Time-scale (wavelet)","title":"Time-scale (wavelet)","text":"TimeScaleMODWT","category":"page"},{"location":"TimeScaleMODWT/#Entropies.TimeScaleMODWT","page":"Time-scale (wavelet)","title":"Entropies.TimeScaleMODWT","text":"TimeScaleMODWT <: WaveletProbabilitiesEstimator\nTimeScaleMODWT(wl::Wavelets.WT.OrthoWaveletClass = Wavelets.WT.Daubechies{12}())\n\nApply the maximal overlap discrete wavelet transform (MODWT) to a signal, then compute probabilities/entropy from the energies at different wavelet scales. This implementation is based on Rosso et al. (2001)[Rosso2001]. Optionally specify a wavelet to be used.\n\nThe probability p[i] is the relative/total energy for the i-th wavelet scale.\n\nExample\n\nManually picking a wavelet is done as follows.\n\nusing Entropies, Wavelets\nN = 200\na = 10\nt = LinRange(0, 2*a*Ï€, N)\nx = sin.(t .+  cos.(t/0.1)) .- 0.1;\n\n# Pick a wavelet (if no wavelet provided, defaults to Wavelets.WL.Daubechies{12}())\nwl = Wavelets.WT.Daubechies{12}()\n\n# Compute the probabilities (relative energies) at the different wavelet scales\nprobabilities(x, TimeScaleMODWT(wl))\n\nIf no wavelet provided, the default is Wavelets.WL.Daubechies{12}()).\n\n[Rosso2001]: Rosso, O. A., Blanco, S., Yordanova, J., Kolev, V., Figliola, A., SchÃ¼rmann, M., & BaÅŸar, E. (2001). Wavelet entropy: a new tool for analysis of short duration brain electrical signals. Journal of neuroscience methods, 105(1), 65-75.\n\n\n\n\n\n","category":"type"},{"location":"TimeScaleMODWT/#Example","page":"Time-scale (wavelet)","title":"Example","text":"","category":"section"},{"location":"TimeScaleMODWT/","page":"Time-scale (wavelet)","title":"Time-scale (wavelet)","text":"The scale-resolved wavelet entropy should be lower for very regular signals (most of the  energy is contained at one scale) and higher for very irregular signals (energy spread more out across scales).","category":"page"},{"location":"TimeScaleMODWT/","page":"Time-scale (wavelet)","title":"Time-scale (wavelet)","text":"using Entropies, PyPlot\nN, a = 1000, 10\nt = LinRange(0, 2*a*Ï€, N)\n\nx = sin.(t);\ny = sin.(t .+  cos.(t/0.5));\nz = sin.(rand(1:15, N) ./ rand(1:10, N))\n\nest = TimeScaleMODWT()\nh_x = Entropies.genentropy(x, est)\nh_y = Entropies.genentropy(y, est)\nh_z = Entropies.genentropy(z, est)\n\nf = figure(figsize = (10,6))\nax = subplot(311)\npx = plot(t, x; color = \"C1\", label = \"h=$(h=round(h_x, sigdigits = 5))\"); \nylabel(\"x\"); legend()\nay = subplot(312)\npy = plot(t, y; color = \"C2\", label = \"h=$(h=round(h_y, sigdigits = 5))\"); \nylabel(\"y\"); legend()\naz = subplot(313)\npz = plot(t, z; color = \"C3\", label = \"h=$(h=round(h_z, sigdigits = 5))\"); \nylabel(\"z\"); xlabel(\"Time\"); legend()\ntight_layout()\nsavefig(\"waveletentropy.png\")","category":"page"},{"location":"TimeScaleMODWT/","page":"Time-scale (wavelet)","title":"Time-scale (wavelet)","text":"(Image: )","category":"page"},{"location":"SymbolicAmplitudeAwarePermutation/#Amplitude-aware-permutation-(symbolic)","page":"Amplitude-aware permutation (symbolic)","title":"Amplitude-aware permutation (symbolic)","text":"","category":"section"},{"location":"SymbolicAmplitudeAwarePermutation/","page":"Amplitude-aware permutation (symbolic)","title":"Amplitude-aware permutation (symbolic)","text":"SymbolicAmplitudeAwarePermutation","category":"page"},{"location":"SymbolicAmplitudeAwarePermutation/#Entropies.SymbolicAmplitudeAwarePermutation","page":"Amplitude-aware permutation (symbolic)","title":"Entropies.SymbolicAmplitudeAwarePermutation","text":"SymbolicAmplitudeAwarePermutation(; Ï„ = 1, m = 3, A = 0.5) <: PermutationProbabilityEstimator\n\nA symbolic, amplitude-aware permutation based probabilities/entropy estimator.\n\nProperties of original signal preserved\n\nAmplitude-aware permutations of a signal preserve not only ordinal patterns (sorting  information), but also encodes amplitude information (see description below for explanation  of the parameter A). This implementation is based on Azami & Escudero (2016) [Azami2016].\n\nEstimation\n\nUnivariate time series\n\nTo estimate probabilities or entropies from univariate time series, use the following methods:\n\nprobabilities(x::AbstractVector, est::SymbolicAmplitudeAwarePermutation). Constructs state vectors    from x using embedding lag Ï„ and embedding dimension m. The ordinal patterns of the    state vectors are then symbolized, and probabilities are taken as the relative    frequency of symbols.\ngenentropy(x::AbstractVector, est::SymbolicAmplitudeAwarePermutation; Î±=1, base = 2) computes   probabilities by calling probabilities(x::AbstractVector, est::SymbolicAmplitudeAwarePermutation),   then computer the order-Î± generalized entropy to the given base.\n\nSee below for in-place versions below allow you to provide a pre-allocated symbol array s for faster repeated computations of input data of the same length.\n\ninfo: Default embedding dimension and embedding lag\nBy default, embedding dimension m = 3 with embedding lag tau = 1 is used when embedding a time series for symbolization. You should probably make a more informed decision about embedding parameters when computing the permutation entropy of a real time series. In all cases, m must be at least 2 (there are no permutations of a single-element state vector, so need m geq 2).\n\nMultivariate datasets\n\nAs for regular permutation entropy, numerically speaking,  amplitude-adjusted permutation entropy can also be computed for multivariate datasets with  dimension â‰¥ 2. Such datasets may be, for example, preembedded time series. Then, just skip the delay  reconstruction step, compute and symbols directly from the L existing state vectors  mathbfx_1 mathbfx_2 ldots mathbfx_L.\n\nprobabilities(x::Dataset, est::SymbolicAmplitudeAwarePermutation). Compute ordinal patterns of the    state vectors of x directly (without doing any embedding), symbolize those patterns,   and compute probabilities as relative frequencies of symbols.\ngenentropy(x::Dataset, est::SymbolicAmplitudeAwarePermutation). Computes probabilities from    symbol frequencies using probabilities(x::Dataset, est::SymbolicAmplitudeAwarePermutation),   then computes the order-Î± generalized (permutation) entropy to the given base.\n\nwarn: Dynamical interpretation\nA dynamical interpretation of the permutation entropy does not necessarily hold if computing it on generic multivariate datasets. Method signatures for Datasets are provided for convenience, and should only be applied if you understand the relation between your input data, the numerical value for the permutation entropy, and its interpretation.\n\nDescription\n\nEmbedding, ordinal patterns and symbolization\n\nConsider the n-element univariate time series x(t) = x_1 x_2 ldots x_n.  Let mathbfx_i^m tau = x_j x_j+tau ldots x_j+(m-1)tau for  j = 1 2 ldots n - (m-1)tau be the i-th state vector in a delay reconstruction  with embedding dimension m and reconstruction lag tau. There are then  N = n - (m-1)tau state vectors. \n\nFor an m-dimensional vector, there are m possible ways of sorting it in ascending  order of magnitude. Each such possible sorting ordering is called a motif.  Let pi_i^m tau denote the motif associated with the m-dimensional state  vector mathbfx_i^m tau, and let R be the number of distinct motifs that  can be constructed from the N state vectors. Then there are at most R motifs;  R = N precisely when all motifs are unique, and R = 1 when all motifs are the same.  Each unique motif pi_i^m tau can be mapped to a unique integer symbol  0 leq s_i leq M-1. Let S(pi)  mathbbR^m to mathbbN_0 be the  function that maps the motif pi to its symbol s, and let Pi denote the set      of symbols Pi =  s_i _iin  1 ldots R.\n\nProbability computation\n\nAmplitude-aware permutation entropy is computed analogously to regular permutation entropy (see SymbolicPermutation), but probabilities are weighted by amplitude information as follows.\n\np(pi_i^m tau) = dfracsum_k=1^N mathbf1_uS(u) = s_i left( mathbfx_k^m tau right)  a_ksum_k=1^N mathbf1_uS(u) in Pi left( mathbfx_k^m tau right) a_k = dfracsum_k=1^N mathbf1_uS(u) = s_i left( mathbfx_k^m tau right)  a_ksum_k=1^N a_k\n\nThe weights encoding amplitude information about state vector mathbfx_i = (x_1^i x_2^i ldots x_m^i) are \n\na_i = dfracAm sum_k=1^m x_k^i  + dfrac1-Ad-1 sum_k=2^d x_k^i - x_k-1^i\n\nwith 0 leq A leq 1. When A=0 , only internal differences between the elements of  mathbfx_i are weighted. Only mean amplitude of the state vector  elements are weighted when A=1. With, 0A1, a combined weighting is used.\n\nEntropy computation\n\nThe generalized order-Î± Renyi entropy[RÃ©nyi1960] can be computed over the probability  distribution of symbols as  H(m tau alpha) = dfracalpha1-alpha log  left( sum_j=1^R p_j^alpha right). Permutation entropy, as described in  Bandt and Pompe (2002), is just the limiting case as Î± to1, that is H(m tau) = - sum_j^R p(pi_j^m tau) ln p(pi_j^m tau).\n\nhint: Generalized entropy order vs. permutation order\nDo not confuse the order of the generalized entropy (Î±) with the order m of the permutation entropy (m, which controls the symbol size). Amplitude-aware permutation entropy is usually estimated with Î± = 1, but the implementation here  allows the generalized entropy of any dimension to be computed from the symbol  frequency distribution.\n\n[Azami2016]: Azami, H., & Escudero, J. (2016). Amplitude-aware permutation entropy: Illustration in spike detection and signal segmentation. Computer methods and programs in biomedicine, 128, 40-51.\n\n\n\n\n\n","category":"type"},{"location":"CountOccurrences/#CountOccurrences-(counting)","page":"CountOccurrences (counting)","title":"CountOccurrences (counting)","text":"","category":"section"},{"location":"CountOccurrences/","page":"CountOccurrences (counting)","title":"CountOccurrences (counting)","text":"CountOccurrences","category":"page"},{"location":"CountOccurrences/#Entropies.CountOccurrences","page":"CountOccurrences (counting)","title":"Entropies.CountOccurrences","text":"CountOccurrences  <: CountingBasedProbabilityEstimator\n\nA probabilities/entropy estimator based on straight-forward counting of distinct elements in a univariate time series or multivariate dataset. From these counts, construct histograms. Sum-normalize histograms to obtain probability distributions.\n\n\n\n\n\n","category":"type"},{"location":"SymbolicWeightedPermutation/#Weighted-permutation-(symbolic)","page":"Weighted permutation (symbolic)","title":"Weighted permutation (symbolic)","text":"","category":"section"},{"location":"SymbolicWeightedPermutation/","page":"Weighted permutation (symbolic)","title":"Weighted permutation (symbolic)","text":"SymbolicWeightedPermutation","category":"page"},{"location":"SymbolicWeightedPermutation/#Entropies.SymbolicWeightedPermutation","page":"Weighted permutation (symbolic)","title":"Entropies.SymbolicWeightedPermutation","text":"SymbolicWeightedPermutation(; Ï„ = 1, m = 3) <: PermutationProbabilityEstimator\n\nA symbolic, weighted permutation based probabilities/entropy estimator.\n\nProperties of original signal preserved\n\nWeighted permutations of a signal preserve not only ordinal patterns (sorting information), but also encodes amplitude information. This implementation is based on Fadlallah et al. (2013)[Fadlallah2013].\n\nEstimation\n\nUnivariate time series\n\nTo estimate probabilities or entropies from univariate time series, use the following methods:\n\nprobabilities(x::AbstractVector, est::SymbolicWeightedPermutation). Constructs state vectors    from x using embedding lag Ï„ and embedding dimension m. The ordinal patterns of the    state vectors are then symbolized, and probabilities are taken as the (weighted) relative    frequency of symbols.\ngenentropy(x::AbstractVector, est::SymbolicWeightedPermutation; Î±=1, base = 2) computes   weighted probabilities by calling probabilities(x::AbstractVector, est::SymbolicPermutation),   then computer the order-Î± generalized entropy to the given base.\n\nSee below for in-place versions below allow you to provide a pre-allocated symbol array s for faster repeated computations of input data of the same length.\n\ninfo: Default embedding dimension and embedding lag\nBy default, embedding dimension m = 3 with embedding lag tau = 1 is used when embedding a time series for symbolization. You should probably make a more informed decision about embedding parameters when computing the permutation entropy of a real time series. In all cases, m must be at least 2 (there are no permutations of a single-element state vector, so need m geq 2).\n\nMultivariate datasets\n\nAs for regular permutation entropy, numerically speaking, weighted permutation entropy  can also be computed for multivariate datasets with dimension â‰¥ 2. Such datasets may be, for example, preembedded time series. Then, just skip the delay  reconstruction step, compute and symbols directly from the L existing state vectors  mathbfx_1 mathbfx_2 ldots mathbfx_L.\n\nprobabilities(x::Dataset, est::SymbolicWeightedPermutation). Compute ordinal patterns of the    state vectors of x directly (without doing any embedding), symbolize those patterns,   and compute probabilities as relative frequencies of symbols.\ngenentropy(x::Dataset, est::SymbolicWeightedPermutation). Computes probabilities from    symbol frequencies using probabilities(x::Dataset, est::SymbolicWeightedPermutation),   then computes the order-Î± generalized (permutation) entropy to the given base.\n\nwarn: Dynamical interpretation\nA dynamical interpretation of the permutation entropy does not necessarily hold if computing it on generic multivariate datasets. Method signatures for Datasets are provided for convenience, and should only be applied if you understand the relation between your input data, the numerical value for the permutation entropy, and its interpretation.\n\nDescription\n\nEmbedding, ordinal patterns and symbolization\n\nConsider the n-element univariate time series x(t) = x_1 x_2 ldots x_n. Let mathbfx_i^m tau = x_j x_j+tau ldots x_j+(m-1)tau for j = 1 2 ldots n - (m-1)tau be the i-th state vector in a delay reconstruction with embedding dimension m and reconstruction lag tau. There are then N = n - (m-1)tau state vectors.\n\nFor an m-dimensional vector, there are m possible ways of sorting it in ascending order of magnitude. Each such possible sorting ordering is called a motif. Let pi_i^m tau denote the motif associated with the m-dimensional state vector mathbfx_i^m tau, and let R be the number of distinct motifs that can be constructed from the N state vectors. Then there are at most R motifs; R = N precisely when all motifs are unique, and R = 1 when all motifs are the same. Each unique motif pi_i^m tau can be mapped to a unique integer symbol 0 leq s_i leq M-1. Let S(pi)  mathbbR^m to mathbbN_0 be the function that maps the motif pi to its symbol s, and let Pi denote the set     of symbols Pi =  s_i _iin  1 ldots R.\n\nProbability computation\n\nWeighted permutation entropy is computed analogously to regular permutation entropy (see SymbolicPermutation), but adds weights that encode amplitude information too:\n\np(pi_i^m tau) = dfracsum_k=1^N mathbf1_uS(u) = s_i \nleft( mathbfx_k^m tau right) \n w_ksum_k=1^N mathbf1_uS(u) in Pi \nleft( mathbfx_k^m tau right) w_k = dfracsum_k=1^N \nmathbf1_uS(u) = s_i \nleft( mathbfx_k^m tau right)  w_ksum_k=1^N w_k\n\nThe weighted permutation entropy is equivalent to regular permutation entropy when weights are positive and identical (w_j = beta  forall  j leq N and beta  0). Weights are dictated by the variance of the state vectors.\n\nLet the aritmetic mean of state vector mathbfx_i be denoted by\n\nmathbfhatx_j^m tau = frac1m sum_k=1^m x_j + (k+1)tau\n\nWeights are then computed as\n\nw_j = dfrac1msum_k=1^m (x_j+(k+1)tau - mathbfhatx_j^m tau)^2\n\nquestion: Implementation details\nNote: in equation 7, section III, of the original paper, the authors writew_j = dfrac1msum_k=1^m (x_j-(k-1)tau - mathbfhatx_j^m tau)^2But given the formula they give for the arithmetic mean, this is not the variance of mathbfx_i, because the indices are mixed: x_j+(k-1)tau in the weights formula, vs. x_j+(k+1)tau in the arithmetic mean formula. This seems to imply that amplitude information about previous delay vectors are mixed with mean amplitude information about current vectors. The authors also mix the terms \"vector\" and \"neighboring vector\" (but uses the same notation for both), making it hard to interpret whether the sign switch is a typo or intended. Here, we use the notation above, which actually computes the variance for mathbfx_i.\n\nEntropy computation\n\nThe generalized order-Î± Renyi entropy[RÃ©nyi1960] can be computed over the probability  distribution of symbols as  H(m tau alpha) = dfracalpha1-alpha log  left( sum_j=1^R p_j^alpha right). Permutation entropy, as described in  Bandt and Pompe (2002), is just the limiting case as Î± to1, that is H(m tau) = - sum_j^R p(pi_j^m tau) ln p(pi_j^m tau).\n\nhint: Generalized entropy order vs. permutation order\nDo not confuse the order of the generalized entropy (Î±) with the order m of the permutation entropy (m, which controls the symbol size). Weighted permutation entropy is usually estimated with Î± = 1, but the implementation here allows the generalized entropy of any dimension to be computed from the symbol frequency distribution.\n\nSee also: SymbolicPermutation, SymbolicAmplitudeAwarePermutation.\n\n[Fadlallah2013]: Fadlallah, Bilal, et al. \"Weighted-permutation entropy: A complexity measure for time series incorporating amplitude information.\" Physical Review E 87.2 (2013): 022911.\n\n\n\n\n\n","category":"type"},{"location":"VisitationFrequency/#Visitation-frequency-(binning)","page":"Visitation frequency (binning)","title":"Visitation frequency (binning)","text":"","category":"section"},{"location":"VisitationFrequency/","page":"Visitation frequency (binning)","title":"Visitation frequency (binning)","text":"VisitationFrequency","category":"page"},{"location":"VisitationFrequency/#Entropies.VisitationFrequency","page":"Visitation frequency (binning)","title":"Entropies.VisitationFrequency","text":"VisitationFrequency(r::RectangularBinning) <: BinningProbabilitiesEstimator\n\nA probability estimator based on binning data into rectangular boxes dictated by the binning scheme r.\n\nExample\n\n# Construct boxes by dividing each coordinate axis into 5 equal-length chunks.\nb = RectangularBinning(5)\n\n# A probabilities estimator that, when applied a dataset, computes visitation frequencies\n# over the boxes of the binning, constructed as describedon the previous line.\nest = VisitationFrequency(b)\n\nSee also: RectangularBinning.\n\n\n\n\n\n","category":"type"},{"location":"VisitationFrequency/#Specifying-binning/boxes","page":"Visitation frequency (binning)","title":"Specifying binning/boxes","text":"","category":"section"},{"location":"VisitationFrequency/","page":"Visitation frequency (binning)","title":"Visitation frequency (binning)","text":"RectangularBinning","category":"page"},{"location":"VisitationFrequency/#Entropies.RectangularBinning","page":"Visitation frequency (binning)","title":"Entropies.RectangularBinning","text":"RectangularBinning(Ïµ) <: RectangularBinningScheme\n\nInstructions for creating a rectangular box partition using the binning scheme Ïµ.  Binning instructions are deduced from the type of Ïµ.\n\nRectangular binnings may be automatically adjusted to the data in which the RectangularBinning  is applied, as follows:\n\nÏµ::Int divides each coordinate axis into Ïµ equal-length intervals,   extending the upper bound 1/100th of a bin size to ensure all points are covered.\nÏµ::Float64 divides each coordinate axis into intervals of fixed size Ïµ, starting   from the axis minima until the data is completely covered by boxes.\nÏµ::Vector{Int} divides the i-th coordinate axis into Ïµ[i] equal-length   intervals, extending the upper bound 1/100th of a bin size to ensure all points are   covered.\nÏµ::Vector{Float64} divides the i-th coordinate axis into intervals of fixed size Ïµ[i], starting   from the axis minima until the data is completely covered by boxes.\n\nRectangular binnings may also be specified on arbitrary min-max ranges. \n\nÏµ::Tuple{Vector{Tuple{Float64,Float64}},Int64} creates intervals   along each coordinate axis from ranges indicated by a vector of (min, max) tuples, then divides   each coordinate axis into an integer number of equal-length intervals. Note: this does not ensure   that all points are covered by the data (points outside the binning are ignored).\n\nExample 1: Grid deduced automatically from data (partition guaranteed to cover data points)\n\nFlexible box sizes\n\nThe following binning specification finds the minima/maxima along each coordinate axis, then  split each of those data ranges (with some tiny padding on the edges) into 10 equal-length  intervals. This gives (hyper-)rectangular boxes, and works for data of any dimension.\n\nusing Entropies\nRectangularBinning(10)\n\nNow, assume the data consists of 2-dimensional points, and that we want a finer grid along one of the dimensions than over the other dimension.\n\nThe following binning specification finds the minima/maxima along each coordinate axis, then  splits the range along the first coordinate axis (with some tiny padding on the edges)  into 10 equal-length intervals, and the range along the second coordinate axis (with some  tiny padding on the edges) into 5 equal-length intervals. This gives (hyper-)rectangular boxes.\n\nusing Entropies\nRectangularBinning([10, 5])\n\nFixed box sizes\n\nThe following binning specification finds the minima/maxima along each coordinate axis,  then split the axis ranges into equal-length intervals of fixed size 0.5 until the all data  points are covered by boxes. This approach yields (hyper-)cubic boxes, and works for  data of any dimension.\n\nusing Entropies\nRectangularBinning(0.5)\n\nAgain, assume the data consists of 2-dimensional points, and that we want a finer grid along one of the dimensions than over the other dimension.\n\nThe following binning specification finds the minima/maxima along each coordinate axis, then splits the range along the first coordinate axis into equal-length intervals of size 0.3, and the range along the second axis into equal-length intervals of size 0.1 (in both cases,  making sure the data are completely covered by the boxes). This approach gives a (hyper-)rectangular boxes. \n\nusing Entropies\nRectangularBinning([0.3, 0.1])\n\nExample 2: Custom grids (partition not guaranteed to cover data points):\n\nAssume the data consists of 3-dimensional points (x, y, z), and that we want a grid  that is fixed over the intervals [xâ‚, xâ‚‚] for the first dimension, over [yâ‚, yâ‚‚] for the second dimension, and over [zâ‚, zâ‚‚] for the third dimension. We when want to split each of those ranges into 4 equal-length pieces. Beware: some points may fall  outside the partition if the intervals are not chosen properly (these points are  simply discarded). \n\nThe following binning specification produces the desired (hyper-)rectangular boxes. \n\nusing Entropies, DelayEmbeddings\n\nD = Dataset(rand(100, 3));\n\nxâ‚, xâ‚‚ = 0.5, 1 # not completely covering the data, which are on [0, 1]\nyâ‚, yâ‚‚ = -2, 1.5 # covering the data, which are on [0, 1]\nzâ‚, zâ‚‚ = 0, 0.5 # not completely covering the data, which are on [0, 1]\n\nÏµ = [(xâ‚, xâ‚‚), (yâ‚, yâ‚‚), (zâ‚, zâ‚‚)], 4 # [interval 1, interval 2, ...], n_subdivisions\n\nRectangularBinning(Ïµ)\n\n\n\n\n\n","category":"type"},{"location":"VisitationFrequency/#Utility-methods","page":"Visitation frequency (binning)","title":"Utility methods","text":"","category":"section"},{"location":"VisitationFrequency/","page":"Visitation frequency (binning)","title":"Visitation frequency (binning)","text":"Some convenience functions bin encoding are provided.","category":"page"},{"location":"VisitationFrequency/","page":"Visitation frequency (binning)","title":"Visitation frequency (binning)","text":"encode_as_bin\njoint_visits\nmarginal_visits","category":"page"},{"location":"VisitationFrequency/#Entropies.encode_as_bin","page":"Visitation frequency (binning)","title":"Entropies.encode_as_bin","text":"encode_as_bin(point, reference_point, edgelengths) â†’ Vector{Int}\n\nEncode a point into its integer bin labels relative to some reference_point (always counting from lowest to highest magnitudes), given a set of box  edgelengths (one for each axis). The first bin on the positive side of  the reference point is indexed with 0, and the first bin on the negative  side of the reference point is indexed with -1.\n\nSee also: joint_visits, marginal_visits.\n\nExample\n\nusing Entropies\n\nrefpoint = [0, 0, 0]\nsteps = [0.2, 0.2, 0.3]\nencode_as_bin(rand(3), refpoint, steps)\n\n\n\n\n\n","category":"function"},{"location":"VisitationFrequency/#Entropies.joint_visits","page":"Visitation frequency (binning)","title":"Entropies.joint_visits","text":"joint_visits(points, binning_scheme::RectangularBinning) â†’ Vector{Vector{Int}}\n\nDetermine which bins are visited by points given the rectangular binning scheme Ïµ. Bins are referenced relative to the axis minima, and are  encoded as integers, such that each box in the binning is assigned a unique integer array (one element for each dimension). \n\nFor example, if a bin is visited three times, then the corresponding  integer array will appear three times in the array returned.\n\nSee also: marginal_visits, encode_as_bin.\n\nExample\n\nusing DelayEmbeddings, Entropies\n\npts = Dataset([rand(5) for i = 1:100]);\njoint_visits(pts, RectangularBinning(0.2))\n\n\n\n\n\n","category":"function"},{"location":"VisitationFrequency/#Entropies.marginal_visits","page":"Visitation frequency (binning)","title":"Entropies.marginal_visits","text":"marginal_visits(points, binning_scheme::RectangularBinning, dims) â†’ Vector{Vector{Int}}\n\nDetermine which bins are visited by points given the rectangular binning scheme Ïµ, but only along the desired dimensions dims. Bins are referenced  relative to the axis minima, and are encoded as integers, such that each box  in the binning is assigned a unique integer array (one element for each  dimension in dims). \n\nFor example, if a bin is visited three times, then the corresponding  integer array will appear three times in the array returned.\n\nSee also: joint_visits, encode_as_bin.\n\nExample\n\nusing DelayEmbeddings, Entropies\npts = Dataset([rand(5) for i = 1:100]);\n\n# Marginal visits along dimension 3 and 5\nmarginal_visits(pts, RectangularBinning(0.3), [3, 5])\n\n# Marginal visits along dimension 2 through 5\nmarginal_visits(pts, RectangularBinning(0.3), 2:5)\n\n\n\n\n\nmarginal_visits(joint_visits, dims) â†’ Vector{Vector{Int}}\n\nIf joint visits have been precomputed using joint_visits, marginal  visits can be returned directly without providing the binning again  using the marginal_visits(joint_visits, dims) signature.\n\nSee also: joint_visits, encode_as_bin.\n\nExample\n\nusing DelayEmbeddings, Entropies\npts = Dataset([rand(5) for i = 1:100]);\n\n# First compute joint visits, then marginal visits along dimensions 1 and 4\njv = joint_visits(pts, RectangularBinning(0.2))\nmarginal_visits(jv, [1, 4])\n\n# Marginals along dimension 2\nmarginal_visits(jv, 2)\n\n\n\n\n\n","category":"function"}]
}
