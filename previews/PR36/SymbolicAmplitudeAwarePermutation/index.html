<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Amplitude-aware permutation (symbolic) · Entropies.jl</title><link href="https://fonts.googleapis.com/css?family=Lato|Roboto+Mono" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.11.2/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.11.2/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.11.2/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.11.1/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL=".."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../assets/documenter.js"></script><script src="../siteinfo.js"></script><script src="../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-dark.css" data-theme-name="documenter-dark"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../assets/themeswap.js"></script><link href="https://fonts.googleapis.com/css?family=Montserrat|Source+Code+Pro&amp;display=swap" rel="stylesheet" type="text/css"/></head><body><div id="documenter"><nav class="docs-sidebar"><div class="docs-package-name"><span class="docs-autofit">Entropies.jl</span></div><form class="docs-search" action="../search/"><input class="docs-search-query" id="documenter-search-query" name="q" type="text" placeholder="Search docs"/></form><ul class="docs-menu"><li><a class="tocitem" href="../">Entropies.jl</a></li><li><span class="tocitem">Estimators</span><ul><li><a class="tocitem" href="../CountOccurrences/">CountOccurrences (counting)</a></li><li><a class="tocitem" href="../VisitationFrequency/">Visitation frequency (binning)</a></li><li><a class="tocitem" href="../SymbolicPermutation/">Permutation (symbolic)</a></li><li><a class="tocitem" href="../SymbolicWeightedPermutation/">Weighted permutation (symbolic)</a></li><li class="is-active"><a class="tocitem" href>Amplitude-aware permutation (symbolic)</a></li><li><a class="tocitem" href="../NearestNeighbors/">Nearest neighbor estimators</a></li><li><a class="tocitem" href="../NaiveKernel/">Kernel density</a></li><li><a class="tocitem" href="../TimeScaleMODWT/">Time-scale (wavelet)</a></li></ul></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><nav class="breadcrumb"><ul class="is-hidden-mobile"><li><a class="is-disabled">Estimators</a></li><li class="is-active"><a href>Amplitude-aware permutation (symbolic)</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>Amplitude-aware permutation (symbolic)</a></li></ul></nav><div class="docs-right"><a class="docs-edit-link" href="https://github.com/JuliaDynamics/Entropies.jl/blob/master/docs/src/SymbolicAmplitudeAwarePermutation.md" title="Edit on GitHub"><span class="docs-icon fab"></span><span class="docs-label is-hidden-touch">Edit on GitHub</span></a><a class="docs-settings-button fas fa-cog" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-sidebar-button fa fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a></div></header><article class="content" id="documenter-page"><h1 id="Amplitude-aware-permutation-(symbolic)"><a class="docs-heading-anchor" href="#Amplitude-aware-permutation-(symbolic)">Amplitude-aware permutation (symbolic)</a><a id="Amplitude-aware-permutation-(symbolic)-1"></a><a class="docs-heading-anchor-permalink" href="#Amplitude-aware-permutation-(symbolic)" title="Permalink"></a></h1><article class="docstring"><header><a class="docstring-binding" id="Entropies.SymbolicAmplitudeAwarePermutation" href="#Entropies.SymbolicAmplitudeAwarePermutation"><code>Entropies.SymbolicAmplitudeAwarePermutation</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia">SymbolicAmplitudeAwarePermutation(; τ = 1, m = 3, A = 0.5) &lt;: PermutationProbabilityEstimator</code></pre><p>A symbolic, amplitude-aware permutation based probabilities/entropy estimator.</p><p><strong>Properties of original signal preserved</strong></p><p>Amplitude-aware permutations of a signal preserve not only ordinal patterns (sorting  information), but also encodes amplitude information (see description below for explanation  of the parameter <code>A</code>). This implementation is based on Azami &amp; Escudero (2016) <sup class="footnote-reference"><a id="citeref-Azami2016" href="#footnote-Azami2016">[Azami2016]</a></sup>.</p><p><strong>Estimation</strong></p><p><strong>Univariate time series</strong></p><p>To estimate probabilities or entropies from univariate time series, use the following methods:</p><ul><li><code>probabilities(x::AbstractVector, est::SymbolicAmplitudeAwarePermutation)</code>. Constructs state vectors    from <code>x</code> using embedding lag <code>τ</code> and embedding dimension <code>m</code>. The ordinal patterns of the    state vectors are then symbolized, and probabilities are taken as the relative    frequency of symbols.</li><li><code>genentropy(x::AbstractVector, est::SymbolicAmplitudeAwarePermutation; α=1, base = 2)</code> computes   probabilities by calling <code>probabilities(x::AbstractVector, est::SymbolicAmplitudeAwarePermutation)</code>,   then computer the order-<code>α</code> generalized entropy to the given base.</li></ul><p>See below for in-place versions below allow you to provide a pre-allocated symbol array <code>s</code> for faster repeated computations of input data of the same length.</p><div class="admonition is-info"><header class="admonition-header">Default embedding dimension and embedding lag</header><div class="admonition-body"><p>By default, embedding dimension <span>$m = 3$</span> with embedding lag <span>$\tau = 1$</span> is used when embedding a time series for symbolization. You should probably make a more informed decision about embedding parameters when computing the permutation entropy of a real time series. In all cases, <span>$m$</span> must be at least 2 (there are no permutations of a single-element state vector, so need <span>$m \geq 2$</span>).</p></div></div><p><strong>Multivariate datasets</strong></p><p>As for regular permutation entropy, numerically speaking,  amplitude-adjusted permutation entropy can also be computed for multivariate datasets with  dimension ≥ 2. Such datasets may be, for example, preembedded time series. Then, just skip the delay  reconstruction step, compute and symbols directly from the <span>$L$</span> existing state vectors  <span>$\{\mathbf{x}_1, \mathbf{x}_2, \ldots, \mathbf{x_L}\}$</span>.</p><ul><li><code>probabilities(x::Dataset, est::SymbolicAmplitudeAwarePermutation)</code>. Compute ordinal patterns of the    state vectors of <code>x</code> directly (without doing any embedding), symbolize those patterns,   and compute probabilities as relative frequencies of symbols.</li><li><code>genentropy(x::Dataset, est::SymbolicAmplitudeAwarePermutation)</code>. Computes probabilities from    symbol frequencies using <code>probabilities(x::Dataset, est::SymbolicAmplitudeAwarePermutation)</code>,   then computes the order-<code>α</code> generalized (permutation) entropy to the given base.</li></ul><div class="admonition is-category-warn"><header class="admonition-header">Dynamical interpretation</header><div class="admonition-body"><p>A dynamical interpretation of the permutation entropy does not necessarily hold if computing it on generic multivariate datasets. Method signatures for <code>Dataset</code>s are provided for convenience, and should only be applied if you understand the relation between your input data, the numerical value for the permutation entropy, and its interpretation.</p></div></div><p><strong>Description</strong></p><p><strong>Embedding, ordinal patterns and symbolization</strong></p><p>Consider the <span>$n$</span>-element univariate time series <span>$\{x(t) = x_1, x_2, \ldots, x_n\}$</span>.  Let <span>$\mathbf{x_i}^{m, \tau} = \{x_j, x_{j+\tau}, \ldots, x_{j+(m-1)\tau}\}$</span> for  <span>$j = 1, 2, \ldots n - (m-1)\tau$</span> be the <span>$i$</span>-th state vector in a delay reconstruction  with embedding dimension <span>$m$</span> and reconstruction lag <span>$\tau$</span>. There are then  <span>$N = n - (m-1)\tau$</span> state vectors. </p><p>For an <span>$m$</span>-dimensional vector, there are <span>$m!$</span> possible ways of sorting it in ascending  order of magnitude. Each such possible sorting ordering is called a <em>motif</em>.  Let <span>$\pi_i^{m, \tau}$</span> denote the motif associated with the <span>$m$</span>-dimensional state  vector <span>$\mathbf{x_i}^{m, \tau}$</span>, and let <span>$R$</span> be the number of distinct motifs that  can be constructed from the <span>$N$</span> state vectors. Then there are at most <span>$R$</span> motifs;  <span>$R = N$</span> precisely when all motifs are unique, and <span>$R = 1$</span> when all motifs are the same.  Each unique motif <span>$\pi_i^{m, \tau}$</span> can be mapped to a unique integer symbol  <span>$0 \leq s_i \leq M!-1$</span>. Let <span>$S(\pi) : \mathbb{R}^m \to \mathbb{N}_0$</span> be the  function that maps the motif <span>$\pi$</span> to its symbol <span>$s$</span>, and let <span>$\Pi$</span> denote the set      of symbols <span>$\Pi = \{ s_i \}_{i\in \{ 1, \ldots, R\}}$</span>.</p><p><strong>Probability computation</strong></p><p>Amplitude-aware permutation entropy is computed analogously to regular permutation entropy (see <a href="../SymbolicPermutation/#Entropies.SymbolicPermutation"><code>SymbolicPermutation</code></a>), but probabilities are weighted by amplitude information as follows.</p><div>\[p(\pi_i^{m, \tau}) = \dfrac{\sum_{k=1}^N \mathbf{1}_{u:S(u) = s_i} \left( \mathbf{x}_k^{m, \tau} \right) \, a_k}{\sum_{k=1}^N \mathbf{1}_{u:S(u) \in \Pi} \left( \mathbf{x}_k^{m, \tau} \right) \,a_k} = \dfrac{\sum_{k=1}^N \mathbf{1}_{u:S(u) = s_i} \left( \mathbf{x}_k^{m, \tau} \right) \, a_k}{\sum_{k=1}^N a_k}.\]</div><p>The weights encoding amplitude information about state vector <span>$\mathbf{x}_i = (x_1^i, x_2^i, \ldots, x_m^i)$</span> are </p><div>\[a_i = \dfrac{A}{m} \sum_{k=1}^m |x_k^i | + \dfrac{1-A}{d-1} \sum_{k=2}^d |x_{k}^i - x_{k-1}^i|,\]</div><p>with <span>$0 \leq A \leq 1$</span>. When <span>$A=0$</span> , only internal differences between the elements of  <span>$\mathbf{x}_i$</span> are weighted. Only mean amplitude of the state vector  elements are weighted when <span>$A=1$</span>. With, <span>$0&lt;A&lt;1$</span>, a combined weighting is used.</p><p><strong>Entropy computation</strong></p><p>The generalized order-<code>α</code> Renyi entropy<sup class="footnote-reference"><a id="citeref-Rényi1960" href="#footnote-Rényi1960">[Rényi1960]</a></sup> can be computed over the probability  distribution of symbols as  <span>$H(m, \tau, \alpha) = \dfrac{\alpha}{1-\alpha} \log  \left( \sum_{j=1}^R p_j^\alpha \right)$</span>. Permutation entropy, as described in  Bandt and Pompe (2002), is just the limiting case as <span>$α \to1$</span>, that is <span>$H(m, \tau) = - \sum_j^R p(\pi_j^{m, \tau}) \ln p(\pi_j^{m, \tau})$</span>.</p><div class="admonition is-category-hint"><header class="admonition-header">Generalized entropy order vs. permutation order</header><div class="admonition-body"><p>Do not confuse the order of the generalized entropy (<code>α</code>) with the order <code>m</code> of the permutation entropy (<code>m</code>, which controls the symbol size). Amplitude-aware permutation entropy is usually estimated with <code>α = 1</code>, but the implementation here  allows the generalized entropy of any dimension to be computed from the symbol  frequency distribution.</p></div></div></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaDynamics/Entropies.jl/blob/265c76be276b5c72b5858dfbfea702599645650f/src/symbolic/SymbolicAmplitudeAware.jl#L3-L124">source</a></section></article><section class="footnotes is-size-7"><ul><li class="footnote" id="footnote-Azami2016"><a class="tag is-link" href="#citeref-Azami2016">Azami2016</a>Azami, H., &amp; Escudero, J. (2016). Amplitude-aware permutation entropy: Illustration in spike detection and signal segmentation. Computer methods and programs in biomedicine, 128, 40-51.</li></ul></section></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../SymbolicWeightedPermutation/">« Weighted permutation (symbolic)</a><a class="docs-footer-nextpage" href="../NearestNeighbors/">Nearest neighbor estimators »</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> on <span class="colophon-date" title="Wednesday 18 November 2020 00:16">Wednesday 18 November 2020</span>. Using Julia version 1.5.3.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
