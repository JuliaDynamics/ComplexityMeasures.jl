var documenterSearchIndex = {"docs":
[{"location":"entropies/#entropies","page":"Entropies","title":"Entropies","text":"","category":"section"},{"location":"entropies/","page":"Entropies","title":"Entropies","text":"entropy\nentropy!","category":"page"},{"location":"entropies/#Entropies.entropy","page":"Entropies","title":"Entropies.entropy","text":"entropy([e::Entropy,] x, est::ProbabilitiesEstimator) → h::Real\nentropy([e::Entropy,] probs::Probabilities) → h::Real\n\nCompute a (generalized) entropy h from x according to the specified entropy type e and the given probability estimator est. Alternatively compute the entropy directly from the existing probabilities probs. In fact, the first method is a 2-lines-of-code wrapper that calls probabilities and gives the result to the second method.\n\nx is typically an Array or a Dataset, see Input data for Entropies.jl.\n\nThe entropy types that support this interface are \"direct\" entropies. They always yield an entropy value given a probability distribution. Such entropies are theoretically well-founded and are typically called \"generalized entropies\". Currently implemented types are:\n\nRenyi.\nTsallis.\nShannon, which is a subcase of the above two in the limit q → 1.\nCurado.\nStretchedExponential.\n\nThe entropy (first argument) is optional: if not given, Shannon() is used instead.\n\nThese entropies also have a well defined maximum value for a given probability estimator. To obtain this value one only needs to call the entropy_maximum function with the chosen entropy type and probability estimator. Or, one can use entropy_normalized to obtain the normalized form of the entropy (divided by the maximum).\n\nExamples\n\nx = [rand(Bool) for _ in 1:10000] # coin toss\nps = probabilities(x) # gives about [0.5, 0.5] by definition\nh = entropy(ps) # gives 1, about 1 bit by definition\nh = entropy(Shannon(), ps) # syntactically equivalent to above\nh = entropy(Shannon(), x, CountOccurrences()) # syntactically equivalent to above\nh = entropy(x, SymbolicPermutation(;m=3)) # gives about 2, again by definition\nh = entropy(Renyi(2.0), ps) # also gives 1, order `q` doesn't matter for coin toss\n\n\n\n\n\nentropy(e::IndirectEntropy, x) → h::Real\n\nCompute the entropy of x, here named h, according to the specified indirect entropy estimator e.\n\nIn contrast to the \"typical\" way one obtains entropies in the above methods, indirect entropy estimators compute Shannon entropies via alternate means, without explicitly computing probability distributions. The available indirect entropies are:\n\nKraskov.\nKozachenkoLeonenko.\nVasicek.\n\n\n\n\n\n","category":"function"},{"location":"entropies/#Entropies.entropy!","page":"Entropies","title":"Entropies.entropy!","text":"entropy!(s, [e::Entropy,] x, est::ProbabilitiesEstimator)\n\nSimilar to probabilities!, this is an in-place version of entropy that allows pre-allocation of temporarily used containers.\n\nThe entropy (second argument) is optional: if not given, Shannon() is used instead.\n\nOnly works for certain estimators. See for example SymbolicPermutation.\n\n\n\n\n\n","category":"function"},{"location":"entropies/#Rényi-(generalized)-entropy","page":"Entropies","title":"Rényi (generalized) entropy","text":"","category":"section"},{"location":"entropies/","page":"Entropies","title":"Entropies","text":"Renyi","category":"page"},{"location":"entropies/#Entropies.Renyi","page":"Entropies","title":"Entropies.Renyi","text":"Renyi <: Entropy\nRenyi(q, base = 2)\nRenyi(; q = 1.0, base = 2)\n\nThe Rényi[Rényi1960] generalized order-q entropy, used with entropy to compute an entropy with units given by base (typically 2 or MathConstants.e).\n\nDescription\n\nLet p be an array of probabilities (summing to 1). Then the Rényi generalized entropy is\n\nH_q(p) = frac11-q log left(sum_i pi^qright)\n\nand generalizes other known entropies, like e.g. the information entropy (q = 1, see [Shannon1948]), the maximum entropy (q=0, also known as Hartley entropy), or the correlation entropy (q = 2, also known as collision entropy).\n\nIf the probability estimator has known alphabet length L, then the maximum value of the Rényi entropy is log_base(L), which is the entropy of the uniform distribution with given alphabet length.\n\n[Rényi1960]: A. Rényi, Proceedings of the fourth Berkeley Symposium on Mathematics, Statistics and Probability, pp 547 (1960)\n\n[Shannon1948]: C. E. Shannon, Bell Systems Technical Journal 27, pp 379 (1948)\n\n\n\n\n\n","category":"type"},{"location":"entropies/#Tsallis-(generalized)-entropy","page":"Entropies","title":"Tsallis (generalized) entropy","text":"","category":"section"},{"location":"entropies/","page":"Entropies","title":"Entropies","text":"Tsallis","category":"page"},{"location":"entropies/#Entropies.Tsallis","page":"Entropies","title":"Entropies.Tsallis","text":"Tsallis <: Entropy\nTsallis(q; k = 1.0, base = 2)\nTsallis(; q = 1.0, k = 1.0, base = 2)\n\nThe Tsallis[Tsallis1988] generalized order-q entropy, used with entropy to compute an entropy.\n\nbase only applies in the limiting case q == 1, in which the Tsallis entropy reduces to Shannon entropy.\n\nDescription\n\nThe Tsallis entropy is a generalization of the Boltzmann-Gibbs entropy, with k standing for the Boltzmann constant. It is defined as\n\nS_q(p) = frackq - 1left(1 - sum_i pi^qright)\n\nIf the probability estimator has known alphabet length L, then the maximum value of the Tsallis entropy is k(L^1 - q - 1)(1 - q).\n\n[Tsallis1988]: Tsallis, C. (1988). Possible generalization of Boltzmann-Gibbs statistics. Journal of statistical physics, 52(1), 479-487.\n\n\n\n\n\n","category":"type"},{"location":"entropies/#Shannon-entropy-(convenience)","page":"Entropies","title":"Shannon entropy (convenience)","text":"","category":"section"},{"location":"entropies/","page":"Entropies","title":"Entropies","text":"Shannon","category":"page"},{"location":"entropies/#Entropies.Shannon","page":"Entropies","title":"Entropies.Shannon","text":"Shannon(; base = 2)\n\nThe Shannon[Shannon1948] entropy, used with entropy to compute:\n\nH(p) = - sum_i pi log(pi)\n\nwith the log at the given base.\n\nShannon(base) is syntactically equivalent to Renyi(; base).\n\n[Shannon1948]: C. E. Shannon, Bell Systems Technical Journal 27, pp 379 (1948)\n\n\n\n\n\n","category":"function"},{"location":"entropies/#Curado-entropy","page":"Entropies","title":"Curado entropy","text":"","category":"section"},{"location":"entropies/","page":"Entropies","title":"Entropies","text":"Curado","category":"page"},{"location":"entropies/#Entropies.Curado","page":"Entropies","title":"Entropies.Curado","text":"Curado <: Entropy\nCurado(; b = 1.0)\n\nThe Curado entropy (Curado & Nobre, 2004)[Curado2004], used with entropy to compute\n\nH_C(p) = left( sum_i=1^N e^-b p_i right) + e^-b - 1\n\nwith b ∈ ℛ, b > 0, where the terms outside the sum ensures that H_C(0) = H_C(1) = 0.\n\n[Curado2004]: Curado, E. M., & Nobre, F. D. (2004). On the stability of analytic entropic forms. Physica A: Statistical Mechanics and its Applications, 335(1-2), 94-106.\n\n\n\n\n\n","category":"type"},{"location":"entropies/#Stretched-exponental-entropy","page":"Entropies","title":"Stretched exponental entropy","text":"","category":"section"},{"location":"entropies/","page":"Entropies","title":"Entropies","text":"StretchedExponential","category":"page"},{"location":"entropies/#Entropies.StretchedExponential","page":"Entropies","title":"Entropies.StretchedExponential","text":"StretchedExponential <: Entropy\nStretchedExponential(; η = 2.0, base = 2)\n\nThe stretched exponential, or Anteneodo-Plastino, entropy (Anteneodo & Plastino, 1999[Anteneodo1999]), used with entropy to compute\n\nS_eta(p) = sum_i = 1^N\nGamma left( dfraceta + 1eta - log_base(p_i) right) -\np_i Gamma left( dfraceta + 1eta right)\n\nwhere eta geq 0, Gamma(cdot cdot) is the upper incomplete Gamma function, and Gamma(cdot) = Gamma(cdot 0) is the Gamma function. Reduces to Shannon entropy for η = 1.0.\n\n[Anteneodo1999]: Anteneodo, C., & Plastino, A. R. (1999). Maximum entropy approach to stretched exponential probability distributions. Journal of Physics A: Mathematical and General, 32(7), 1089.\n\n\n\n\n\n","category":"type"},{"location":"entropies/#Normalized-entropies","page":"Entropies","title":"Normalized entropies","text":"","category":"section"},{"location":"entropies/","page":"Entropies","title":"Entropies","text":"entropy_maximum\nentropy_normalized","category":"page"},{"location":"entropies/#Entropies.entropy_maximum","page":"Entropies","title":"Entropies.entropy_maximum","text":"entropy_maximum(e::Entropy, x, est::ProbabilitiesEstimator) → m::Real\n\nReturn the maximum value m of the given entropy type based on the given estimator and the given input x (whose values are not important, but layout and type are).\n\nThis function only works if the maximum value is deducable, which is possible only when the estimator has a known total_outcomes.\n\nentropy_maximum(e::Entropy, L::Int) → m::Real\n\nAlternatively, compute the maximum entropy from the total outcomes L directly.\n\n\n\n\n\n","category":"function"},{"location":"entropies/#Entropies.entropy_normalized","page":"Entropies","title":"Entropies.entropy_normalized","text":"entropy_normalized([e::Entropy,] x, est::ProbabilitiesEstimator) → h̃ ∈ [0, 1]\n\nReturn the normalized entropy of x, i.e., the value of entropy divided by the maximum value for e, according to the given probability estimator. If e is not given, it defaults to Shannon().\n\nNotice that unlike for entropy, here there is no method entropy_normalized(e::Entropy, probs::Probabilities) because there is no way to know the amount of possible events (i.e., the total_outcomes) from probs.\n\n\n\n\n\n","category":"function"},{"location":"entropies/#Indirect-entropies","page":"Entropies","title":"Indirect entropies","text":"","category":"section"},{"location":"entropies/","page":"Entropies","title":"Entropies","text":"Here we list functions which compute Shannon entropies via alternate means, without explicitly computing some probability distributions and then using the Shannon formula.","category":"page"},{"location":"entropies/#Nearest-neighbors-entropy","page":"Entropies","title":"Nearest neighbors entropy","text":"","category":"section"},{"location":"entropies/","page":"Entropies","title":"Entropies","text":"Kraskov\nKozachenkoLeonenko\nZhu\nZhuSingh","category":"page"},{"location":"entropies/#Entropies.Kraskov","page":"Entropies","title":"Entropies.Kraskov","text":"Kraskov <: IndirectEntropy\nKraskov(; k::Int = 1, w::Int = 1, base = 2)\n\nAn indirect entropy used in entropy(Kraskov(), x) to estimate the Shannon entropy of x (a multi-dimensional Dataset) to the given base using k-th nearest neighbor searches as in [Kraskov2004].\n\nw is the Theiler window, which determines if temporal neighbors are excluded during neighbor searches (defaults to 0, meaning that only the point itself is excluded when searching for neighbours).\n\nSee also: KozachenkoLeonenko.\n\n[Kraskov2004]: Kraskov, A., Stögbauer, H., & Grassberger, P. (2004). Estimating mutual information. Physical review E, 69(6), 066138.\n\n\n\n\n\n","category":"type"},{"location":"entropies/#Entropies.KozachenkoLeonenko","page":"Entropies","title":"Entropies.KozachenkoLeonenko","text":"KozachenkoLeonenko <: IndirectEntropy\nKozachenkoLeonenko(; k::Int = 1, w::Int = 1, base = 2)\n\nAn indirect entropy estimator used in entropy(KozachenkoLeonenko(), x) to estimate the Shannon entropy of x (a multi-dimensional Dataset) to the given base using nearest neighbor searches using the method from Kozachenko & Leonenko[KozachenkoLeonenko1987], as described in Charzyńska and Gambin[Charzyńska2016].\n\nw is the Theiler window, which determines if temporal neighbors are excluded during neighbor searches (defaults to 0, meaning that only the point itself is excluded when searching for neighbours).\n\nIn contrast to Kraskov, this estimator uses only the closest neighbor.\n\n[Charzyńska2016]: Charzyńska, A., & Gambin, A. (2016). Improvement of the k-NN entropy estimator with applications in systems biology. Entropy, 18(1), 13.\n\n[KozachenkoLeonenko1987]: Kozachenko, L. F., & Leonenko, N. N. (1987). Sample estimate of the entropy of a random vector. Problemy Peredachi Informatsii, 23(2), 9-16.\n\n\n\n\n\n","category":"type"},{"location":"entropies/#Entropies.Zhu","page":"Entropies","title":"Entropies.Zhu","text":"Zhu <: IndirectEntropy\nZhu(k = 1, w = 0, base = MathConstants.e)\n\nThe Zhu indirect entropy estimator (Zhu et al., 2015)[Zhu2015] estimates the Shannon entropy of x (a multi-dimensional Dataset) to the given base, by approximating probabilities within hyperrectangles surrounding each point xᵢ ∈ x using using k nearest neighbor searches.\n\nThis estimator is an extension to KozachenkoLeonenko.\n\nw is the Theiler window, which determines if temporal neighbors are excluded during neighbor searches (defaults to 0, meaning that only the point itself is excluded when searching for neighbours).\n\n[Zhu2015]: Zhu, J., Bellanger, J. J., Shu, H., & Le Bouquin Jeannès, R. (2015). Contribution to transfer entropy estimation via the k-nearest-neighbors approach. Entropy, 17(6), 4173-4201.\n\n\n\n\n\n","category":"type"},{"location":"entropies/#Entropies.ZhuSingh","page":"Entropies","title":"Entropies.ZhuSingh","text":"ZhuSingh <: IndirectEntropy\nZhuSingh(k = 1, w = 0, base = MathConstants.e)\n\nThe ZhuSingh indirect entropy estimator (Zhu et al., 2015)[Zhu2015] estimates the Shannon entropy of x (a multi-dimensional Dataset) to the given base.\n\nLike Zhu, this estimator approximates probabilities within hyperrectangles surrounding each point xᵢ ∈ x using using k nearest neighbor searches. However, it also considers the number of neighbors falling on the borders of these hyperrectangles. This estimator is an extension to the entropy estimator in Singh et al. (2003).\n\nw is the Theiler window, which determines if temporal neighbors are excluded during neighbor searches (defaults to 0, meaning that only the point itself is excluded when searching for neighbours).\n\n[Zhu2015]: Zhu, J., Bellanger, J. J., Shu, H., & Le Bouquin Jeannès, R. (2015). Contribution to transfer entropy estimation via the k-nearest-neighbors approach. Entropy, 17(6), 4173-4201.\n\n[Singh2003]: Singh, H., Misra, N., Hnizdo, V., Fedorowicz, A., & Demchuk, E. (2003). Nearest neighbor estimates of entropy. American journal of mathematical and management sciences, 23(3-4), 301-321.\n\n\n\n\n\n","category":"type"},{"location":"entropies/#Order-statistics-entropy","page":"Entropies","title":"Order statistics entropy","text":"","category":"section"},{"location":"entropies/","page":"Entropies","title":"Entropies","text":"Vasicek\nAlizadehArghami\nEbrahimi\nCorrea","category":"page"},{"location":"entropies/#Entropies.Vasicek","page":"Entropies","title":"Entropies.Vasicek","text":"Vasicek <: IndirectEntropy\nVasicek(; m::Int = 1, base = 2)\n\nAn indirect entropy estimator used in entropy(Vasicek(), x) to estimate the Shannon entropy of the timeseries x to the given base using the method from Vasicek (1976)[Vasicek1976].\n\nDescription\n\nThe Vasicek entropy estimator first computes the order statistics X_(1) leq X_(2) leq cdots leq X_(n) for a random sample of length n, i.e. the input timeseries. The Shannon entropy is then estimated as\n\nH_V(m) =\ndfrac1n sum_i = 1^n log left dfracn2m (X_(i+m) - X_(i-m)) right\n\nUsage\n\nIn practice, choice of m influences how fast the entropy converges to the true value. For small value of m, convergence is slow, so we recommend to scale m according to the time series length n and use m >= n/100 (this is just a heuristic based on the tests written for this package).\n\n[Vasicek1976]: Vasicek, O. (1976). A test for normality based on sample entropy. Journal of the Royal Statistical Society: Series B (Methodological), 38(1), 54-59.\n\n\n\n\n\n","category":"type"},{"location":"entropies/#Entropies.AlizadehArghami","page":"Entropies","title":"Entropies.AlizadehArghami","text":"AlizadehArghami <: IndirectEntropy     AlizadehArghami(; m::Int = 1, base = 2)\n\nAn indirect entropy estimator used in entropy(Alizadeh(), x) to estimate the Shannon entropy of the timeseries x to the given base using the method from Alizadeh & Arghami (2010)[Alizadeh2010].\n\nDescription\n\nThe Alizadeh entropy estimator first computes the order statistics X_(1) leq X_(2) leq cdots leq X_(n) for a random sample of length n, i.e. the input timeseries. The entropy for the length-n sample is then estimated as the Vasicek entropy estimate, plus a correction factor\n\nH_A(m n) = H_V(m n) + dfrac2nleft(m log_base(2) right)\n\n[Alizadeh2010]: Alizadeh, N. H., & Arghami, N. R. (2010). A new estimator of entropy. Journal of the Iranian Statistical Society (JIRSS).\n\n\n\n\n\n","category":"type"},{"location":"entropies/#Entropies.Ebrahimi","page":"Entropies","title":"Entropies.Ebrahimi","text":"Ebrahimi <: IndirectEntropy\nEbrahimi(; m::Int = 1, base = 2)\n\nAn indirect entropy estimator used in entropy(Ebrahimi(), x) to estimate the Shannon entropy of the timeseries x to the given base using the method from Ebrahimi (1994)[Ebrahimi1994].\n\nDescription\n\nThe Ebrahimi entropy estimator first computes the order statistics X_(1) leq X_(2) leq cdots leq X_(n) for a random sample of length n, i.e. the input timeseries. The Shannon entropy is then estimated as\n\nH_E(m) =\ndfrac1n sum_i = 1^n log left dfracnc_i m (X_(i+m) - X_(i-m)) right\n\nwhere\n\nc_i =\nbegincases\n    1 + fraci - 1m  1 geq i geq m \n    2                     m + 1 geq i geq n - m \n    1 + fracn - im  n - m + 1 geq i geq n\nendcases\n\n[Ebrahimi1994]: Ebrahimi, N., Pflughoeft, K., & Soofi, E. S. (1994). Two measures of sample entropy. Statistics & Probability Letters, 20(3), 225-234.\n\n\n\n\n\n","category":"type"},{"location":"entropies/#Entropies.Correa","page":"Entropies","title":"Entropies.Correa","text":"Correa <: IndirectEntropy\nCorrea(; m::Int = 1, base = 2)\n\nAn indirect entropy estimator used in entropy(Correa(), x) to estimate the Shannon entropy of the timeseries x to the given base using the method from Correa (1995)[Correa1995].\n\nDescription\n\nThe Correa entropy estimator first computes the order statistics like Vasicek, ensuring that edge points are included, then estimates entropy as\n\nH_C(m n) =\ndfrac1n sum_i = 1^n log\nleft dfrac sum_j=i-m^i+m(X_(j) - barX_(i))(j - i)n sum_j=i-m^i+m (X_(j) - barX_(i))^2 right\n\nwhere\n\nbarX_(i) = dfrac12m + 1 sum_j = i - m^i + m X_(j)\n\n[Correa1995]: Correa, J. C. (1995). A new estimator of entropy. Communications in Statistics-Theory and Methods, 24(10), 2439-2449.\n\n\n\n\n\n","category":"type"},{"location":"entropies/#Convenience-functions","page":"Entropies","title":"Convenience functions","text":"","category":"section"},{"location":"entropies/","page":"Entropies","title":"Entropies","text":"In this subsection we expand documentation strings of \"entropy names\" that are used commonly in the literature, such as \"permutation entropy\". As we made clear in API & terminology, these are just the existing Shannon entropy with a particularly chosen probability estimator. We have only defined convenience functions for the most used names, and arbitrary more specialized convenience functions can be easily defined in a couple lines of code.","category":"page"},{"location":"entropies/","page":"Entropies","title":"Entropies","text":"entropy_permutation\nentropy_spatial_permutation\nentropy_wavelet\nentropy_dispersion","category":"page"},{"location":"entropies/#Entropies.entropy_permutation","page":"Entropies","title":"Entropies.entropy_permutation","text":"entropy_permutation(x; τ = 1, m = 3, base = 2)\n\nCompute the permutation entropy of x of order m with delay/lag τ. This function is just a convenience call to:\n\nest = SymbolicPermutation(; m, τ)\nentropy(Shannon(base), x, est)\n\nSee SymbolicPermutation for more info. Similarly, one can use SymbolicWeightedPermutation or SymbolicAmplitudeAwarePermutation for the weighted/amplitude-aware versions.\n\n\n\n\n\n","category":"function"},{"location":"entropies/#Entropies.entropy_spatial_permutation","page":"Entropies","title":"Entropies.entropy_spatial_permutation","text":"entropy_spatial_permutation(x, stencil, periodic = true; kwargs...)\n\nCompute the spatial permutation entropy of x given the stencil. Here x must be a matrix or higher dimensional Array containing spatial data. This function is just a convenience call to:\n\nest = SpatialSymbolicPermutation(stencil, x, periodic)\nentropy(Renyi(;kwargs...), x, est)\n\nSee SpatialSymbolicPermutation for more info, or how to encode stencils.\n\n\n\n\n\n","category":"function"},{"location":"entropies/#Entropies.entropy_wavelet","page":"Entropies","title":"Entropies.entropy_wavelet","text":"entropy_wavelet(x; wavelet = Wavelets.WT.Daubechies{12}(), base = 2)\n\nCompute the wavelet entropy. This function is just a convenience call to:\n\nest = WaveletOverlap(wavelet)\nentropy(Shannon(base), x, est)\n\nSee WaveletOverlap for more info.\n\n\n\n\n\n","category":"function"},{"location":"entropies/#Entropies.entropy_dispersion","page":"Entropies","title":"Entropies.entropy_dispersion","text":"entropy_dispersion(x; base = 2, kwargs...)\n\nCompute the dispersion entropy. This function is just a convenience call to:\n\nest = Dispersion(kwargs...)\nentropy(Shannon(base), x, est)\n\nSee Dispersion for more info.\n\n\n\n\n\n","category":"function"},{"location":"probabilities/#probabilities_estimators","page":"Probabilities","title":"Probabilities","text":"","category":"section"},{"location":"probabilities/#Probabilities-API","page":"Probabilities","title":"Probabilities API","text":"","category":"section"},{"location":"probabilities/","page":"Probabilities","title":"Probabilities","text":"The probabilities API is defined by","category":"page"},{"location":"probabilities/","page":"Probabilities","title":"Probabilities","text":"ProbabilitiesEstimator\nprobabilities\nprobabilities_and_outcomes","category":"page"},{"location":"probabilities/","page":"Probabilities","title":"Probabilities","text":"ProbabilitiesEstimator\nprobabilities\nprobabilities!\nProbabilities\nprobabilities_and_outcomes\noutcomes\noutcome_space\ntotal_outcomes\nmissing_outcomes","category":"page"},{"location":"probabilities/#Entropies.ProbabilitiesEstimator","page":"Probabilities","title":"Entropies.ProbabilitiesEstimator","text":"The supertype for all probabilities estimators.\n\nIn Entropies.jl, probability distributions are estimated from data by defining a set of possible outcomes Omega = omega_1 omega_2 ldots omega_L , and assigning to each outcome omega_i a probability p(omega_i), such that sum_i=1^N p(omega_i) = 1. It is the role a ProbabilitiesEstimator to\n\nDefine Omega, the \"outcome space\", which is the set of all possible outcomes over  which probabilities are estimated. The cardinality of this set can be obtained using  total_outcomes.\nDefine how probabilities p_i = p(omega_i) are assigned to outcomes\\omega_i``.\n\nIn practice, probability estimation is done by calling probabilities with some input data and one of the following probabilities estimators. The result is a Probabilities p (Vector-like), where each element p[i] is the probability of the outcome ω[i]. Use probabilities_and_outcomes if you need both the probabilities and the outcomes and outcome_space to obtain Omega. The element type of Omega varies between estimators, but it is guranteed to be hashable. This allows for conveniently tracking the probability of a specific event across experimental realizations, by using the outcome as a dictionary key and the probability as the value for that key (or, alternatively, the key remains the outcome and one has a vector of probabilities, one for each experimental realization).\n\nAll currently implemented probability estimators are:\n\nCountOccurrences.\nValueHistogram.\nTransferOperator.\nDispersion.\nWaveletOverlap.\nPowerSpectrum.\nSymbolicPermutation.\nSymbolicWeightedPermutation.\nSymbolicAmplitudeAwarePermutation.\nSpatialSymbolicPermutation.\nNaiveKernel.\n\n\n\n\n\n","category":"type"},{"location":"probabilities/#Entropies.probabilities","page":"Probabilities","title":"Entropies.probabilities","text":"probabilities(x::Array_or_Dataset, est::ProbabilitiesEstimator) → p::Probabilities\n\nCompute a probability distribution over the set of possible outcomes defined by the probabilities estimator est, given input data x. To obtain the outcomes use outcomes.\n\nThe returned probabilities p may or may not be ordered, and may or may not contain 0s; see the documentation of the individual estimators for more. Configuration options are always given as arguments to the chosen estimator. x is typically an Array or a Dataset; see Input data for Entropies.jl.\n\nprobabilities(x::Array_or_Dataset) → p::Probabilities\n\nEstimate probabilities by directly counting the elements of x, assuming that Ω = sort(unique(x)), i.e. that the outcome space is the unique elements of x. This is mostly useful when x contains categorical or integer data.\n\nSee also: Probabilities, ProbabilitiesEstimator.\n\n\n\n\n\n","category":"function"},{"location":"probabilities/#Entropies.probabilities!","page":"Probabilities","title":"Entropies.probabilities!","text":"probabilities!(s, args...)\n\nSimilar to probabilities(args...), but allows pre-allocation of temporarily used containers s.\n\nOnly works for certain estimators. See for example SymbolicPermutation.\n\n\n\n\n\n","category":"function"},{"location":"probabilities/#Entropies.Probabilities","page":"Probabilities","title":"Entropies.Probabilities","text":"Probabilities <: AbstractVector\nProbabilities(x) → p\n\nProbabilities is a simple wrapper around AbstractVector that ensures its values sum to 1, so that p can be interpreted as probability distribution.\n\n\n\n\n\n","category":"type"},{"location":"probabilities/#Entropies.probabilities_and_outcomes","page":"Probabilities","title":"Entropies.probabilities_and_outcomes","text":"probabilities_and_outcomes(x, est) → (probs, Ω::Vector)\n\nReturn probs, Ω, where probs = probabilities(x, est) and Ω[i] is the outcome with probability probs[i]. The element type of Ω depends on the estimator.\n\nSee also outcomes, total_outcomes, and outcome_space.\n\n\n\n\n\n","category":"function"},{"location":"probabilities/#Entropies.outcomes","page":"Probabilities","title":"Entropies.outcomes","text":"outcomes(x, est::ProbabilitiesEstimator)\n\nReturn all (unique) outcomes contained in x according to the given estimator. Equivalent with probabilities_and_outcomes(x, est)[2], but for some estimators it may be explicitly extended for better performance.\n\n\n\n\n\n","category":"function"},{"location":"probabilities/#Entropies.outcome_space","page":"Probabilities","title":"Entropies.outcome_space","text":"outcome_space([x,] est::ProbabilitiesEstimator) → Ω\n\nReturn a container (typically Vector) containing all possible outcomes of est, i.e., the outcome space Ω. Only possible for estimators that implement total_outcomes, and similarly, for some estimators x is not needed. The values of x are never needed; but some times the type and dimensional layout of x is.\n\n\n\n\n\n","category":"function"},{"location":"probabilities/#Entropies.total_outcomes","page":"Probabilities","title":"Entropies.total_outcomes","text":"total_outcomes([x::Array_or_Dataset,] est::ProbabilitiesEstimator) → Int\n\nReturn the size/cardinality of the outcome space Omega defined by the probabilities estimator est imposed on the input data x.\n\nFor some estimators, the total number of outcomes is independent of x, in which case the input x is ignored and the method total_outcomes(est) is called. If the total number of states cannot be known a priori, an error is thrown. Primarily used in entropy_normalized.\n\nExamples\n\njulia> est = SymbolicPermutation(m = 4);\n\njulia> total_outcomes(rand(42), est) # same as `factorial(m)` for any `x`\n24\n\n\n\n\n\n","category":"function"},{"location":"probabilities/#Entropies.missing_outcomes","page":"Probabilities","title":"Entropies.missing_outcomes","text":"missing_outcomes(x, est::ProbabilitiesEstimator) → n_missing::Int\n\nEstimate a probability distribution for x using the given estimator, then count the number of missing (i.e. zero-probability) outcomes.\n\nWorks for estimators that implement total_outcomes.\n\nSee also: MissingDispersionPatterns.\n\n\n\n\n\n","category":"function"},{"location":"probabilities/#Count-occurrences-(counting)","page":"Probabilities","title":"Count occurrences (counting)","text":"","category":"section"},{"location":"probabilities/","page":"Probabilities","title":"Probabilities","text":"CountOccurrences","category":"page"},{"location":"probabilities/#Entropies.CountOccurrences","page":"Probabilities","title":"Entropies.CountOccurrences","text":"CountOccurrences()\n\nA probabilities/entropy estimator based on straight-forward counting of distinct elements in a univariate time series or multivariate dataset. This is the same as giving no estimator to probabilities.\n\nOutcome space\n\nThe outcome space is the unique sorted values of the input.\n\n\n\n\n\n","category":"type"},{"location":"probabilities/#Visitation-frequency-(histograms)","page":"Probabilities","title":"Visitation frequency (histograms)","text":"","category":"section"},{"location":"probabilities/","page":"Probabilities","title":"Probabilities","text":"ValueHistogram\nRectangularBinning\nFixedRectangularBinning","category":"page"},{"location":"probabilities/#Entropies.ValueHistogram","page":"Probabilities","title":"Entropies.ValueHistogram","text":"ValueHistogram(b::AbstractBinning) <: ProbabilitiesEstimator\n\nA probability estimator based on binning the values of the data as dictated by the binning scheme b and formally computing their histogram, i.e., the frequencies of points in the bins. An alias to this is VisitationFrequency. Available binnings are:\n\nRectangularBinning\nFixedRectangularBinning\n\nThe ValueHistogram estimator has a linearithmic time complexity (n log(n) for n = length(x)) and a linear space complexity (l for l = dimension(x)). This allows computation of probabilities (histograms) of high-dimensional datasets and with small box sizes ε without memory overflow and with maximum performance. For performance reasons, the probabilities returned never contain 0s and are arbitrarily ordered.\n\nValueHistogram(ϵ::Union{Real,Vector})\n\nA convenience method that accepts same input as RectangularBinning and initializes this binning directly.\n\nOutcomes\n\nThe outcome space for ValueHistogram is the set of unique bins constructed from b. Each bin is identified by its left (lowest-value) corner. The bins are in data units, not integer (cartesian indices units), and are returned as SVectors.\n\nSee also: RectangularBinning.\n\n\n\n\n\n","category":"type"},{"location":"probabilities/#Entropies.RectangularBinning","page":"Probabilities","title":"Entropies.RectangularBinning","text":"RectangularBinning(ϵ) <: AbstractBinning\n\nRectangular box partition of state space using the scheme ϵ, deducing the coordinates of the grid axis minima from the input data.\n\nBinning instructions are deduced from the type of ϵ as follows:\n\nϵ::Int divides each coordinate axis into ϵ equal-length intervals  that cover all data.\nϵ::Float64 divides each coordinate axis into intervals of fixed size ϵ, starting  from the axis minima until the data is completely covered by boxes.\nϵ::Vector{Int} divides the i-th coordinate axis into ϵ[i] equal-length  intervals that cover all data.\nϵ::Vector{Float64} divides the i-th coordinate axis into intervals of fixed size  ϵ[i], starting from the axis minima until the data is completely covered by boxes.\n\n\n\n\n\n","category":"type"},{"location":"probabilities/#Entropies.FixedRectangularBinning","page":"Probabilities","title":"Entropies.FixedRectangularBinning","text":"FixedRectangularBinning <: AbstractBinning\nFixedRectangularBinning(ϵmin::E, ϵmax::E, N::Int) where E\n\nRectangular box partition of state space where the extent of the grid is explicitly specified by ϵmin and emax, and along each dimension, the grid is subdivided into N subintervals.\n\nBinning instructions are deduced from the types of ϵmin/emax as follows:\n\nE<:Float64 sets the grid range along all dimensions to to [ϵmin, ϵmax].\nE::NTuple{D, Float64} sets ranges along each dimension individually, i.e.  [ϵmin[i], ϵmax[i]] is the range along the i-th dimension.\n\nIf the grid spans the range [r1, r2] along a particular dimension, then this range is subdivided into N subintervals of equal length nextfloat((r2 - r1)/N). Thus, for D-dimensional data, there are N^D boxes.\n\n\n\n\n\n","category":"type"},{"location":"probabilities/#Permutation-(symbolic)","page":"Probabilities","title":"Permutation (symbolic)","text":"","category":"section"},{"location":"probabilities/","page":"Probabilities","title":"Probabilities","text":"SymbolicPermutation\nSymbolicWeightedPermutation\nSymbolicAmplitudeAwarePermutation\nSpatialSymbolicPermutation","category":"page"},{"location":"probabilities/#Entropies.SymbolicPermutation","page":"Probabilities","title":"Entropies.SymbolicPermutation","text":"SymbolicPermutation(; m = 3, τ = 1, lt::Function = Entropies.isless_rand)\n\nA probabilities estimator based on ordinal permutation patterns, originally used by Bandt & Pompe (2002)[BandtPompe2002] to compute permutation entropy.\n\nIf applied to a univariate time series, then the time series is first embedded using embedding delay τ and dimension m, and then converted to a symbol time series using outcomes with OrdinalPatternEncoding, from which probabilities are estimated. If applied to a Dataset, then τ and m are ignored, and probabilities are computed directly from the state vectors.\n\nOutcome space\n\nThe outcome space Ω for SymbolicPermutation is the set of length-m ordinal patterns (i.e. permutations) that can be formed by the integers 1, 2, …, m, ordered lexicographically. There are factorial(m) such patterns.\n\nIn-place symbolization\n\nSymbolicPermutation also implements the in-place entropy! and probabilities!. The length of the pre-allocated symbol vector must match the length of the embedding: N - (m-1)τ for univariate time series, and M for length-M Datasets), i.e.\n\nusing DelayEmbeddings, Entropies\nm, τ, N = 2, 1, 100\nest = SymbolicPermutation(; m, τ)\n\n# For a time series\nx_ts = rand(N)\nπs_ts = zeros(Int, N - (m - 1)*τ)\np = probabilities!(πs_ts, x_ts, est)\nh = entropy!(πs_ts, Renyi(),  x_ts, est)\n\n# For a pre-discretized `Dataset`\nx_symb = outcomes(x_ts, OrdinalPatternEncoding(m = 2, τ = 1))\nx_d = genembed(x_symb, (0, -1, -2))\nπs_d = zeros(Int, length(x_d))\np = probabilities!(πs_d, x_d, est)\nh = entropy!(πs_d, Renyi(), x_d, est)\n\nSee SymbolicWeightedPermutation and SymbolicAmplitudeAwarePermutation for estimators that not only consider ordinal (sorting) patterns, but also incorporate information about within-state-vector amplitudes.\n\nnote: Handling equal values in ordinal patterns\nIn Bandt & Pompe (2002), equal values are ordered after their order of appearance, but this can lead to erroneous temporal correlations, especially for data with low-amplitude resolution [Zunino2017]. Here, by default, if two values are equal, then one of the is randomly assigned as \"the largest\", using lt = Entropies.isless_rand. To get the behaviour from Bandt and Pompe (2002), use lt = Base.isless).\n\n[BandtPompe2002]: Bandt, Christoph, and Bernd Pompe. \"Permutation entropy: a natural complexity measure for time series.\" Physical review letters 88.17 (2002): 174102.\n\n[Zunino2017]: Zunino, L., Olivares, F., Scholkmann, F., & Rosso, O. A. (2017). Permutation entropy based time series analysis: Equalities in the input signal can lead to false conclusions. Physics Letters A, 381(22), 1883-1892.\n\n\n\n\n\n","category":"type"},{"location":"probabilities/#Entropies.SymbolicWeightedPermutation","page":"Probabilities","title":"Entropies.SymbolicWeightedPermutation","text":"SymbolicWeightedPermutation(; τ = 1, m = 3, lt = Entropies.isless_rand)\n\nA variant of SymbolicPermutation that also incorporates amplitude information, based on the weighted permutation entropy (Fadlallah et al., 2013).\n\nOutcome space\n\nLike for SymbolicPermutation, the outcome space Ω for SymbolicWeightedPermutation is the lexiographically ordered set of length-m ordinal patterns (i.e. permutations) that can be formed by the integers 1, 2, …, m. There are factorial(m) such patterns.\n\nDescription\n\nProbabilities are computed as\n\np(pi_i^m tau) = dfracsum_k=1^N mathbf1_uS(u) = s_i\nleft( mathbfx_k^m tau right)\n w_ksum_k=1^N mathbf1_uS(u) in Pi\nleft( mathbfx_k^m tau right) w_k = dfracsum_k=1^N\nmathbf1_uS(u) = s_i\nleft( mathbfx_k^m tau right)  w_ksum_k=1^N w_k\n\nwhere weights are computed based on the variance of the state vectors as\n\nw_j = dfrac1msum_k=1^m (x_j+(k+1)tau - mathbfhatx_j^m tau)^2\n\nand mathbfx_i is the aritmetic mean of state vector:\n\nmathbfhatx_j^m tau = frac1m sum_k=1^m x_j + (k+1)tau\n\nThe weighted permutation entropy is equivalent to regular permutation entropy when weights are positive and identical (w_j = beta  forall  j leq N and beta  0).\n\nSee SymbolicPermutation for an estimator that only incorporates ordinal/sorting information and disregards amplitudes, and SymbolicAmplitudeAwarePermutation for another estimator that incorporates amplitude information.\n\nnote: An implementation note\nNote: in equation 7, section III, of the original paper, the authors writew_j = dfrac1msum_k=1^m (x_j-(k-1)tau - mathbfhatx_j^m tau)^2But given the formula they give for the arithmetic mean, this is not the variance of mathbfx_i, because the indices are mixed: x_j+(k-1)tau in the weights formula, vs. x_j+(k+1)tau in the arithmetic mean formula. This seems to imply that amplitude information about previous delay vectors are mixed with mean amplitude information about current vectors. The authors also mix the terms \"vector\" and \"neighboring vector\" (but uses the same notation for both), making it hard to interpret whether the sign switch is a typo or intended. Here, we use the notation above, which actually computes the variance for mathbfx_i.\n\n[Fadlallah2013]: Fadlallah, Bilal, et al. \"Weighted-permutation entropy: A complexity measure for time series incorporating amplitude information.\" Physical Review E 87.2 (2013): 022911.\n\n\n\n\n\n","category":"type"},{"location":"probabilities/#Entropies.SymbolicAmplitudeAwarePermutation","page":"Probabilities","title":"Entropies.SymbolicAmplitudeAwarePermutation","text":"SymbolicAmplitudeAwarePermutation(; τ = 1, m = 3, A = 0.5, lt = Entropies.isless_rand)\n\nA variant of SymbolicPermutation that also incorporates amplitude information, based on the amplitude-aware permutation entropy (Azami & Escudero, 2016).\n\nOutcome space\n\nLike for SymbolicPermutation, the outcome space Ω for SymbolicAmplitudeAwarePermutation is the lexiographically ordered set of length-m ordinal patterns (i.e. permutations) that can be formed by the integers 1, 2, …, m. There are factorial(m) such patterns.\n\nDescription\n\nProbabilities are computed as\n\np(pi_i^m tau) =\ndfracsum_k=1^N\nmathbf1_uS(u) = s_i left( mathbfx_k^m tau right)  a_ksum_k=1^N\nmathbf1_uS(u) in Pi left( mathbfx_k^m tau right) a_k =\ndfracsum_k=1^N mathbf1_uS(u) = s_i\nleft( mathbfx_k^m tau right)  a_ksum_k=1^N a_k\n\nThe weights encoding amplitude information about state vector mathbfx_i = (x_1^i x_2^i ldots x_m^i) are\n\na_i = dfracAm sum_k=1^m x_k^i  + dfrac1-Ad-1\nsum_k=2^d x_k^i - x_k-1^i\n\nwith 0 leq A leq 1. When A=0 , only internal differences between the elements of mathbfx_i are weighted. Only mean amplitude of the state vector elements are weighted when A=1. With, 0A1, a combined weighting is used.\n\nSee SymbolicPermutation for an estimator that only incorporates ordinal/sorting information and disregards amplitudes, and SymbolicWeightedPermutation for another estimator that incorporates amplitude information.\n\n[Azami2016]: Azami, H., & Escudero, J. (2016). Amplitude-aware permutation entropy: Illustration in spike detection and signal segmentation. Computer methods and programs in biomedicine, 128, 40-51.\n\n\n\n\n\n","category":"type"},{"location":"probabilities/#Entropies.SpatialSymbolicPermutation","page":"Probabilities","title":"Entropies.SpatialSymbolicPermutation","text":"SpatialSymbolicPermutation(stencil, x, periodic = true)\n\nA symbolic, permutation-based probabilities estimator for spatiotemporal systems.\n\nThe input data x are high-dimensional arrays, for example 2D arrays [Ribeiro2012] or 3D arrays [Schlemmer2018]. This approach is also known as spatiotemporal permutation entropy. x is given because we need to know its size for optimization and bound checking.\n\nA stencil defines what local area (which points) around each pixel to consider, and compute ordinal patterns from. The number of included points in a stencil (m) determines the length of the vectors to be discretized, i.e. there are m! possible ordinal patterns around each pixel.\n\nExample usage:\n\ndata = [rand(50, 50) for _ in 1:50]\nx = data[1] # first \"time slice\" of a spatial system evolution\nstencil = ...\nest = SpatialSymbolicPermutation(stencil, x)\n\nStencils are passed in one of the following three ways:\n\nAs vectors of CartesianIndex which encode the pixels to include in the  stencil, with respect to the current pixel, or integer arrays of the same dimensionality  as the data. For example\njulia  stencil = CartesianIndex.([(0,0), (0,1), (1,1), (1,0)])  Don't forget to include the zero offset index if you want to include the point itself,  which is almost always the case.  Here the stencil creates a 2x2 square extending to the bottom and right of the pixel  (directions here correspond to the way Julia prints matrices by default).  When passing a stencil as a vector of CartesianIndex, m = length(stencil).\nAs a D-dimensional array (where D matches the dimensionality of the input data)  containing 0s and 1s, where if stencil[index] == 1, the corresponding pixel is  included, and if stencil[index] == 0, it is not included.  To generate the same estimator as in 1., use\njulia  stencil = [1 1; 1 1]  When passing a stencil as a D-dimensional array, m = sum(stencil)\nAs a Tuple containing two Tuples, both of length D, for D-dimensional data.  The first tuple specifies the extent of the stencil, where extent[i]  dictates the number of pixels to be included along the ith axis and lag[i]  the separation of pixels along the same axis.  This method can only generate (hyper)rectangular stencils. To create the same estimator as  in the previous examples, use here\njulia  stencil = ((2, 2), (1, 1))  When passing a stencil using extent and lag, m = prod(extent)!.\n\nAfter having defined est, one calculates the spatial permutation entropy by calling entropy with est, and with the array data. To apply this to timeseries of spatial data, simply loop over the call, e.g.:\n\nh = entropy(x, est)\nh_vs_t = [entropy(d, est) for d in data]\n\nThe argument periodic decides whether the stencil should wrap around at the end of the array. If periodic = false, pixels whose stencil exceeds the array bounds are skipped.\n\n[Ribeiro2012]: Ribeiro et al. (2012). Complexity-entropy causality plane as a complexity measure for two-dimensional patterns. https://doi.org/10.1371/journal.pone.0040689\n\n[Schlemmer2018]: Schlemmer et al. (2018). Spatiotemporal Permutation Entropy as a Measure for Complexity of Cardiac Arrhythmia. https://doi.org/10.3389/fphy.2018.00039\n\n\n\n\n\n","category":"type"},{"location":"probabilities/#Dispersion-(symbolic)","page":"Probabilities","title":"Dispersion (symbolic)","text":"","category":"section"},{"location":"probabilities/","page":"Probabilities","title":"Probabilities","text":"Dispersion","category":"page"},{"location":"probabilities/#Entropies.Dispersion","page":"Probabilities","title":"Entropies.Dispersion","text":"Dispersion(; c = 5, m = 2, τ = 1, check_unique = true)\n\nA probability estimator based on dispersion patterns, originally used by Rostaghi & Azami, 2016[Rostaghi2016] to compute the \"dispersion entropy\", which characterizes the complexity and irregularity of a time series.\n\nRecommended parameter values[Li2018] are m ∈ [2, 3], τ = 1 for the embedding, and c ∈ [3, 4, …, 8] categories for the Gaussian symbol mapping.\n\nDescription\n\nAssume we have a univariate time series X = x_i_i=1^N. First, this time series is discretized a \"Gaussian encoding\", which uses the normal cumulative distribution function (CDF) to encode a timeseries x as integers like so: each x_i to a new real number y_i in 0 1 by using the normal cumulative distribution function (CDF), x_i to y_i  y_i = dfrac1 sigma sqrt2 pi int_-infty^x_i e^(-(x_i - mu)^2)(2 sigma^2) dx, where mu and sigma are the empirical mean and standard deviation of X. Other choices of CDFs are also possible, but currently only Gaussian is implemented. Next, each y_i is linearly mapped to an integer z_i in 1 2 ldots c using the map y_i to z_i  z_i = R(y_i(c-1) + 05), where R indicates rounding up to the nearest integer. This procedure subdivides the interval 0 1 into c different subintervals that form a covering of 0 1, and assigns each y_i to one of these subintervals. The original time series X is thus transformed to a symbol time series S =  s_i _i=1^N, where s_i in 1 2 ldots c.\n\nNext, the symbol time series S is embedded into an m-dimensional time series, using an embedding lag of tau = 1, which yields a total of N - (m - 1)tau points, or \"dispersion patterns\". Because each z_i can take on c different values, and each embedding point has m values, there are c^m possible dispersion patterns. This number is used for normalization when computing dispersion entropy.\n\nComputing dispersion probabilities and entropy\n\nA probability distribution P = p_i _i=1^c^m, where sum_i^c^m p_i = 1, can then be estimated by counting and sum-normalising the distribution of dispersion patterns among the embedding vectors. Note that dispersion patterns that are not present are not counted. Therefore, you'll always get non-zero probabilities using the Dispersion probability estimator.\n\nOutcome space\n\nThe outcome space for Dispersion is the unique delay vectos with elements the the symbols (integers) encoded by the Gaussian PDF. Hence, the outcome space is all m-dimensional delay vectors whose elements are all possible values in 1:c.\n\nData requirements and parameters\n\nThe input must have more than one unique element for the Gaussian mapping to be well-defined. Li et al. (2018) recommends that x has at least 1000 data points.\n\nIf check_unique == true (default), then it is checked that the input has more than one unique value. If check_unique == false and the input only has one unique element, then a InexactError is thrown when trying to compute probabilities.\n\nnote: Why 'dispersion patterns'?\nEach embedding vector is called a \"dispersion pattern\". Why? Let's consider the case when m = 5 and c = 3, and use some very imprecise terminology for illustration:When c = 3, values clustering far below mean are in one group, values clustered around the mean are in one group, and values clustering far above the mean are in a third group. Then the embedding vector 2 2 2 2 2 consists of values that are relatively close together (close to the mean), so it represents a set of numbers that are not very spread out (less dispersed). The embedding vector 1 1 2 3 3, however, represents numbers that are much more spread out (more dispersed), because the categories representing \"outliers\" both above and below the mean are represented, not only values close to the mean.\n\n[Rostaghi2016]: Rostaghi, M., & Azami, H. (2016). Dispersion entropy: A measure for time-series analysis. IEEE Signal Processing Letters, 23(5), 610-614.\n\n[Li2018]: Li, G., Guan, Q., & Yang, H. (2018). Noise reduction method of underwater acoustic signals based on CEEMDAN, effort-to-compress complexity, refined composite multiscale dispersion entropy and wavelet threshold denoising. Entropy, 21(1), 11.\n\n\n\n\n\n","category":"type"},{"location":"probabilities/#Transfer-operator-(binning)","page":"Probabilities","title":"Transfer operator (binning)","text":"","category":"section"},{"location":"probabilities/","page":"Probabilities","title":"Probabilities","text":"TransferOperator","category":"page"},{"location":"probabilities/#Entropies.TransferOperator","page":"Probabilities","title":"Entropies.TransferOperator","text":"TransferOperator(ϵ::RectangularBinning) <: ProbabilitiesEstimator\n\nA probability estimator based on binning data into rectangular boxes dictated by the binning scheme ϵ, then approxmating the transfer (Perron-Frobenius) operator over the bins, then taking the invariant measure associated with that transfer operator as the bin probabilities. Assumes that the input data are sequential (time-ordered).\n\nThis implementation follows the grid estimator approach in Diego et al. (2019)[Diego2019].\n\nDescription\n\nThe transfer operator P^Nis computed as an N-by-N matrix of transition probabilities between the states defined by the partition elements, where N is the number of boxes in the partition that is visited by the orbit/points.\n\nIf  x_t^(D) _n=1^L are the L different D-dimensional points over which the transfer operator is approximated,  C_k=1^N  are the N different partition elements (as dictated by ϵ) that gets visited by the points, and  phi(x_t) = x_t+1, then\n\nP_ij = dfrac\n x_n  phi(x_n) in C_j cap x_n in C_i \n x_m  x_m in C_i \n\nwhere  denotes the cardinal. The element P_ij thus indicates how many points that are initially in box C_i end up in box C_j when the points in C_i are projected one step forward in time. Thus, the row P_ik^N where k in 1 2 ldots N  gives the probability of jumping from the state defined by box C_i to any of the other N states. It follows that sum_k=1^N P_ik = 1 for all i. Thus, P^N is a row/right stochastic matrix.\n\nInvariant measure estimation from transfer operator\n\nThe left invariant distribution mathbfrho^N is a row vector, where mathbfrho^N P^N = mathbfrho^N. Hence, mathbfrho^N is a row eigenvector of the transfer matrix P^N associated with eigenvalue 1. The distribution mathbfrho^N approximates the invariant density of the system subject to the partition ϵ, and can be taken as a probability distribution over the partition elements.\n\nIn practice, the invariant measure mathbfrho^N is computed using invariantmeasure, which also approximates the transfer matrix. The invariant distribution is initialized as a length-N random distribution which is then applied to P^N. The resulting length-N distribution is then applied to P^N again. This process repeats until the difference between the distributions over consecutive iterations is below some threshold.\n\nProbability and entropy estimation\n\nprobabilities(x::AbstractDataset, est::TransferOperator{RectangularBinning}) estimates   probabilities for the bins defined by the provided binning (est.ϵ)\nentropy_renyi(x::AbstractDataset, est::TransferOperator{RectangularBinning}) does the same,   but computes generalized entropy using the probabilities.\n\nSee also: RectangularBinning, invariantmeasure.\n\n[Diego2019]: Diego, D., Haaga, K. A., & Hannisdal, B. (2019). Transfer entropy computation using the Perron-Frobenius operator. Physical Review E, 99(4), 042212.\n\n\n\n\n\n","category":"type"},{"location":"probabilities/#Utility-methods/types","page":"Probabilities","title":"Utility methods/types","text":"","category":"section"},{"location":"probabilities/","page":"Probabilities","title":"Probabilities","text":"InvariantMeasure\ninvariantmeasure\ntransfermatrix","category":"page"},{"location":"probabilities/#Entropies.InvariantMeasure","page":"Probabilities","title":"Entropies.InvariantMeasure","text":"InvariantMeasure(to, ρ)\n\nMinimal return struct for invariantmeasure that contains the estimated invariant measure ρ, as well as the transfer operator to from which it is computed (including bin information).\n\nSee also: invariantmeasure.\n\n\n\n\n\n","category":"type"},{"location":"probabilities/#Entropies.invariantmeasure","page":"Probabilities","title":"Entropies.invariantmeasure","text":"invariantmeasure(x::AbstractDataset, ϵ::RectangularBinning) → iv::InvariantMeasure\n\nEstimate an invariant measure over the points in x based on binning the data into rectangular boxes dictated by the binning scheme ϵ, then approximate the transfer (Perron-Frobenius) operator over the bins. From the approximation to the transfer operator, compute an invariant distribution over the bins. Assumes that the input data are sequential.\n\nDetails on the estimation procedure is found the TransferOperator docstring.\n\nExample\n\nusing DynamicalSystems, Plots, Entropies\nD = 4\nds = Systems.lorenz96(D; F = 32.0)\nN, dt = 20000, 0.1\norbit = trajectory(ds, N*dt; dt = dt, Ttr = 10.0)\n\n# Estimate the invariant measure over some coarse graining of the orbit.\niv = invariantmeasure(orbit, RectangularBinning(15))\n\n# Get the probabilities and bins\ninvariantmeasure(iv)\n\nProbabilities and bin information\n\ninvariantmeasure(iv::InvariantMeasure) → (ρ::Probabilities, bins::Vector{<:SVector})\n\nFrom a pre-computed invariant measure, return the probabilities and associated bins. The element ρ[i] is the probability of visitation to the box bins[i]. Analogous to binhist.\n\nhint: Transfer operator approach vs. naive histogram approach\nWhy bother with the transfer operator instead of using regular histograms to obtain probabilities?In fact, the naive histogram approach and the transfer operator approach are equivalent in the limit of long enough time series (as n to intfy), which is guaranteed by the ergodic theorem. There is a crucial difference, however:The naive histogram approach only gives the long-term probabilities that orbits visit a certain region of the state space. The transfer operator encodes that information too, but comes with the added benefit of knowing the transition probabilities between states (see transfermatrix).\n\nSee also: InvariantMeasure.\n\n\n\n\n\n","category":"function"},{"location":"probabilities/#Entropies.transfermatrix","page":"Probabilities","title":"Entropies.transfermatrix","text":"transfermatrix(iv::InvariantMeasure) → (M::AbstractArray{<:Real, 2}, bins::Vector{<:SVector})\n\nReturn the transfer matrix/operator and corresponding bins. Here, bins[i] corresponds to the i-th row/column of the transfer matrix. Thus, the entry M[i, j] is the probability of jumping from the state defined by bins[i] to the state defined by bins[j].\n\nSee also: TransferOperator.\n\n\n\n\n\n","category":"function"},{"location":"probabilities/#Kernel-density","page":"Probabilities","title":"Kernel density","text":"","category":"section"},{"location":"probabilities/","page":"Probabilities","title":"Probabilities","text":"NaiveKernel","category":"page"},{"location":"probabilities/#Entropies.NaiveKernel","page":"Probabilities","title":"Entropies.NaiveKernel","text":"NaiveKernel(ϵ::Real, ss = KDTree; w = 0, metric = Euclidean()) <: ProbabilitiesEstimator\n\nEstimate probabilities/entropy using a \"naive\" kernel density estimation approach (KDE), as discussed in Prichard and Theiler (1995) [PrichardTheiler1995].\n\nProbabilities P(mathbfx epsilon) are assigned to every point mathbfx by counting how many other points occupy the space spanned by a hypersphere of radius ϵ around mathbfx, according to:\n\nP_i( X epsilon) approx dfrac1N sum_s B(X_i - X_j  epsilon)\n\nwhere B gives 1 if the argument is true. Probabilities are then normalized.\n\nThe search structure ss is any search structure supported by Neighborhood.jl. Specifically, use KDTree to use a tree-based neighbor search, or BruteForce for the direct distances between all points. KDTrees heavily outperform direct distances when the dimensionality of the data is much smaller than the data length.\n\nThe keyword w stands for the Theiler window, and excludes indices s that are within i - s  w from the given point X_i.\n\nOutcome space\n\nThe outcome space Ω for NaiveKernel are the indices of the input data, 1:length(x). The reason to not return the data points themselves is because duplicate data points may not have same probabilities (due to having different neighbors).\n\n[PrichardTheiler1995]: Prichard, D., & Theiler, J. (1995). Generalized redundancies for time series analysis. Physica D: Nonlinear Phenomena, 84(3-4), 476-493.\n\n\n\n\n\n","category":"type"},{"location":"probabilities/#Timescales","page":"Probabilities","title":"Timescales","text":"","category":"section"},{"location":"probabilities/","page":"Probabilities","title":"Probabilities","text":"WaveletOverlap\nPowerSpectrum","category":"page"},{"location":"probabilities/#Entropies.WaveletOverlap","page":"Probabilities","title":"Entropies.WaveletOverlap","text":"WaveletOverlap([wavelet]) <: ProbabilitiesEstimator\n\nApply the maximal overlap discrete wavelet transform (MODWT) to a signal, then compute probabilities as the (normalized) energies at different wavelet scales. These probabilities are used to compute the wavelet entropy, according to Rosso et al. (2001)[Rosso2001].\n\nBy default the wavelet Wavelets.WT.Daubechies{12}() is used. Otherwise, you may choose a wavelet from the Wavelets package (it must subtype OrthoWaveletClass).\n\nOutcome space\n\nThe outcome space for WaveletOverlap are the integers 1, 2, …, N enumerating the wavelet scales. To obtain a better understanding of what these mean, we prepared a notebook you can view online. As such, this estimator only works for timeseries input.\n\n[Rosso2001]: Rosso et al. (2001). Wavelet entropy: a new tool for analysis of short duration brain electrical signals. Journal of neuroscience methods, 105(1), 65-75.\n\n\n\n\n\n","category":"type"},{"location":"probabilities/#Entropies.PowerSpectrum","page":"Probabilities","title":"Entropies.PowerSpectrum","text":"PowerSpectrum() <: ProbabilitiesEstimator\n\nCalculate the power spectrum of a timeseries (amplitude square of its Fourier transform), and return the spectrum normalized to sum = 1 as probabilities. The Shannon entropy of these probabilities is typically referred in the literature as spectral entropy, e.g. [Llanos2016],[Tian2017].\n\nThe closer the spectrum is to flat, i.e., white noise, the higher the entropy. However, you can't compare entropies of timeseries with different length, because the binning in spectral space depends on the length of the input.\n\nOutcome space\n\nThe outcome space Ω for PowerSpectrum is the set of frequencies in Fourier space. They should be multiplied with the sampling rate of the signal, which is assumed to be 1.\n\n[Llanos2016]: Llanos et al., Power spectral entropy as an information-theoretic correlate of manner of articulation in American English, The Journal of the Acoustical Society of America 141, EL127 (2017)\n\n[Tian2017]: Tian et al, Spectral Entropy Can Predict Changes of Working Memory Performance Reduced by Short-Time Training in the Delayed-Match-to-Sample Task, Front. Hum. Neurosci.\n\n\n\n\n\n","category":"type"},{"location":"probabilities/#Diversity","page":"Probabilities","title":"Diversity","text":"","category":"section"},{"location":"probabilities/","page":"Probabilities","title":"Probabilities","text":"Diversity","category":"page"},{"location":"probabilities/#Entropies.Diversity","page":"Probabilities","title":"Entropies.Diversity","text":"Diversity(; m::Int, τ::Int, nbins::Int)\n\nA ProbabilitiesEstimator based on the cosine similarity. It can be used with entropy to compute diversity entropy (Wang et al., 2020)[Wang2020].\n\nThe implementation here allows for τ != 1, which was not considered in the original paper.\n\nDescription\n\nDiversity probabilities are computed as follows.\n\nFrom the input time series x, using embedding lag τ and embedding dimension m,  construct the embedding  Y = bf x_i  = (x_i x_i+tau x_i+2tau ldots x_i+mtau - 1_i = 1^N-mτ.\nCompute D = d(bf x_t bf x_t+1) _t=1^N-mτ-1,  where d(cdot cdot) is the cosine similarity between two m-dimensional  vectors in the embedding.\nDivide the interval [-1, 1] into nbins equally sized subintervals.\nConstruct a histogram of cosine similarities d in D over those subintervals.\nSum-normalize the histogram to obtain probabilities.\n\nOutcome space\n\nThe outcome space for Diversity is the bins of the [-1, 1] interval. The left side of each bin is returned in outcome_space.\n\nprobabilities_and_outcomes. Events are the corners of the cosine similarity bins.   Each bin has width nextfloat(2 / nbins).\ntotal_outcomes. The total number of states is given by nbins.\n\n[Wang2020]: Wang, X., Si, S., & Li, Y. (2020). Multiscale diversity entropy: A novel dynamical measure for fault diagnosis of rotating machinery. IEEE Transactions on Industrial Informatics, 17(8), 5419-5429.\n\n\n\n\n\n","category":"type"},{"location":"examples/#Entropies.jl-Examples","page":"Entropies.jl Examples","title":"Entropies.jl Examples","text":"","category":"section"},{"location":"examples/#Indirect-entropy-(nearest-neighbors)","page":"Entropies.jl Examples","title":"Indirect entropy (nearest neighbors)","text":"","category":"section"},{"location":"examples/","page":"Entropies.jl Examples","title":"Entropies.jl Examples","text":"Here, we reproduce Figure 1 in Charzyńska & Gambin (2016)[Charzyńska2016]. Their example demonstrates how the Kraskov and KozachenkoLeonenko nearest neighbor based estimators converge towards the true entropy value for increasing time series length. We extend their example with Zhu and ZhuSingh estimators, which are also based on nearest neighbor searches.","category":"page"},{"location":"examples/","page":"Entropies.jl Examples","title":"Entropies.jl Examples","text":"Input data are from a uniform 1D distribution U(0 1), for which the true entropy is ln(1 - 0) = 0).","category":"page"},{"location":"examples/","page":"Entropies.jl Examples","title":"Entropies.jl Examples","text":"using Entropies\nusing DynamicalSystemsBase, CairoMakie, Statistics\nusing Distributions: Uniform, Normal\n\n# Define estimators\nbase = MathConstants.e # shouldn't really matter here, because the target entropy is 0.\nw = 0 # Theiler window of 0 (only exclude the point itself during neighbor searches)\nestimators = [\n    # with k = 1, Kraskov is virtually identical to\n    # Kozachenko-Leonenko, so pick a higher number of neighbors for Kraskov\n    Kraskov(; k = 3, w, base),\n    KozachenkoLeonenko(; w, base),\n    Zhu(; k = 3, w, base),\n    ZhuSingh(; k = 3, w, base),\n]\nlabels = [\"KozachenkoLeonenko\", \"Kraskov\", \"Zhu\", \"ZhuSingh\"]\n\n# Test each estimator `nreps` times over time series of varying length.\nnreps = 50\nNs = [100:100:500; 1000:1000:10000]\n\nHs_uniform = [[zeros(nreps) for N in Ns] for e in estimators]\nfor (i, e) in enumerate(estimators)\n    for j = 1:nreps\n        pts = rand(Uniform(0, 1), maximum(Ns)) |> Dataset\n        for (k, N) in enumerate(Ns)\n            Hs_uniform[i][k][j] = entropy(e, pts[1:N])\n        end\n    end\nend\n\nfig = Figure(resolution = (600, length(estimators) * 200))\nfor (i, e) in enumerate(estimators)\n    Hs = Hs_uniform[i]\n    ax = Axis(fig[i,1]; ylabel = \"h (nats)\")\n    lines!(ax, Ns, mean.(Hs); color = Cycled(i), label = labels[i])\n    band!(ax, Ns, mean.(Hs) .+ std.(Hs), mean.(Hs) .- std.(Hs);\n    color = (Main.COLORS[i], 0.5))\n    ylims!(-0.25, 0.25)\n    axislegend()\nend\n\nfig","category":"page"},{"location":"examples/#Indirect-entropy-(order-statistics)","page":"Entropies.jl Examples","title":"Indirect entropy (order statistics)","text":"","category":"section"},{"location":"examples/","page":"Entropies.jl Examples","title":"Entropies.jl Examples","text":"Entropies.jl also provides entropy estimators based on order statistics. These estimators are only defined for scalar-valued vectors, so we pass the data as Vector{<:Real}s instead of Datasets, as we did for the nearest-neighbor estimators above.","category":"page"},{"location":"examples/","page":"Entropies.jl Examples","title":"Entropies.jl Examples","text":"Here, we show how the Vasicek, Ebrahimi, AlizadehArghami  and Correa direct Shannon entropy estimators, with increasing sample size, approach zero for samples from a uniform distribution on  [0, 1]. The true entropy value in nats for this distribution is ln(1 - 0) = 0.","category":"page"},{"location":"examples/","page":"Entropies.jl Examples","title":"Entropies.jl Examples","text":"using Entropies\nusing Statistics\nusing Distributions\nusing CairoMakie\n\n# Define estimators\nbase = MathConstants.e # shouldn't really matter here, because the target entropy is 0.\n# just provide types here, they are instantiated inside the loop\nestimators = [Vasicek, Ebrahimi, AlizadehArghami, Correa]\nlabels = [\"Vasicek\", \"Ebrahimi\", \"AlizadehArghami\", \"Correa\"]\n\n# Test each estimator `nreps` times over time series of varying length.\nNs = [100:100:500; 1000:1000:10000]\nnreps = 30\n\nHs_uniform = [[zeros(nreps) for N in Ns] for e in estimators]\nfor (i, e) in enumerate(estimators)\n    for j = 1:nreps\n        pts = rand(Uniform(0, 1), maximum(Ns)) # raw timeseries, not a `Dataset`\n        for (k, N) in enumerate(Ns)\n            m = floor(Int, N / 100) # Scale `m` to timeseries length\n            est = e(; m, base) # Instantiate estimator with current `m`\n            Hs_uniform[i][k][j] = entropy(est, pts[1:N])\n        end\n    end\nend\n\nfig = Figure(resolution = (600, length(estimators) * 200))\nfor (i, e) in enumerate(estimators)\n    Hs = Hs_uniform[i]\n    ax = Axis(fig[i,1]; ylabel = \"h (nats)\")\n    lines!(ax, Ns, mean.(Hs); color = Cycled(i), label = labels[i])\n    band!(ax, Ns, mean.(Hs) .+ std.(Hs), mean.(Hs) .- std.(Hs);\n    color = (Main.COLORS[i], 0.5))\n    ylims!(-0.25, 0.25)\n    axislegend()\nend\n\nfig","category":"page"},{"location":"examples/","page":"Entropies.jl Examples","title":"Entropies.jl Examples","text":"As for the nearest neighbor estimators, both estimators also approach the true entropy value for this example, but is negatively biased for small sample sizes.","category":"page"},{"location":"examples/#Permutation-entropy-example","page":"Entropies.jl Examples","title":"Permutation entropy example","text":"","category":"section"},{"location":"examples/","page":"Entropies.jl Examples","title":"Entropies.jl Examples","text":"This example reproduces an example from Bandt and Pompe (2002), where the permutation entropy is compared with the largest Lyapunov exponents from time series of the chaotic logistic map. Entropy estimates using SymbolicWeightedPermutation and SymbolicAmplitudeAwarePermutation are added here for comparison.","category":"page"},{"location":"examples/","page":"Entropies.jl Examples","title":"Entropies.jl Examples","text":"using DynamicalSystemsBase, CairoMakie, ChaosTools\n\nds = Systems.logistic()\nrs = 3.4:0.001:4\nN_lyap, N_ent = 100000, 10000\nm, τ = 6, 1 # Symbol size/dimension and embedding lag\n\n# Generate one time series for each value of the logistic parameter r\nlyaps, hs_perm, hs_wtperm, hs_ampperm = [zeros(length(rs)) for _ in 1:4]\n\nfor (i, r) in enumerate(rs)\n    ds.p[1] = r\n    lyaps[i] = lyapunov(ds, N_lyap)\n\n    x = trajectory(ds, N_ent) # time series\n    hperm = entropy(x, SymbolicPermutation(; m, τ))\n    hwtperm = entropy(x, SymbolicWeightedPermutation(; m, τ))\n    hampperm = entropy(x, SymbolicAmplitudeAwarePermutation(; m, τ))\n\n    hs_perm[i] = hperm; hs_wtperm[i] = hwtperm; hs_ampperm[i] = hampperm\nend\n\nfig = Figure()\na1 = Axis(fig[1,1]; ylabel = L\"\\lambda\")\nlines!(a1, rs, lyaps); ylims!(a1, (-2, log(2)))\na2 = Axis(fig[2,1]; ylabel = L\"h_6 (SP)\")\nlines!(a2, rs, hs_perm; color = Cycled(2))\na3 = Axis(fig[3,1]; ylabel = L\"h_6 (WT)\")\nlines!(a3, rs, hs_wtperm; color = Cycled(3))\na4 = Axis(fig[4,1]; ylabel = L\"h_6 (SAAP)\")\nlines!(a4, rs, hs_ampperm; color = Cycled(4))\na4.xlabel = L\"r\"\n\nfor a in (a1,a2,a3)\n    hidexdecorations!(a, grid = false)\nend\nfig","category":"page"},{"location":"examples/#Kernel-density-example","page":"Entropies.jl Examples","title":"Kernel density example","text":"","category":"section"},{"location":"examples/","page":"Entropies.jl Examples","title":"Entropies.jl Examples","text":"Here, we draw some random points from a 2D normal distribution. Then, we use kernel density estimation to associate a probability to each point p, measured by how many points are within radius 1.5 of p. Plotting the actual points, along with their associated probabilities estimated by the KDE procedure, we get the following surface plot.","category":"page"},{"location":"examples/","page":"Entropies.jl Examples","title":"Entropies.jl Examples","text":"using Entropies\nusing DelayEmbeddings\nusing DynamicalSystemsBase, CairoMakie, Distributions\n𝒩 = MvNormal([1, -4], 2)\nN = 500\nD = Dataset(sort([rand(𝒩) for i = 1:N]))\nx, y = columns(D)\np = probabilities(D, NaiveKernel(1.5))\nfig, ax = scatter(D[:, 1], D[:, 2], zeros(N);\n    markersize=8, axis=(type = Axis3,)\n)\nsurface!(ax, x, y, p.p)\nax.zlabel = \"P\"\nax.zticklabelsvisible = false\nfig","category":"page"},{"location":"examples/#Wavelet-entropy-example","page":"Entropies.jl Examples","title":"Wavelet entropy example","text":"","category":"section"},{"location":"examples/","page":"Entropies.jl Examples","title":"Entropies.jl Examples","text":"The scale-resolved wavelet entropy should be lower for very regular signals (most of the energy is contained at one scale) and higher for very irregular signals (energy spread more out across scales).","category":"page"},{"location":"examples/","page":"Entropies.jl Examples","title":"Entropies.jl Examples","text":"using DynamicalSystemsBase, CairoMakie\nN, a = 1000, 10\nt = LinRange(0, 2*a*π, N)\n\nx = sin.(t);\ny = sin.(t .+ cos.(t/0.5));\nz = sin.(rand(1:15, N) ./ rand(1:10, N))\n\nh_x = entropy_wavelet(x)\nh_y = entropy_wavelet(y)\nh_z = entropy_wavelet(z)\n\nfig = Figure()\nax = Axis(fig[1,1]; ylabel = \"x\")\nlines!(ax, t, x; color = Cycled(1), label = \"h=$(h=round(h_x, sigdigits = 5))\");\nay = Axis(fig[2,1]; ylabel = \"y\")\nlines!(ay, t, y; color = Cycled(2), label = \"h=$(h=round(h_y, sigdigits = 5))\");\naz = Axis(fig[3,1]; ylabel = \"z\", xlabel = \"time\")\nlines!(az, t, z; color = Cycled(3), label = \"h=$(h=round(h_z, sigdigits = 5))\");\nfor a in (ax, ay, az); axislegend(a); end\nfor a in (ax, ay); hidexdecorations!(a; grid=false); end\nfig","category":"page"},{"location":"examples/#Properties-of-different-entropies","page":"Entropies.jl Examples","title":"Properties of different entropies","text":"","category":"section"},{"location":"examples/","page":"Entropies.jl Examples","title":"Entropies.jl Examples","text":"Here, we show the sensitivity of the various entropies to variations in their parameters.","category":"page"},{"location":"examples/#Curado-entropy","page":"Entropies.jl Examples","title":"Curado entropy","text":"","category":"section"},{"location":"examples/","page":"Entropies.jl Examples","title":"Entropies.jl Examples","text":"Here, we reproduce Figure 2 from Curado & Nobre (2004)[Curado2004], showing how the Curado entropy changes as function of the parameter a for a range of two-element probability distributions given by Probabilities([p, 1 - p] for p in 1:0.0:0.01:1.0).","category":"page"},{"location":"examples/","page":"Entropies.jl Examples","title":"Entropies.jl Examples","text":"using Entropies, CairoMakie\nbs = [1.0, 1.5, 2.0, 3.0, 4.0, 10.0]\nps = [Probabilities([p, 1 - p]) for p = 0.0:0.01:1.0]\nhs = [[entropy(Curado(; b = b), p) for p in ps] for b in bs]\nfig = Figure()\nax = Axis(fig[1,1]; xlabel = \"p\", ylabel = \"H(p)\")\npp = [p[1] for p in ps]\nfor (i, b) in enumerate(bs)\n    lines!(ax, pp, hs[i], label = \"b=$b\", color = Cycled(i))\nend\naxislegend(ax)\nfig","category":"page"},{"location":"examples/","page":"Entropies.jl Examples","title":"Entropies.jl Examples","text":"[Curado2004]: Curado, E. M., & Nobre, F. D. (2004). On the stability of analytic entropic forms. Physica A: Statistical Mechanics and its Applications, 335(1-2), 94-106.","category":"page"},{"location":"examples/#Stretched-exponential-entropy","page":"Entropies.jl Examples","title":"Stretched exponential entropy","text":"","category":"section"},{"location":"examples/","page":"Entropies.jl Examples","title":"Entropies.jl Examples","text":"Here, we reproduce the example from Anteneodo & Plastino (1999)[Anteneodo1999], showing how the stretched exponential entropy changes as function of the parameter η for a range of two-element probability distributions given by Probabilities([p, 1 - p] for p in 1:0.0:0.01:1.0).","category":"page"},{"location":"examples/","page":"Entropies.jl Examples","title":"Entropies.jl Examples","text":"using Entropies, SpecialFunctions, CairoMakie\nηs = [0.01, 0.2, 0.3, 0.5, 0.7, 1.0, 1.5, 3.0]\nps = [Probabilities([p, 1 - p]) for p = 0.0:0.01:1.0]\n\nhs_norm = [[entropy(StretchedExponential( η = η), p) / gamma((η + 1)/η) for p in ps] for η in ηs]\nfig = Figure()\nax = Axis(fig[1,1]; xlabel = \"p\", ylabel = \"H(p)\")\npp = [p[1] for p in ps]\n\nfor (i, η) in enumerate(ηs)\n    lines!(ax, pp, hs_norm[i], label = \"η=$η\")\nend\naxislegend(ax)\nfig","category":"page"},{"location":"examples/","page":"Entropies.jl Examples","title":"Entropies.jl Examples","text":"[Anteneodo1999]: Anteneodo, C., & Plastino, A. R. (1999). Maximum entropy approach to stretched exponential probability distributions. Journal of Physics A: Mathematical and General, 32(7), 1089.","category":"page"},{"location":"examples/#dispersion_example","page":"Entropies.jl Examples","title":"Dispersion entropy","text":"","category":"section"},{"location":"examples/","page":"Entropies.jl Examples","title":"Entropies.jl Examples","text":"Here we compute dispersion entropy (Rostaghi et al. 2016)[Rostaghi2016], using the use the Dispersion probabilities estimator, for a time series consisting of normally distributed noise with a single spike in the middle of the signal. We compute the entropies over a range subsets of the data, using a sliding window consisting of 70 data points, stepping the window 10 time steps at a time. This example is adapted from Li et al. (2021)[Li2019].","category":"page"},{"location":"examples/","page":"Entropies.jl Examples","title":"Entropies.jl Examples","text":"using Entropies\nusing DynamicalSystemsBase\nusing Random\nusing CairoMakie\nusing Distributions\n\nn = 1000\nts = 1:n\nx = [i == n ÷ 2 ? 50.0 : 0.0 for i in ts]\nrng = Random.default_rng()\ns = rand(rng, Normal(0, 1), n)\ny = x .+ s\n\nws = 70\nwindows = [t:t+ws for t in 1:10:n-ws]\nrdes = zeros(length(windows))\ndes = zeros(length(windows))\npes = zeros(length(windows))\n\nm, c = 2, 6\nest_de = Dispersion(encoding = GaussianCDFEncoding(c), m = m, τ = 1)\nfor (i, window) in enumerate(windows)\n    des[i] = entropy_normalized(Renyi(), y[window], est_de)\nend\n\nfig = Figure()\na1 = Axis(fig[1,1]; xlabel = \"Time step\", ylabel = \"Value\")\nlines!(a1, ts, y)\ndisplay(fig)\na2 = Axis(fig[2, 1]; xlabel = \"Time step\", ylabel = \"Value\")\np_de = scatterlines!([first(w) for w in windows], des,\n    label = \"Dispersion entropy\",\n    color = :red,\n    markercolor = :red, marker = '●', markersize = 20)\n\naxislegend(position = :rc)\nylims!(0, max(maximum(pes), 1))\nfig","category":"page"},{"location":"examples/","page":"Entropies.jl Examples","title":"Entropies.jl Examples","text":"[Rostaghi2016]: Rostaghi, M., & Azami, H. (2016). Dispersion entropy: A measure for time-series analysis. IEEE Signal Processing Letters, 23(5), 610-614.","category":"page"},{"location":"examples/","page":"Entropies.jl Examples","title":"Entropies.jl Examples","text":"[Li2019]: Li, Y., Gao, X., & Wang, L. (2019). Reverse dispersion entropy: a new complexity measure for sensor signal. Sensors, 19(23), 5203.","category":"page"},{"location":"examples/#Normalized-entropy-for-comparing-different-signals","page":"Entropies.jl Examples","title":"Normalized entropy for comparing different signals","text":"","category":"section"},{"location":"examples/","page":"Entropies.jl Examples","title":"Entropies.jl Examples","text":"When comparing different signals or signals that have different length, it is best to normalize entropies so that the \"complexity\" or \"disorder\" quantification is directly comparable between signals. Here is an example based on the Wavelet entropy example (where we use the spectral entropy instead of the wavelet entropy):","category":"page"},{"location":"examples/","page":"Entropies.jl Examples","title":"Entropies.jl Examples","text":"using DynamicalSystemsBase\nN1, N2, a = 101, 100001, 10\n\nfor N in (N1, N2)\n    local t = LinRange(0, 2*a*π, N)\n    local x = sin.(t) # periodic\n    local y = sin.(t .+ cos.(t/0.5)) # periodic, complex spectrum\n    local z = sin.(rand(1:15, N) ./ rand(1:10, N)) # random\n    local w = trajectory(Systems.lorenz(), N÷10; Δt = 0.1, Ttr = 100)[:, 1] # chaotic\n\n    for q in (x, y, z, w)\n        h = entropy(q, PowerSpectrum())\n        n = entropy_normalized(q, PowerSpectrum())\n        println(\"entropy: $(h), normalized: $(n).\")\n    end\nend","category":"page"},{"location":"examples/","page":"Entropies.jl Examples","title":"Entropies.jl Examples","text":"You see that while the direct entropy values of the chaotic and noisy signals change massively with N but they are almost the same for the normalized version. For the regular signals, the entropy decreases nevertheless because the noise contribution of the Fourier computation becomes less significant.","category":"page"},{"location":"#Entropies.jl","page":"Entropies.jl","title":"Entropies.jl","text":"","category":"section"},{"location":"","page":"Entropies.jl","title":"Entropies.jl","text":"Entropies","category":"page"},{"location":"#Entropies","page":"Entropies.jl","title":"Entropies","text":"A Julia package that provides estimators for probabilities and entropies for nonlinear dynamics, nonlinear timeseries analysis, and complex systems. It can be used as a standalone package, or as part of several projects in the JuliaDynamics organization, such as DynamicalSystems.jl or CausalityTools.jl.\n\nTo install it, run import Pkg; Pkg.add(\"Entropies\").\n\n\n\n\n\n","category":"module"},{"location":"","page":"Entropies.jl","title":"Entropies.jl","text":"info: Info\nYou are reading the development version of the documentation of Entropies.jl, that will become version 2.0.","category":"page"},{"location":"#API-and-terminology","page":"Entropies.jl","title":"API & terminology","text":"","category":"section"},{"location":"","page":"Entropies.jl","title":"Entropies.jl","text":"note: Note\nThe documentation here follows (loosely) chapter 5 of Nonlinear Dynamics, Datseris & Parlitz, Springer 2022.","category":"page"},{"location":"","page":"Entropies.jl","title":"Entropies.jl","text":"In the literature, the term \"entropy\" is used (and abused) in multiple contexts. The API and documentation of Entropies.jl aim to clarify some aspects of its usage, and to provide a simple way to obtain probabilities, entropies, or other complexity measures.","category":"page"},{"location":"#Probabilities","page":"Entropies.jl","title":"Probabilities","text":"","category":"section"},{"location":"","page":"Entropies.jl","title":"Entropies.jl","text":"Entropies and other complexity measures are typically computed based on probability distributions. These are obtained from Input data for Entropies.jl in a plethora of different ways. The central API function that returns a probability distribution (in fact, just a vector of probabilities) is probabilities, which takes in a subtype of ProbabilitiesEstimator to specify how the probabilities are computed. All estimators available in Entropies.jl can be found in the estimators page.","category":"page"},{"location":"#Entropies","page":"Entropies.jl","title":"Entropies","text":"","category":"section"},{"location":"","page":"Entropies.jl","title":"Entropies.jl","text":"Entropy is an established concept in statistics, information theory, and nonlinear dynamics. However it is also an umbrella term that may mean several computationally different quantities. In Entropies.jl, we provide the generic function entropy that tries to both clarify the disparate \"entropy concepts\", while unifying them under a common interface that highlights the modular nature of the word \"entropy\".","category":"page"},{"location":"","page":"Entropies.jl","title":"Entropies.jl","text":"Most of the time, computing an entropy boils down to two simple steps: first estimating a probability distribution, and then applying one of the so-called \"generalized entropy\" formulas to the distributions. Thus, any of the implemented probabilities estimators can be used to compute generalized entropies.","category":"page"},{"location":"","page":"Entropies.jl","title":"Entropies.jl","text":"tip: There aren't many entropies, really.\nA crucial thing to clarify is that many quantities that are named as entropies (e.g., permutation entropy entropy_permutation, wavelet entropy entropy_wavelet, etc.), are not really new entropies. They are new probabilities estimators. They simply devise a new way to calculate probabilities from data, and then plug those probabilities into formal entropy formulas such as the Shannon entropy. The probabilities estimators are smartly created so that they elegantly highlight important aspects of the data relevant to complexity.These names are common place, and so in Entropies.jl we provide convenience functions like entropy_wavelet. However, it should be noted that these functions really aren't anything more than 2-lines-of-code wrappers that call entropy with the appropriate ProbabilitiesEstimator.There are only a few exceptions to this rule, which are quantities that are able to compute Shannon entropies via alternate means, without explicitly computing some probability distributions. These are IndirectEntropy instances, such as Kraskov.","category":"page"},{"location":"#Other-complexity-measures","page":"Entropies.jl","title":"Other complexity measures","text":"","category":"section"},{"location":"","page":"Entropies.jl","title":"Entropies.jl","text":"Other complexity measures, which strictly speaking don't compute entropies, and may or may not explicitly compute probability distributions, are found in Complexity.jl package. This includes measures like sample entropy and approximate entropy.","category":"page"},{"location":"#input_data","page":"Entropies.jl","title":"Input data for Entropies.jl","text":"","category":"section"},{"location":"","page":"Entropies.jl","title":"Entropies.jl","text":"The input data type typically depend on the probability estimator chosen. In general though, the standard DynamicalSystems.jl approach is taken and as such we have three types of input data:","category":"page"},{"location":"","page":"Entropies.jl","title":"Entropies.jl","text":"Timeseries, which are AbstractVector{<:Real}, used in e.g. with WaveletOverlap.\nMulti-dimensional timeseries, or datasets, or state space sets, which are Dataset, used e.g. with NaiveKernel.\nSpatial data, which are higher dimensional standard Arrays, used e.g. with  SpatialSymbolicPermutation.","category":"page"},{"location":"devdocs/#Entropies.jl-Dev-Docs","page":"Entropies.jl Dev Docs","title":"Entropies.jl Dev Docs","text":"","category":"section"},{"location":"devdocs/","page":"Entropies.jl Dev Docs","title":"Entropies.jl Dev Docs","text":"Good practices in developing a code base apply in every Pull Request. The Good Scientific Code Workshop is worth checking out for this.","category":"page"},{"location":"devdocs/#Adding-a-new-ProbabilitiesEstimator","page":"Entropies.jl Dev Docs","title":"Adding a new ProbabilitiesEstimator","text":"","category":"section"},{"location":"devdocs/#Mandatory-steps","page":"Entropies.jl Dev Docs","title":"Mandatory steps","text":"","category":"section"},{"location":"devdocs/","page":"Entropies.jl Dev Docs","title":"Entropies.jl Dev Docs","text":"Decide on the outcome space and how the estimator will map probabilities to outcomes.\nDefine your type and make it subtype ProbabilitiesEstimator.\nAdd a docstring to your type following the style of the docstrings of other estimators.\nImplement dispatch for probabilities_and_outcomes.\nImplement dispatch for outcome_space.\nAdd your type to the list in the docstring of ProbabilitiyEstimator.","category":"page"},{"location":"devdocs/#Optional-steps","page":"Entropies.jl Dev Docs","title":"Optional steps","text":"","category":"section"},{"location":"devdocs/","page":"Entropies.jl Dev Docs","title":"Entropies.jl Dev Docs","text":"You may extend any of the following functions if there are potential performance benefits in doing so:","category":"page"},{"location":"devdocs/","page":"Entropies.jl Dev Docs","title":"Entropies.jl Dev Docs","text":"probabilities. By default it calls probabilities_and_outcomes and returns the first value.\noutcomes. By default calls probabilities_and_outcomes and returns the second value.\ntotal_outcomes. By default it returns the length of outcome_space. This is the function that most typically has performance benefits if implemented explicitly, so most existing estimators extend it by default.","category":"page"},{"location":"devdocs/#Tests","page":"Entropies.jl Dev Docs","title":"Tests","text":"","category":"section"},{"location":"devdocs/","page":"Entropies.jl Dev Docs","title":"Entropies.jl Dev Docs","text":"You also need to add tests for all functions that you explicitly extended. Not extended functions do not need to be tested.","category":"page"}]
}
