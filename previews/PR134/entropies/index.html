<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Entropies · Entropies.jl</title><script data-outdated-warner src="../assets/warner.js"></script><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.045/juliamono.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.24/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL=".."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../assets/documenter.js"></script><script src="../siteinfo.js"></script><script src="../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../assets/themeswap.js"></script><link href="https://fonts.googleapis.com/css?family=Montserrat|Source+Code+Pro&amp;display=swap" rel="stylesheet" type="text/css"/></head><body><div id="documenter"><nav class="docs-sidebar"><div class="docs-package-name"><span class="docs-autofit"><a href="../">Entropies.jl</a></span></div><form class="docs-search" action="../search/"><input class="docs-search-query" id="documenter-search-query" name="q" type="text" placeholder="Search docs"/></form><ul class="docs-menu"><li><a class="tocitem" href="../">Entropies.jl</a></li><li><a class="tocitem" href="../probabilities/">Probabilities</a></li><li class="is-active"><a class="tocitem" href>Entropies</a><ul class="internal"><li><a class="tocitem" href="#Entropy-API"><span>Entropy API</span></a></li><li><a class="tocitem" href="#Rényi-(generalized)-entropy"><span>Rényi (generalized) entropy</span></a></li><li><a class="tocitem" href="#Tsallis-(generalized)-entropy"><span>Tsallis (generalized) entropy</span></a></li><li><a class="tocitem" href="#Shannon-entropy-(convenience)"><span>Shannon entropy (convenience)</span></a></li><li><a class="tocitem" href="#Curado-entropy"><span>Curado entropy</span></a></li><li><a class="tocitem" href="#Stretched-exponental-entropy"><span>Stretched exponental entropy</span></a></li><li><a class="tocitem" href="#Normalized-entropies"><span>Normalized entropies</span></a></li><li><a class="tocitem" href="#Indirect-entropies"><span>Indirect entropies</span></a></li><li><a class="tocitem" href="#Convenience-functions"><span>Convenience functions</span></a></li></ul></li><li><a class="tocitem" href="../complexity_measures/">Complexity measures</a></li><li><a class="tocitem" href="../examples/">Entropies.jl examples</a></li><li><a class="tocitem" href="../utils/">Utility methods</a></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><nav class="breadcrumb"><ul class="is-hidden-mobile"><li class="is-active"><a href>Entropies</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>Entropies</a></li></ul></nav><div class="docs-right"><a class="docs-edit-link" href="https://github.com/JuliaDynamics/Entropies.jl/blob/main/docs/src/entropies.md" title="Edit on GitHub"><span class="docs-icon fab"></span><span class="docs-label is-hidden-touch">Edit on GitHub</span></a><a class="docs-settings-button fas fa-cog" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-sidebar-button fa fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a></div></header><article class="content" id="documenter-page"><h2 id="Entropy-API"><a class="docs-heading-anchor" href="#Entropy-API">Entropy API</a><a id="Entropy-API-1"></a><a class="docs-heading-anchor-permalink" href="#Entropy-API" title="Permalink"></a></h2><article class="docstring"><header><a class="docstring-binding" id="Entropies.entropy" href="#Entropies.entropy"><code>Entropies.entropy</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">entropy([e::Entropy,] x, est::ProbabilitiesEstimator) → h::Real
entropy([e::Entropy,] probs::Probabilities) → h::Real</code></pre><p>Compute a (generalized) entropy <code>h</code> from <code>x</code> according to the specified entropy type <code>e</code> and the given probability estimator <code>est</code>.</p><p>Alternatively compute the entropy directly from the existing probabilities <code>probs</code>. In fact, the first method is a 2-lines-of-code wrapper that calls <a href="../probabilities/#Entropies.probabilities"><code>probabilities</code></a> and gives the result to the second method.</p><p><code>x</code> is typically an <code>Array</code> or a <code>Dataset</code>, see <a href="@ref">Input data for Entropies.jl</a>.</p><p>The entropy types that support this interface are &quot;direct&quot; entropies. They always yield an entropy value given a probability distribution. Such entropies are theoretically well-founded and are typically called &quot;generalized entropies&quot;. Currently implemented types are:</p><ul><li><a href="#Entropies.Renyi"><code>Renyi</code></a>.</li><li><a href="#Entropies.Tsallis"><code>Tsallis</code></a>.</li><li><a href="#Entropies.Shannon"><code>Shannon</code></a>, which is a subcase of the above two in the limit <code>q → 1</code>.</li><li><a href="#Entropies.Curado"><code>Curado</code></a>.</li><li><a href="#Entropies.StretchedExponential"><code>StretchedExponential</code></a>.</li></ul><p>The entropy (first argument) is optional: if not given, <code>Shannon()</code> is used instead.</p><p>These entropies also have a well defined maximum value for a given probability estimator. To obtain this value one only needs to call the <a href="#Base.maximum-Tuple{Entropy, Any, ProbabilitiesEstimator}"><code>maximum</code></a> function with the chosen entropy type and probability estimator. Or, one can use <a href="#Entropies.entropy_normalized"><code>entropy_normalized</code></a> to obtain the normalized form of the entropy (divided by the maximum).</p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">x = [rand(Bool) for _ in 1:10000] # coin toss
ps = probabilities(x) # gives about [0.5, 0.5] by definition
h = entropy(ps) # gives 1, about 1 bit by definition
h = entropy(Shannon(), ps) # syntactically equivalent to above
h = entropy(Shannon(), x, CountOccurrences()) # syntactically equivalent to above
h = entropy(x, SymbolicPermutation(;m=3)) # gives about 2, again by definition
h = entropy(Renyi(2.0), ps) # also gives 1, order `q` doesn&#39;t matter for coin toss</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaDynamics/Entropies.jl/blob/3cbcbfaeb0b4a75e5b786b97bb423214ca66a5f3/src/entropy.jl#L13-L54">source</a></section><section><div><pre><code class="nohighlight hljs">entropy(e::IndirectEntropy, x) → h::Real</code></pre><p>Compute the entropy of <code>x</code>, here named <code>h</code>, according to the specified indirect entropy estimator <code>e</code>.</p><p>In contrast to the &quot;typical&quot; way one obtains entropies in the above methods, indirect entropy estimators compute Shannon entropies via alternate means, without explicitly computing probability distributions. The available indirect entropies are:</p><ul><li><a href="#Entropies.Kraskov"><code>Kraskov</code></a>.</li><li><a href="#Entropies.KozachenkoLeonenko"><code>KozachenkoLeonenko</code></a>.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaDynamics/Entropies.jl/blob/3cbcbfaeb0b4a75e5b786b97bb423214ca66a5f3/src/entropy.jl#L128-L140">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Entropies.entropy!" href="#Entropies.entropy!"><code>Entropies.entropy!</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">entropy!(s, [e::Entropy,] x, est::ProbabilitiesEstimator)</code></pre><p>Similar to <code>probabilities!</code>, this is an in-place version of <a href="#Entropies.entropy"><code>entropy</code></a> that allows pre-allocation of temporarily used containers.</p><p>The entropy (second argument) is optional: if not given, <code>Shannon()</code> is used instead.</p><p>Only works for certain estimators. See for example <a href="../probabilities/#Entropies.SymbolicPermutation"><code>SymbolicPermutation</code></a>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaDynamics/Entropies.jl/blob/3cbcbfaeb0b4a75e5b786b97bb423214ca66a5f3/src/entropy.jl#L65-L74">source</a></section></article><h2 id="Rényi-(generalized)-entropy"><a class="docs-heading-anchor" href="#Rényi-(generalized)-entropy">Rényi (generalized) entropy</a><a id="Rényi-(generalized)-entropy-1"></a><a class="docs-heading-anchor-permalink" href="#Rényi-(generalized)-entropy" title="Permalink"></a></h2><article class="docstring"><header><a class="docstring-binding" id="Entropies.Renyi" href="#Entropies.Renyi"><code>Entropies.Renyi</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">Renyi &lt;: Entropy
Renyi(q, base = 2)
Renyi(; q = 1.0, base = 2)</code></pre><p>The Rényi<sup class="footnote-reference"><a id="citeref-Rényi1960" href="#footnote-Rényi1960">[Rényi1960]</a></sup> generalized order-<code>q</code> entropy, used with <a href="#Entropies.entropy"><code>entropy</code></a> to compute an entropy with units given by <code>base</code> (typically <code>2</code> or <code>MathConstants.e</code>).</p><p><strong>Description</strong></p><p>Let <span>$p$</span> be an array of probabilities (summing to 1). Then the Rényi generalized entropy is</p><p class="math-container">\[H_q(p) = \frac{1}{1-q} \log \left(\sum_i p[i]^q\right)\]</p><p>and generalizes other known entropies, like e.g. the information entropy (<span>$q = 1$</span>, see <sup class="footnote-reference"><a id="citeref-Shannon1948" href="#footnote-Shannon1948">[Shannon1948]</a></sup>), the maximum entropy (<span>$q=0$</span>, also known as Hartley entropy), or the correlation entropy (<span>$q = 2$</span>, also known as collision entropy).</p><p>If the probability estimator has known alphabet length <span>$L$</span>, then the maximum value of the Rényi entropy is <span>$\log_{base}(L)$</span>, which is the entropy of the uniform distribution with given alphabet length.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaDynamics/Entropies.jl/blob/3cbcbfaeb0b4a75e5b786b97bb423214ca66a5f3/src/entropies/renyi.jl#L3-L34">source</a></section></article><h2 id="Tsallis-(generalized)-entropy"><a class="docs-heading-anchor" href="#Tsallis-(generalized)-entropy">Tsallis (generalized) entropy</a><a id="Tsallis-(generalized)-entropy-1"></a><a class="docs-heading-anchor-permalink" href="#Tsallis-(generalized)-entropy" title="Permalink"></a></h2><article class="docstring"><header><a class="docstring-binding" id="Entropies.Tsallis" href="#Entropies.Tsallis"><code>Entropies.Tsallis</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">Tsallis &lt;: Entropy
Tsallis(q; k = 1.0, base = 2)
Tsallis(; q = 1.0, k = 1.0, base = 2)</code></pre><p>The Tsallis<sup class="footnote-reference"><a id="citeref-Tsallis1988" href="#footnote-Tsallis1988">[Tsallis1988]</a></sup> generalized order-<code>q</code> entropy, used with <a href="#Entropies.entropy"><code>entropy</code></a> to compute an entropy.</p><p><code>base</code> only applies in the limiting case <code>q == 1</code>, in which the Tsallis entropy reduces to Shannon entropy.</p><p><strong>Description</strong></p><p>The Tsallis entropy is a generalization of the Boltzmann-Gibbs entropy, with <code>k</code> standing for the Boltzmann constant. It is defined as</p><p class="math-container">\[S_q(p) = \frac{k}{q - 1}\left(1 - \sum_{i} p[i]^q\right)\]</p><p>If the probability estimator has known alphabet length <span>$L$</span>, then the maximum value of the Tsallis entropy is <span>$k(L^{1 - q} - 1)/(1 - q)$</span>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaDynamics/Entropies.jl/blob/3cbcbfaeb0b4a75e5b786b97bb423214ca66a5f3/src/entropies/tsallis.jl#L6-L32">source</a></section></article><h2 id="Shannon-entropy-(convenience)"><a class="docs-heading-anchor" href="#Shannon-entropy-(convenience)">Shannon entropy (convenience)</a><a id="Shannon-entropy-(convenience)-1"></a><a class="docs-heading-anchor-permalink" href="#Shannon-entropy-(convenience)" title="Permalink"></a></h2><article class="docstring"><header><a class="docstring-binding" id="Entropies.Shannon" href="#Entropies.Shannon"><code>Entropies.Shannon</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">Shannon(; base = 2)</code></pre><p>The Shannon<sup class="footnote-reference"><a id="citeref-Shannon1948" href="#footnote-Shannon1948">[Shannon1948]</a></sup> entropy, used with <a href="#Entropies.entropy"><code>entropy</code></a> to compute:</p><p class="math-container">\[H(p) = - \sum_i p[i] \log(p[i])\]</p><p>with the <span>$log$</span> at the given <code>base</code>.</p><p><code>Shannon(base)</code> is syntactically equivalent to <code>Renyi(; base)</code>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaDynamics/Entropies.jl/blob/3cbcbfaeb0b4a75e5b786b97bb423214ca66a5f3/src/entropies/renyi.jl#L60-L73">source</a></section></article><h2 id="Curado-entropy"><a class="docs-heading-anchor" href="#Curado-entropy">Curado entropy</a><a id="Curado-entropy-1"></a><a class="docs-heading-anchor-permalink" href="#Curado-entropy" title="Permalink"></a></h2><article class="docstring"><header><a class="docstring-binding" id="Entropies.Curado" href="#Entropies.Curado"><code>Entropies.Curado</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">Curado &lt;: Entropy
Curado(; b = 1.0)</code></pre><p>The Curado entropy (Curado &amp; Nobre, 2004)<sup class="footnote-reference"><a id="citeref-Curado2004" href="#footnote-Curado2004">[Curado2004]</a></sup>, used with <a href="#Entropies.entropy"><code>entropy</code></a> to compute</p><p class="math-container">\[H_C(p) = \left( \sum_{i=1}^N e^{-b p_i} \right) + e^{-b} - 1,\]</p><p>with <code>b ∈ ℛ, b &gt; 0</code>, where the terms outside the sum ensures that <span>$H_C(0) = H_C(1) = 0$</span>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaDynamics/Entropies.jl/blob/3cbcbfaeb0b4a75e5b786b97bb423214ca66a5f3/src/entropies/curado.jl#L3-L18">source</a></section></article><h2 id="Stretched-exponental-entropy"><a class="docs-heading-anchor" href="#Stretched-exponental-entropy">Stretched exponental entropy</a><a id="Stretched-exponental-entropy-1"></a><a class="docs-heading-anchor-permalink" href="#Stretched-exponental-entropy" title="Permalink"></a></h2><article class="docstring"><header><a class="docstring-binding" id="Entropies.StretchedExponential" href="#Entropies.StretchedExponential"><code>Entropies.StretchedExponential</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">StretchedExponential &lt;: Entropy
StretchedExponential(; η = 2.0, base = 2)</code></pre><p>The stretched exponential, or Anteneodo-Plastino, entropy (Anteneodo &amp; Plastino, 1999<sup class="footnote-reference"><a id="citeref-Anteneodo1999" href="#footnote-Anteneodo1999">[Anteneodo1999]</a></sup>), used with <a href="#Entropies.entropy"><code>entropy</code></a> to compute</p><p class="math-container">\[S_{\eta}(p) = \sum_{i = 1}^N
\Gamma \left( \dfrac{\eta + 1}{\eta}, - \log_{base}(p_i) \right) -
p_i \Gamma \left( \dfrac{\eta + 1}{\eta} \right),\]</p><p>where <span>$\eta \geq 0$</span>, <span>$\Gamma(\cdot, \cdot)$</span> is the upper incomplete Gamma function, and <span>$\Gamma(\cdot) = \Gamma(\cdot, 0)$</span> is the Gamma function. Reduces to <a href="@ref">Shannon</a> entropy for <code>η = 1.0</code>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaDynamics/Entropies.jl/blob/3cbcbfaeb0b4a75e5b786b97bb423214ca66a5f3/src/entropies/streched_exponential.jl#L6-L26">source</a></section></article><h2 id="Normalized-entropies"><a class="docs-heading-anchor" href="#Normalized-entropies">Normalized entropies</a><a id="Normalized-entropies-1"></a><a class="docs-heading-anchor-permalink" href="#Normalized-entropies" title="Permalink"></a></h2><article class="docstring"><header><a class="docstring-binding" id="Base.maximum-Tuple{Entropy, Any, ProbabilitiesEstimator}" href="#Base.maximum-Tuple{Entropy, Any, ProbabilitiesEstimator}"><code>Base.maximum</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">maximum(e::Entropy, x, est::ProbabilitiesEstimator) → m::Real</code></pre><p>Return the maximum value <code>m</code> of the given entropy type based on the given estimator and the given input <code>x</code> (whose values are not important, but layout and type are).</p><p>This function only works if the maximum value is dedicable, which is possible only when the estimator has a known <a href="../utils/#Entropies.alphabet_length"><code>alphabet_length</code></a>.</p><pre><code class="nohighlight hljs">maximum(e::Entropy, L::Int) → m::Real</code></pre><p>Alternatively, compute the maximum entropy from the alphabet length <code>L</code> directly.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaDynamics/Entropies.jl/blob/3cbcbfaeb0b4a75e5b786b97bb423214ca66a5f3/src/entropy.jl#L86-L98">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Entropies.entropy_normalized" href="#Entropies.entropy_normalized"><code>Entropies.entropy_normalized</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">entropy_normalized([e::Entropy,] x, est::ProbabilitiesEstimator) → h̃ ∈ [0, 1]</code></pre><p>Return the normalized entropy of <code>x</code>, i.e., the value of <a href="#Entropies.entropy"><code>entropy</code></a> divided by the maximum value for <code>e</code>, according to the given probability estimator. If <code>e</code> is not given, it defaults to <code>Shannon()</code>.</p><p>Notice that unlike for <a href="#Entropies.entropy"><code>entropy</code></a>, here there is no method <code>entropy_normalized(e::Entropy, probs::Probabilities)</code> because there is no way to know the amount of <em>possible</em> events (i.e., the <a href="../utils/#Entropies.alphabet_length"><code>alphabet_length</code></a>) from <code>probs</code>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaDynamics/Entropies.jl/blob/3cbcbfaeb0b4a75e5b786b97bb423214ca66a5f3/src/entropy.jl#L107-L117">source</a></section></article><h2 id="Indirect-entropies"><a class="docs-heading-anchor" href="#Indirect-entropies">Indirect entropies</a><a id="Indirect-entropies-1"></a><a class="docs-heading-anchor-permalink" href="#Indirect-entropies" title="Permalink"></a></h2><p>Here we list functions which compute Shannon entropies via alternate means, without explicitly computing some probability distributions and then using the Shannon formula.</p><h3 id="Nearest-neighbors-entropy"><a class="docs-heading-anchor" href="#Nearest-neighbors-entropy">Nearest neighbors entropy</a><a id="Nearest-neighbors-entropy-1"></a><a class="docs-heading-anchor-permalink" href="#Nearest-neighbors-entropy" title="Permalink"></a></h3><article class="docstring"><header><a class="docstring-binding" id="Entropies.Kraskov" href="#Entropies.Kraskov"><code>Entropies.Kraskov</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">Kraskov &lt;: IndirectEntropy
Kraskov(; k::Int = 1, w::Int = 1, base = 2)</code></pre><p>An indirect entropy used in <a href="#Entropies.entropy"><code>entropy</code></a><code>(Kraskov(), x)</code> to estimate the Shannon entropy of <code>x</code> (a multi-dimensional <code>Dataset</code>) to the given <code>base</code> using <code>k</code>-th nearest neighbor searches as in <sup class="footnote-reference"><a id="citeref-Kraskov2004" href="#footnote-Kraskov2004">[Kraskov2004]</a></sup>.</p><p><code>w</code> is the Theiler window, which determines if temporal neighbors are excluded during neighbor searches (defaults to <code>0</code>, meaning that only the point itself is excluded when searching for neighbours).</p><p>See also: <a href="#Entropies.KozachenkoLeonenko"><code>KozachenkoLeonenko</code></a>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaDynamics/Entropies.jl/blob/3cbcbfaeb0b4a75e5b786b97bb423214ca66a5f3/src/entropies/direct_entropies/nearest_neighbors/Kraskov.jl#L3-L20">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Entropies.KozachenkoLeonenko" href="#Entropies.KozachenkoLeonenko"><code>Entropies.KozachenkoLeonenko</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">KozachenkoLeonenko &lt;: IndirectEntropy
KozachenkoLeonenko(; k::Int = 1, w::Int = 1, base = 2)</code></pre><p>An indirect entropy estimator used in <a href="#Entropies.entropy"><code>entropy</code></a><code>(KozachenkoLeonenko(), x)</code> to estimate the Shannon entropy of <code>x</code> (a multi-dimensional <code>Dataset</code>) to the given <code>base</code> using <code>k</code>-th nearest neighbor searches using the method from Kozachenko &amp; Leonenko<sup class="footnote-reference"><a id="citeref-KozachenkoLeonenko1987" href="#footnote-KozachenkoLeonenko1987">[KozachenkoLeonenko1987]</a></sup>, as described in Charzyńska and Gambin<sup class="footnote-reference"><a id="citeref-Charzyńska2016" href="#footnote-Charzyńska2016">[Charzyńska2016]</a></sup>.</p><p><code>w</code> is the Theiler window, which determines if temporal neighbors are excluded during neighbor searches (defaults to <code>0</code>, meaning that only the point itself is excluded when searching for neighbours).</p><p>See also: <a href="#Entropies.Kraskov"><code>Kraskov</code></a>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaDynamics/Entropies.jl/blob/3cbcbfaeb0b4a75e5b786b97bb423214ca66a5f3/src/entropies/direct_entropies/nearest_neighbors/KozachenkoLeonenko.jl#L3-L23">source</a></section></article><h2 id="Convenience-functions"><a class="docs-heading-anchor" href="#Convenience-functions">Convenience functions</a><a id="Convenience-functions-1"></a><a class="docs-heading-anchor-permalink" href="#Convenience-functions" title="Permalink"></a></h2><p>In this subsection we expand documentation strings of &quot;entropy names&quot; that are used commonly in the literature, such as &quot;permutation entropy&quot;. As we made clear in <a href="../#API-and-terminology">API &amp; terminology</a>, these are just the existing Shannon entropy with a particularly chosen probability estimator. We have only defined convenience functions for the most used names, and arbitrary more specialized convenience functions can be easily defined in a couple lines of code.</p><article class="docstring"><header><a class="docstring-binding" id="Entropies.entropy_permutation" href="#Entropies.entropy_permutation"><code>Entropies.entropy_permutation</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">entropy_permutation(x; τ = 1, m = 3, base = 2)</code></pre><p>Compute the permutation entropy of <code>x</code> of order <code>m</code> with delay/lag <code>τ</code>. This function is just a convenience call to:</p><pre><code class="language-julia hljs">est = SymbolicPermutation(; m, τ)
entropy(Shannon(base), x, est)</code></pre><p>See <a href="../probabilities/#Entropies.SymbolicPermutation"><code>SymbolicPermutation</code></a> for more info. Similarly, one can use <code>SymbolicWeightedPermutation</code> or <code>SymbolicAmplitudeAwarePermutation</code> for the weighted/amplitude-aware versions.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaDynamics/Entropies.jl/blob/3cbcbfaeb0b4a75e5b786b97bb423214ca66a5f3/src/entropies/convenience_definitions.jl#L9-L23">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Entropies.entropy_spatial_permutation" href="#Entropies.entropy_spatial_permutation"><code>Entropies.entropy_spatial_permutation</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">entropy_spatial_permutation(x, stencil, periodic = true; kwargs...)</code></pre><p>Compute the spatial permutation entropy of <code>x</code> given the <code>stencil</code>. Here <code>x</code> must be a matrix or higher dimensional <code>Array</code> containing spatial data. This function is just a convenience call to:</p><pre><code class="language-julia hljs">est = SpatialSymbolicPermutation(stencil, x, periodic)
entropy(Renyi(;kwargs...), x, est)</code></pre><p>See <a href="../probabilities/#Entropies.SpatialSymbolicPermutation"><code>SpatialSymbolicPermutation</code></a> for more info, or how to encode stencils.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaDynamics/Entropies.jl/blob/3cbcbfaeb0b4a75e5b786b97bb423214ca66a5f3/src/entropies/convenience_definitions.jl#L29-L42">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Entropies.entropy_wavelet" href="#Entropies.entropy_wavelet"><code>Entropies.entropy_wavelet</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">entropy_wavelet(x; wavelet = Wavelets.WT.Daubechies{12}(), base = 2)</code></pre><p>Compute the wavelet entropy. This function is just a convenience call to:</p><pre><code class="language-julia hljs">est = WaveletOverlap(wavelet)
entropy(Shannon(base), x, est)</code></pre><p>See <a href="../probabilities/#Entropies.WaveletOverlap"><code>WaveletOverlap</code></a> for more info.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaDynamics/Entropies.jl/blob/3cbcbfaeb0b4a75e5b786b97bb423214ca66a5f3/src/entropies/convenience_definitions.jl#L48-L59">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Entropies.entropy_dispersion" href="#Entropies.entropy_dispersion"><code>Entropies.entropy_dispersion</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">entropy_dispersion(x; base = 2, kwargs...)</code></pre><p>Compute the dispersion entropy. This function is just a convenience call to:</p><pre><code class="language-julia hljs">est = Dispersion(kwargs...)
entropy(Shannon(base), x, est)</code></pre><p>See <a href="../probabilities/#Entropies.Dispersion"><code>Dispersion</code></a> for more info.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaDynamics/Entropies.jl/blob/3cbcbfaeb0b4a75e5b786b97bb423214ca66a5f3/src/entropies/convenience_definitions.jl#L65-L76">source</a></section></article><section class="footnotes is-size-7"><ul><li class="footnote" id="footnote-Rényi1960"><a class="tag is-link" href="#citeref-Rényi1960">Rényi1960</a>A. Rényi, <em>Proceedings of the fourth Berkeley Symposium on Mathematics, Statistics and Probability</em>, pp 547 (1960)</li><li class="footnote" id="footnote-Shannon1948"><a class="tag is-link" href="#citeref-Shannon1948">Shannon1948</a>C. E. Shannon, Bell Systems Technical Journal <strong>27</strong>, pp 379 (1948)</li><li class="footnote" id="footnote-Tsallis1988"><a class="tag is-link" href="#citeref-Tsallis1988">Tsallis1988</a>Tsallis, C. (1988). Possible generalization of Boltzmann-Gibbs statistics. Journal of statistical physics, 52(1), 479-487.</li><li class="footnote" id="footnote-Shannon1948"><a class="tag is-link" href="#citeref-Shannon1948">Shannon1948</a>C. E. Shannon, Bell Systems Technical Journal <strong>27</strong>, pp 379 (1948)</li><li class="footnote" id="footnote-Curado2004"><a class="tag is-link" href="#citeref-Curado2004">Curado2004</a>Curado, E. M., &amp; Nobre, F. D. (2004). On the stability of analytic entropic forms. Physica A: Statistical Mechanics and its Applications, 335(1-2), 94-106.</li><li class="footnote" id="footnote-Anteneodo1999"><a class="tag is-link" href="#citeref-Anteneodo1999">Anteneodo1999</a>Anteneodo, C., &amp; Plastino, A. R. (1999). Maximum entropy approach to stretched exponential probability distributions. Journal of Physics A: Mathematical and General, 32(7), 1089.</li><li class="footnote" id="footnote-Kraskov2004"><a class="tag is-link" href="#citeref-Kraskov2004">Kraskov2004</a>Kraskov, A., Stögbauer, H., &amp; Grassberger, P. (2004). Estimating mutual information. Physical review E, 69(6), 066138.</li><li class="footnote" id="footnote-Charzyńska2016"><a class="tag is-link" href="#citeref-Charzyńska2016">Charzyńska2016</a>Charzyńska, A., &amp; Gambin, A. (2016). Improvement of the k-NN entropy estimator with applications in systems biology. Entropy, 18(1), 13.</li><li class="footnote" id="footnote-KozachenkoLeonenko1987"><a class="tag is-link" href="#citeref-KozachenkoLeonenko1987">KozachenkoLeonenko1987</a>Kozachenko, L. F., &amp; Leonenko, N. N. (1987). Sample estimate of the entropy of a random vector. Problemy Peredachi Informatsii, 23(2), 9-16.</li></ul></section></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../probabilities/">« Probabilities</a><a class="docs-footer-nextpage" href="../complexity_measures/">Complexity measures »</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 0.27.23 on <span class="colophon-date" title="Tuesday 18 October 2022 15:25">Tuesday 18 October 2022</span>. Using Julia version 1.8.2.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
