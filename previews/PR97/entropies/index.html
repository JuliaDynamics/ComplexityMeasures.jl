<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Entropies · Entropies.jl</title><script data-outdated-warner src="../assets/warner.js"></script><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.045/juliamono.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.24/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL=".."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../assets/documenter.js"></script><script src="../siteinfo.js"></script><script src="../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../assets/themeswap.js"></script><link href="https://fonts.googleapis.com/css?family=Montserrat|Source+Code+Pro&amp;display=swap" rel="stylesheet" type="text/css"/></head><body><div id="documenter"><nav class="docs-sidebar"><div class="docs-package-name"><span class="docs-autofit"><a href="../">Entropies.jl</a></span></div><form class="docs-search" action="../search/"><input class="docs-search-query" id="documenter-search-query" name="q" type="text" placeholder="Search docs"/></form><ul class="docs-menu"><li><a class="tocitem" href="../">Entropies.jl</a></li><li><a class="tocitem" href="../probabilities/">Probabilities</a></li><li class="is-active"><a class="tocitem" href>Entropies</a><ul class="internal"><li><a class="tocitem" href="#Rényi-(generalized)-entropy"><span>Rényi (generalized) entropy</span></a></li><li><a class="tocitem" href="#Tsallis-(generalized)-entropy"><span>Tsallis (generalized) entropy</span></a></li><li><a class="tocitem" href="#Shannon-entropy-(convenience)"><span>Shannon entropy (convenience)</span></a></li><li><a class="tocitem" href="#Indirect-entropies"><span>Indirect entropies</span></a></li><li><a class="tocitem" href="#Convenience-functions"><span>Convenience functions</span></a></li></ul></li><li><a class="tocitem" href="../complexity_measures/">Complexity measures</a></li><li><a class="tocitem" href="../examples/">Entropies.jl examples</a></li><li><a class="tocitem" href="../utils/">Utility methods</a></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><nav class="breadcrumb"><ul class="is-hidden-mobile"><li class="is-active"><a href>Entropies</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>Entropies</a></li></ul></nav><div class="docs-right"><a class="docs-edit-link" href="https://github.com/JuliaDynamics/Entropies.jl/blob/main/docs/src/entropies.md" title="Edit on GitHub"><span class="docs-icon fab"></span><span class="docs-label is-hidden-touch">Edit on GitHub</span></a><a class="docs-settings-button fas fa-cog" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-sidebar-button fa fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a></div></header><article class="content" id="documenter-page"><h1 id="Entropies"><a class="docs-heading-anchor" href="#Entropies">Entropies</a><a id="Entropies-1"></a><a class="docs-heading-anchor-permalink" href="#Entropies" title="Permalink"></a></h1><h2 id="Rényi-(generalized)-entropy"><a class="docs-heading-anchor" href="#Rényi-(generalized)-entropy">Rényi (generalized) entropy</a><a id="Rényi-(generalized)-entropy-1"></a><a class="docs-heading-anchor-permalink" href="#Rényi-(generalized)-entropy" title="Permalink"></a></h2><article class="docstring"><header><a class="docstring-binding" id="Entropies.entropy_renyi" href="#Entropies.entropy_renyi"><code>Entropies.entropy_renyi</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">entropy_renyi(p::Probabilities; q = 1.0, base = MathConstants.e)</code></pre><p>Compute the Rényi<sup class="footnote-reference"><a id="citeref-Rényi1960" href="#footnote-Rényi1960">[Rényi1960]</a></sup> generalized order-<code>q</code> entropy of some probabilities (typically returned by the <a href="../probabilities/#Entropies.probabilities"><code>probabilities</code></a> function).</p><pre><code class="nohighlight hljs">entropy_renyi(x::Array_or_Dataset, est; q = 1.0, base)</code></pre><p>A convenience syntax, which calls first <code>probabilities(x, est)</code> and then calculates the entropy of the result (and thus <code>est</code> can be anything the <a href="../probabilities/#Entropies.probabilities"><code>probabilities</code></a> function accepts).</p><p><strong>Description</strong></p><p>Let <span>$p$</span> be an array of probabilities (summing to 1). Then the Rényi generalized entropy is</p><p class="math-container">\[H_q(p) = \frac{1}{1-q} \log \left(\sum_i p[i]^q\right)\]</p><p>and generalizes other known entropies, like e.g. the information entropy (<span>$q = 1$</span>, see <sup class="footnote-reference"><a id="citeref-Shannon1948" href="#footnote-Shannon1948">[Shannon1948]</a></sup>), the maximum entropy (<span>$q=0$</span>, also known as Hartley entropy), or the correlation entropy (<span>$q = 2$</span>, also known as collision entropy).</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaDynamics/Entropies.jl/blob/7c3fc690adbe62b91a8050f359cba7345124668d/src/entropies/renyi.jl#L3-L34">source</a></section></article><h2 id="Tsallis-(generalized)-entropy"><a class="docs-heading-anchor" href="#Tsallis-(generalized)-entropy">Tsallis (generalized) entropy</a><a id="Tsallis-(generalized)-entropy-1"></a><a class="docs-heading-anchor-permalink" href="#Tsallis-(generalized)-entropy" title="Permalink"></a></h2><article class="docstring"><header><a class="docstring-binding" id="Entropies.entropy_tsallis" href="#Entropies.entropy_tsallis"><code>Entropies.entropy_tsallis</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">entropy_tsallis(p::Probabilities; k = 1, q = 0)</code></pre><p>Compute the Tsallis entropy of <code>x</code> (Tsallis, 1998)<sup class="footnote-reference"><a id="citeref-Tsallis1988" href="#footnote-Tsallis1988">[Tsallis1988]</a></sup>.</p><pre><code class="nohighlight hljs">entropy_tsallis(x::Array_or_Dataset, est; k = 1, q = 0)</code></pre><p>A convenience syntax, which calls first <code>probabilities(x, est)</code> and then calculates the entropy of the result (and thus <code>est</code> can be anything the <a href="../probabilities/#Entropies.probabilities"><code>probabilities</code></a> function accepts).</p><p><strong>Description</strong></p><p>The Tsallis entropy is a generalization of the Boltzmann–Gibbs entropy, with <code>k</code> standing for the Boltzmann constant. It is defined as</p><p class="math-container">\[S_q(p) = \frac{k}{q - 1}\left(1 - \sum_{i} p[i]^q\right)\]</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaDynamics/Entropies.jl/blob/7c3fc690adbe62b91a8050f359cba7345124668d/src/entropies/tsallis.jl#L3-L25">source</a></section></article><h2 id="Shannon-entropy-(convenience)"><a class="docs-heading-anchor" href="#Shannon-entropy-(convenience)">Shannon entropy (convenience)</a><a id="Shannon-entropy-(convenience)-1"></a><a class="docs-heading-anchor-permalink" href="#Shannon-entropy-(convenience)" title="Permalink"></a></h2><article class="docstring"><header><a class="docstring-binding" id="Entropies.entropy_shannon" href="#Entropies.entropy_shannon"><code>Entropies.entropy_shannon</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">entropy_shannon(args...; base = MathConstants.e)</code></pre><p>Equivalent to <code>entropy_renyi(args...; base, q = 1)</code> and provided solely for convenience. Compute the Shannon entropy, given by</p><p class="math-container">\[H(p) = - \sum_i p[i] \log(p[i])\]</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaDynamics/Entropies.jl/blob/7c3fc690adbe62b91a8050f359cba7345124668d/src/entropies/shannon.jl#L3-L10">source</a></section></article><h2 id="Indirect-entropies"><a class="docs-heading-anchor" href="#Indirect-entropies">Indirect entropies</a><a id="Indirect-entropies-1"></a><a class="docs-heading-anchor-permalink" href="#Indirect-entropies" title="Permalink"></a></h2><p>Here we list functions which compute Shannon entropies via alternate means, without explicitly computing some probability distributions and then using the Shannon formulat.</p><h3 id="Nearest-neighbors-entropy"><a class="docs-heading-anchor" href="#Nearest-neighbors-entropy">Nearest neighbors entropy</a><a id="Nearest-neighbors-entropy-1"></a><a class="docs-heading-anchor-permalink" href="#Nearest-neighbors-entropy" title="Permalink"></a></h3><article class="docstring"><header><a class="docstring-binding" id="Entropies.entropy_kraskov" href="#Entropies.entropy_kraskov"><code>Entropies.entropy_kraskov</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">entropy_kraskov(x::AbstractDataset{D, T}; k::Int = 1, w::Int = 0,
    base::Real = MathConstants.e) where {D, T}</code></pre><p>Estimate Shannon entropy to the given <code>base</code> using <code>k</code>-th nearest neighbor searches (Kraskov, 2004)<sup class="footnote-reference"><a id="citeref-Kraskov2004" href="#footnote-Kraskov2004">[Kraskov2004]</a></sup>.</p><p><code>w</code> is the Theiler window, which determines if temporal neighbors are excluded during neighbor searches (defaults to <code>0</code>, meaning that only the point itself is excluded when searching for neighbours).</p><p>See also: <a href="#Entropies.entropy_kozachenkoleonenko"><code>entropy_kozachenkoleonenko</code></a>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaDynamics/Entropies.jl/blob/7c3fc690adbe62b91a8050f359cba7345124668d/src/entropies/direct_entropies/nearest_neighbors/Kraskov.jl#L3-L17">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Entropies.entropy_kozachenkoleonenko" href="#Entropies.entropy_kozachenkoleonenko"><code>Entropies.entropy_kozachenkoleonenko</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">entropy_kozachenkoleonenko(x::AbstractDataset{D, T}; k::Int = 1, w::Int = 0,
    base::Real = MathConstants.e) where {D, T}</code></pre><p>Estimate Shannon entropy to the given <code>base</code> using <code>k</code>-th nearest neighbor searches, using the method from Kozachenko &amp; Leonenko (1987)<sup class="footnote-reference"><a id="citeref-KozachenkoLeonenko1987" href="#footnote-KozachenkoLeonenko1987">[KozachenkoLeonenko1987]</a></sup>, as described in Charzyńska and Gambin (2016)<sup class="footnote-reference"><a id="citeref-Charzyńska2016" href="#footnote-Charzyńska2016">[Charzyńska2016]</a></sup>.</p><p><code>w</code> is the Theiler window, which determines if temporal neighbors are excluded during neighbor searches (defaults to <code>0</code>, meaning that only the point itself is excluded when searching for neighbours).</p><p>See also: <a href="#Entropies.entropy_kraskov"><code>entropy_kraskov</code></a>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaDynamics/Entropies.jl/blob/7c3fc690adbe62b91a8050f359cba7345124668d/src/entropies/direct_entropies/nearest_neighbors/KozachenkoLeonenko.jl#L3-L21">source</a></section></article><h2 id="Convenience-functions"><a class="docs-heading-anchor" href="#Convenience-functions">Convenience functions</a><a id="Convenience-functions-1"></a><a class="docs-heading-anchor-permalink" href="#Convenience-functions" title="Permalink"></a></h2><p>In this subsection we expand documentation strings of &quot;entropy names&quot; that are used commonly in the literature, such as &quot;permutation entropy&quot;. As we made clear in <a href="../#API-and-terminology">API &amp; terminology</a>, these are just the existing Shannon/Rényi/Tsallis entropy with a particularly chosen probability estimator. We have only defined convenience functions for the most used names, and arbitrary more specialized convenience functions can be easily defined in a couple lines of code.</p><article class="docstring"><header><a class="docstring-binding" id="Entropies.entropy_permutation" href="#Entropies.entropy_permutation"><code>Entropies.entropy_permutation</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">entropy_permutation(x; τ = 1, m = 3, base = MathConstants.e)</code></pre><p>Compute the permutation entropy of order <code>m</code> with delay/lag <code>τ</code>. This function is just a convenience call to:</p><pre><code class="language-julia hljs">est = SymbolicPermutation(; m, τ)
entropy_shannon(x, est; base)</code></pre><p>See <a href="../probabilities/#Entropies.SymbolicPermutation"><code>SymbolicPermutation</code></a> for more info. Similarly, one can use <code>SymbolicWeightedPermutation</code> or <code>SymbolicAmplitudeAwarePermutation</code> for the weighted/amplitude-aware versions.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaDynamics/Entropies.jl/blob/7c3fc690adbe62b91a8050f359cba7345124668d/src/entropies/convenience_definitions.jl#L6-L18">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Entropies.entropy_spatial_permutation" href="#Entropies.entropy_spatial_permutation"><code>Entropies.entropy_spatial_permutation</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">entropy_spatial_permutation(x, stencil, periodic = true; kwargs...)</code></pre><p>Compute the spatial permutation entropy of <code>x</code> given the <code>stencil</code>. Here <code>x</code> must be a matrix or higher dimensional <code>Array</code> containing spatial data. This function is just a convenience call to:</p><pre><code class="language-julia hljs">est = SpatialSymbolicPermutation(stencil, x, periodic)
entropy_shannon(x, est; kwargs...)</code></pre><p>See <a href="../probabilities/#Entropies.SpatialSymbolicPermutation"><code>SpatialSymbolicPermutation</code></a> for more info, or how to encode stencils.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaDynamics/Entropies.jl/blob/7c3fc690adbe62b91a8050f359cba7345124668d/src/entropies/convenience_definitions.jl#L24-L35">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Entropies.entropy_wavelet" href="#Entropies.entropy_wavelet"><code>Entropies.entropy_wavelet</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">entropy_wavelet(x; wavelet = Wavelets.WT.Daubechies{12}(), base = MathConstants.e)</code></pre><p>Compute the wavelet entropy. This function is just a convenience call to:</p><pre><code class="language-julia hljs">est = WaveletOverlap(wavelet)
entropy_shannon(x, est; base)</code></pre><p>See <a href="../probabilities/#Entropies.WaveletOverlap"><code>WaveletOverlap</code></a> for more info.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaDynamics/Entropies.jl/blob/7c3fc690adbe62b91a8050f359cba7345124668d/src/entropies/convenience_definitions.jl#L41-L50">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Entropies.entropy_dispersion" href="#Entropies.entropy_dispersion"><code>Entropies.entropy_dispersion</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">entropy_dispersion(x, s = GaussianSymbolization(n_categories = 5);
    m = 3, τ = 1, q = 1, base = MathConstants.e)</code></pre><p>Compute the (order-<code>q</code> generalized) dispersion entropy to the given <code>base</code> of the univariate time series <code>x</code>. Relative frequencies of dispersion patterns are computed using the symbolization scheme <code>s</code> with embedding dimension <code>m</code> and embedding delay <code>τ</code>.</p><p>Recommended parameter values<sup class="footnote-reference"><a id="citeref-Li2018" href="#footnote-Li2018">[Li2018]</a></sup> are <code>m ∈ [2, 3]</code>, <code>τ = 1</code>, and <code>n_categories ∈ [3, 4, …, 8]</code> for the Gaussian mapping (defaults to 5).</p><p><strong>Description</strong></p><p>Dispersion entropy characterizes the complexity and irregularity of a time series. This implementation follows the description in Li et al. (2018)<sup class="footnote-reference"><a id="citeref-Li2018" href="#footnote-Li2018">[Li2018]</a></sup>, which is based on Azami &amp; Escudero (2018)<sup class="footnote-reference"><a id="citeref-Azami2018" href="#footnote-Azami2018">[Azami2018]</a></sup>, additionally allowing the computation of generalized dispersion entropy of order <code>q</code> (default is <code>q = 1</code>, which is the Shannon entropy).</p><p><strong>Data requirements</strong></p><p>Li et al. (2018) recommends that <code>x</code> has at least 1000 data points.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaDynamics/Entropies.jl/blob/7c3fc690adbe62b91a8050f359cba7345124668d/src/entropies/direct_entropies/entropy_dispersion.jl#L27-L51">source</a></section></article><section class="footnotes is-size-7"><ul><li class="footnote" id="footnote-Rényi1960"><a class="tag is-link" href="#citeref-Rényi1960">Rényi1960</a>A. Rényi, <em>Proceedings of the fourth Berkeley Symposium on Mathematics, Statistics and Probability</em>, pp 547 (1960)</li><li class="footnote" id="footnote-Shannon1948"><a class="tag is-link" href="#citeref-Shannon1948">Shannon1948</a>C. E. Shannon, Bell Systems Technical Journal <strong>27</strong>, pp 379 (1948)</li><li class="footnote" id="footnote-Tsallis1988"><a class="tag is-link" href="#citeref-Tsallis1988">Tsallis1988</a>Tsallis, C. (1988). Possible generalization of Boltzmann-Gibbs statistics. Journal of statistical physics, 52(1), 479-487.</li><li class="footnote" id="footnote-Kraskov2004"><a class="tag is-link" href="#citeref-Kraskov2004">Kraskov2004</a>Kraskov, A., Stögbauer, H., &amp; Grassberger, P. (2004). Estimating mutual information. Physical review E, 69(6), 066138.</li><li class="footnote" id="footnote-Charzyńska2016"><a class="tag is-link" href="#citeref-Charzyńska2016">Charzyńska2016</a>Charzyńska, A., &amp; Gambin, A. (2016). Improvement of the k-NN entropy estimator with applications in systems biology. Entropy, 18(1), 13.</li><li class="footnote" id="footnote-KozachenkoLeonenko1987"><a class="tag is-link" href="#citeref-KozachenkoLeonenko1987">KozachenkoLeonenko1987</a>Kozachenko, L. F., &amp; Leonenko, N. N. (1987). Sample estimate of the entropy of a random vector. Problemy Peredachi Informatsii, 23(2), 9-16.</li><li class="footnote" id="footnote-Li2018"><a class="tag is-link" href="#citeref-Li2018">Li2018</a>Li, G., Guan, Q., &amp; Yang, H. (2018). Noise reduction method of underwater acoustic signals based on CEEMDAN, effort-to-compress complexity, refined composite multiscale dispersion entropy and wavelet threshold denoising. Entropy, 21(1), 11.</li><li class="footnote" id="footnote-Azami2018"><a class="tag is-link" href="#citeref-Azami2018">Azami2018</a>Azami, H., &amp; Escudero, J. (2018). Coarse-graining approaches in univariate multiscale sample and dispersion entropy. Entropy, 20(2), 138.</li></ul></section></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../probabilities/">« Probabilities</a><a class="docs-footer-nextpage" href="../complexity_measures/">Complexity measures »</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 0.27.23 on <span class="colophon-date" title="Friday 23 September 2022 12:46">Friday 23 September 2022</span>. Using Julia version 1.8.1.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
