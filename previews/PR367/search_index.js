var documenterSearchIndex = {"docs":
[{"location":"complexity/#complexity_measures","page":"Complexity measures","title":"Complexity measures","text":"","category":"section"},{"location":"complexity/","page":"Complexity measures","title":"Complexity measures","text":"note: Note\nBe sure you have gone through the Tutorial before going through the API here to have a good idea of the terminology used in ComplexityMeasures.jl.","category":"page"},{"location":"complexity/#Complexity-measures-API","page":"Complexity measures","title":"Complexity measures API","text":"","category":"section"},{"location":"complexity/","page":"Complexity measures","title":"Complexity measures","text":"The complexity measure API is defined by the complexity function, which may take as an input an ComplexityEstimator. The function complexity_normalized is also useful.","category":"page"},{"location":"complexity/","page":"Complexity measures","title":"Complexity measures","text":"complexity\ncomplexity_normalized\nComplexityEstimator","category":"page"},{"location":"complexity/#ComplexityMeasures.complexity","page":"Complexity measures","title":"ComplexityMeasures.complexity","text":"complexity(c::ComplexityEstimator, x) → m::Real\n\nEstimate a complexity measure according to c for input data x, where c is an instance of any subtype of ComplexityEstimator:\n\nApproximateEntropy.\nLempelZiv76.\nMissingDispersionPatterns.\nReverseDispersion.\nSampleEntropy.\nStatisticalComplexity.\n\n\n\n\n\n","category":"function"},{"location":"complexity/#ComplexityMeasures.complexity_normalized","page":"Complexity measures","title":"ComplexityMeasures.complexity_normalized","text":"complexity_normalized(c::ComplexityEstimator, x) → m::Real ∈ [a, b]\n\nThe same as complexity, but the result is normalized to the interval [a, b], where [a, b] depends on c.\n\n\n\n\n\n","category":"function"},{"location":"complexity/#ComplexityMeasures.ComplexityEstimator","page":"Complexity measures","title":"ComplexityMeasures.ComplexityEstimator","text":"ComplexityEstimator\n\nSupertype for estimators for various complexity measures that are not entropies in the strict mathematical sense.\n\nSee complexity for all available estimators.\n\n\n\n\n\n","category":"type"},{"location":"complexity/#Approximate-entropy","page":"Complexity measures","title":"Approximate entropy","text":"","category":"section"},{"location":"complexity/","page":"Complexity measures","title":"Complexity measures","text":"ApproximateEntropy","category":"page"},{"location":"complexity/#ComplexityMeasures.ApproximateEntropy","page":"Complexity measures","title":"ComplexityMeasures.ApproximateEntropy","text":"ApproximateEntropy <: ComplexityEstimator\nApproximateEntropy([x]; r = 0.2std(x), kwargs...)\n\nAn estimator for the approximate entropy (Pincus, 1991) complexity measure, used with complexity.\n\nThe keyword argument r is mandatory if an input timeseries x is not provided.\n\nKeyword arguments\n\nr::Real: The radius used when querying for nearest neighbors around points. Its value   should be determined from the input data, for example as some proportion of the   standard deviation of the data.\nm::Int = 2: The embedding dimension.\nτ::Int = 1: The embedding lag.\nbase::Real = MathConstants.e: The base to use for the logarithm. Pincus (1991) uses the   natural logarithm.\n\nDescription\n\nApproximate entropy (ApEn) is defined as\n\nApEn(m r) = lim_N to infty left phi(x m r) - phi(x m + 1 r) right\n\nApproximate entropy is estimated for a timeseries x, by first embedding x using embedding dimension m and embedding lag τ, then searching for similar vectors within tolerance radius r, using the estimator described below, with logarithms to the given base (natural logarithm is used in Pincus, 1991).\n\nSpecifically, for a finite-length timeseries x, an estimator for ApEn(m r) is\n\nApEn(m r N) = phi(x m r N) -  phi(x m + 1 r N)\n\nwhere N = length(x) and\n\nphi(x k r N) =\ndfrac1N-(k-1)tau sum_i=1^N - (k-1)tau\nlogleft(\n    sum_j = 1^N-(k-1)tau dfractheta(d(bf x_i^m bf x_j^m) leq r)N-(k-1)tau\n    right)\n\nHere, theta(cdot) returns 1 if the argument is true and 0 otherwise,  d(bf x_i bf x_j) returns the Chebyshev distance between vectors  bf x_i and bf x_j, and the k-dimensional embedding vectors are constructed from the input timeseries x(t) as\n\nbf x_i^k = (x(i) x(i+τ) x(i+2τ) ldots x(i+(k-1)tau))\n\nnote: Flexible embedding lag\nIn the original paper, they fix τ = 1. In our implementation, the normalization constant is modified to account for embeddings with τ != 1.\n\n\n\n\n\n","category":"type"},{"location":"complexity/#Sample-entropy","page":"Complexity measures","title":"Sample entropy","text":"","category":"section"},{"location":"complexity/","page":"Complexity measures","title":"Complexity measures","text":"SampleEntropy","category":"page"},{"location":"complexity/#ComplexityMeasures.SampleEntropy","page":"Complexity measures","title":"ComplexityMeasures.SampleEntropy","text":"SampleEntropy([x]; r = 0.2std(x), kwargs...) <: ComplexityEstimator\n\nAn estimator for the sample entropy complexity measure (Richman and Moorman, 2000), used with complexity and complexity_normalized.\n\nThe keyword argument r is mandatory if an input timeseries x is not provided.\n\nKeyword arguments\n\nr::Real: The radius used when querying for nearest neighbors around points. Its value   should be determined from the input data, for example as some proportion of the   standard deviation of the data.\nm::Int = 2: The embedding dimension.\nτ::Int = 1: The embedding lag.\n\nDescription\n\nAn estimator for sample entropy using radius r, embedding dimension m, and embedding lag τ is\n\nSampEn(mr N) = -lndfracA(r N)B(r N)\n\nHere,\n\nbeginaligned\nB(r m N) = sum_i = 1^N-mtau sum_j = 1 j neq i^N-mtau theta(d(bf x_i^m bf x_j^m) leq r) \nA(r m N) = sum_i = 1^N-mtau sum_j = 1 j neq i^N-mtau theta(d(bf x_i^m+1 bf x_j^m+1) leq r) \nendaligned\n\nwhere theta(cdot) returns 1 if the argument is true and 0 otherwise, and d(x y) computes the Chebyshev distance between x and y, and  bf x_i^m and bf x_i^m+1 are m-dimensional and m+1-dimensional embedding vectors, where k-dimensional embedding vectors are constructed from the input timeseries x(t) as\n\nbf x_i^k = (x(i) x(i+τ) x(i+2τ) ldots x(i+(k-1)tau))\n\nQuoting Richman & Moorman (2002): \"SampEn(m,r,N) will be defined except when B = 0, in which case no regularity has been detected, or when A = 0, which corresponds to a conditional probability of 0 and an infinite value of SampEn(m,r,N)\". In these cases, NaN is returned.\n\nIf computing the normalized measure, then the resulting sample entropy is on [0, 1].\n\nnote: Flexible embedding lag\nThe original algorithm fixes τ = 1. All formulas here are modified to account for any τ.\n\nSee also: entropy_sample.\n\n\n\n\n\n","category":"type"},{"location":"complexity/#Missing-dispersion-patterns","page":"Complexity measures","title":"Missing dispersion patterns","text":"","category":"section"},{"location":"complexity/","page":"Complexity measures","title":"Complexity measures","text":"MissingDispersionPatterns","category":"page"},{"location":"complexity/#ComplexityMeasures.MissingDispersionPatterns","page":"Complexity measures","title":"ComplexityMeasures.MissingDispersionPatterns","text":"MissingDispersionPatterns <: ComplexityEstimator\nMissingDispersionPatterns(est = Dispersion())\n\nAn estimator for the number of missing dispersion patterns (N_MDP), a complexity measure which can be used to detect nonlinearity in time series (Zhou et al., 2023).\n\nUsed with complexity or complexity_normalized, whose implementation uses missing_outcomes.\n\nDescription\n\nIf used with complexity, N_MDP is computed by first symbolising each xᵢ ∈ x, then embedding the resulting symbol sequence using the dispersion pattern estimator est, and computing the quantity\n\nN_MDP = L - N_ODP\n\nwhere L = total_outcomes(est) (i.e. the total number of possible dispersion patterns), and N_ODP is defined as the number of occurring dispersion patterns.\n\nIf used with complexity_normalized, then N_MDP^N = (L - N_ODP)L is computed. The authors recommend that total_outcomes(est.symbolization)^est.m << length(x) - est.m*est.τ + 1 to avoid undersampling.\n\nnote: Encoding\nDispersion's linear mapping from CDFs to integers is based on equidistant partitioning of the interval [0, 1]. This is slightly different from Zhou et al. (2023), which uses the linear mapping s_i = textround(y + 05).\n\nUsage\n\nIn Zhou et al. (2023), MissingDispersionPatterns is used to detect nonlinearity in time series by comparing the N_MDP for a time series x to N_MDP values for an ensemble of surrogates of x. If N_MDP  q_MDP^WIAAFT, where q_MDP^WIAAFT is some q-th quantile of the surrogate ensemble, then it is taken as evidence for nonlinearity.\n\nSee also: Dispersion, ReverseDispersion, total_outcomes.\n\n\n\n\n\n","category":"type"},{"location":"complexity/#Reverse-dispersion-entropy","page":"Complexity measures","title":"Reverse dispersion entropy","text":"","category":"section"},{"location":"complexity/","page":"Complexity measures","title":"Complexity measures","text":"ReverseDispersion","category":"page"},{"location":"complexity/#ComplexityMeasures.ReverseDispersion","page":"Complexity measures","title":"ComplexityMeasures.ReverseDispersion","text":"ReverseDispersion <: ComplexityEstimator\nReverseDispersion(; c = 3, m = 2, τ = 1, check_unique = true)\n\nEstimator for the reverse dispersion entropy complexity measure (Li et al., 2019).\n\nDescription\n\nLi et al. (2019) defines the reverse dispersion entropy as\n\nH_rde = sum_i = 1^c^m left(p_i - dfrac1c^m right)^2 =\nleft( sum_i=1^c^m p_i^2 right) - dfrac1c^m\n\nwhere the probabilities p_i are obtained precisely as for the Dispersion probability estimator. Relative frequencies of dispersion patterns are computed using the given encoding scheme , which defaults to encoding using the normal cumulative distribution function (NCDF), as implemented by GaussianCDFEncoding, using embedding dimension m and embedding delay τ. Recommended parameter values(Li et al., 2019) are m ∈ [2, 3], τ = 1 for the embedding, and c ∈ [3, 4, …, 8] categories for the Gaussian mapping.\n\nIf normalizing, then the reverse dispersion entropy is normalized to [0, 1].\n\nThe minimum value of H_rde is zero and occurs precisely when the dispersion pattern distribution is flat, which occurs when all p_is are equal to 1c^m. Because H_rde geq 0, H_rde can therefore be said to be a measure of how far the dispersion pattern probability distribution is from white noise.\n\nData requirements\n\nThe input must have more than one unique element for the default GaussianCDFEncoding to be well-defined. Li et al. (2019) recommends that x has at least 1000 data points.\n\nIf check_unique == true (default), then it is checked that the input has more than one unique value. If check_unique == false and the input only has one unique element, then a InexactError is thrown when trying to compute probabilities.\n\n\n\n\n\n","category":"type"},{"location":"complexity/#Statistical-complexity","page":"Complexity measures","title":"Statistical complexity","text":"","category":"section"},{"location":"complexity/","page":"Complexity measures","title":"Complexity measures","text":"StatisticalComplexity\nentropy_complexity\nentropy_complexity_curves","category":"page"},{"location":"complexity/#ComplexityMeasures.StatisticalComplexity","page":"Complexity measures","title":"ComplexityMeasures.StatisticalComplexity","text":"StatisticalComplexity <: ComplexityEstimator\nStatisticalComplexity([x]; kwargs...)\n\nAn estimator for the statistical complexity and entropy, originally by (Rosso et al., 2007), but here generalized see Rosso et al. (2013) to work with any ProbabilitiesEstimator in combination with any OutcomeSpace with a priori known total_outcomes, any valid distance metric, and any normalizable discrete information measure (e.g. entropies like Shannon, Renyi, or extropies like ShannonExtropy, the latter of which are not treated in Rosso et al.'s papers). Used with complexity.\n\nKeyword arguments\n\nest::ProbabilitiesEstimator = RelativeAmount(OrdinalPatterns()): The   ProbabilitiesEstimator used to estimate probabilities from the input data.   An OutcomeSpace must be given as the first argument to the estimator to   control how discretization within pixel windows is performed.\ndist<:SemiMetric = JSDivergence(): The distance measure (from Distances.jl) to use for   estimating the distance between the estimated probability distribution and a uniform   distribution with the same maximal number of outcomes.\nentr::InformationMeasure = Renyi(): An   InformationMeasure of choice. Any   information measure that defines information_maximum is valid here. Typically,   an entropy is used, e.g. Shannon or Renyi is used.\n\nDescription\n\nStatistical complexity is defined as\n\nC_qP = mathcalH_qcdot mathcalQ_qP\n\nwhere Q_q is a \"disequilibrium\" obtained from a distance-measure and H_q a disorder measure. In the original paper(Rosso et al., 2007), this complexity measure was defined via an ordinal pattern-based probability distribution, the Shannon entropy and the Jensen-Shannon divergence as a distance measure. This implementation allows for a generalization of the complexity measure as developed in Rosso et al. (2013). Here, H_qcan be the (q-order) Shannon-, Renyi or Tsallis entropy andQ_q` based either on the Euclidean, Wooters, Kullback, q-Kullback, Jensen or q-Jensen distance as\n\nQ_qP = Q_q^0cdot DP P_e\n\nwhere DP P_e is the distance between the obtained distribution P and a uniform distribution with the same maximum number of bins, measured by the distance measure dist.\n\nUsage\n\nThe statistical complexity is exclusively used in combination with the related information measure (typically an entropy). complexity(c::StatisticalComplexity, x) returns only the statistical complexity.\n\nThe entropy (or other information measure) can be accessed as a Ref value of the struct as\n\nx = randn(100)\nc = StatisticalComplexity()\ncompl = complexity(c, x)\nentr = c.entr_val[]\n\nTo obtain both the entropy (or other information measure) and the statistical complexity together as a Tuple, use the wrapper entropy_complexity.\n\n\n\n\n\n","category":"type"},{"location":"complexity/#ComplexityMeasures.entropy_complexity","page":"Complexity measures","title":"ComplexityMeasures.entropy_complexity","text":"entropy_complexity(c::StatisticalComplexity, x)\n\nReturn both the entropy and the corresponding StatisticalComplexity. Useful when wanting to plot data on the \"entropy-complexity plane\". See also entropy_complexity_curves.\n\n\n\n\n\n","category":"function"},{"location":"complexity/#ComplexityMeasures.entropy_complexity_curves","page":"Complexity measures","title":"ComplexityMeasures.entropy_complexity_curves","text":"entropy_complexity_curves(c::StatisticalComplexity; num_max=1, num_min=1000) -> (min_entropy_complexity, max_entropy_complexity)\n\nCalculate the maximum complexity-entropy curve for the statistical complexity according to Rosso et al. (2007) for num_max * total_outcomes(c.est) different values of the normalized information measure of choice (in case of the maximum complexity curves) and num_min different values of the normalized information measure of choice (in case of the minimum complexity curve).\n\nThis function can also be used to compute the maximum \"complexity-extropy curve\" if c.entr is e.g. ShannonExtropy, which is the equivalent of the complexity-entropy curves, but using extropy instead of information.\n\nDescription\n\nThe way the statistical complexity is designed, there is a minimum and maximum possible complexity for data with a given permutation entropy. The calculation time of the maximum complexity curve grows as O(total_outcomes(c.est)^2), and thus takes very long for high numbers of outcomes. This function is inspired by S. Sippels implementation in statcomp (Sippel et al., 2016).\n\nThis function will work with any ProbabilitiesEstimator where total_outcomes is known a priori.\n\n\n\n\n\n","category":"function"},{"location":"complexity/#Lempel-Ziv-complexity","page":"Complexity measures","title":"Lempel-Ziv complexity","text":"","category":"section"},{"location":"complexity/","page":"Complexity measures","title":"Complexity measures","text":"LempelZiv76","category":"page"},{"location":"complexity/#ComplexityMeasures.LempelZiv76","page":"Complexity measures","title":"ComplexityMeasures.LempelZiv76","text":"LempelZiv76 <: ComplexityEstimator\nLempelZiv76()\n\nThe Lempel-Ziv, or LempelZiv76, complexity measure (Lempel and Ziv, 1976), which is used with complexity and complexity_normalized.\n\nFor results to be comparable across sequences with different length, use the normalized version. Normalized LempelZiv76-complexity is implemented as given in Amigó et al. (2004). The normalized measure is close to zero for very regular signals, while for random sequences, it is close to 1 with high probability[Amigó2004]. Note: the normalized LempelZiv76 complexity can be higher than 1[Amigó2004].\n\nThe LempelZiv76 measure applies only to binary sequences, i.e. sequences with a two-element alphabet (precisely two distinct outcomes). For performance optimization, we do not check the number of unique elements in the input. If your input sequence is not binary, you must encode it first using one of the implemented Encoding schemes (or encode your data manually).\n\n\n\n\n\n","category":"type"},{"location":"probabilities/#Probabilities","page":"Probabilities","title":"Probabilities","text":"","category":"section"},{"location":"probabilities/","page":"Probabilities","title":"Probabilities","text":"note: Note\nBe sure you have gone through the Tutorial before going through the API here to have a good idea of the terminology used in ComplexityMeasures.jl.","category":"page"},{"location":"probabilities/","page":"Probabilities","title":"Probabilities","text":"ComplexityMeasures.jl implements an interface for probabilities that exactly follows the mathematically rigorous formulation of probability spaces. Probability spaces are formalized by an OutcomeSpace Omega. Probabilities are extracted from data then by referencing an outcome space in the functions counts and probabilities. The mathematical formulation of probabilities spaces is further enhanced by ProbabilitiesEstimator and its subtypes, which may correct theoretically known biases when estimating probabilities from finite data.","category":"page"},{"location":"probabilities/","page":"Probabilities","title":"Probabilities","text":"In reality, probabilities can be either discrete (mass functions) or continuous (density functions). Currently in ComplexityMeasures.jl, only probability mass functions (i.e., countable Omega) are implemented explicitly. Quantities that are estimated from probability density functions (i.e., uncountable Omega) also exist and are implemented in ComplexityMeasures.jl. However, these are estimated by a one-step processes without the intermediate estimation of probabilities.","category":"page"},{"location":"probabilities/","page":"Probabilities","title":"Probabilities","text":"If Omega is countable, the process of estimating the outcomes from input data is also called discretization of the input data.","category":"page"},{"location":"probabilities/#outcome_spaces","page":"Probabilities","title":"Outcome spaces","text":"","category":"section"},{"location":"probabilities/","page":"Probabilities","title":"Probabilities","text":"OutcomeSpace\noutcomes\noutcome_space\ntotal_outcomes\nmissing_outcomes","category":"page"},{"location":"probabilities/#ComplexityMeasures.OutcomeSpace","page":"Probabilities","title":"ComplexityMeasures.OutcomeSpace","text":"OutcomeSpace\n\nThe supertype for all outcome space implementation.\n\nDescription\n\nIn ComplexityMeasures.jl, an outcome space defines a set of possible outcomes Omega = omega_1 omega_2 ldots omega_L  (some form of discretization). In the literature, the outcome space is often also called an \"alphabet\", while each outcome is called a \"symbol\" or an \"event\".\n\nAn outcome space also defines a set of rules for mapping input data to to each outcome omega_i, a processes called encoding or symbolizing or discretizing in the literature (see encodings). Some OutcomeSpaces first apply a transformation, e.g. a delay embedding, to the data before discretizing/encoding, while other OutcomeSpaces discretize/encode the data directly.\n\nImplementations\n\nOutcome space Principle Input data Counting-compatible\nUniqueElements Count of unique elements Any ✔\nValueBinning Binning (histogram) Vector, StateSpaceSet ✔\nOrdinalPatterns Ordinal patterns Vector, StateSpaceSet ✔\nSpatialOrdinalPatterns Ordinal patterns in space Array ✔\nDispersion Dispersion patterns Vector ✔\nSpatialDispersion Dispersion patterns in space Array ✔\nCosineSimilarityBinning Cosine similarity Vector ✔\nTransferOperator Binning (transfer operator) Vector, StateSpaceSet ✖\nNaiveKernel Kernel density estimation StateSpaceSet ✖\nWeightedOrdinalPatterns Ordinal patterns Vector, StateSpaceSet ✖\nAmplitudeAwareOrdinalPatterns Ordinal patterns Vector, StateSpaceSet ✖\nWaveletOverlap Wavelet transform Vector ✖\nPowerSpectrum Fourier transform Vector ✖\n\nIn the column \"input data\" it is assumed that the eltype of the input is <: Real.\n\nUsage\n\nOutcome spaces are used as input to\n\nprobabilities/allprobabilities_and_outcomes for computing   probability mass functions.\noutcome_space, which returns the elements of the outcome space.\ntotal_outcomes, which returns the cardinality of the outcome space.\ncounts/counts_and_outcomes/allcounts_and_outcomes, for    obtaining raw counts instead of probabilities (only for counting-compatible outcome   spaces).\n\nCounting-compatible vs. non-counting compatible outcome spaces\n\nThere are two main types of outcome spaces.\n\nCounting-compatible outcome spaces have a well-defined   way of counting how often each point in the (encoded) input data is mapped to a   particular outcome omega_i. These outcome spaces use   encode to discretize the input data. Examples are   OrdinalPatterns (which encodes input data into ordinal patterns) or   ValueBinning (which discretizes points onto a regular grid).   The table below lists which outcome spaces are counting compatible.\nNon-counting compatible outcome spaces have no well-defined way of counting explicitly   how often each point in the input data is mapped to a particular outcome omega_i.   Instead, these outcome spaces returns a vector of pre-normalized \"relative counts\", one   for each outcome omega_i. Examples are WaveletOverlap or   PowerSpectrum.\n\nCounting-compatible outcome spaces can be used with any ProbabilitiesEstimator to convert counts into probability mass functions. Non-counting-compatible outcome spaces can only be used with the maximum likelihood (RelativeAmount) probabilities estimator, which estimates probabilities precisely by the relative frequency of each outcome (formally speaking, the RelativeAmount estimator also requires counts, but for the sake of code consistency, we allow it to be used with relative frequencies as well).\n\nThe function is_counting_based can be used to check whether an outcome space is based on counting.\n\nDeducing the outcome space (from data)\n\nSome outcome space models can deduce Omega without knowledge of the input, such as OrdinalPatterns. Other outcome spaces require knowledge of the input data for concretely specifying Omega, such as ValueBinning with RectangularBinning. If o is some outcome space model and x some input data, then outcome_space(o, x) returns the possible outcomes Omega. To get the cardinality of Omega, use total_outcomes.\n\nImplementation details\n\nThe element type of Omega varies between outcome space models, but it is guaranteed to be hashable and sortable. This allows for conveniently tracking the counts of a specific event across experimental realizations, by using the outcome as a dictionary key and the counts as the value for that key (or, alternatively, the key remains the outcome and one has a vector of probabilities, one for each experimental realization).\n\n\n\n\n\n","category":"type"},{"location":"probabilities/#ComplexityMeasures.outcomes","page":"Probabilities","title":"ComplexityMeasures.outcomes","text":"outcomes(o::OutcomeSpace, x)\n\nReturn all (unique) outcomes that appear in the (encoded) input data x, according to the given OutcomeSpace. Equivalent to probabilities_and_outcomes(o, x)[2], but for some estimators it may be explicitly extended for better performance.\n\n\n\n\n\n","category":"function"},{"location":"probabilities/#ComplexityMeasures.outcome_space","page":"Probabilities","title":"ComplexityMeasures.outcome_space","text":"outcome_space(o::OutcomeSpace, x) → Ω\n\nReturn a sorted container containing all possible outcomes of o for input x.\n\nFor some estimators the concrete outcome space is known without knowledge of input x, in which case the function dispatches to outcome_space(o). In general it is recommended to use the 2-argument version irrespectively of estimator.\n\n\n\n\n\n","category":"function"},{"location":"probabilities/#ComplexityMeasures.total_outcomes","page":"Probabilities","title":"ComplexityMeasures.total_outcomes","text":"total_outcomes(o::OutcomeSpace, x)\n\nReturn the length (cardinality) of the outcome space Omega of est.\n\nFor some OutcomeSpace, the cardinality is known without knowledge of input x, in which case the function dispatches to total_outcomes(est). In general it is recommended to use the 2-argument version irrespectively of estimator.\n\n\n\n\n\n","category":"function"},{"location":"probabilities/#ComplexityMeasures.missing_outcomes","page":"Probabilities","title":"ComplexityMeasures.missing_outcomes","text":"missing_outcomes(o::OutcomeSpace, x; all = true) → n_missing::Int\n\nCount the number of missing (i.e., zero-probability) outcomes specified by o, given input data x, using RelativeAmount probabilities estimation.\n\nIf all == true, then allprobabilities_and_outcomes is used to compute the probabilities. If all == false, then probabilities is used to compute the probabilities.\n\nThis is syntactically equivalent to missing_outcomes(RelativeAmount(o), x).\n\nmissing_outcomes(est::ProbabilitiesEstimator, o::OutcomeSpace, x) → n_missing::Int\n\nLike above, but specifying a custom ProbabilitiesEstimator too.\n\nSee also: MissingDispersionPatterns.\n\n\n\n\n\n","category":"function"},{"location":"probabilities/#Count-occurrences","page":"Probabilities","title":"Count occurrences","text":"","category":"section"},{"location":"probabilities/","page":"Probabilities","title":"Probabilities","text":"UniqueElements","category":"page"},{"location":"probabilities/#ComplexityMeasures.UniqueElements","page":"Probabilities","title":"ComplexityMeasures.UniqueElements","text":"UniqueElements()\n\nAn OutcomeSpace based on straight-forward counting of distinct elements in a univariate time series or multivariate dataset. This is the same as giving no estimator to probabilities.\n\nOutcome space\n\nThe outcome space is the unique sorted values of the input. Hence, input x is needed for a well-defined outcome_space.\n\nImplements\n\ncodify. Used for encoding inputs where ordering matters (e.g. time series).\n\n\n\n\n\n","category":"type"},{"location":"probabilities/#Histograms","page":"Probabilities","title":"Histograms","text":"","category":"section"},{"location":"probabilities/","page":"Probabilities","title":"Probabilities","text":"ValueBinning\nAbstractBinning\nRectangularBinning\nFixedRectangularBinning","category":"page"},{"location":"probabilities/#ComplexityMeasures.ValueBinning","page":"Probabilities","title":"ComplexityMeasures.ValueBinning","text":"ValueBinning(b::AbstractBinning) <: OutcomeSpace\n\nAn OutcomeSpace based on binning the values of the data as dictated by the binning scheme b and formally computing their histogram, i.e., the frequencies of points in the bins. An alias to this is VisitationFrequency. Available binnings are subtypes of AbstractBinning.\n\nThe ValueBinning estimator has a linearithmic time complexity (n log(n) for n = length(x)) and a linear space complexity (l for l = dimension(x)). This allows computation of probabilities (histograms) of high-dimensional datasets and with small box sizes ε without memory overflow and with maximum performance. For performance reasons, the probabilities returned never contain 0s and are arbitrarily ordered.\n\nValueBinning(ϵ::Union{Real,Vector})\n\nA convenience method that accepts same input as RectangularBinning and initializes this binning directly.\n\nOutcomes\n\nThe outcome space for ValueBinning is the unique bins constructed from b. Each bin is identified by its left (lowest-value) corner, because bins are always left-closed-right-open intervals [a, b). The bins are in data units, not integer (cartesian indices units), and are returned as SVectors, i.e., same type as input data.\n\nFor convenience, outcome_space returns the outcomes in the same array format as the underlying binning (e.g., Matrix for 2D input).\n\nFor FixedRectangularBinning the outcome_space is well-defined from the binning, but for RectangularBinning input x is needed as well.\n\nImplements\n\ncodify. Used for encoding inputs where ordering matters (e.g. time series).\n\n\n\n\n\n","category":"type"},{"location":"probabilities/#ComplexityMeasures.AbstractBinning","page":"Probabilities","title":"ComplexityMeasures.AbstractBinning","text":"AbstractBinning\n\nSupertype encompassing RectangularBinning and FixedRectangularBinning.\n\n\n\n\n\n","category":"type"},{"location":"probabilities/#ComplexityMeasures.RectangularBinning","page":"Probabilities","title":"ComplexityMeasures.RectangularBinning","text":"RectangularBinning(ϵ, precise = false) <: AbstractBinning\n\nRectangular box partition of state space using the scheme ϵ, deducing the histogram extent and bin width from the input data.\n\nRectangularBinning is a convenience struct. It is re-cast into FixedRectangularBinning once the data are provided, so see that docstring for info on the bin calculation and the meaning of precise.\n\nBinning instructions are deduced from the type of ϵ as follows:\n\nϵ::Int divides each coordinate axis into ϵ equal-length intervals  that cover all data.\nϵ::Float64 divides each coordinate axis into intervals of fixed size ϵ, starting  from the axis minima until the data is completely covered by boxes.\nϵ::Vector{Int} divides the i-th coordinate axis into ϵ[i] equal-length  intervals that cover all data.\nϵ::Vector{Float64} divides the i-th coordinate axis into intervals of fixed size  ϵ[i], starting from the axis minima until the data is completely covered by boxes.\n\nRectangularBinning ensures all input data are covered by extending the created ranges if need be.\n\n\n\n\n\n","category":"type"},{"location":"probabilities/#ComplexityMeasures.FixedRectangularBinning","page":"Probabilities","title":"ComplexityMeasures.FixedRectangularBinning","text":"FixedRectangularBinning <: AbstractBinning\nFixedRectangularBinning(ranges::Tuple{<:AbstractRange...}, precise = false)\n\nRectangular box partition of state space where the partition along each dimension is explicitly given by each range ranges, which is a tuple of AbstractRange subtypes. Typically, each range is the output of the range Base function, e.g., ranges = (0:0.1:1, range(0, 1; length = 101), range(2.1, 3.2; step = 0.33)). All ranges must be sorted.\n\nThe optional second argument precise dictates whether Julia Base's TwicePrecision is used for when searching where a point falls into the range. Useful for edge cases of points being almost exactly on the bin edges, but it is exactly four times as slow, so by default it is false.\n\nPoints falling outside the partition do not contribute to probabilities. Bins are always left-closed-right-open: [a, b). This means that the last value of each of the ranges dictates the last right-closing value. This value does not belong to the histogram! E.g., if given a range r = range(0, 1; length = 11), with r[end] = 1, the value 1 is outside the partition and would not attribute any increase of the probability corresponding to the last bin (here [0.9, 1))!\n\nEquivalently, the size of the histogram is histsize = map(r -> length(r)-1, ranges)!\n\nFixedRectangularBinning leads to a well-defined outcome space without knowledge of input data, see ValueBinning.\n\n\n\n\n\n","category":"type"},{"location":"probabilities/#Symbolic-permutations","page":"Probabilities","title":"Symbolic permutations","text":"","category":"section"},{"location":"probabilities/","page":"Probabilities","title":"Probabilities","text":"OrdinalPatterns\nWeightedOrdinalPatterns\nAmplitudeAwareOrdinalPatterns","category":"page"},{"location":"probabilities/#ComplexityMeasures.OrdinalPatterns","page":"Probabilities","title":"ComplexityMeasures.OrdinalPatterns","text":"OrdinalPatterns <: OutcomeSpace\nOrdinalPatterns{m}(τ = 1, lt::Function = ComplexityMeasures.isless_rand)\n\nAn OutcomeSpace based on lengh-m ordinal permutation patterns, originally introduced in Bandt and Pompe (2002)'s paper on permutation entropy. Note that m is given as a type parameter, so that when it is a literal integer there are performance accelerations.\n\nWhen passed to probabilities the output depends on the input data type:\n\nUnivariate data. If applied to a univariate timeseries (AbstractVector), then the timeseries   is first embedded using embedding delay τ and dimension m, resulting in embedding   vectors  bfx_i _i=1^N-(m-1)tau. Then, for each bfx_i,   we find its permutation pattern pi_i. Probabilities are then   estimated as the frequencies of the encoded permutation symbols   by using UniqueElements. When giving the resulting probabilities to   information, the original permutation entropy is computed (Bandt and Pompe, 2002).\nMultivariate data. If applied to a an D-dimensional StateSpaceSet,   then no embedding is constructed, m must be equal to D and τ is ignored.   Each vector bfx_i of the dataset is mapped   directly to its permutation pattern pi_i by comparing the   relative magnitudes of the elements of bfx_i.   Like above, probabilities are estimated as the frequencies of the permutation symbols.   The resulting probabilities can be used to compute multivariate permutation   entropy (He et al., 2016), although here we don't perform any further subdivision   of the permutation patterns (as in Figure 3 of He et al. (2016)).\n\nInternally, OrdinalPatterns uses the OrdinalPatternEncoding to represent ordinal patterns as integers for efficient computations.\n\nSee WeightedOrdinalPatterns and AmplitudeAwareOrdinalPatterns for estimators that not only consider ordinal (sorting) patterns, but also incorporate information about within-state-vector amplitudes. For a version of this estimator that can be used on spatial data, see SpatialOrdinalPatterns.\n\nnote: Handling equal values in ordinal patterns\nIn Bandt and Pompe (2002), equal values are ordered after their order of appearance, but this can lead to erroneous temporal correlations, especially for data with low amplitude resolution (Zunino et al., 2017). Here, by default, if two values are equal, then one of the is randomly assigned as \"the largest\", using lt = ComplexityMeasures.isless_rand. To get the behaviour from Bandt and Pompe (2002), use lt = Base.isless.\n\nOutcome space\n\nThe outcome space Ω for OrdinalPatterns is the set of length-m ordinal patterns (i.e. permutations) that can be formed by the integers 1, 2, …, m. There are factorial(m) such patterns.\n\nFor example, the outcome [2, 3, 1] corresponds to the ordinal pattern of having the smallest value in the second position, the next smallest value in the third position, and the next smallest, i.e. the largest value in the first position. See also OrdinalPatternEncoding.\n\nIn-place symbolization\n\nOrdinalPatterns also implements the in-place probabilities! for StateSpaceSet input (or embedded vector input) for reducing allocations in looping scenarios. The length of the pre-allocated symbol vector must be the length of the dataset. For example\n\nusing ComplexityMeasures\nm, N = 2, 100\nest = OrdinalPatterns{m}(τ)\nx = StateSpaceSet(rand(N, m)) # some input dataset\nπs_ts = zeros(Int, N) # length must match length of `x`\np = probabilities!(πs_ts, est, x)\n\n\n\n\n\n","category":"type"},{"location":"probabilities/#ComplexityMeasures.WeightedOrdinalPatterns","page":"Probabilities","title":"ComplexityMeasures.WeightedOrdinalPatterns","text":"WeightedOrdinalPatterns <: OutcomeSpace\nWeightedOrdinalPatterns{m}(τ = 1, lt::Function = ComplexityMeasures.isless_rand)\n\nA variant of OrdinalPatterns that also incorporates amplitude information, based on the weighted permutation entropy (Fadlallah et al., 2013). The outcome space and arguments are the same as in OrdinalPatterns.\n\nDescription\n\nFor each ordinal pattern extracted from each state (or delay) vector, a weight is attached to it which is the variance of the vector. Probabilities are then estimated by summing the weights corresponding to the same pattern, instead of just counting the occurrence of the same pattern.\n\nnote: An implementation note\nNote: in equation 7, section III, of the original paper, the authors writew_j = dfrac1msum_k=1^m (x_j-(k-1)tau - mathbfhatx_j^m tau)^2*But given the formula they give for the arithmetic mean, this is not the variance of the delay vector mathbfx_i, because the indices are mixed: x_j+(k-1)tau in the weights formula, vs. x_j+(k+1)tau in the arithmetic mean formula. Here, delay embedding and computation of the patterns and their weights are completely separated processes, ensuring that we compute the arithmetic mean correctly for each vector of the input dataset (which may be a delay-embedded timeseries).\n\n\n\n\n\n","category":"type"},{"location":"probabilities/#ComplexityMeasures.AmplitudeAwareOrdinalPatterns","page":"Probabilities","title":"ComplexityMeasures.AmplitudeAwareOrdinalPatterns","text":"AmplitudeAwareOrdinalPatterns <: OutcomeSpace\nAmplitudeAwareOrdinalPatterns{m}(τ = 1, A = 0.5, lt = ComplexityMeasures.isless_rand)\n\nA variant of OrdinalPatterns that also incorporates amplitude information, based on the amplitude-aware permutation entropy (Azami and Escudero, 2016). The outcome space and arguments are the same as in OrdinalPatterns.\n\nDescription\n\nSimilarly to WeightedOrdinalPatterns, a weight w_i is attached to each ordinal pattern extracted from each state (or delay) vector mathbfx_i = (x_1^i x_2^i ldots x_m^i) as\n\nw_i = dfracAm sum_k=1^m x_k^i  + dfrac1-Ad-1\nsum_k=2^d x_k^i - x_k-1^i\n\nwith 0 leq A leq 1. When A=0 , only internal differences between the elements of mathbfx_i are weighted. Only mean amplitude of the state vector elements are weighted when A=1. With, 0A1, a combined weighting is used.\n\n\n\n\n\n","category":"type"},{"location":"probabilities/#Dispersion-patterns","page":"Probabilities","title":"Dispersion patterns","text":"","category":"section"},{"location":"probabilities/","page":"Probabilities","title":"Probabilities","text":"Dispersion","category":"page"},{"location":"probabilities/#ComplexityMeasures.Dispersion","page":"Probabilities","title":"ComplexityMeasures.Dispersion","text":"Dispersion(; c = 5, m = 2, τ = 1, check_unique = true)\n\nAn OutcomeSpace based on dispersion patterns, originally used by Rostaghi and Azami (2016) to compute the \"dispersion entropy\", which characterizes the complexity and irregularity of a time series.\n\nRecommended parameter values (Li et al., 2019) are m ∈ [2, 3], τ = 1 for the embedding, and c ∈ [3, 4, …, 8] categories for the Gaussian symbol mapping.\n\nDescription\n\nAssume we have a univariate time series X = x_i_i=1^N. First, this time series is encoded into a symbol timeseries S using the Gaussian encoding GaussianCDFEncoding with empirical mean μ and empirical standard deviation σ (both determined from X), and c as given to Dispersion.\n\nThen, S is embedded into an m-dimensional time series, using an embedding lag of tau, which yields a total of N - (m - 1)tau delay vectors z_i, or \"dispersion patterns\". Since each element of z_i can take on c different values, and each delay vector has m entries, there are c^m possible dispersion patterns. This number is used for normalization when computing dispersion entropy.\n\nThe returned probabilities are simply the frequencies of the unique dispersion patterns present in S (i.e., the UniqueElements of S).\n\nOutcome space\n\nThe outcome space for Dispersion is the unique delay vectors whose elements are the the symbols (integers) encoded by the Gaussian CDF, i.e., the unique elements of S.\n\nData requirements and parameters\n\nThe input must have more than one unique element for the Gaussian mapping to be well-defined. Li et al. (2019) recommends that x has at least 1000 data points.\n\nIf check_unique == true (default), then it is checked that the input has more than one unique value. If check_unique == false and the input only has one unique element, then a InexactError is thrown when trying to compute probabilities.\n\nnote: Why 'dispersion patterns'?\nEach embedding vector is called a \"dispersion pattern\". Why? Let's consider the case when m = 5 and c = 3, and use some very imprecise terminology for illustration:When c = 3, values clustering far below mean are in one group, values clustered around the mean are in one group, and values clustering far above the mean are in a third group. Then the embedding vector 2 2 2 2 2 consists of values that are close together (close to the mean), so it represents a set of numbers that are not very spread out (less dispersed). The embedding vector 1 1 2 3 3, however, represents numbers that are much more spread out (more dispersed), because the categories representing \"outliers\" both above and below the mean are represented, not only values close to the mean.\n\nFor a version of this estimator that can be used on high-dimensional arrays, see SpatialDispersion.\n\nImplements\n\ncodify. Used for encoding inputs where ordering matters (e.g. time series).\n\n\n\n\n\n","category":"type"},{"location":"probabilities/#Transfer-operator","page":"Probabilities","title":"Transfer operator","text":"","category":"section"},{"location":"probabilities/","page":"Probabilities","title":"Probabilities","text":"TransferOperator","category":"page"},{"location":"probabilities/#ComplexityMeasures.TransferOperator","page":"Probabilities","title":"ComplexityMeasures.TransferOperator","text":"TransferOperator <: OutcomeSpace\nTransferOperator(b::AbstractBinning; warn_precise = true, rng = Random.default_rng())\n\nAn OutcomeSpace based on binning data into rectangular boxes dictated by the given binning scheme b.\n\nWhen used with probabilities, then the transfer (Perron-Frobenius) operator is approximated over the bins, then bin probabilities are estimated as the invariant measure associated with that transfer operator. Assumes that the input data are sequential (time-ordered).\n\nThis implementation follows the grid estimator approach in Diego et al. (2019).\n\nPrecision\n\nThe default behaviour when using RectangularBinning or FixedRectangularBinning is to accept some loss of precision on the  bin boundaries for speed-ups, but this may lead to issues for TransferOperator where some points may be encoded as the symbol -1 (\"outside the binning\"). The warn_precise keyword controls whether the user is warned when a less  precise binning is used.\n\nOutcome space\n\nThe outcome space for TransferOperator is the set of unique bins constructed from b. Bins are identified by their left (lowest-value) corners, are given in data units, and are returned as SVectors.\n\nBin ordering\n\nBins returned by probabilities_and_outcomes are ordered according to first appearance (i.e. the first time the input (multivariate) timeseries visits the bin). Thus, if\n\nb = RectangularBinning(4)\nest = TransferOperator(b)\nprobs, outcomes = probabilities_and_outcomes(x, est) # x is some timeseries\n\nthen probs[i] is the invariant measure (probability) of the bin outcomes[i], which is the i-th bin visited by the timeseries with nonzero measure.\n\nDescription\n\nThe transfer operator P^Nis computed as an N-by-N matrix of transition probabilities between the states defined by the partition elements, where N is the number of boxes in the partition that is visited by the orbit/points.\n\nIf  x_t^(D) _n=1^L are the L different D-dimensional points over which the transfer operator is approximated,  C_k=1^N  are the N different partition elements (as dictated by ϵ) that gets visited by the points, and  phi(x_t) = x_t+1, then\n\nP_ij = dfrac\n x_n  phi(x_n) in C_j cap x_n in C_i \n x_m  x_m in C_i \n\nwhere  denotes the cardinal. The element P_ij thus indicates how many points that are initially in box C_i end up in box C_j when the points in C_i are projected one step forward in time. Thus, the row P_ik^N where k in 1 2 ldots N  gives the probability of jumping from the state defined by box C_i to any of the other N states. It follows that sum_k=1^N P_ik = 1 for all i. Thus, P^N is a row/right stochastic matrix.\n\nInvariant measure estimation from transfer operator\n\nThe left invariant distribution mathbfrho^N is a row vector, where mathbfrho^N P^N = mathbfrho^N. Hence, mathbfrho^N is a row eigenvector of the transfer matrix P^N associated with eigenvalue 1. The distribution mathbfrho^N approximates the invariant density of the system subject to binning, and can be taken as a probability distribution over the partition elements.\n\nIn practice, the invariant measure mathbfrho^N is computed using invariantmeasure, which also approximates the transfer matrix. The invariant distribution is initialized as a length-N random distribution which is then applied to P^N. For reproducibility in this step, set the rng. The resulting length-N distribution is then applied to P^N again. This process repeats until the difference between the distributions over consecutive iterations is below some threshold.\n\nSee also: RectangularBinning, FixedRectangularBinning, invariantmeasure.\n\n\n\n\n\n","category":"type"},{"location":"probabilities/#Utility-methods/types","page":"Probabilities","title":"Utility methods/types","text":"","category":"section"},{"location":"probabilities/","page":"Probabilities","title":"Probabilities","text":"InvariantMeasure\ninvariantmeasure\ntransfermatrix","category":"page"},{"location":"probabilities/#ComplexityMeasures.InvariantMeasure","page":"Probabilities","title":"ComplexityMeasures.InvariantMeasure","text":"InvariantMeasure(to, ρ)\n\nMinimal return struct for invariantmeasure that contains the estimated invariant measure ρ, as well as the transfer operator to from which it is computed (including bin information).\n\nSee also: invariantmeasure.\n\n\n\n\n\n","category":"type"},{"location":"probabilities/#ComplexityMeasures.invariantmeasure","page":"Probabilities","title":"ComplexityMeasures.invariantmeasure","text":"invariantmeasure(x::AbstractStateSpaceSet, binning::RectangularBinning;\n    rng = Random.default_rng()) → iv::InvariantMeasure\n\nEstimate an invariant measure over the points in x based on binning the data into rectangular boxes dictated by the binning, then approximate the transfer (Perron-Frobenius) operator over the bins. From the approximation to the transfer operator, compute an invariant distribution over the bins. Assumes that the input data are sequential.\n\nDetails on the estimation procedure is found the TransferOperator docstring.\n\nExample\n\nusing DynamicalSystems\nhenon_rule(x, p, n) = SVector{2}(1.0 - p[1]*x[1]^2 + x[2], p[2]*x[1])\nhenon = DeterministicIteratedMap(henon_rule, zeros(2), [1.4, 0.3])\norbit, t = trajectory(ds, 20_000; Ttr = 10)\n\n# Estimate the invariant measure over some coarse graining of the orbit.\niv = invariantmeasure(orbit, RectangularBinning(15))\n\n# Get the probabilities and bins\ninvariantmeasure(iv)\n\nProbabilities and bin information\n\ninvariantmeasure(iv::InvariantMeasure) → (ρ::Probabilities, bins::Vector{<:SVector})\n\nFrom a pre-computed invariant measure, return the probabilities and associated bins. The element ρ[i] is the probability of visitation to the box bins[i].\n\nhint: Transfer operator approach vs. naive histogram approach\nWhy bother with the transfer operator instead of using regular histograms to obtain probabilities?In fact, the naive histogram approach and the transfer operator approach are equivalent in the limit of long enough time series (as n to intfy), which is guaranteed by the ergodic theorem. There is a crucial difference, however:The naive histogram approach only gives the long-term probabilities that orbits visit a certain region of the state space. The transfer operator encodes that information too, but comes with the added benefit of knowing the transition probabilities between states (see transfermatrix).\n\nSee also: InvariantMeasure.\n\n\n\n\n\n","category":"function"},{"location":"probabilities/#ComplexityMeasures.transfermatrix","page":"Probabilities","title":"ComplexityMeasures.transfermatrix","text":"transfermatrix(iv::InvariantMeasure) → (M::AbstractArray{<:Real, 2}, bins::Vector{<:SVector})\n\nReturn the transfer matrix/operator and corresponding bins. Here, bins[i] corresponds to the i-th row/column of the transfer matrix. Thus, the entry M[i, j] is the probability of jumping from the state defined by bins[i] to the state defined by bins[j].\n\nSee also: TransferOperator.\n\n\n\n\n\n","category":"function"},{"location":"probabilities/#Kernel-density","page":"Probabilities","title":"Kernel density","text":"","category":"section"},{"location":"probabilities/","page":"Probabilities","title":"Probabilities","text":"NaiveKernel","category":"page"},{"location":"probabilities/#ComplexityMeasures.NaiveKernel","page":"Probabilities","title":"ComplexityMeasures.NaiveKernel","text":"NaiveKernel(ϵ::Real; method = KDTree, w = 0, metric = Euclidean()) <: OutcomeSpace\n\nAn OutcomeSpace based on a \"naive\" kernel density estimation approach (KDE), as discussed in Prichard and Theiler (1995).\n\nProbabilities P(mathbfx epsilon) are assigned to every point mathbfx by counting how many other points occupy the space spanned by a hypersphere of radius ϵ around mathbfx, according to:\n\nP_i( X epsilon) approx dfrac1N sum_s B(X_i - X_j  epsilon)\n\nwhere B gives 1 if the argument is true. Probabilities are then normalized.\n\nKeyword arguments\n\nmethod = KDTree: the search structure supported by Neighborhood.jl. Specifically, use KDTree to use a tree-based neighbor search, or BruteForce for the direct distances between all points. KDTrees heavily outperform direct distances when the dimensionality of the data is much smaller than the data length.\nw = 0: the Theiler window, which excludes indices s that are within i - s  w from the given point x_i.\nmetric = Euclidean(): the distance metric.\n\nOutcome space\n\nThe outcome space Ω for NaiveKernel are the indices of the input data, eachindex(x). Hence, input x is needed for a well-defined outcome_space. The reason to not return the data points themselves is because duplicate data points may not get assigned same probabilities (due to having different neighbors).\n\n\n\n\n\n","category":"type"},{"location":"probabilities/#Timescales","page":"Probabilities","title":"Timescales","text":"","category":"section"},{"location":"probabilities/","page":"Probabilities","title":"Probabilities","text":"WaveletOverlap\nPowerSpectrum","category":"page"},{"location":"probabilities/#ComplexityMeasures.WaveletOverlap","page":"Probabilities","title":"ComplexityMeasures.WaveletOverlap","text":"WaveletOverlap([wavelet]) <: OutcomeSpace\n\nAn OutcomeSpace based on the maximal overlap discrete wavelet transform (MODWT).\n\nWhen used with probabilities, the MODWT is applied to a signal, then probabilities are computed as the (normalized) energies at different wavelet scales. These probabilities are used to compute the wavelet entropy according to Rosso et al. (2001). Input timeseries x is needed for a well-defined outcome space.\n\nBy default the wavelet Wavelets.WT.Daubechies{12}() is used. Otherwise, you may choose a wavelet from the Wavelets package (it must subtype OrthoWaveletClass).\n\nOutcome space\n\nThe outcome space for WaveletOverlap are the integers 1, 2, …, N enumerating the wavelet scales. To obtain a better understanding of what these mean, we prepared a notebook you can view online. As such, this estimator only works for timeseries input and input x is needed for a well-defined outcome_space.\n\n\n\n\n\n","category":"type"},{"location":"probabilities/#ComplexityMeasures.PowerSpectrum","page":"Probabilities","title":"ComplexityMeasures.PowerSpectrum","text":"PowerSpectrum() <: OutcomeSpace\n\nAn OutcomeSpace based on the power spectrum of a timeseries (amplitude square of its Fourier transform).\n\nIf used with probabilities, then the spectrum normalized to sum = 1 is returned as probabilities. The Shannon entropy of these probabilities is typically referred in the literature as spectral entropy, e.g. Llanos et al. (2017) and Tian et al. (2017).\n\nThe closer the spectrum is to flat, i.e., white noise, the higher the entropy. However, you can't compare entropies of timeseries with different length, because the binning in spectral space depends on the length of the input.\n\nOutcome space\n\nThe outcome space Ω for PowerSpectrum is the set of frequencies in Fourier space. They should be multiplied with the sampling rate of the signal, which is assumed to be 1. Input x is needed for a well-defined outcome_space.\n\n\n\n\n\n","category":"type"},{"location":"probabilities/#Cosine-similarity-binning","page":"Probabilities","title":"Cosine similarity binning","text":"","category":"section"},{"location":"probabilities/","page":"Probabilities","title":"Probabilities","text":"CosineSimilarityBinning\nDiversity","category":"page"},{"location":"probabilities/#ComplexityMeasures.CosineSimilarityBinning","page":"Probabilities","title":"ComplexityMeasures.CosineSimilarityBinning","text":"CosineSimilarityBinning(; m::Int, τ::Int, nbins::Int)\n\nA OutcomeSpace based on the cosine similarity (Wang et al., 2020).\n\nIt can be used with information to compute the \"diversity entropy\" of an input timeseries (Wang et al., 2020).\n\nThe implementation here allows for τ != 1, which was not considered in the original paper.\n\nDescription\n\nCosineSimilarityBinning probabilities are computed as follows.\n\nFrom the input time series x, using embedding lag τ and embedding dimension m,  construct the embedding  Y = bf x_i  = (x_i x_i+tau x_i+2tau ldots x_i+mtau - 1_i = 1^N-mτ.\nCompute D = d(bf x_t bf x_t+1) _t=1^N-mτ-1,  where d(cdot cdot) is the cosine similarity between two m-dimensional  vectors in the embedding.\nDivide the interval [-1, 1] into nbins equally sized subintervals (including the value +1).\nConstruct a histogram of cosine similarities d in D over those subintervals.\nSum-normalize the histogram to obtain probabilities.\n\nOutcome space\n\nThe outcome space for CosineSimilarityBinning is the bins of the [-1, 1] interval, and the return configuration is the same as in ValueBinning (left bin edge).\n\nImplements\n\ncodify. Used for encoding inputs where ordering matters (e.g. time series).\n\n\n\n\n\n","category":"type"},{"location":"probabilities/#ComplexityMeasures.Diversity","page":"Probabilities","title":"ComplexityMeasures.Diversity","text":"Diversity\n\nAn alias to CosineSimilarityBinning.\n\n\n\n\n\n","category":"type"},{"location":"probabilities/#Spatial-outcome-spaces","page":"Probabilities","title":"Spatial outcome spaces","text":"","category":"section"},{"location":"probabilities/","page":"Probabilities","title":"Probabilities","text":"SpatialOrdinalPatterns\nSpatialDispersion","category":"page"},{"location":"probabilities/#ComplexityMeasures.SpatialOrdinalPatterns","page":"Probabilities","title":"ComplexityMeasures.SpatialOrdinalPatterns","text":"SpatialOrdinalPatterns <: OutcomeSpaceModel\nSpatialOrdinalPatterns(stencil, x; periodic = true)\n\nA symbolic, permutation-based OutcomeSpace for spatiotemporal systems that generalises OrdinalPatterns to high-dimensional arrays. The order m of the permutation pattern is extracted from the stencil, see below.\n\nSpatialOrdinalPatterns is based on the 2D and 3D spatiotemporal permutation entropy estimators by Ribeiro et al. (2012) and Schlemmer et al. (2018), respectively, but is here implemented as a pure probabilities probabilities estimator that is generalized for D-dimensional input array x, with arbitrary regions (stencils) to get patterns form and (possibly) periodic boundary conditions.\n\nSee below for ways to specify the stencil. If periodic = true, then the stencil wraps around at the ends of the array. If false, then collected regions with indices which exceed the array bounds are skipped.\n\nIn combination with information and information_normalized, this probabilities estimator can be used to compute generalized spatiotemporal permutation InformationMeasure of any type.\n\nOutcome space\n\nThe outcome space Ω for SpatialOrdinalPatterns is the set of length-m ordinal patterns (i.e. permutations) that can be formed by the integers 1, 2, …, m, ordered lexicographically. There are factorial(m) such patterns. Here m refers to the number of points included in stencil.\n\nStencils\n\nThe stencil defines what local area to use to group hypervoxels. Each grouping of hypervoxels is mapped to an order-m permutation pattern, which is then mapped to an integer as in OrdinalPatterns. The stencil is moved around the input array, in a sense \"scanning\" the input array, to collect all possible groupings allowed by the boundary condition (periodic or not).\n\nStencils are passed in one of the following three ways:\n\nAs vectors of CartesianIndex which encode the offset of indices to include in the  stencil, with respect to the current array index when scanning over the array.  For example stencil = CartesianIndex.([(0,0), (0,1), (1,1), (1,0)]).  Don't forget to include the zero offset index if you want to include the hypervoxel  itself, which is almost always the case.  Here the stencil creates a 2x2 square extending to the bottom and right of the pixel  (directions here correspond to the way Julia prints matrices by default).  When passing a stencil as a vector of CartesianIndex, m = length(stencil).\nAs a D-dimensional array (where D matches the dimensionality of the input data)  containing 0s and 1s, where if stencil[index] == 1, the corresponding pixel is  included, and if stencil[index] == 0, it is not included.  To generate the same estimator as in 1., use stencil = [1 1; 1 1].  When passing a stencil as a D-dimensional array, m = sum(stencil)\nAs a Tuple containing two Tuples, both of length D, for D-dimensional data.  The first tuple specifies the extent of the stencil, where extent[i]  dictates the number of hypervoxels to be included along the ith axis and lag[i]  the separation of hypervoxels along the same axis.  This method can only generate (hyper)rectangular stencils. To create the same estimator as  in the previous examples, use here stencil = ((2, 2), (1, 1)).  When passing a stencil using extent and lag, m = prod(extent).\n\n\n\n\n\n","category":"type"},{"location":"probabilities/#ComplexityMeasures.SpatialDispersion","page":"Probabilities","title":"ComplexityMeasures.SpatialDispersion","text":"SpatialDispersion <: OutcomeSpace\nSpatialDispersion(stencil, x::AbstractArray;\n    periodic = true,\n    c = 5,\n    skip_encoding = false,\n    L = nothing,\n)\n\nA dispersion-based OutcomeSpace that generalises Dispersion for input data that are high-dimensional arrays.\n\nSpatialDispersion is based on Azami et al. (2019)'s 2D square dispersion (Shannon) entropy estimator, but is here implemented as a pure probabilities probabilities estimator that is generalized for N-dimensional input data x, with arbitrary neighborhood regions (stencils) and (optionally) periodic boundary conditions.\n\nIn combination with information and information_normalized, this probabilities estimator can be used to compute (normalized) generalized spatiotemporal dispersion InformationMeasure of any type.\n\nArguments\n\nstencil. Defines what local area (hyperrectangle), or which points within this area,   to include around each hypervoxel (i.e. pixel in 2D). The examples below demonstrate   different ways of specifying stencils. For details, see   SpatialOrdinalPatterns. See SpatialOrdinalPatterns for   more information about stencils.\nx::AbstractArray. The input data. Must be provided because we need to know its size   for optimization and bound checking.\n\nKeyword arguments\n\nperiodic::Bool. If periodic == true, then the stencil should wrap around at the   end of the array. If periodic = false, then pixels whose stencil exceeds the array   bounds are skipped.\nc::Int. Determines how many discrete categories to use for the Gaussian encoding.\nskip_encoding. If skip_encoding == true, encoding is ignored, and dispersion   patterns are computed directly from x, under the assumption that L is the alphabet   length for x (useful for categorical or integer data). Thus, if   skip_encoding == true, then L must also be specified. This is useful for   categorical or integer-valued data.\nL. If L == nothing (default), then the number of total outcomes is inferred from   stencil and encoding. If L is set to an integer, then the data is considered   pre-encoded and the number of total outcomes is set to L.\n\nOutcome space\n\nThe outcome space for SpatialDispersion is the unique delay vectors whose elements are the the symbols (integers) encoded by the Gaussian CDF. Hence, the outcome space is all m-dimensional delay vectors whose elements are all possible values in 1:c. There are c^m such vectors.\n\nDescription\n\nEstimating probabilities/entropies from higher-dimensional data is conceptually simple.\n\nDiscretize each value (hypervoxel) in x relative to all other values xᵢ ∈ x using the  provided encoding scheme.\nUse stencil to extract relevant (discretized) points around each hypervoxel.\nConstruct a symbol these points.\nTake the sum-normalized histogram of the symbol as a probability distribution.\nOptionally, compute information or information_normalized from this  probability distribution.\n\nUsage\n\nHere's how to compute spatial dispersion entropy using the three different ways of specifying stencils.\n\nx = rand(50, 50) # first \"time slice\" of a spatial system evolution\n\n# Cartesian stencil\nstencil_cartesian = CartesianIndex.([(0,0), (1,0), (1,1), (0,1)])\nest = SpatialDispersion(stencil_cartesian, x)\ninformation_normalized(est, x)\n\n# Extent/lag stencil\nextent = (2, 2); lag = (1, 1); stencil_ext_lag = (extent, lag)\nest = SpatialDispersion(stencil_ext_lag, x)\ninformation_normalized(est, x)\n\n# Matrix stencil\nstencil_matrix = [1 1; 1 1]\nest = SpatialDispersion(stencil_matrix, x)\ninformation_normalized(est, x)\n\nTo apply this to timeseries of spatial data, simply loop over the call (broadcast), e.g.:\n\nimgs = [rand(50, 50) for i = 1:100]; # one image per second over 100 seconds\nstencil = ((2, 2), (1, 1)) # a 2x2 stencil (i.e. dispersion patterns of length 4)\nest = SpatialDispersion(stencil, first(imgs))\nh_vs_t = information_normalized.(Ref(est), imgs)\n\nComputing generalized spatiotemporal dispersion entropy is trivial, e.g. with Renyi:\n\nx = reshape(repeat(1:5, 500) .+ 0.1*rand(500*5), 50, 50)\nest = SpatialDispersion(stencil, x)\ninformation(Renyi(q = 2), est, x)\n\nSee also: SpatialOrdinalPatterns, GaussianCDFEncoding, codify.\n\n\n\n\n\n","category":"type"},{"location":"probabilities/#Probabilities-and-related-functions","page":"Probabilities","title":"Probabilities and related functions","text":"","category":"section"},{"location":"probabilities/","page":"Probabilities","title":"Probabilities","text":"Probabilities\nprobabilities\nprobabilities_and_outcomes\nallprobabilities_and_outcomes\nprobabilities!","category":"page"},{"location":"probabilities/#ComplexityMeasures.Probabilities","page":"Probabilities","title":"ComplexityMeasures.Probabilities","text":"Probabilities <: Array{N, <: AbstractFloat}\nProbabilities(counts/probs [, outcomes [, outnames]]) → p\n\nProbabilities stores an N-dimensional array of probabilities, while ensuring that the array sums to 1 (normalized probability mass). The probabilities correspond to outcomes that describe the axes of the array. In most cases the array is a standard vector.\n\nIf p isa Probabilities, then p.outcomes[i] is the outcomes along the i-th dimension, each being an abstract vector whose order is the same one corresponding to p, and p.dimlabels[i] is the label of the i-th dimension. Both labels and outcomes are assigned automatically if not given. p itself can be manipulated and iterated over like its stored array.\n\n\n\n\n\n","category":"type"},{"location":"probabilities/#ComplexityMeasures.probabilities","page":"Probabilities","title":"ComplexityMeasures.probabilities","text":"probabilities(\n    [est::ProbabilitiesEstimator], o::OutcomeSpace, x::Array_or_SSSet\n) → (p::Probabilities, Ω)\n\nLike probabilities_and_outcomes, but returns the Probabilities p directly.\n\nCompute the same probabilities as in the probabilities_and_outcomes function, with two differences:\n\nDo not explicitly return the outcomes.\nIf the outcomes are not estimated for free while estimating the counts, a special integer type is used to enumerate the outcomes, to avoid the computational cost of estimating the outcomes.\n\nprobabilities([est::ProbabilitiesEstimator], counts::Counts) → (p::Probabilities, Ω)\n\nThe same as above, but estimate the probability directly from a set of Counts.\n\n\n\n\n\n","category":"function"},{"location":"probabilities/#ComplexityMeasures.probabilities_and_outcomes","page":"Probabilities","title":"ComplexityMeasures.probabilities_and_outcomes","text":"probabilities_and_outcomes(\n    [est::ProbabilitiesEstimator], o::OutcomeSpace, x::Array_or_SSSet\n) → (p::Probabilities, Ω)\n\nEstimate a probability distribution over the set of possible outcomes Ω defined by the OutcomeSpace o, given input data x. Probabilities are estimated according to the given probabilities estimator est, which defaults to RelativeAmount.\n\nThe input data is typically an Array or a StateSpaceSet (or SSSet for short); see Input data for ComplexityMeasures.jl. Configuration options are always given as arguments to the chosen outcome space and probabilities estimator.\n\nReturn a tuple where the first element is a Probabilities instance, which is vector-like and contains the probabilities, and where the second element Ω are the outcomes corresponding to the probabilities, such that p[i] is the probability for the outcome Ω[i].\n\nThe outcomes are actually included in p, and you can use the outcomes function on the p to get them. probabilities_and_outcomes returns both for backwards compatibility.\n\nprobabilities_and_outcomes(\n    [est::ProbabilitiesEstimator], counts::Counts\n) → (p::Probabilities, Ω)\n\nEstimate probabilities from the pre-computed counts using the given ProbabilitiesEstimator est.\n\nDescription\n\nProbabilities are computed by:\n\nDiscretizing/encoding x into a finite set of outcomes Ω specified by the provided  OutcomeSpace o.\nAssigning to each outcome Ωᵢ ∈ Ω either a count (how often it appears among the  discretized data points), or a pseudo-count (some pre-normalized probability such  that sum(Ωᵢ for Ωᵢ in Ω) == 1).\n\nFor outcome spaces that result in pseudo counts, such as PowerSpectrum, these pseudo counts are simply treated as probabilities and returned directly (that is, est is ignored). For counting-based outcome spaces (see OutcomeSpace docstring), probabilities are estimated from the counts using some ProbabilitiesEstimator (first signature).\n\nObserved vs all probabilities\n\nDue to performance optimizations, whether the returned probabilities contain 0s as entries or not depends on the outcome space. E.g., in ValueBinning 0s are skipped, while in PowerSpectrum 0 are not skipped, because we get them for free.\n\nUse allprobabilities_and_outcomes to guarantee that zero probabilities are also returned (may be slower).\n\n\n\n\n\n","category":"function"},{"location":"probabilities/#ComplexityMeasures.allprobabilities_and_outcomes","page":"Probabilities","title":"ComplexityMeasures.allprobabilities_and_outcomes","text":"allprobabilities_and_outcomes(est::ProbabilitiesEstimator, x::Array_or_SSSet) → (p::Probabilities, outs)\nallprobabilities_and_outcomes(o::OutcomeSpace, x::Array_or_SSSet) → (p::Probabilities, outs)\n\nThe same as probabilities_and_outcomes, but ensures that outcomes with 0 probability are explicitly added in the returned vector. This means that p[i] is the probability of ospace[i], with ospace =outcome_space(est, x).\n\nThis function is useful in cases where one wants to compare the probability mass functions of two different input data x, y under the same estimator. E.g., to compute the KL-divergence of the two PMFs assumes that the obey the same indexing. This is not true for probabilities even with the same est, due to the skipping of 0 entries, but it is true for allprobabilities_and_outcomes.\n\n\n\n\n\n","category":"function"},{"location":"probabilities/#ComplexityMeasures.probabilities!","page":"Probabilities","title":"ComplexityMeasures.probabilities!","text":"probabilities!(s, args...)\n\nSimilar to probabilities(args...), but allows pre-allocation of temporarily used containers s.\n\nOnly works for certain estimators. See for example OrdinalPatterns.\n\n\n\n\n\n","category":"function"},{"location":"probabilities/#Counts","page":"Probabilities","title":"Counts","text":"","category":"section"},{"location":"probabilities/","page":"Probabilities","title":"Probabilities","text":"Counts\ncounts_and_outcomes\ncounts\nallcounts_and_outcomes\nis_counting_based","category":"page"},{"location":"probabilities/#ComplexityMeasures.Counts","page":"Probabilities","title":"ComplexityMeasures.Counts","text":"Counts <: Array{N, <: Integer}\nCounts(counts [, outcomes [, outnames]]) → c\n\nCounts stores an N-dimensional array of integer counts corresponding to a set of outcomes. This is typically called a \"frequency table\" or \"contingency table\".\n\nIf c isa Counts, then c.outcomes[i] is the outcomes along the i-th dimension, each being an abstract vector whose order is the same one corresponding to c, and c.dimlabels[i] is the label of the i-th dimension. Both labels and outcomes are assigned automatically if not given. c itself can be manipulated and iterated over like its stored array.\n\n\n\n\n\n","category":"type"},{"location":"probabilities/#ComplexityMeasures.counts_and_outcomes","page":"Probabilities","title":"ComplexityMeasures.counts_and_outcomes","text":"counts_and_outcomes(o::OutcomeSpace, x) → (cts::Counts, Ω)\n\nDiscretize/encode x (which must be sortable) into a finite set of outcomes Ω specified by the provided OutcomeSpace o, and then count how often each outcome Ωᵢ ∈ Ω (i.e. each \"discretized value\", or \"encoded symbol\") appears.\n\nReturn a tuple where the first element is a Counts instance, which is vector-like and contains the counts, and where the second element Ω are the outcomes corresponding to the counts, such that cts[i] is the count for the outcome Ω[i].\n\nThe outcomes are actually included in cts, and you can use the outcomes function on the cts to get them. counts_and_outcomes returns both for backwards compatibility.\n\ncounts_and_outcomes(x) → cts::Counts\n\nIf no OutcomeSpace is specified, then UniqueElements is used as the outcome space.\n\nDescription\n\nFor OutcomeSpaces that uses encode to discretize, it is possible to count how often each outcome omega_i in Omega, where Omega is the set of possible outcomes, is observed in the discretized/encoded input data. Thus, we can assign to each outcome omega_i a count f(omega_i), such that sum_i=1^N f(omega_i) = N, where N is the number of observations in the (encoded) input data. counts returns the counts f(omega_i)_obs and outcomes only for the observed outcomes omega_i^obs (those outcomes that actually appear in the input data). If you need the counts for unobserved outcomes as well, use allcounts_and_outcomes.\n\n\n\n\n\n","category":"function"},{"location":"probabilities/#ComplexityMeasures.counts","page":"Probabilities","title":"ComplexityMeasures.counts","text":"counts(o::OutcomeSpace, x) → cts::Counts\n\nCompute the same counts as in the counts_and_outcomes function, with two differences:\n\nDo not explicitly return the outcomes.\nIf the outcomes are not estimated for free while estimating the counts, a special integer type is used to enumerate the outcomes, to avoid the computational cost of estimating the outcomes.\n\n\n\n\n\n","category":"function"},{"location":"probabilities/#ComplexityMeasures.allcounts_and_outcomes","page":"Probabilities","title":"ComplexityMeasures.allcounts_and_outcomes","text":"allcounts_and_outcomes(o::OutcomeSpace, x::Array_or_SSSet) → (cts::Counts{<:Integer, 1}, Ω)\n\nLike counts_and_outcomes, but ensures that all outcomes Ωᵢ ∈ Ω, where Ω = outcome_space(o, x)), are included.\n\nOutcomes that do not occur in the data x get a 0 count.\n\n\n\n\n\n","category":"function"},{"location":"probabilities/#ComplexityMeasures.is_counting_based","page":"Probabilities","title":"ComplexityMeasures.is_counting_based","text":"is_counting_based(o::OutcomeSpace)\n\nReturn true if the OutcomeSpace o is counting-based, and false otherwise.\n\n\n\n\n\n","category":"function"},{"location":"probabilities/#probability_estimators","page":"Probabilities","title":"Probability estimators","text":"","category":"section"},{"location":"probabilities/","page":"Probabilities","title":"Probabilities","text":"ProbabilitiesEstimator\nRelativeAmount\nBayesianRegularization\nShrinkage\nAddConstant","category":"page"},{"location":"probabilities/#ComplexityMeasures.ProbabilitiesEstimator","page":"Probabilities","title":"ComplexityMeasures.ProbabilitiesEstimator","text":"ProbabilitiesEstimator\n\nThe supertype for all probabilities estimators.\n\nThe role of the probabilities estimator is to convert (pseudo-)counts to probabilities. Currently, the implementation of all probabilities estimators assume finite outcome space with known cardinality. Therefore, ProbabilitiesEstimator accept an OutcomeSpace as the first argument, which specifies the set of possible outcomes.\n\nProbabilities estimators are used with probabilities and allprobabilities_and_outcomes.\n\nImplementations\n\nThe default probabilities estimator is RelativeAmount, which is compatible with any OutcomeSpace. The following estimators only support counting-based outcomes.\n\nShrinkage.\nBayesianRegularization.\nAddConstant.\n\nDescription\n\nIn ComplexityMeasures.jl, probability mass functions are estimated from data by defining a set of possible outcomes Omega = omega_1 omega_2 ldots omega_L  (by specifying an OutcomeSpace), and assigning to each outcome omega_i a probability p(omega_i), such that sum_i=1^N p(omega_i) = 1 (by specifying a ProbabilitiesEstimator).\n\n\n\n\n\n","category":"type"},{"location":"probabilities/#ComplexityMeasures.RelativeAmount","page":"Probabilities","title":"ComplexityMeasures.RelativeAmount","text":"RelativeAmount <: ProbabilitiesEstimator\nRelativeAmount()\n\nThe RelativeAmount estimator is used with probabilities and related functions to estimate probabilities over the given OutcomeSpace using maximum likelihood estimation (MLE), also called plug-in estimation. See ProbabilitiesEstimator for usage.\n\nDescription\n\nConsider a length-m outcome space Omega and random sample of length N. The maximum likelihood estimate of the probability of the k-th outcome omega_k is\n\np(omega_k) = dfracn_kN\n\nwhere n_k is the number of times the k-th outcome was observed in the (encoded) sample.\n\nThis estimation is known as maximum likelihood estimation. However, RelativeAmount also serves as the fall-back probabilities estimator for OutcomeSpaces that are not count-based and only yield \"pseudo-counts\", for example WaveletOverlap or PowerSpectrum. These outcome spaces do not yield counts, but pre-normalized numbers that can be treated as \"relative frequencies\" or \"relative power\". Hence, this estimator is called RelativeAmount.\n\nExamples\n\nusing ComplexityMeasures\nx = cumsum(randn(100))\nps = probabilities(OrdinalPatterns{3}(), x) # `RelativeAmount` is the default estimator\nps_mle = probabilities(RelativeAmount(), OrdinalPatterns{3}(), x) # equivalent\nps == ps_mle # true\n\nSee also: BayesianRegularization, Shrinkage.\n\n\n\n\n\n","category":"type"},{"location":"probabilities/#ComplexityMeasures.BayesianRegularization","page":"Probabilities","title":"ComplexityMeasures.BayesianRegularization","text":"BayesianRegularization <: ProbabilitiesEstimator\nBayesianRegularization(; a = 1.0)\n\nThe BayesianRegularization estimator is used with probabilities and related functions to estimate probabilities an m-element counting-based OutcomeSpace using Bayesian regularization of cell counts (Hausser and Strimmer, 2009). See ProbabilitiesEstimator for usage.\n\nOutcome space requirements\n\nThis estimator only works with counting-compatible outcome spaces.\n\nDescription\n\nThe BayesianRegularization estimator estimates the probability of the k-th outcome omega_k is\n\nomega_k^textBayesianRegularization = dfracn_k + a_kn + A\n\nwhere n is the number of samples in the input data, n_k is the observed counts for the outcome omega_k, and A = sum_i=1^k a_k.\n\nPicking a\n\nThere are many common choices of priors, some of which are listed in Hausser and Strimmer (2009). They include\n\na == 0, which is equivalent to the RelativeAmount estimator.\na == 0.5 (Jeffrey's prior)\na == 1 (Bayes-Laplace uniform prior)\n\na can also be chosen as a vector of real numbers. Then, if used with allprobabilities_and_outcomes, it is required that  length(a) == total_outcomes(o, x), where x is the input data and o is the OutcomeSpace. If used with probabilities, then length(a) must match the number of observed outcomes (you can check this using probabilities_and_outcomes). The choice of a can severely impact the estimation errors of the probabilities, and the errors depend both on the choice of a and on the sampling scenario (Hausser and Strimmer, 2009).\n\nAssumptions\n\nThe BayesianRegularization estimator assumes a fixed and known m. Thus, using it with probabilities_and_outcomes and allprobabilities_and_outcomes will  yield different results, depending on whether all outcomes are observed in the input data or not. For probabilities_and_outcomes, m is the number of observed outcomes. For allprobabilities_and_outcomes, m = total_outcomes(o, x), where o is the OutcomeSpace and x is the input data.\n\nnote: Note\nIf used with allprobabilities_and_outcomes, then outcomes which have not been observed may be assigned non-zero probabilities. This might affect your results if using e.g. missing_outcomes.\n\nExamples\n\nusing ComplexityMeasures\nx = cumsum(randn(100))\nps_bayes = probabilities(BayesianRegularization(a = 0.5), OrdinalPatterns{3}(), x)\n\nSee also: RelativeAmount, Shrinkage.\n\n\n\n\n\n","category":"type"},{"location":"probabilities/#ComplexityMeasures.Shrinkage","page":"Probabilities","title":"ComplexityMeasures.Shrinkage","text":"Shrinkage{<:OutcomeSpace} <: ProbabilitiesEstimator\nShrinkage(; t = nothing, λ = nothing)\n\nThe Shrinkage estimator is used with probabilities and related functions to estimate probabilities over the given m-element counting-based OutcomeSpace using James-Stein-type shrinkage (James and Stein, 1992), as presented in Hausser and Strimmer (2009).\n\nDescription\n\nThe Shrinkage estimator estimates a cell probability theta_k^textShrink as\n\ntheta_k^textShrink = lambda t_k + (1-lambda) hattheta_k^RelativeAmount\n\nwhere lambda in 0 1 is the shrinkage intensity (lambda = 0 means no shrinkage, and lambda = 1 means full shrinkage), and t_k is the shrinkage target. Hausser and Strimmer (2009) picks t_k = 1m, i.e. the uniform distribution.\n\nIf t == nothing, then t_k is set to 1m for all k, as in Hausser and Strimmer (2009). If λ == nothing (the default), then the shrinkage intensity is optimized according to Hausser and Strimmer (2009). Hence, you should probably not pick λ nor t manually, unless you know what you are doing.\n\nAssumptions\n\nThe Shrinkage estimator assumes a fixed and known number of outcomes m. Thus, using it with probabilities_and_outcomes) and  allprobabilities_and_outcomes will yield different results, depending on whether all outcomes are observed in the input data or not. For probabilities_and_outcomes, m is the number of observed outcomes. For allprobabilities_and_outcomes, m = total_outcomes(o, x), where o is the OutcomeSpace and x is the input data.\n\nnote: Note\nIf used with allprobabilities_and_outcomes, then outcomes which have not been observed may be assigned non-zero probabilities. This might affect your results if using e.g. missing_outcomes.\n\nExamples\n\nusing ComplexityMeasures\nx = cumsum(randn(100))\nps_shrink = probabilities(Shrinkage(), OrdinalPatterns{3}(), x)\n\nSee also: RelativeAmount, BayesianRegularization.\n\n\n\n\n\n","category":"type"},{"location":"probabilities/#ComplexityMeasures.AddConstant","page":"Probabilities","title":"ComplexityMeasures.AddConstant","text":"AddConstant <: ProbabilitiesEstimator\nAddConstant(; c = 1.0)\n\nA generic add-constant probabilities estimator for counting-based OutcomeSpaces, where several literature estimators can be obtained tuning c. Currently c can only be a scalar.\n\nc = 1.0 is the Laplace estimator, or the \"add-one\" estimator.\n\nDescription\n\nProbabilities for the k-th outcome omega_k are estimated as\n\np(omega_k) = dfrac(n_k + c)n + mc\n\nwhere m is the cardinality of the outcome space, and n is the number of (encoded) input data points, and n_k is the number of times the outcome omega_k is observed in the (encoded) input data points.\n\nIf the AddConstant estimator used with probabilities_and_outcomes, then m is set to the number of observed outcomes. If used with allprobabilities_and_outcomes, then m is set to the number of possible outcomes.\n\nnote: Unobserved outcomes are assigned nonzero probability!\nLooking at the formula above, if n_k = 0, then unobserved outcomes are assigned a non-zero probability of dfraccn + mc. This means that if the estimator is used with allprobabilities_and_outcomes, then all outcomes, even those that are not observed, are assigned non-zero probabilities. This might affect your results if using e.g. missing_outcomes.\n\n\n\n\n\n","category":"type"},{"location":"probabilities/#encodings","page":"Probabilities","title":"Encodings/Symbolizations API","text":"","category":"section"},{"location":"probabilities/","page":"Probabilities","title":"Probabilities","text":"Count-based OutcomeSpaces first \"encode\" input data into an intermediate representation indexed by the positive integers. This intermediate representation is called an \"encoding\". Alternative names for \"encode\" in the literature is \"symbolize\" or \"codify\", and in this package we use the latter.","category":"page"},{"location":"probabilities/","page":"Probabilities","title":"Probabilities","text":"The encodings API is defined by:","category":"page"},{"location":"probabilities/","page":"Probabilities","title":"Probabilities","text":"Encoding\nencode\ndecode\ncodify","category":"page"},{"location":"probabilities/","page":"Probabilities","title":"Probabilities","text":"Encoding\nencode\ndecode\ncodify","category":"page"},{"location":"probabilities/#ComplexityMeasures.Encoding","page":"Probabilities","title":"ComplexityMeasures.Encoding","text":"Encoding\n\nThe supertype for all encoding schemes. Encodings always encode elements of input data into the positive integers. The encoding API is defined by the functions encode and decode. Some probability estimators utilize encodings internally.\n\nCurrent available encodings are:\n\nOrdinalPatternEncoding.\nGaussianCDFEncoding.\nRectangularBinEncoding.\nRelativeMeanEncoding.\nRelativeFirstDifferenceEncoding.\nUniqueElementsEncoding.\nCombinationEncoding, which can combine any of the above encodings.\n\n\n\n\n\n","category":"type"},{"location":"probabilities/#ComplexityMeasures.encode","page":"Probabilities","title":"ComplexityMeasures.encode","text":"encode(c::Encoding, χ) -> i::Int\n\nEncode an element χ ∈ x of input data x (those given to counts) into the positive integers i ≥ 0 using encoding c. The special value of i = -1 is used as a return value for inappropriate elements χ that cannot be encoded according to c.\n\n\n\n\n\n","category":"function"},{"location":"probabilities/#ComplexityMeasures.decode","page":"Probabilities","title":"ComplexityMeasures.decode","text":"decode(c::Encoding, i::Integer) -> ω\n\nDecode an encoded element i into the outcome ω ∈ Ω it corresponds to. Ω is the outcome_space that uses encoding c.\n\n\n\n\n\n","category":"function"},{"location":"probabilities/#ComplexityMeasures.codify","page":"Probabilities","title":"ComplexityMeasures.codify","text":"codify(o::OutcomeSpace, x::Vector) → s::Vector{Int}\ncodify(o::OutcomeSpace, x::AbstractStateSpaceSet{D}) → s::NTuple{D, Vector{Int}\n\nCodify x according to the outcome space o. If x is a Vector, then a Vector{<:Integer} is returned. If x is a StateSpaceSet{D}, then symbolization is done column-wise and an NTuple{D, Vector{<:Integer}} is returned, where D = dimension(x).\n\nDescription\n\nThe reason this function exists is that we don't always want to encode the entire input x at once. Sometimes, it is desirable to first apply some transformation to x first, then apply Encodings in a point-wise manner in the transformed space. (the OutcomeSpace dictates this transformation). This is useful for encoding timeseries data.\n\nThe length of the returned s depends on the OutcomeSpace. Some outcome spaces preserve the input data length (e.g. UniqueElements), while some outcome spaces (e.g. OrdinalPatterns) do e.g. delay embeddings before encoding, so that length(s) < length(x).\n\n\n\n\n\n","category":"function"},{"location":"probabilities/#Available-encodings","page":"Probabilities","title":"Available encodings","text":"","category":"section"},{"location":"probabilities/","page":"Probabilities","title":"Probabilities","text":"OrdinalPatternEncoding\nGaussianCDFEncoding\nRectangularBinEncoding\nRelativeMeanEncoding\nRelativeFirstDifferenceEncoding\nUniqueElementsEncoding\nCombinationEncoding","category":"page"},{"location":"probabilities/#ComplexityMeasures.OrdinalPatternEncoding","page":"Probabilities","title":"ComplexityMeasures.OrdinalPatternEncoding","text":"OrdinalPatternEncoding <: Encoding\nOrdinalPatternEncoding{m}(lt = ComplexityMeasures.isless_rand)\n\nAn encoding scheme that encodes length-m vectors into their permutation/ordinal patterns and then into the integers based on the Lehmer code. It is used by OrdinalPatterns and similar estimators, see that for a description of the outcome space.\n\nThe ordinal/permutation pattern of a vector χ is simply sortperm(χ), which gives the indices that would sort χ in ascending order.\n\nDescription\n\nThe Lehmer code, as implemented here, is a bijection between the set of factorial(m) possible permutations for a length-m sequence, and the integers 1, 2, …, factorial(m). The encoding step uses algorithm 1 in Berger et al. (2019), which is highly optimized. The decoding step is much slower due to missing optimizations (pull requests welcomed!).\n\nExample\n\njulia> using ComplexityMeasures\n\njulia> χ = [4.0, 1.0, 9.0];\n\njulia> c = OrdinalPatternEncoding(3);\n\njulia> i = encode(c, χ)\n3\n\njulia> decode(c, i)\n3-element SVector{3, Int64} with indices SOneTo(3):\n 2\n 1\n 3\n\nIf you want to encode something that is already a permutation pattern, then you can use the non-exported permutation_to_integer function.\n\n\n\n\n\n","category":"type"},{"location":"probabilities/#ComplexityMeasures.GaussianCDFEncoding","page":"Probabilities","title":"ComplexityMeasures.GaussianCDFEncoding","text":"GaussianCDFEncoding <: Encoding\nGaussianCDFEncoding{m}(; μ, σ, c::Int = 3)\n\nAn encoding scheme that encodes a scalar or vector χ into one of the integers sᵢ ∈ [1, 2, …, c] based on the normal cumulative distribution function (NCDF), and decodes the sᵢ into subintervals of [0, 1] (with some loss of information).\n\nInitializing a GaussianCDFEncoding\n\nThe size of the input to be encoded must be known beforehand. One must therefore set m = length(χ), where χ is the input (m = 1 for scalars, m ≥ 2 for vectors). To do so, one must explicitly give m as a type parameter: e.g. encoding = GaussianCDFEncoding{3}(; μ = 0.0, σ = 0.1) to encode 3-element vectors, or encoding = GaussianCDFEncoding{1}(; μ = 0.0, σ = 0.1) to encode scalars.\n\nDescription\n\nEncoding/decoding scalars\n\nGaussianCDFEncoding first maps an input scalar χ to a new real number y_ in 0 1 by using the normal cumulative distribution function (CDF) with the given mean μ and standard deviation σ, according to the map\n\nx to y  y = dfrac1 sigma\n    sqrt2 pi int_-infty^x e^(-(x - mu)^2)(2 sigma^2) dx\n\nNext, the interval [0, 1] is equidistantly binned and enumerated 1 2 ldots c,  and y is linearly mapped to one of these integers using the linear map  y to z  z = textfloor(y(c-1)) + 1.\n\nBecause of the floor operation, some information is lost, so when used with decode, each decoded sᵢ is mapped to a subinterval of [0, 1]. This subinterval is returned as a length-1 Vector{SVector}.\n\nNotice that the decoding step does not yield an element of any outcome space of the estimators that use GaussianCDFEncoding internally, such as Dispersion. That is because these estimators additionally delay embed the encoded data.\n\nEncoding/decoding vectors\n\nIf GaussianCDFEncoding is used with a vector χ, then each element of χ is encoded separately, resulting in a length(χ) sequence of integers which may be treated as a CartesianIndex. The encoded symbol s ∈ [1, 2, …, c] is then just the linear index corresponding to this cartesian index (similar to how CombinationEncoding works).\n\nWhen decoded, the integer symbol s is converted back into its CartesianIndex representation,  which is just a sequence of integers that refer to subdivisions of the [0, 1] interval. The relevant subintervals are then returned as a length-χ Vector{SVector}.\n\nExamples\n\njulia> using ComplexityMeasures, Statistics\n\njulia> x = [0.1, 0.4, 0.7, -2.1, 8.0];\n\njulia> μ, σ = mean(x), std(x); encoding = GaussianCDFEncoding(; μ, σ, c = 5)\n\njulia> es = encode.(Ref(encoding), x)\n5-element Vector{Int64}:\n 2\n 2\n 3\n 1\n 5\n\njulia> decode(encoding, 3)\n2-element SVector{2, Float64} with indices SOneTo(2):\n 0.4\n 0.6\n\n\n\n\n\n","category":"type"},{"location":"probabilities/#ComplexityMeasures.RectangularBinEncoding","page":"Probabilities","title":"ComplexityMeasures.RectangularBinEncoding","text":"RectangularBinEncoding <: Encoding\nRectangularBinEncoding(binning::RectangularBinning, x)\nRectangularBinEncoding(binning::FixedRectangularBinning)\n\nAn encoding scheme that encodes points χ ∈ x into their histogram bins.\n\nThe first call signature simply initializes a FixedRectangularBinning and then calls the second call signature.\n\nSee FixedRectangularBinning for info on mapping points to bins.\n\n\n\n\n\n","category":"type"},{"location":"probabilities/#ComplexityMeasures.RelativeMeanEncoding","page":"Probabilities","title":"ComplexityMeasures.RelativeMeanEncoding","text":"RelativeMeanEncoding <: Encoding\nRelativeMeanEncoding(minval::Real, maxval::Real; n = 2)\n\nRelativeMeanEncoding encodes a vector based on the relative position the mean of the vector has with respect to a predefined minimum and maximum value (minval and maxval, respectively).\n\nDescription\n\nThis encoding is inspired by Azami and Escudero (2016)'s algorithm for amplitude-aware permutation entropy. They use a linear combination of amplitude information and first differences information of state vectors to correct probabilities. Here, however, we explicitly encode the amplitude-part of the correction as an a integer symbol Λ ∈ [1, 2, …, n]. The first-difference part of the encoding is available as the RelativeFirstDifferenceEncoding encoding.\n\nEncoding/decoding\n\nWhen used with encode, an m-element state vector bfx = (x_1 x_2 ldots x_m) is encoded as Λ = dfrac1Nsum_i=1^m abs(x_i). The value of Λ is then normalized to lie on the interval [0, 1], assuming that the minimum/maximum value any single element x_i can take is minval/maxval, respectively. Finally, the interval [0, 1] is discretized into n discrete bins, enumerated by positive integers 1, 2, …, n, and the number of the bin that the normalized Λ falls into is returned.\n\nWhen used with decode, the left-edge of the bin that the normalized Λ fell into is returned.\n\n\n\n\n\n","category":"type"},{"location":"probabilities/#ComplexityMeasures.RelativeFirstDifferenceEncoding","page":"Probabilities","title":"ComplexityMeasures.RelativeFirstDifferenceEncoding","text":"RelativeFirstDifferenceEncoding <: Encoding\nRelativeFirstDifferenceEncoding(minval::Real, maxval::Real; n = 2)\n\nRelativeFirstDifferenceEncoding encodes a vector based on the relative position the average of the first differences of the vectors has with respect to a predefined minimum and maximum value (minval and maxval, respectively).\n\nDescription\n\nThis encoding is inspired by Azami and Escudero (2016)'s algorithm for amplitude-aware permutation entropy. They use a linear combination of amplitude information and first differences information of state vectors to correct probabilities. Here, however, we explicitly encode the first differences part of the correction as an a integer symbol Λ ∈ [1, 2, …, n]. The amplitude part of the encoding is available as the RelativeMeanEncoding encoding.\n\nEncoding/decoding\n\nWhen used with encode, an m-element state vector bfx = (x_1 x_2 ldots x_m) is encoded as Λ = dfrac1m - 1sum_k=2^m x_k - x_k-1. The value of Λ is then normalized to lie on the interval [0, 1], assuming that the minimum/maximum value any single abs(x_k - x_k-1) can take is minval/maxval, respectively. Finally, the interval [0, 1] is discretized into n discrete bins, enumerated by positive integers 1, 2, …, n, and the number of the bin that the normalized Λ falls into is returned. The smaller the mean first difference of the state vector is, the smaller the bin number is. The higher the mean first difference of the state vectors is, the higher the bin number is.\n\nWhen used with decode, the left-edge of the bin that the normalized Λ fell into is returned.\n\nPerformance tips\n\nIf you are encoding multiple input vectors, it is more efficient to construct a RelativeFirstDifferenceEncoding instance and re-use it:\n\nminval, maxval = 0, 1\nencoding = RelativeFirstDifferenceEncoding(minval, maxval; n = 4)\npts = [rand(3) for i = 1:1000]\n[encode(encoding, x) for x in pts]\n\n\n\n\n\n","category":"type"},{"location":"probabilities/#ComplexityMeasures.UniqueElementsEncoding","page":"Probabilities","title":"ComplexityMeasures.UniqueElementsEncoding","text":"UniqueElementsEncoding <: Encoding\nUniqueElementsEncoding(x)\n\nUniqueElementsEncoding is a generic encoding that encodes each xᵢ ∈ unique(x) to one of the positive integers. The xᵢ are encoded according to the order of their first appearance in the input data.\n\nThe constructor requires the input data x, since the number of possible symbols is length(unique(x)).\n\nExample\n\nusing ComplexityMeasures\nx = ['a', 2, 5, 2, 5, 'a']\ne = UniqueElementsEncoding(x)\nencode.(Ref(e), x) == [1, 2, 3, 2, 3, 1] # true\n\n\n\n\n\n","category":"type"},{"location":"probabilities/#ComplexityMeasures.CombinationEncoding","page":"Probabilities","title":"ComplexityMeasures.CombinationEncoding","text":"CombinationEncoding <: Encoding\nCombinationEncoding(encodings)\n\nA CombinationEncoding takes multiple Encodings and creates a combined encoding that can be used to encode inputs that are compatible with the given encodings.\n\nEncoding/decoding\n\nWhen used with encode, each Encoding in encodings returns integers in the set 1, 2, …, n_e, where n_e is the total number of outcomes for a particular encoding. For k different encodings, we can thus construct the cartesian coordinate (c₁, c₂, …, cₖ) (cᵢ ∈ 1, 2, …, n_i), which can uniquely be identified by an integer. We can thus identify each unique combined encoding with a single integer.\n\nWhen used with decode, the integer symbol is converted to its corresponding cartesian coordinate, which is used to retrieve the decoded symbols for each of the encodings, and a tuple of the decoded symbols are returned.\n\nThe total number of outcomes is prod(total_outcomes(e) for e in encodings).\n\nExamples\n\nusing ComplexityMeasures\n\n# We want to encode the vector `x`.\nx = [0.9, 0.2, 0.3]\n\n# To do so, we will use a combination of first-difference encoding, amplitude encoding,\n# and ordinal pattern encoding.\n\nencodings = (\n    RelativeFirstDifferenceEncoding(0, 1; n = 2),\n    RelativeMeanEncoding(0, 1; n = 5),\n    OrdinalPatternEncoding(3) # x is a three-element vector\n    )\nc = CombinationEncoding(encodings)\n\n# Encode `x` as integer\nω = encode(c, x)\n\n# Decode symbol (into a vector of decodings, one for each encodings `e ∈ encodings`).\n# In this particular case, the first two element will be left-bin edges, and\n# the last element will be the decoded ordinal pattern (indices that would sort `x`).\nd = decode(c, ω)\n\n\n\n\n\n","category":"type"},{"location":"references/#References","page":"References","title":"References","text":"","category":"section"},{"location":"references/","page":"References","title":"References","text":"Alizadeh, N. H. and Arghami, N. R. (2010). A new estimator of entropy. Journal of the Iranian Statistical Society (JIRSS).\n\n\n\nAmigó, J. M.; Balogh, S. G. and Hernández, S. (2018). A brief review of generalized entropies. Entropy 20, 813.\n\n\n\nAmigó, J. M.; Szczepański, J.; Wajnryb, E. and Sanchez-Vives, M. V. (2004). Estimating the Entropy Rate of Spike Trains via Lempel-Ziv Complexity. Neural Computation 16, 717–736, arXiv:https://direct.mit.edu/neco/article-pdf/16/4/717/815838/089976604322860677.pdf.\n\n\n\nAnteneodo, C. and Plastino, A. R. (1999). Maximum entropy approach to stretched exponential probability distributions. Journal of Physics A: Mathematical and General 32, 1089.\n\n\n\nArora, A.; Meister, C. and Cotterell, R. (2022). Estimating the Entropy of Linguistic Distributions, arXiv, arXiv:2204.01469 [cs.CL].\n\n\n\nAzami, H. and Escudero, J. (2016). Amplitude-aware permutation entropy: Illustration in spike detection and signal segmentation. Computer Methods and Programs in Biomedicine 128, 40–51.\n\n\n\nAzami, H.; da Silva, L. E.; Omoto, A. C. and Humeau-Heurtier, A. (2019). Two-dimensional dispersion entropy: An information-theoretic method for irregularity analysis of images. Signal Processing: Image Communication 75, 178–187.\n\n\n\nBandt, C. and Pompe, B. (2002). Permutation Entropy: A Natural Complexity Measure for Time Series. Phys. Rev. Lett. 88, 174102.\n\n\n\nBerger, S.; Kravtsiv, A.; Schneider, G. and Jordan, D. (2019). Teaching Ordinal Patterns to a Computer: Efficient Encoding Algorithms Based on the Lehmer Code. Entropy 21.\n\n\n\nChao, A. and Shen, T.-J. (2003). Nonparametric estimation of Shannon's index of diversity when there are unseen species in sample. Environmental and Ecological Statistics 10, 429–443.\n\n\n\nCharzyńska, A. and Gambin, A. (2016). Improvement of the k-nn Entropy Estimator with Applications in Systems Biology. Entropy 18.\n\n\n\nCorrea, J. C. (1995). A new estimator of entropy. Communications in Statistics - Theory and Methods 24, 2439–2449, arXiv:https://doi.org/10.1080/03610929508831626.\n\n\n\nCurado, E. M. and Nobre, F. D. (2004). On the stability of analytic entropic forms. Physica A: Statistical Mechanics and its Applications 335, 94–106.\n\n\n\nDatseris, G. and Parlitz, U. (2022). Nonlinear dynamics: a concise introduction interlaced with code (Springer Nature).\n\n\n\nDiego, D.; Haaga, K. A. and Hannisdal, B. (2019). Transfer entropy computation using the Perron-Frobenius operator. Phys. Rev. E 99, 042212.\n\n\n\nEbrahimi, N.; Pflughoeft, K. and Soofi, E. S. (1994). Two measures of sample entropy. Statistics & Probability Letters 20, 225–234.\n\n\n\nFadlallah, B.; Chen, B.; Keil, A. and Prı́ncipe, J. (2013). Weighted-permutation entropy: A complexity measure for time series incorporating amplitude information. Phys. Rev. E 87, 022911.\n\n\n\nGao, S.; Ver Steeg, G. and Galstyan, A. (09–12 May 2015). Efficient Estimation of Mutual Information for Strongly Dependent Variables. In: Proceedings of the Eighteenth International Conference on Artificial Intelligence and Statistics, Vol. 38 of Proceedings of Machine Learning Research, edited by Lebanon, G. and Vishwanathan, S. V. (PMLR, San Diego, California, USA); pp. 277–286.\n\n\n\nGoria, M. N.; Leonenko, N. N.; Mergel, V. V. and Inverardi, P. L. (2005). A new class of random vector entropy estimators and its applications in testing statistical hypotheses. Journal of Nonparametric Statistics 17, 277–297, arXiv:https://doi.org/10.1080/104852504200026815.\n\n\n\nGrassberger, P. (2022). On Generalized Schürmann Entropy Estimators. Entropy 24.\n\n\n\nHausser, J. and Strimmer, K. (2009). Entropy inference and the James-Stein estimator, with application to nonlinear gene association networks. Journal of Machine Learning Research 10.\n\n\n\nHe, S.; Sun, K. and Wang, H. (2016). Multivariate permutation entropy and its application for complexity analysis of chaotic systems. Physica A: Statistical Mechanics and its Applications 461, 812–823.\n\n\n\nHorvitz, D. G. and Thompson, D. J. (1952). A Generalization of Sampling Without Replacement from a Finite Universe. Journal of the American Statistical Association 47, 663–685, arXiv:https://www.tandfonline.com/doi/pdf/10.1080/01621459.1952.10483446.\n\n\n\nJames, W. and Stein, C. (1992). Estimation with quadratic loss. In: Breakthroughs in statistics: Foundations and basic theory (Springer); pp. 443–460.\n\n\n\nKozachenko, L. F. and Leonenko, N. N. (1987). Sample estimate of the entropy of a random vector. Problemy Peredachi Informatsii 23, 9–16.\n\n\n\nKraskov, A.; Stögbauer, H. and Grassberger, P. (2004). Estimating mutual information. Phys. Rev. E 69, 066138.\n\n\n\nLad, F.; Sanfilippo, G. and Agrò, G. (2015). Extropy: Complementary Dual of Entropy. Statistical Science 30, 40–58.\n\n\n\nLempel, A. and Ziv, J. (1976). On the Complexity of Finite Sequences. IEEE Transactions on Information Theory 22, 75–81.\n\n\n\nLeonenko, N.; Pronzato, L. and Savani, V. (2008). A class of Rényi information estimators for multidimensional densities. The Annals of Statistics 36, 2153–2182.\n\n\n\nLi, G.; Guan, Q. and Yang, H. (2019). Noise Reduction Method of Underwater Acoustic Signals Based on CEEMDAN, Effort-To-Compress Complexity, Refined Composite Multiscale Dispersion Entropy and Wavelet Threshold Denoising. Entropy 21.\n\n\n\nLi, Y.; Gao, X. and Wang, L. (2019). Reverse Dispersion Entropy: A New Complexity Measure for Sensor Signal. Sensors 19.\n\n\n\nLiu, J. and Xiao, F. (2023). Renyi extropy. Communications in Statistics, Theory and Methods 52, 5836–5847.\n\n\n\nLlanos, F.; Alexander, J. M.; Stilp, C. E. and Kluender, K. R. (2017). Power spectral entropy as an information-theoretic correlate of manner of articulation in American English. The Journal of the Acoustical Society of America 141, EL127–EL133.\n\n\n\nLord, W. M.; Sun, J. and Bollt, E. M. (2018). Geometric k-nearest neighbor estimation of entropy and mutual information. Chaos: An Interdisciplinary Journal of Nonlinear Science 28.\n\n\n\nMiller, G. (1955). Note on the bias of information estimates. Information theory in psychology: Problems and methods.\n\n\n\nPaninski, L. (2003). Estimation of entropy and mutual information. Neural computation 15, 1191–1253.\n\n\n\nPincus, S. M. (1991). Approximate entropy as a measure of system complexity. Proceedings of the National Academy of Sciences 88, 2297–2301.\n\n\n\nPrichard, D. and Theiler, J. (1995). Generalized redundancies for time series analysis. Physica D: Nonlinear Phenomena 84, 476–493.\n\n\n\nRibeiro, H. V.; Zunino, L.; Lenzi, E. K.; Santoro, P. A. and Mendes, R. S. (2012). Complexity-Entropy Causality Plane as a Complexity Measure for Two-Dimensional Patterns. PLOS ONE 7, 1–9.\n\n\n\nRichman, J. S. and Moorman, J. R. (2000). Physiological time-series analysis using approximate entropy and sample entropy. American journal of physiology-heart and circulatory physiology 278, H2039–H2049.\n\n\n\nRosso, O. A.; Blanco, S.; Yordanova, J.; Kolev, V.; Figliola, A.; Schürmann, M. and Başar, E. (2001). Wavelet entropy: a new tool for analysis of short duration brain electrical signals. Journal of Neuroscience Methods 105, 65–75.\n\n\n\nRosso, O. A.; Larrondo, H.; Martin, M. T.; Plastino, A. and Fuentes, M. A. (2007). Distinguishing noise from chaos. Physical review letters 99, 154102.\n\n\n\nRosso, O. A.; Martín, M.; Larrondo, H. A.; Kowalski, A. and Plastino, A. (2013). Generalized statistical complexity: A new tool for dynamical systems. Concepts and recent advances in generalized information measures and statistics, 169–215.\n\n\n\nRostaghi, M. and Azami, H. (2016). Dispersion entropy: A measure for time-series analysis. IEEE Signal Processing Letters 23, 610–614.\n\n\n\nRényi, A. (1961). On measures of entropy and information. In: Proceedings of the Fourth Berkeley Symposium on Mathematical Statistics and Probability, Volume 1: Contributions to the Theory of Statistics, Vol. 4 (University of California Press); pp. 547–562.\n\n\n\nSchlemmer, A.; Berg, S.; Lilienkamp, T.; Luther, S. and Parlitz, U. (2018). Spatiotemporal permutation entropy as a measure for complexity of cardiac arrhythmia. Frontiers in Physics 6, 39.\n\n\n\nSchürmann, T. (2004). Bias analysis in entropy estimation. Journal of Physics A: Mathematical and General 37, L295.\n\n\n\nShannon, C. E. (1948). A mathematical theory of communication. The Bell system technical journal 27, 379–423.\n\n\n\nSingh, H.; Misra, N.; Hnizdo, V.; Fedorowicz, A. and Demchuk, E. (2003). Nearest neighbor estimates of entropy. American journal of mathematical and management sciences 23, 301–321.\n\n\n\nSippel, S.; Lange, H. and Gans, F. (2016), statcomp: Statistical Complexity and Information measures for time series analysis. R package version.\n\n\n\nTian, Y.; Zhang, H.; Xu, W.; Zhang, H.; Yang, L.; Zheng, S. and Shi, Y. (2017). Spectral entropy can predict changes of working memory performance reduced by short-time training in the delayed-match-to-sample task. Frontiers in human neuroscience 11, 437.\n\n\n\nTsallis, C. (1988). Possible generalization of Boltzmann-Gibbs statistics. Journal of statistical physics 52, 479–487.\n\n\n\nTsallis, C. (2009). Introduction to nonextensive statistical mechanics: approaching a complex world. Vol. 1 no. 1 (Springer).\n\n\n\nVasicek, O. (1976). A test for normality based on sample entropy. Journal of the Royal Statistical Society Series B: Statistical Methodology 38, 54–59.\n\n\n\nWang, X.; Si, S. and Li, Y. (2020). Multiscale diversity entropy: A novel dynamical measure for fault diagnosis of rotating machinery. IEEE Transactions on Industrial Informatics 17, 5419–5429.\n\n\n\nXue, Y. and Deng, Y. (2023). Tsallis extropy. Communications in Statistics-Theory and Methods 52, 751–762.\n\n\n\nZahl, S. (1977). Jackknifing an index of diversity. Ecology 58, 907–913.\n\n\n\nZhou, Q.; Shang, P. and Zhang, B. (2023). Using missing dispersion patterns to detect determinism and nonlinearity in time series data. Nonlinear Dynamics 111, 439–458.\n\n\n\nZhu, J.; Bellanger, J.-J.; Shu, H. and Le Bouquin Jeannès, R. (2015). Contribution to transfer entropy estimation via the k-nearest-neighbors approach. Entropy 17, 4173–4201.\n\n\n\nZunino, L.; Olivares, F.; Scholkmann, F. and Rosso, O. A. (2017). Permutation entropy based time series analysis: Equalities in the input signal can lead to false conclusions. Physics Letters A 381, 1883–1892.\n\n\n\n","category":"page"},{"location":"dev/multiscale/#Complexity:-multiscale","page":"-","title":"Complexity: multiscale","text":"","category":"section"},{"location":"dev/multiscale/","page":"-","title":"-","text":"using ComplexityMeasures\nusing CairoMakie\n\nN, a = 2000, 20\nt = LinRange(0, 2*a*π, N)\n\nx = repeat([-5:5 |> collect; 4:-1:-4 |> collect], N ÷ 20);\ny = sin.(t .+ cos.(t/0.5)) .+ 0.2 .* x\nmaxscale = 10\nhs = ComplexityMeasures.multiscale_normalized(Regular(), SampleEntropy(y), y; maxscale)\n\nfig = Figure()\nax1 = Axis(fig[1,1]; ylabel = \"y\")\nlines!(ax1, t, y; color = Cycled(1));\nax2 = Axis(fig[2, 1]; ylabel = \"Sample entropy (h)\", xlabel = \"Scale\")\nscatterlines!(ax2, 1:maxscale |> collect, hs; color = Cycled(1));\nfig","category":"page"},{"location":"convenience/#convenience","page":"Convenience functions","title":"Convenience functions","text":"","category":"section"},{"location":"convenience/","page":"Convenience functions","title":"Convenience functions","text":"We provide a few convenience functions for widely used names for entropy or \"entropy-like\" quantities. Other arbitrary specialized convenience functions can easily be defined in a couple lines of code.","category":"page"},{"location":"convenience/","page":"Convenience functions","title":"Convenience functions","text":"We emphasize that these functions really aren't anything more than 2-lines-of-code wrappers that call information with the appropriate OutcomeSpace and InformationMeasure.","category":"page"},{"location":"convenience/","page":"Convenience functions","title":"Convenience functions","text":"entropy_permutation\nentropy_wavelet\nentropy_dispersion\nentropy_approx\nentropy_sample","category":"page"},{"location":"convenience/#ComplexityMeasures.entropy_permutation","page":"Convenience functions","title":"ComplexityMeasures.entropy_permutation","text":"entropy_permutation(x; τ = 1, m = 3, base = 2)\n\nCompute the permutation entropy of x of order m with delay/lag τ. This function is just a convenience call to:\n\nest = OrdinalPatterns(; m, τ)\ninformation(Shannon(base), est, x)\n\nSee OrdinalPatterns for more info. Similarly, one can use WeightedOrdinalPatterns or AmplitudeAwareOrdinalPatterns for the weighted/amplitude-aware versions.\n\n\n\n\n\n","category":"function"},{"location":"convenience/#ComplexityMeasures.entropy_wavelet","page":"Convenience functions","title":"ComplexityMeasures.entropy_wavelet","text":"entropy_wavelet(x; wavelet = Wavelets.WT.Daubechies{12}(), base = 2)\n\nCompute the wavelet entropy. This function is just a convenience call to:\n\nest = WaveletOverlap(wavelet)\ninformation(Shannon(base), est, x)\n\nSee WaveletOverlap for more info.\n\n\n\n\n\n","category":"function"},{"location":"convenience/#ComplexityMeasures.entropy_dispersion","page":"Convenience functions","title":"ComplexityMeasures.entropy_dispersion","text":"entropy_dispersion(x; base = 2, kwargs...)\n\nCompute the dispersion entropy. This function is just a convenience call to:\n\nest = Dispersion(kwargs...)\ninformation(Shannon(base), est, x)\n\nSee Dispersion for more info.\n\n\n\n\n\n","category":"function"},{"location":"convenience/#ComplexityMeasures.entropy_approx","page":"Convenience functions","title":"ComplexityMeasures.entropy_approx","text":"entropy_approx(x; m = 2, τ = 1, r = 0.2 * Statistics.std(x), base = MathConstants.e)\n\nConvenience syntax for computing the approximate entropy (Pincus, 1991) for timeseries x.\n\nThis is just a wrapper for complexity(ApproximateEntropy(; m, τ, r, base), x) (see also ApproximateEntropy).\n\n\n\n\n\n","category":"function"},{"location":"convenience/#ComplexityMeasures.entropy_sample","page":"Convenience functions","title":"ComplexityMeasures.entropy_sample","text":"entropy_sample(x; r = 0.2std(x), m = 2, τ = 1, normalize = true)\n\nConvenience syntax for estimating the (normalized) sample entropy (Richman & Moorman, 2000) of timeseries x.\n\nThis is just a wrapper for complexity(SampleEntropy(; r, m, τ, base), x).\n\nSee also: SampleEntropy, complexity, complexity_normalized).\n\n\n\n\n\n","category":"function"},{"location":"examples/#examples","page":"ComplexityMeasures.jl Examples","title":"ComplexityMeasures.jl Examples","text":"","category":"section"},{"location":"examples/#Probabilities:-kernel-density","page":"ComplexityMeasures.jl Examples","title":"Probabilities: kernel density","text":"","category":"section"},{"location":"examples/","page":"ComplexityMeasures.jl Examples","title":"ComplexityMeasures.jl Examples","text":"Here, we draw some random points from a 2D normal distribution. Then, we use kernel density estimation to associate a probability to each point p, measured by how many points are within radius 1.5 of p. Plotting the actual points, along with their associated probabilities estimated by the KDE procedure, we get the following surface plot.","category":"page"},{"location":"examples/","page":"ComplexityMeasures.jl Examples","title":"ComplexityMeasures.jl Examples","text":"using ComplexityMeasures\nusing CairoMakie\nusing Distributions: MvNormal\nusing LinearAlgebra\n\nμ = [1.0, -4.0]\nσ = [2.0, 2.0]\n𝒩 = MvNormal(μ, LinearAlgebra.Diagonal(map(abs2, σ)))\nN = 500\nD = StateSpaceSet(sort([rand(𝒩) for i = 1:N]))\nx, y = columns(D)\np = probabilities(NaiveKernel(1.5), D)\nfig, ax = scatter(D[:, 1], D[:, 2], zeros(N);\n    markersize=8, axis=(type = Axis3,)\n)\nsurface!(ax, x, y, p.p)\nax.zlabel = \"P\"\nax.zticklabelsvisible = false\nfig","category":"page"},{"location":"examples/#Probabilities:-KL-divergence-of-histograms","page":"ComplexityMeasures.jl Examples","title":"Probabilities: KL-divergence of histograms","text":"","category":"section"},{"location":"examples/","page":"ComplexityMeasures.jl Examples","title":"ComplexityMeasures.jl Examples","text":"In this example we show how simple it is to compute the KL-divergence (or any other distance function for probability distributions) using ComplexityMeasures.jl. For simplicity, we will compute the KL-divergence between the ValueBinnings of two timeseries.","category":"page"},{"location":"examples/","page":"ComplexityMeasures.jl Examples","title":"ComplexityMeasures.jl Examples","text":"Note that it is crucial to use allprobabilities_and_outcomes instead of probabilities_and_outcomes.","category":"page"},{"location":"examples/","page":"ComplexityMeasures.jl Examples","title":"ComplexityMeasures.jl Examples","text":"using ComplexityMeasures\n\nN = 1000\nt = range(0, 20π; length=N)\nx = @. clamp(sin(t), -0.5, 1)\ny = @. sin(t + cos(2t))\n\nr = -1:0.1:1\nest = ValueBinning(FixedRectangularBinning(r))\npx, outsx = allprobabilities_and_outcomes(est, x)\npy, outsy = allprobabilities_and_outcomes(est, y)\n\n# Visualize\nusing CairoMakie\nbins = r[1:end-1] .+ step(r)/2\nfig, ax = barplot(bins, px; label = L\"p_x\")\nbarplot!(ax, bins, py; label = L\"p_y\")\naxislegend(ax; labelsize = 30)\nfig","category":"page"},{"location":"examples/","page":"ComplexityMeasures.jl Examples","title":"ComplexityMeasures.jl Examples","text":"using StatsBase: kldivergence\n\nkldivergence(px, py)","category":"page"},{"location":"examples/","page":"ComplexityMeasures.jl Examples","title":"ComplexityMeasures.jl Examples","text":"kldivergence(py, px)","category":"page"},{"location":"examples/","page":"ComplexityMeasures.jl Examples","title":"ComplexityMeasures.jl Examples","text":"(Inf because there are events with 0 probability in px)","category":"page"},{"location":"examples/#Differential-entropy:-estimator-comparison","page":"ComplexityMeasures.jl Examples","title":"Differential entropy: estimator comparison","text":"","category":"section"},{"location":"examples/#Shannon-entropy","page":"ComplexityMeasures.jl Examples","title":"Shannon entropy","text":"","category":"section"},{"location":"examples/","page":"ComplexityMeasures.jl Examples","title":"ComplexityMeasures.jl Examples","text":"Here, we compare how the nearest neighbor differential entropy estimators (Kraskov, KozachenkoLeonenko, Zhu, ZhuSingh, etc.) converge towards the true Shannon entropy value for increasing time series length.","category":"page"},{"location":"examples/","page":"ComplexityMeasures.jl Examples","title":"ComplexityMeasures.jl Examples","text":"ComplexityMeasures.jl also provides entropy estimators based on order statistics. These estimators are only defined for scalar-valued vectors, in this example, so we compute these estimates separately, and add these estimators (Vasicek, Ebrahimi, AlizadehArghami and Correa) to the comparison.","category":"page"},{"location":"examples/","page":"ComplexityMeasures.jl Examples","title":"ComplexityMeasures.jl Examples","text":"Input data are from a normal 1D distribution mathcalN(0 1), for which the true entropy is 0.5*log(2π) + 0.5 nats when using natural logarithms.","category":"page"},{"location":"examples/","page":"ComplexityMeasures.jl Examples","title":"ComplexityMeasures.jl Examples","text":"using ComplexityMeasures\nusing CairoMakie, Statistics\nnreps = 30\nNs = [100:100:500; 1000:1000:5000]\ne = Shannon(; base = MathConstants.e)\n\n# --------------------------\n# kNN estimators\n# --------------------------\nw = 0 # Theiler window of 0 (only exclude the point itself during neighbor searches)\nent = Shannon(; base = ℯ)\nknn_estimators = [\n    # with k = 1, Kraskov is virtually identical to\n    # Kozachenko-Leonenko, so pick a higher number of neighbors for Kraskov\n    Kraskov(ent; k = 3, w),\n    KozachenkoLeonenko(ent; w),\n    Zhu(ent; k = 3, w),\n    ZhuSingh(ent; k = 3, w),\n    Gao(ent; k = 3, corrected = false, w),\n    Gao(ent; k = 3, corrected = true, w),\n    Goria(ent; k = 3, w),\n    Lord(ent; k = 20, w), # more neighbors for accurate ellipsoid estimation\n    LeonenkoProzantoSavani(ent; k = 3),\n]\n\n# Test each estimator `nreps` times over time series of varying length.\nHs_uniform_knn = [[zeros(nreps) for N in Ns] for e in knn_estimators]\nfor (i, est) in enumerate(knn_estimators)\n    for j = 1:nreps\n        pts = randn(maximum(Ns)) |> StateSpaceSet\n        for (k, N) in enumerate(Ns)\n            Hs_uniform_knn[i][k][j] = information(est, pts[1:N])\n        end\n    end\nend\n\n# --------------------------\n# Order statistic estimators\n# --------------------------\n\n# Just provide types here, they are instantiated inside the loop\nestimators_os = [Vasicek, Ebrahimi, AlizadehArghami, Correa]\nHs_uniform_os = [[zeros(nreps) for N in Ns] for e in estimators_os]\nfor (i, est_os) in enumerate(estimators_os)\n    for j = 1:nreps\n        pts = randn(maximum(Ns)) # raw timeseries, not a `StateSpaceSet`\n        for (k, N) in enumerate(Ns)\n            m = floor(Int, N / 100) # Scale `m` to timeseries length\n            est = est_os(ent; m) # Instantiate estimator with current `m`\n            Hs_uniform_os[i][k][j] = information(est, pts[1:N])\n        end\n    end\nend\n\n# -------------\n# Plot results\n# -------------\nfig = Figure(resolution = (700, 11 * 200))\nlabels_knn = [\"KozachenkoLeonenko\", \"Kraskov\", \"Zhu\", \"ZhuSingh\", \"Gao (not corrected)\",\n    \"Gao (corrected)\", \"Goria\", \"Lord\", \"LeonenkoProzantoSavani\"]\nlabels_os = [\"Vasicek\", \"Ebrahimi\", \"AlizadehArghami\", \"Correa\"]\n\nfor (i, e) in enumerate(knn_estimators)\n    Hs = Hs_uniform_knn[i]\n    ax = Axis(fig[i,1]; ylabel = \"h (nats)\")\n    lines!(ax, Ns, mean.(Hs); color = Cycled(i), label = labels_knn[i])\n    band!(ax, Ns, mean.(Hs) .+ std.(Hs), mean.(Hs) .- std.(Hs); alpha = 0.5,\n        color = (Main.COLORS[i], 0.5))\n    hlines!(ax, [(0.5*log(2π) + 0.5)], color = :black, lw = 5, linestyle = :dash)\n\n    ylims!(1.2, 1.6)\n    axislegend()\nend\n\nfor (i, e) in enumerate(estimators_os)\n    Hs = Hs_uniform_os[i]\n    ax = Axis(fig[i + length(knn_estimators),1]; ylabel = \"h (nats)\")\n    lines!(ax, Ns, mean.(Hs); color = Cycled(i), label = labels_os[i])\n    band!(ax, Ns, mean.(Hs) .+ std.(Hs), mean.(Hs) .- std.(Hs), alpha = 0.5,\n        color = (Main.COLORS[i], 0.5))\n    hlines!(ax, [(0.5*log(2π) + 0.5)], color = :black, lw = 5, linestyle = :dash)\n    ylims!(1.2, 1.6)\n    axislegend()\nend\n\nfig","category":"page"},{"location":"examples/","page":"ComplexityMeasures.jl Examples","title":"ComplexityMeasures.jl Examples","text":"All estimators approach the true differential entropy, but those based on order statistics are negatively biased for small sample sizes.","category":"page"},{"location":"examples/#Rényi-entropy","page":"ComplexityMeasures.jl Examples","title":"Rényi entropy","text":"","category":"section"},{"location":"examples/","page":"ComplexityMeasures.jl Examples","title":"ComplexityMeasures.jl Examples","text":"Here, we see how the LeonenkoProzantoSavani estimator approaches the known target Renyi entropy of a multivariate normal distribution for increasing time series length. We'll consider the Rényi entropy with q = 2.","category":"page"},{"location":"examples/","page":"ComplexityMeasures.jl Examples","title":"ComplexityMeasures.jl Examples","text":"\nusing ComplexityMeasures\nimport ComplexityMeasures: information # we're overriding this function in the example\nusing CairoMakie, Statistics\nusing Distributions: MvNormal\nimport Distributions.entropy as dentropy\nusing Random\nrng = MersenneTwister(1234)\n\n\"\"\"\n    information(e::Renyi, 𝒩::MvNormal; base = 2)\n\nCompute the analytical value of the `Renyi` entropy for a multivariate normal distribution.\n\"\"\"\nfunction information(e::Renyi, 𝒩::MvNormal; base = 2)\n    q = e.q\n    if q ≈ 1.0\n        h = dentropy(𝒩)\n    else\n        Σ = 𝒩.Σ\n        D = length(𝒩.μ)\n        h = dentropy(𝒩) - (D / 2) * (1 + log(q) / (1 - q))\n    end\n    return convert_logunit(h, ℯ, base)\nend\n\nnreps = 30\nNs = [100:100:500; 1000:1000:5000]\ndef = Renyi(q = 2, base = 2)\n\nμ = [-1, 1]\nσ = [1, 0.5]\n𝒩 = MvNormal(μ, LinearAlgebra.Diagonal(map(abs2, σ)))\nh_true = information(def, 𝒩; base = 2)\n\n# Estimate `nreps` times for each time series length\n\nhs = [zeros(nreps) for N in Ns]\nfor (i, N) in enumerate(Ns)\n    for j = 1:nreps\n        pts = StateSpaceSet(transpose(rand(rng, 𝒩, N)))\n        hs[i][j] = information(LeonenkoProzantoSavani(def; k = 5), pts)\n    end\nend\n\n# We plot the mean and standard deviation of the estimator again the true value\nhs_mean, hs_stdev = mean.(hs), std.(hs)\n\nfig = Figure()\nax = Axis(fig[1, 1]; ylabel = \"h (bits)\")\nlines!(ax, Ns, hs_mean; color = Cycled(1), label = \"LeonenkoProzantoSavani\")\nband!(ax, Ns, hs_mean .+ hs_stdev, hs_mean .- hs_stdev, \n    alpha = 0.5, color = (Main.COLORS[1], 0.5))\nhlines!(ax, [h_true], color = :black, lw = 5, linestyle = :dash)\naxislegend()\nfig","category":"page"},{"location":"examples/#Tsallis-entropy","page":"ComplexityMeasures.jl Examples","title":"Tsallis entropy","text":"","category":"section"},{"location":"examples/","page":"ComplexityMeasures.jl Examples","title":"ComplexityMeasures.jl Examples","text":"Here, we see how the LeonenkoProzantoSavani estimator approaches the known target Tsallis entropy of a multivariate normal distribution for increasing time series length. We'll consider the Rényi entropy with q = 2.","category":"page"},{"location":"examples/","page":"ComplexityMeasures.jl Examples","title":"ComplexityMeasures.jl Examples","text":"using ComplexityMeasures\nimport ComplexityMeasures: information # we're overriding this function in the example\nusing CairoMakie, Statistics\nusing Distributions: MvNormal\nimport Distributions.entropy as dentropy\nusing Random\nrng = MersenneTwister(1234)\n\n\"\"\"\n    information(e::Tsallis, 𝒩::MvNormal; base = 2)\n\nCompute the analytical value of the `Tsallis` entropy for a multivariate normal distribution.\n\"\"\"\nfunction information(e::Tsallis, 𝒩::MvNormal; base = 2)\n    q = e.q\n    Σ = 𝒩.Σ\n    D = length(𝒩.μ)\n    # uses the function from the example above\n    hr = information(Renyi(q = q), 𝒩; base = ℯ) # stick with natural log, convert after\n    h = (exp((1 - q) * hr) - 1) / (1 - q)\n    return convert_logunit(h, ℯ, base)\nend\n\nnreps = 30\nNs = [100:100:500; 1000:1000:5000]\ndef = Tsallis(q = 2, base = 2)\n\nμ = [-1, 1]\nσ = [1, 0.5]\n𝒩 = MvNormal(μ, LinearAlgebra.Diagonal(map(abs2, σ)))\nh_true = information(def, 𝒩; base = 2)\n\n# Estimate `nreps` times for each time series length\n\nhs = [zeros(nreps) for N in Ns]\nfor (i, N) in enumerate(Ns)\n    for j = 1:nreps\n        pts = StateSpaceSet(transpose(rand(rng, 𝒩, N)))\n        hs[i][j] = information(LeonenkoProzantoSavani(def; k = 5), pts)\n    end\nend\n\n# We plot the mean and standard deviation of the estimator again the true value\nhs_mean, hs_stdev = mean.(hs), std.(hs)\n\nfig = Figure()\nax = Axis(fig[1, 1]; ylabel = \"h (bits)\")\nlines!(ax, Ns, hs_mean; color = Cycled(1), label = \"LeonenkoProzantoSavani\")\nband!(ax, Ns, hs_mean .+ hs_stdev, hs_mean .- hs_stdev, \n    alpha = 0.5, color = (Main.COLORS[1], 0.5))\nhlines!(ax, [h_true], color = :black, lw = 5, linestyle = :dash)\naxislegend()\nfig","category":"page"},{"location":"examples/#Discrete-entropy:-permutation-entropy","page":"ComplexityMeasures.jl Examples","title":"Discrete entropy: permutation entropy","text":"","category":"section"},{"location":"examples/","page":"ComplexityMeasures.jl Examples","title":"ComplexityMeasures.jl Examples","text":"This example plots permutation entropy for time series of the chaotic logistic map. Entropy estimates using WeightedOrdinalPatterns and AmplitudeAwareOrdinalPatterns are added here for comparison. The entropy behaviour can be parallelized with the ChaosTools.lyapunov of the map.","category":"page"},{"location":"examples/","page":"ComplexityMeasures.jl Examples","title":"ComplexityMeasures.jl Examples","text":"using DynamicalSystemsBase, CairoMakie\n\nlogistic_rule(x, p, n) = @inbounds SVector(p[1]*x[1]*(1-x[1]))\nds = DeterministicIteratedMap(logistic_rule, [0.4], [4.0])\nrs = 3.4:0.001:4\nN_lyap, N_ent = 100000, 10000\nm, τ = 6, 1 # Symbol size/dimension and embedding lag\n\n# Generate one time series for each value of the logistic parameter r\nhs_perm, hs_wtperm, hs_ampperm = [zeros(length(rs)) for _ in 1:4]\n\nfor (i, r) in enumerate(rs)\n    ds.p[1] = r\n\n    x, t = trajectory(ds, N_ent)\n    ## `x` is a 1D dataset, need to recast into a timeseries\n    x = columns(x)[1]\n    hs_perm[i] = information(OrdinalPatterns(; m, τ), x)\n    hs_wtperm[i] = information(WeightedOrdinalPatterns(; m, τ), x)\n    hs_ampperm[i] = information(AmplitudeAwareOrdinalPatterns(; m, τ), x)\nend\n\nfig = Figure()\na1 = Axis(fig[1,1]; ylabel = L\"h_6 (SP)\")\nlines!(a1, rs, hs_perm; color = Cycled(2))\na2 = Axis(fig[2,1]; ylabel = L\"h_6 (WT)\")\nlines!(a2, rs, hs_wtperm; color = Cycled(3))\na3 = Axis(fig[3,1]; ylabel = L\"h_6 (SAAP)\", xlabel = L\"r\")\nlines!(a3, rs, hs_ampperm; color = Cycled(4))\n\nfor a in (a1,a2,a3)\n    hidexdecorations!(a, grid = false)\nend\nfig","category":"page"},{"location":"examples/#Discrete-entropy:-wavelet-entropy","page":"ComplexityMeasures.jl Examples","title":"Discrete entropy: wavelet entropy","text":"","category":"section"},{"location":"examples/","page":"ComplexityMeasures.jl Examples","title":"ComplexityMeasures.jl Examples","text":"The scale-resolved wavelet entropy should be lower for very regular signals (most of the energy is contained at one scale) and higher for very irregular signals (energy spread more out across scales).","category":"page"},{"location":"examples/","page":"ComplexityMeasures.jl Examples","title":"ComplexityMeasures.jl Examples","text":"using CairoMakie\nN, a = 1000, 10\nt = LinRange(0, 2*a*π, N)\n\nx = sin.(t);\ny = sin.(t .+ cos.(t/0.5));\nz = sin.(rand(1:15, N) ./ rand(1:10, N))\n\nh_x = entropy_wavelet(x)\nh_y = entropy_wavelet(y)\nh_z = entropy_wavelet(z)\n\nfig = Figure()\nax = Axis(fig[1,1]; ylabel = \"x\")\nlines!(ax, t, x; color = Cycled(1), label = \"h=$(h=round(h_x, sigdigits = 5))\");\nay = Axis(fig[2,1]; ylabel = \"y\")\nlines!(ay, t, y; color = Cycled(2), label = \"h=$(h=round(h_y, sigdigits = 5))\");\naz = Axis(fig[3,1]; ylabel = \"z\", xlabel = \"time\")\nlines!(az, t, z; color = Cycled(3), label = \"h=$(h=round(h_z, sigdigits = 5))\");\nfor a in (ax, ay, az); axislegend(a); end\nfor a in (ax, ay); hidexdecorations!(a; grid=false); end\nfig","category":"page"},{"location":"examples/#Discrete-entropies:-properties","page":"ComplexityMeasures.jl Examples","title":"Discrete entropies: properties","text":"","category":"section"},{"location":"examples/","page":"ComplexityMeasures.jl Examples","title":"ComplexityMeasures.jl Examples","text":"Here, we show the sensitivity of the various entropies to variations in their parameters.","category":"page"},{"location":"examples/#Curado-entropy","page":"ComplexityMeasures.jl Examples","title":"Curado entropy","text":"","category":"section"},{"location":"examples/","page":"ComplexityMeasures.jl Examples","title":"ComplexityMeasures.jl Examples","text":"Here, we reproduce Figure 2 from Curado and Nobre (2004), showing how the Curado entropy changes as function of the parameter a for a range of two-element probability distributions given by Probabilities([p, 1 - p] for p in 1:0.0:0.01:1.0).","category":"page"},{"location":"examples/","page":"ComplexityMeasures.jl Examples","title":"ComplexityMeasures.jl Examples","text":"using ComplexityMeasures, CairoMakie\nbs = [1.0, 1.5, 2.0, 3.0, 4.0, 10.0]\nps = [Probabilities([p, 1 - p]) for p = 0.0:0.01:1.0]\nhs = [[information(Curado(; b = b), p) for p in ps] for b in bs]\nfig = Figure()\nax = Axis(fig[1,1]; xlabel = \"p\", ylabel = \"H(p)\")\npp = [p[1] for p in ps]\nfor (i, b) in enumerate(bs)\n    lines!(ax, pp, hs[i], label = \"b=$b\", color = Cycled(i))\nend\naxislegend(ax)\nfig","category":"page"},{"location":"examples/#Kaniadakis-entropy","page":"ComplexityMeasures.jl Examples","title":"Kaniadakis entropy","text":"","category":"section"},{"location":"examples/","page":"ComplexityMeasures.jl Examples","title":"ComplexityMeasures.jl Examples","text":"Here, we show how Kaniadakis entropy changes as function of the parameter a for a range of two-element probability distributions given by Probabilities([p, 1 - p] for p in 1:0.0:0.01:1.0).","category":"page"},{"location":"examples/","page":"ComplexityMeasures.jl Examples","title":"ComplexityMeasures.jl Examples","text":"using ComplexityMeasures\nusing CairoMakie\n\nprobs = [Probabilities([p, 1-p]) for p in 0.0:0.01:1.0]\nps = collect(0.0:0.01:1.0);\nκs = [-0.99, -0.66, -0.33, 0, 0.33, 0.66, 0.99];\nHs = [[information(Kaniadakis(κ = κ), p) for p in probs] for κ in κs];\n\nfig = Figure()\nax = Axis(fig[1, 1], xlabel = \"p\", ylabel = \"H(p)\")\n\nfor (i, H) in enumerate(Hs)\n    lines!(ax, ps, H, label = \"$(κs[i])\")\nend\n\naxislegend()\n\nfig","category":"page"},{"location":"examples/#Stretched-exponential-entropy","page":"ComplexityMeasures.jl Examples","title":"Stretched exponential entropy","text":"","category":"section"},{"location":"examples/","page":"ComplexityMeasures.jl Examples","title":"ComplexityMeasures.jl Examples","text":"Here, we reproduce the example from Anteneodo and Plastino (1999), showing how the stretched exponential entropy changes as function of the parameter η for a range of two-element probability distributions given by Probabilities([p, 1 - p] for p in 1:0.0:0.01:1.0).","category":"page"},{"location":"examples/","page":"ComplexityMeasures.jl Examples","title":"ComplexityMeasures.jl Examples","text":"using ComplexityMeasures, SpecialFunctions, CairoMakie\nηs = [0.01, 0.2, 0.3, 0.5, 0.7, 1.0, 1.5, 3.0]\nps = [Probabilities([p, 1 - p]) for p = 0.0:0.01:1.0]\n\nhs_norm = [[information(StretchedExponential( η = η), p) / gamma((η + 1)/η) for p in ps] for η in ηs]\nfig = Figure()\nax = Axis(fig[1,1]; xlabel = \"p\", ylabel = \"H(p)\")\npp = [p[1] for p in ps]\n\nfor (i, η) in enumerate(ηs)\n    lines!(ax, pp, hs_norm[i], label = \"η=$η\")\nend\naxislegend(ax)\nfig","category":"page"},{"location":"examples/#dispersion_example","page":"ComplexityMeasures.jl Examples","title":"Discrete entropy: dispersion entropy","text":"","category":"section"},{"location":"examples/","page":"ComplexityMeasures.jl Examples","title":"ComplexityMeasures.jl Examples","text":"Here we compute dispersion entropy (Rostaghi and Azami, 2016), using the use the Dispersion probabilities estimator, for a time series consisting of normally distributed noise with a single spike in the middle of the signal. We compute the entropies over a range subsets of the data, using a sliding window consisting of 70 data points, stepping the window 10 time steps at a time. This example is adapted from Li et al. (2019).","category":"page"},{"location":"examples/","page":"ComplexityMeasures.jl Examples","title":"ComplexityMeasures.jl Examples","text":"using ComplexityMeasures\nusing Random\nusing CairoMakie\nusing Distributions: Normal\n\nn = 1000\nts = 1:n\nx = [i == n ÷ 2 ? 50.0 : 0.0 for i in ts]\nrng = Random.default_rng()\ns = rand(rng, Normal(0, 1), n)\ny = x .+ s\n\nws = 70\nwindows = [t:t+ws for t in 1:10:n-ws]\nrdes = zeros(length(windows))\ndes = zeros(length(windows))\npes = zeros(length(windows))\n\nm, c = 2, 6\nest_de = Dispersion(c = c, m = m, τ = 1)\nfor (i, window) in enumerate(windows)\n    des[i] = information_normalized(Renyi(), est_de, y[window])\nend\n\nfig = Figure()\na1 = Axis(fig[1,1]; xlabel = \"Time step\", ylabel = \"Value\")\nlines!(a1, ts, y)\ndisplay(fig)\na2 = Axis(fig[2, 1]; xlabel = \"Time step\", ylabel = \"Value\")\np_de = scatterlines!([first(w) for w in windows], des,\n    label = \"Dispersion entropy\",\n    color = :red,\n    markercolor = :red, marker = '●', markersize = 20)\n\naxislegend(position = :rc)\nylims!(0, max(maximum(pes), 1))\nfig","category":"page"},{"location":"examples/#Discrete-entropy:-normalized-entropy-for-comparing-different-signals","page":"ComplexityMeasures.jl Examples","title":"Discrete entropy: normalized entropy for comparing different signals","text":"","category":"section"},{"location":"examples/","page":"ComplexityMeasures.jl Examples","title":"ComplexityMeasures.jl Examples","text":"When comparing different signals or signals that have different length, it is best to normalize entropies so that the \"complexity\" or \"disorder\" quantification is directly comparable between signals. Here is an example based on the wavelet entropy example where we use the spectral entropy instead of the wavelet entropy:","category":"page"},{"location":"examples/","page":"ComplexityMeasures.jl Examples","title":"ComplexityMeasures.jl Examples","text":"using ComplexityMeasures\nN1, N2, a = 101, 10001, 10\n\nfor N in (N1, N2)\n    local t = LinRange(0, 2*a*π, N)\n    local x = sin.(t) # periodic\n    local y = sin.(t .+ cos.(t/0.5)) # periodic, complex spectrum\n    local z = sin.(rand(1:15, N) ./ rand(1:10, N)) # random\n\n    for q in (x, y, z)\n        h = information(PowerSpectrum(), q)\n        n = information_normalized(PowerSpectrum(), q)\n        println(\"entropy: $(h), normalized: $(n).\")\n    end\nend","category":"page"},{"location":"examples/","page":"ComplexityMeasures.jl Examples","title":"ComplexityMeasures.jl Examples","text":"You see that while the direct entropy values of noisy signal changes strongly with N but they are almost the same for the normalized version. For the regular signals, the entropy decreases nevertheless because the noise contribution of the Fourier computation becomes less significant.","category":"page"},{"location":"examples/#Spatiotemporal-permutation-entropy","page":"ComplexityMeasures.jl Examples","title":"Spatiotemporal permutation entropy","text":"","category":"section"},{"location":"examples/","page":"ComplexityMeasures.jl Examples","title":"ComplexityMeasures.jl Examples","text":"Usage of a SpatialOrdinalPatterns estimator is straightforward. Here we get the spatial permutation entropy of a 2D array (e.g., an image):","category":"page"},{"location":"examples/","page":"ComplexityMeasures.jl Examples","title":"ComplexityMeasures.jl Examples","text":"using ComplexityMeasures\nx = rand(50, 50) # some image\nstencil = [1 1; 0 1] # or one of the other ways of specifying stencils\nest = SpatialOrdinalPatterns(stencil, x)\nh = information(est, x)","category":"page"},{"location":"examples/","page":"ComplexityMeasures.jl Examples","title":"ComplexityMeasures.jl Examples","text":"To apply this to timeseries of spatial data, simply loop over the call, e.g.:","category":"page"},{"location":"examples/","page":"ComplexityMeasures.jl Examples","title":"ComplexityMeasures.jl Examples","text":"data = [rand(50, 50) for i in 1:10] # e.g., evolution of a 2D field of a PDE\nest = SpatialOrdinalPatterns(stencil, first(data))\nh_vs_t = map(d -> information(est, d), data)","category":"page"},{"location":"examples/","page":"ComplexityMeasures.jl Examples","title":"ComplexityMeasures.jl Examples","text":"Computing any other generalized spatiotemporal permutation entropy is trivial, e.g. with Renyi:","category":"page"},{"location":"examples/","page":"ComplexityMeasures.jl Examples","title":"ComplexityMeasures.jl Examples","text":"x = reshape(repeat(1:5, 500) .+ 0.1*rand(500*5), 50, 50)\nest = SpatialOrdinalPatterns(stencil, x)\ninformation(Renyi(q = 2), est, x)","category":"page"},{"location":"examples/#Spatial-discrete-entropy:-Fabio","page":"ComplexityMeasures.jl Examples","title":"Spatial discrete entropy: Fabio","text":"","category":"section"},{"location":"examples/","page":"ComplexityMeasures.jl Examples","title":"ComplexityMeasures.jl Examples","text":"Let's see how the normalized permutation and dispersion entropies increase for an image that gets progressively more noise added to it.","category":"page"},{"location":"examples/","page":"ComplexityMeasures.jl Examples","title":"ComplexityMeasures.jl Examples","text":"using ComplexityMeasures\nusing Distributions: Uniform\nusing CairoMakie\nusing Statistics\nusing TestImages, ImageTransformations, CoordinateTransformations, Rotations\n\nimg = testimage(\"fabio_grey_256\")\nrot = warp(img, recenter(RotMatrix(-3pi/2), center(img));)\noriginal = Float32.(rot)\nnoise_levels = collect(0.0:0.25:1.0) .* std(original) * 5 # % of 1 standard deviation\n\nnoisy_imgs = [i == 1 ? original : original .+ rand(Uniform(0, nL), size(original))\n    for (i, nL) in enumerate(noise_levels)]\n\n# a 2x2 stencil (i.e. dispersion/permutation patterns of length 4)\nstencil = ((2, 2), (1, 1))\n\nest_disp = SpatialDispersion(stencil, original; c = 5, periodic = false)\nest_perm = SpatialOrdinalPatterns(stencil, original; periodic = false)\nhs_disp = [information_normalized(est_disp, img) for img in noisy_imgs]\nhs_perm = [information_normalized(est_perm, img) for img in noisy_imgs]\n\n# Plot the results\nfig = Figure(size = (800, 1000))\nax = Axis(fig[1, 1:length(noise_levels)],\n    xlabel = \"Noise level\",\n    ylabel = \"Normalized entropy\")\nscatterlines!(ax, noise_levels, hs_disp, label = \"Dispersion\")\nscatterlines!(ax, noise_levels, hs_perm, label = \"Permutation\")\nylims!(ax, 0, 1.05)\naxislegend(position = :rb)\nfor (i, nl) in enumerate(noise_levels)\n    ax_i = Axis(fig[2, i])\n    image!(ax_i, Matrix(Float32.(noisy_imgs[i])), label = \"$nl\")\n    hidedecorations!(ax_i)  # hides ticks, grid and lables\n    hidespines!(ax_i)  # hide the frame\nend\nfig","category":"page"},{"location":"examples/","page":"ComplexityMeasures.jl Examples","title":"ComplexityMeasures.jl Examples","text":"While the normalized SpatialOrdinalPatterns entropy quickly approaches its maximum value, the normalized SpatialDispersion entropy much better resolves the increase in entropy as the image gets noiser. This can probably be explained by the fact that the number of possible states (or total_outcomes) for any given stencil is larger for SpatialDispersion than for SpatialOrdinalPatterns, so the dispersion approach is much less sensitive to noise addition (i.e. noise saturation over the possible states is slower for SpatialDispersion).","category":"page"},{"location":"examples/#Complexity:-reverse-dispersion-entropy","page":"ComplexityMeasures.jl Examples","title":"Complexity: reverse dispersion entropy","text":"","category":"section"},{"location":"examples/","page":"ComplexityMeasures.jl Examples","title":"ComplexityMeasures.jl Examples","text":"Here, we compare regular dispersion entropy (Rostaghi and Azami, 2016), and reverse dispersion entropy (Li et al., 2019) for a time series consisting of normally distributed noise with a single spike in the middle of the signal. We compute the entropies over a range subsets of the data, using a sliding window consisting of 70 data points, stepping the window 10 time steps at a time. This example reproduces parts of figure 3 in (Li et al., 2019), but results here are not exactly the same as in the original paper, because their examples are based on randomly generated numbers and do not provide code that specify random number seeds.","category":"page"},{"location":"examples/","page":"ComplexityMeasures.jl Examples","title":"ComplexityMeasures.jl Examples","text":"using ComplexityMeasures\nusing Random\nusing CairoMakie\nusing Distributions: Normal\n\nn = 1000\nts = 1:n\nx = [i == n ÷ 2 ? 50.0 : 0.0 for i in ts]\nrng = Random.default_rng()\ns = rand(rng, Normal(0, 1), n)\ny = x .+ s\n\nws = 70\nwindows = [t:t+ws for t in 1:10:n-ws]\nrdes = zeros(length(windows))\ndes = zeros(length(windows))\npes = zeros(length(windows))\n\nm, c = 2, 6\nest_rd = ReverseDispersion(; c, m, τ = 1)\nest_de = Dispersion(; c, m, τ = 1)\n\nfor (i, window) in enumerate(windows)\n    rdes[i] = complexity_normalized(est_rd, y[window])\n    des[i] = information_normalized(Renyi(), est_de, y[window])\nend\n\nfig = Figure()\n\na1 = Axis(fig[1,1]; xlabel = \"Time step\", ylabel = \"Value\")\nlines!(a1, ts, y)\ndisplay(fig)\n\na2 = Axis(fig[2, 1]; xlabel = \"Time step\", ylabel = \"Value\")\np_rde = scatterlines!([first(w) for w in windows], rdes,\n    label = \"Reverse dispersion entropy\",\n    color = :black,\n    markercolor = :black, marker = '●')\np_de = scatterlines!([first(w) for w in windows], des,\n    label = \"Dispersion entropy\",\n    color = :red,\n    markercolor = :red, marker = 'x', markersize = 20)\n\naxislegend(position = :rc)\nylims!(0, max(maximum(pes), 1))\nfig","category":"page"},{"location":"examples/#Complexity:-missing-dispersion-patterns","page":"ComplexityMeasures.jl Examples","title":"Complexity: missing dispersion patterns","text":"","category":"section"},{"location":"examples/","page":"ComplexityMeasures.jl Examples","title":"ComplexityMeasures.jl Examples","text":"using ComplexityMeasures\nusing CairoMakie\nusing DynamicalSystemsBase\nusing TimeseriesSurrogates\n\nest = MissingDispersionPatterns(Dispersion(m = 3, c = 7))\nlogistic_rule(x, p, n) = @inbounds SVector(p[1]*x[1]*(1-x[1]))\nsys = DeterministicIteratedMap(logistic_rule, [0.6], [4.0])\nLs = collect(100:100:1000)\nnL = length(Ls)\nnreps = 30 # should be higher for real applications\nmethod = WLS(IAAFT(), rescale = true)\n\nr_det, r_noise = zeros(length(Ls)), zeros(length(Ls))\nr_det_surr, r_noise_surr = [zeros(nreps) for L in Ls], [zeros(nreps) for L in Ls]\ny = rand(maximum(Ls))\n\nfor (i, L) in enumerate(Ls)\n    # Deterministic time series\n    x, t = trajectory(sys, L - 1, Ttr = 5000)\n    x = columns(x)[1] # remember to make it `Vector{<:Real}\n    sx = surrogenerator(x, method)\n    r_det[i] = complexity_normalized(est, x)\n    r_det_surr[i][:] = [complexity_normalized(est, sx()) for j = 1:nreps]\n\n    # Random time series\n    r_noise[i] = complexity_normalized(est, y[1:L])\n    sy = surrogenerator(y[1:L], method)\n    r_noise_surr[i][:] = [complexity_normalized(est, sy()) for j = 1:nreps]\nend\n\nfig = Figure()\nax = Axis(fig[1, 1],\n    xlabel = \"Time series length (L)\",\n    ylabel = \"# missing dispersion patterns (normalized)\"\n)\n\nlines!(ax, Ls, r_det, label = \"logistic(x0 = 0.6; r = 4.0)\", color = :black)\nlines!(ax, Ls, r_noise, label = \"Uniform noise\", color = :red)\nfor i = 1:nL\n    if i == 1\n        boxplot!(ax, fill(Ls[i], nL), r_det_surr[i]; width = 50, color = :black,\n            label = \"WIAAFT surrogates (logistic)\")\n         boxplot!(ax, fill(Ls[i], nL), r_noise_surr[i]; width = 50, color = :red,\n            label = \"WIAAFT surrogates (noise)\")\n    else\n        boxplot!(ax, fill(Ls[i], nL), r_det_surr[i]; width = 50, color = :black)\n        boxplot!(ax, fill(Ls[i], nL), r_noise_surr[i]; width = 50, color = :red)\n    end\nend\naxislegend(position = :rc)\nylims!(0, 1.1)\n\nfig","category":"page"},{"location":"examples/","page":"ComplexityMeasures.jl Examples","title":"ComplexityMeasures.jl Examples","text":"We don't need to actually to compute the quantiles here to see that for the logistic map, across all time series lengths, the N_MDP values are above the extremal values of the N_MDP values for the surrogate ensembles. Thus, we conclude that the logistic map time series has nonlinearity (well, of course).","category":"page"},{"location":"examples/","page":"ComplexityMeasures.jl Examples","title":"ComplexityMeasures.jl Examples","text":"For the univariate noise time series, there is considerable overlap between N_MDP for the surrogate distributions and the original signal, so we can't claim nonlinearity for this signal.","category":"page"},{"location":"examples/","page":"ComplexityMeasures.jl Examples","title":"ComplexityMeasures.jl Examples","text":"Of course, to robustly reject the null hypothesis, we'd need to generate a sufficient number of surrogate realizations, and actually compute quantiles to compare with.","category":"page"},{"location":"examples/#Complexity:-approximate-entropy","page":"ComplexityMeasures.jl Examples","title":"Complexity: approximate entropy","text":"","category":"section"},{"location":"examples/","page":"ComplexityMeasures.jl Examples","title":"ComplexityMeasures.jl Examples","text":"Here, we reproduce the Henon map example with R=08 from Pincus (1991), comparing our values with relevant values from table 1 in Pincus (1991).","category":"page"},{"location":"examples/","page":"ComplexityMeasures.jl Examples","title":"ComplexityMeasures.jl Examples","text":"We use DiscreteDynamicalSystem from DynamicalSystemsBase to represent the map, and use the trajectory function from the same package to iterate the map for different initial conditions, for multiple time series lengths.","category":"page"},{"location":"examples/","page":"ComplexityMeasures.jl Examples","title":"ComplexityMeasures.jl Examples","text":"Finally, we summarize our results in box plots and compare the values to those obtained by Pincus (1991).","category":"page"},{"location":"examples/","page":"ComplexityMeasures.jl Examples","title":"ComplexityMeasures.jl Examples","text":"using ComplexityMeasures\nusing DynamicalSystemsBase\nusing DelayEmbeddings\nusing CairoMakie\n\n# Equation 13 in Pincus (1991)\nfunction henon_rule(u, p, n)\n    R = p[1]\n    x, y = u\n    dx = R*y + 1 - 1.4*x^2\n    dy = 0.3*R*x\n    return SVector(dx, dy)\nend\n\nfunction henon(; u₀ = rand(2), R = 0.8)\n    DeterministicIteratedMap(henon_rule, u₀, [R])\nend\n\nts_lengths = [300, 1000, 2000, 3000]\nnreps = 100\napens_08 = [zeros(nreps) for i = 1:length(ts_lengths)]\n\n# For some initial conditions, the Henon map as specified here blows up,\n# so we need to check for infinite values.\ncontainsinf(x) = any(isinf.(x))\n\nc = ApproximateEntropy(r = 0.05, m = 2)\n\nfor (i, L) in enumerate(ts_lengths)\n    k = 1\n    while k <= nreps\n        sys = henon(u₀ = rand(2), R = 0.8)\n        t = trajectory(sys, L; Ttr = 5000)[1]\n\n        if !any([containsinf(tᵢ) for tᵢ in t])\n            x, y = columns(t)\n            apens_08[i][k] = complexity(c, x)\n            k += 1\n        end\n    end\nend\n\nfig = Figure()\n\n# Example time series\na1 = Axis(fig[1,1]; xlabel = \"Time (t)\", ylabel = \"Value\")\nsys = henon(u₀ = [0.5, 0.1], R = 0.8)\nx, y = columns(first(trajectory(sys, 100, Ttr = 500))) # we don't need time indices\nlines!(a1, 1:length(x), x, label = \"x\")\nlines!(a1, 1:length(y), y, label = \"y\")\n\n# Approximate entropy values, compared to those of the original paper (black dots).\na2 = Axis(fig[2, 1];\n    xlabel = \"Time series length (L)\",\n    ylabel = \"ApEn(m = 2, r = 0.05)\")\n\n# hacky boxplot, but this seems to be how it's done in Makie at the moment\nn = length(ts_lengths)\nfor i = 1:n\n    boxplot!(a2, fill(ts_lengths[i], n), apens_08[i];\n        width = 200)\nend\n\nscatter!(a2, ts_lengths, [0.337, 0.385, NaN, 0.394];\n    label = \"Pincus (1991)\", color = :black)\nfig","category":"page"},{"location":"examples/#Complexity:-sample-entropy","page":"ComplexityMeasures.jl Examples","title":"Complexity: sample entropy","text":"","category":"section"},{"location":"examples/","page":"ComplexityMeasures.jl Examples","title":"ComplexityMeasures.jl Examples","text":"Completely regular signals should have sample entropy approaching zero, while less regular signals should have higher sample entropy.","category":"page"},{"location":"examples/","page":"ComplexityMeasures.jl Examples","title":"ComplexityMeasures.jl Examples","text":"using ComplexityMeasures\nusing CairoMakie\nN, a = 2000, 10\nt = LinRange(0, 2*a*π, N)\n\nx = repeat([-5:5 |> collect; 4:-1:-4 |> collect], N ÷ 20);\ny = sin.(t .+ cos.(t/0.5));\nz = rand(N)\n\nh_x, h_y, h_z = map(t -> complexity(SampleEntropy(t), t), (x, y, z))\n\nfig = Figure()\nax = Axis(fig[1,1]; ylabel = \"x\")\nlines!(ax, t, x; color = Cycled(1), label = \"h=$(h=round(h_x, sigdigits = 5))\");\nay = Axis(fig[2,1]; ylabel = \"y\")\nlines!(ay, t, y; color = Cycled(2), label = \"h=$(h=round(h_y, sigdigits = 5))\");\naz = Axis(fig[3,1]; ylabel = \"z\", xlabel = \"time\")\nlines!(az, t, z; color = Cycled(3), label = \"h=$(h=round(h_z, sigdigits = 5))\");\nfor a in (ax, ay, az); axislegend(a); end\nfor a in (ax, ay); hidexdecorations!(a; grid=false); end\nfig","category":"page"},{"location":"examples/","page":"ComplexityMeasures.jl Examples","title":"ComplexityMeasures.jl Examples","text":"Next, we compare the sample entropy obtained for different values of the radius r for uniform noise, normally distributed noise, and a periodic signal.","category":"page"},{"location":"examples/","page":"ComplexityMeasures.jl Examples","title":"ComplexityMeasures.jl Examples","text":"using ComplexityMeasures\nusing CairoMakie\nusing Statistics\nusing Distributions: Normal\nN = 2000\nx_U = rand(N)\nx_N = rand(Normal(0, 3), N)\nx_periodic = repeat(rand(20), N ÷ 20)\n\nx_U .= (x_U .- mean(x_U)) ./ std(x_U)\nx_N .= (x_N .- mean(x_N)) ./ std(x_N)\nx_periodic .= (x_periodic .- mean(x_periodic)) ./ std(x_periodic)\n\nrs = 10 .^ range(-1, 0, length = 30)\nbase = 2\nm = 2\nhs_U = [complexity_normalized(SampleEntropy(m = m, r = r), x_U) for r in rs]\nhs_N = [complexity_normalized(SampleEntropy(m = m, r = r), x_N) for r in rs]\nhs_periodic = [complexity_normalized(SampleEntropy(m = m, r = r), x_periodic) for r in rs]\n\nfig = Figure()\n# Time series\na1 = Axis(fig[1,1]; xlabel = \"r\", ylabel = \"Sample entropy\")\nlines!(a1, rs, hs_U, label = \"Uniform noise, U(0, 1)\")\nlines!(a1, rs, hs_N, label = \"Gaussian noise, N(0, 1)\")\nlines!(a1, rs, hs_periodic, label = \"Periodic signal\")\naxislegend()\nfig","category":"page"},{"location":"examples/#Statistical-complexity-of-iterated-maps","page":"ComplexityMeasures.jl Examples","title":"Statistical complexity of iterated maps","text":"","category":"section"},{"location":"examples/","page":"ComplexityMeasures.jl Examples","title":"ComplexityMeasures.jl Examples","text":"In this example, we reproduce parts of Fig. 1 in Rosso et al. (2007): We compute the statistical complexity of the Henon, logistic and Schuster map, as well as that of k-noise.","category":"page"},{"location":"examples/","page":"ComplexityMeasures.jl Examples","title":"ComplexityMeasures.jl Examples","text":"using ComplexityMeasures\nusing Distances\nusing DynamicalSystemsBase\nusing CairoMakie\nusing FFTW\nusing Statistics\n\nN = 2^15\n\nfunction logistic(x0=0.4; r = 4.0)\n    return DeterministicIteratedMap(logistic_rule, SVector(x0), [r])\nend\nlogistic_rule(x, p, n) = @inbounds SVector(p[1]*x[1]*(1 - x[1]))\nlogistic_jacob(x, p, n) = @inbounds SMatrix{1,1}(p[1]*(1 - 2x[1]))\n\nfunction henon(u0=zeros(2); a = 1.4, b = 0.3)\n    return DeterministicIteratedMap(henon_rule, u0, [a,b])\nend\nhenon_rule(x, p, n) = SVector{2}(1.0 - p[1]*x[1]^2 + x[2], p[2]*x[1])\nhenon_jacob(x, p, n) = SMatrix{2,2}(-2*p[1]*x[1], p[2], 1.0, 0.0)\n\nfunction schuster(x0=0.5, z=3.0/2)\n    return DeterministicIteratedMap(schuster_rule, SVector(x0), [z])\nend\nschuster_rule(x, p, n) = @inbounds SVector((x[1]+x[1]^p[1]) % 1)\n\n# generate noise with power spectrum that falls like 1/f^k\nfunction k_noise(k=3)\n    function f(N)\n        x = rand(Float64, N)\n        # generate power spectrum of random numbers and multiply by f^(-k/2)\n        x_hat = fft(x) .* abs.(vec(fftfreq(length(x)))) .^ (-k/2)\n        # set to zero for frequency zero\n        x_hat[1] = 0\n        return real.(ifft(x_hat))\n    end\n    return f\nend\n\nfig = Figure()\nax = Axis(fig[1, 1]; xlabel=L\"H_S\", ylabel=L\"C_{JS}\")\n\nm, τ = 6, 1\nm_kwargs = (\n        (color=:transparent,\n        strokecolor=:red,\n        marker=:utriangle,\n        strokewidth=2),\n        (color=:transparent,\n        strokecolor=:blue,\n        marker=:rect,\n        strokewidth=2),\n        (color=:magenta,\n        marker=:circle),\n        (color=:blue,\n        marker=:rect)\n    )\n\nn = 100\n\nc = StatisticalComplexity(\n    dist=JSDivergence(),\n    est=OrdinalPatterns(; m, τ),\n    entr=Renyi()\n)\nfor (j, (ds_gen, sym, ds_name)) in enumerate(zip(\n        (logistic, henon, schuster, k_noise),\n        (:utriangle, :rect, :dtriangle, :diamond),\n        (\"Logistic map\", \"Henon map\", \"Schuster map\", \"k-noise (k=3)\"),\n    ))\n\n    if j < 4\n        dim = dimension(ds_gen())\n        hs, cs = zeros(n), zeros(n)\n        for k in 1:n\n            ic = rand(dim) * 0.3\n            ds = ds_gen(SVector{dim}(ic))\n            x, t = trajectory(ds, N, Ttr=100)\n            hs[k], cs[k] = entropy_complexity(c, x[:, 1])\n        end\n        scatter!(ax, mean(hs), mean(cs); label=\"$ds_name\", markersize=25, m_kwargs[j]...)\n    else\n        ds = ds_gen()\n        hs, cs = zeros(n), zeros(n)\n        for k in 1:n\n            x = ds(N)\n            hs[k], cs[k] = entropy_complexity(c, x[:, 1])\n        end\n        scatter!(ax, mean(hs), mean(cs); label=\"$ds_name\", markersize=25, m_kwargs[j]...)\n    end\nend\n\nmin_curve, max_curve = entropy_complexity_curves(c)\nlines!(ax, min_curve; color=:black)\nlines!(ax, max_curve; color=:black)\naxislegend(; position=:lt)\nfig","category":"page"},{"location":"information_measures/#information_measures","page":"Information measures (entropies and co.)","title":"Information measures (entropies and co.)","text":"","category":"section"},{"location":"information_measures/","page":"Information measures (entropies and co.)","title":"Information measures (entropies and co.)","text":"note: Note\nBe sure you have gone through the Tutorial before going through the API here to have a good idea of the terminology used in ComplexityMeasures.jl.","category":"page"},{"location":"information_measures/#Information-measures-API","page":"Information measures (entropies and co.)","title":"Information measures API","text":"","category":"section"},{"location":"information_measures/","page":"Information measures (entropies and co.)","title":"Information measures (entropies and co.)","text":"The information measure API is defined by the information function, which takes as an input an InformationMeasure, or some specialized DiscreteInfoEstimator or DifferentialInfoEstimator for estimating the discrete or differential variant of the measure. The functions information_maximum and information_normalized are also useful.","category":"page"},{"location":"information_measures/","page":"Information measures (entropies and co.)","title":"Information measures (entropies and co.)","text":"InformationMeasure\ninformation(::InformationMeasure, ::OutcomeSpace, ::Any)\ninformation(::DifferentialInfoEstimator, ::Any)\ninformation_maximum\ninformation_normalized","category":"page"},{"location":"information_measures/#ComplexityMeasures.InformationMeasure","page":"Information measures (entropies and co.)","title":"ComplexityMeasures.InformationMeasure","text":"InformationMeasure\n\nInformationMeasure is the supertype of all information measure definitions.\n\nIn this package, we define \"information measures\" as functionals of probability mass functions (\"discrete\" measures), or of probability density functions (\"differential\" measures). Examples are (generalized) entropies such as Shannon or Renyi, or extropies like ShannonExtropy. Amigó et al. (2018) provides a useful review of generalized entropies.\n\nUsed with\n\nAny of the information measures listed below can be used with\n\ninformation, to compute a numerical value for the measure, given some input data.\ninformation_maximum, to compute the maximum possible value for the measure.\ninformation_normalized, to compute the normalized form of the   measure (divided by the maximum possible value).\n\nThe information_maximum/information_normalized functions only works with the discrete version of the measure. See docstrings for the above functions for usage examples.\n\nImplementations\n\nRenyi.\nTsallis.\nShannon, which is a subcase of the above two in the limit q → 1.\nKaniadakis.\nCurado.\nStretchedExponential.\nRenyiExtropy.\nTsallisExtropy.\nShannonExtropy, which is a subcase of the above two in the limit q → 1.\n\nEstimators\n\nA particular information measure may have both a discrete and a continuous/differential definition, which are estimated using a DifferentialInfoEstimator or a DifferentialInfoEstimator, respectively.\n\n\n\n\n\n","category":"type"},{"location":"information_measures/#ComplexityMeasures.information-Tuple{InformationMeasure, OutcomeSpace, Any}","page":"Information measures (entropies and co.)","title":"ComplexityMeasures.information","text":"information(e::DiscreteInfoEstimator, [est::ProbabilitiesEstimator,] o::OutcomeSpace, x) → h::Real\n\nEstimate a discrete information measure from input data x using the provided DiscreteInfoEstimator and ProbabilitiesEstimator over the given OutcomeSpace.\n\nAs an alternative, you can provide an InformationMeasure for the first argument which will default to PlugIn estimation) for the information estimation. You may also skip the first argument, in which case Shannon() will be used. You may also skip the second argument (est) argument, giving only an outcome space, which will default to the RelativeAmount probabilities estimator. Note that some information measure estimators (e.g., GeneralizedSchuermann) operate directly on counts and hence ignore est.\n\ninformation([e::DiscreteInfoEstimator,] p::Probabilities) → h::Real\ninformation([e::DiscreteInfoEstimator,] c::Counts) → h::Real\n\nLike above, but estimate the information measure from the pre-computed Probabilities p or Counts. Counts are converted into probabilities using RelativeAmount, unless the estimator e uses counts directly.\n\nSee also: information_maximum, information_normalized for a normalized version.\n\nExamples (naive estimation)\n\nThe simplest way to estimate a discrete measure is to provide the InformationMeasure directly in combination with an OutcomeSpace. This will use the \"naive\" PlugIn estimator for the measure, and the \"naive\" RelativeAmount estimator for the probabilities.\n\nx = randn(100) # some input data\no = ValueBinning(RectangularBinning(5)) # a 5-bin histogram outcome space\nh_s = information(Shannon(), o, x)\n\nHere are some more examples:\n\nx = [rand(Bool) for _ in 1:10000] # coin toss\nps = probabilities(x) # gives about [0.5, 0.5] by definition\nh = information(ps) # gives 1, about 1 bit by definition (Shannon entropy by default)\nh = information(Shannon(), ps) # syntactically equivalent to the above\nh = information(Shannon(), UniqueElements(), x) # syntactically equivalent to above\nh = information(Renyi(2.0), ps) # also gives 1, order `q` doesn't matter for coin toss\nh = information(OrdinalPatterns(;m=3), x) # gives about 2, again by definition\n\nExamples (bias-corrected estimation)\n\nIt is known that both PlugIn estimation for information measures and RelativeAmount estimation for probabilities are biased. The scientific literature abounds with estimators that correct for this bias, both on the measure-estimation level and on the probability-estimation level. We thus provide the option to use any DiscreteInfoEstimator in combination with any ProbabilitiesEstimator for improved estimates. Note that custom probabilites estimators will only work with counting-compatible OutcomeSpace.\n\nx = randn(100)\no = ValueBinning(RectangularBinning(5))\n\n# Estimate Shannon entropy estimation using various dedicated estimators\nh_s = information(MillerMadow(Shannon()), RelativeAmount(), o, x)\nh_s = information(HorvitzThompson(Shannon()), Shrinkage(), o, x)\nh_s = information(Schuermann(Shannon()), Shrinkage(), o, x)\n\n# Estimate information measures using the generic `Jackknife` estimator\nh_r = information(Jackknife(Renyi()), Shrinkage(), o, x)\nj_t = information(Jackknife(TsallisExtropy()), BayesianRegularization(), o, x)\nj_r = information(Jackknife(RenyiExtropy()), RelativeAmount(), o, x)\n\n\n\n\n\n","category":"method"},{"location":"information_measures/#ComplexityMeasures.information-Tuple{DifferentialInfoEstimator, Any}","page":"Information measures (entropies and co.)","title":"ComplexityMeasures.information","text":"information(est::DifferentialInfoEstimator, x) → h::Real\n\nEstimate a differential information measure using the provided DifferentialInfoEstimator and input data x.\n\nDescription\n\nThe overwhelming majority of differential estimators estimate the Shannon entropy. If the same estimator can estimate different information measures (e.g. it can estimate both Shannon and Tsallis), then the information measure is provided as an argument to the estimator itself.\n\nSee the table of differential information measure estimators in the docs for all differential information measure estimators.\n\nCurrently, unlike for the discrete information measures, this method doesn't involve explicitly first computing a probability density function and then passing this density to an information measure definition. But in the future, we want to establish a density API similar to the probabilities API.\n\nExamples\n\nTo compute the differential version of a measure, give it as the first argument to a DifferentialInfoEstimator and pass it to information.\n\nx = randn(1000)\nh_sh = information(Kraskov(Shannon()), x)\nh_vc = information(Vasicek(Shannon()), x)\n\nA normal distribution has a base-e Shannon differential entropy of 0.5*log(2π) + 0.5 nats.\n\nest = Kraskov(k = 5, base = ℯ) # Base `ℯ` for nats.\nh = information(est, randn(2_000_000))\nabs(h - 0.5*log(2π) - 0.5) # ≈ 0.0001\n\n\n\n\n\n","category":"method"},{"location":"information_measures/#ComplexityMeasures.information_maximum","page":"Information measures (entropies and co.)","title":"ComplexityMeasures.information_maximum","text":"information_maximum(e::InformationMeasure, o::OutcomeSpace [, x])\n\nReturn the maximum value of the given information measure can have, given input data x and the given outcome space (the OutcomeSpace may also be specified by a ProbabilitiesEstimator).\n\nLike in outcome_space, for some outcome spaces, the possible outcomes are known without knowledge of input x, in which case the function dispatches to information_maximum(e, o).\n\ninformation_maximum(e::InformationMeasure, L::Int)\n\nThe same as above, but computed directly from the number of total outcomes L.\n\n\n\n\n\n","category":"function"},{"location":"information_measures/#ComplexityMeasures.information_normalized","page":"Information measures (entropies and co.)","title":"ComplexityMeasures.information_normalized","text":"information_normalized([e::DiscreteInfoEstimator,] [est::ProbabilitiesEstimator,] o::OutcomeSpace, x) → h::Real\n\nEstimate the normalized version of the given discrete information measure, This is just the value of information divided its maximum possible value given o.\n\nThe same convenience syntaxes as in information can be used here.\n\nNotice that there is no method information_normalized(e::DiscreteInfoEstimator, probs::Probabilities), because there is no way to know the number of possible outcomes (i.e., the total_outcomes) from probs.\n\nNormalized values\n\nFor the PlugIn estimator, it is guaranteed that h̃ ∈ [0, 1]. For any other estimator, we can't guarantee this, since the estimator might over-correct. You should know what you're doing if using anything but PlugIn to estimate normalized values.\n\n\n\n\n\n","category":"function"},{"location":"information_measures/#Entropies","page":"Information measures (entropies and co.)","title":"Entropies","text":"","category":"section"},{"location":"information_measures/","page":"Information measures (entropies and co.)","title":"Information measures (entropies and co.)","text":"entropy\nShannon\nRenyi\nTsallis\nKaniadakis\nCurado\nStretchedExponential","category":"page"},{"location":"information_measures/#ComplexityMeasures.entropy","page":"Information measures (entropies and co.)","title":"ComplexityMeasures.entropy","text":"entropy(args...)\n\nentropy is nothing more than a call to information that will simply throw an error if used with an information measure that is not an entropy.\n\n\n\n\n\n","category":"function"},{"location":"information_measures/#ComplexityMeasures.Shannon","page":"Information measures (entropies and co.)","title":"ComplexityMeasures.Shannon","text":"Shannon <: InformationMeasure\nShannon(; base = 2)\n\nThe Shannon (Shannon, 1948) entropy, used with information to compute:\n\nH(p) = - sum_i pi log(pi)\n\nwith the log at the given base.\n\nThe maximum value of the Shannon entropy is log_base(L), which is the entropy of the uniform distribution with L the total_outcomes.\n\n\n\n\n\n","category":"type"},{"location":"information_measures/#ComplexityMeasures.Renyi","page":"Information measures (entropies and co.)","title":"ComplexityMeasures.Renyi","text":"Renyi <: InformationMeasure\nRenyi(q, base = 2)\nRenyi(; q = 1.0, base = 2)\n\nThe Rényi generalized order-q entropy (Rényi, 1961), used with information to compute an entropy with units given by base (typically 2 or MathConstants.e).\n\nDescription\n\nLet p be an array of probabilities (summing to 1). Then the Rényi generalized entropy is\n\nH_q(p) = frac11-q log left(sum_i pi^qright)\n\nand generalizes other known entropies, like e.g. the information entropy (q = 1, see Shannon (1948)), the maximum entropy (q=0, also known as Hartley entropy), or the correlation entropy (q = 2, also known as collision entropy).\n\nThe maximum value of the Rényi entropy is log_base(L), which is the entropy of the uniform distribution with L the total_outcomes.\n\n\n\n\n\n","category":"type"},{"location":"information_measures/#ComplexityMeasures.Tsallis","page":"Information measures (entropies and co.)","title":"ComplexityMeasures.Tsallis","text":"Tsallis <: InformationMeasure\nTsallis(q; k = 1.0, base = 2)\nTsallis(; q = 1.0, k = 1.0, base = 2)\n\nThe Tsallis generalized order-q entropy (Tsallis, 1988), used with information to compute an entropy.\n\nbase only applies in the limiting case q == 1, in which the Tsallis entropy reduces to Shannon entropy.\n\nDescription\n\nThe Tsallis entropy is a generalization of the Boltzmann-Gibbs entropy, with k standing for the Boltzmann constant. It is defined as\n\nS_q(p) = frackq - 1left(1 - sum_i pi^qright)\n\nThe maximum value of the Tsallis entropy is k(L^1 - q - 1)(1 - q), with L the total_outcomes.\n\n\n\n\n\n","category":"type"},{"location":"information_measures/#ComplexityMeasures.Kaniadakis","page":"Information measures (entropies and co.)","title":"ComplexityMeasures.Kaniadakis","text":"Kaniadakis <: InformationMeasure\nKaniadakis(; κ = 1.0, base = 2.0)\n\nThe Kaniadakis entropy (Tsallis, 2009), used with information to compute\n\nH_K(p) = -sum_i=1^N p_i f_kappa(p_i)\n\nf_kappa (x) = dfracx^kappa - x^-kappa2kappa\n\nwhere if kappa = 0, regular logarithm to the given base is used, and 0 probabilities are skipped.\n\n\n\n\n\n","category":"type"},{"location":"information_measures/#ComplexityMeasures.Curado","page":"Information measures (entropies and co.)","title":"ComplexityMeasures.Curado","text":"Curado <: InformationMeasure\nCurado(; b = 1.0)\n\nThe Curado entropy (Curado and Nobre, 2004), used with information to compute\n\nH_C(p) = left( sum_i=1^N e^-b p_i right) + e^-b - 1\n\nwith b ∈ ℛ, b > 0, and the terms outside the sum ensures that H_C(0) = H_C(1) = 0.\n\nThe maximum entropy for Curado is L(1 - exp(-bL)) + exp(-b) - 1 with L the total_outcomes.\n\n\n\n\n\n","category":"type"},{"location":"information_measures/#ComplexityMeasures.StretchedExponential","page":"Information measures (entropies and co.)","title":"ComplexityMeasures.StretchedExponential","text":"StretchedExponential <: InformationMeasure\nStretchedExponential(; η = 2.0, base = 2)\n\nThe stretched exponential, or Anteneodo-Plastino, entropy (Anteneodo and Plastino, 1999), used with information to compute\n\nS_eta(p) = sum_i = 1^N\nGamma left( dfraceta + 1eta - log_base(p_i) right) -\np_i Gamma left( dfraceta + 1eta right)\n\nwhere eta geq 0, Gamma(cdot cdot) is the upper incomplete Gamma function, and Gamma(cdot) = Gamma(cdot 0) is the Gamma function. Reduces to Shannon entropy for η = 1.0.\n\nThe maximum entropy for StrechedExponential is a rather complicated expression involving incomplete Gamma functions (see source code).\n\n\n\n\n\n","category":"type"},{"location":"information_measures/#Other-information-measures","page":"Information measures (entropies and co.)","title":"Other information measures","text":"","category":"section"},{"location":"information_measures/","page":"Information measures (entropies and co.)","title":"Information measures (entropies and co.)","text":"ShannonExtropy\nRenyiExtropy\nTsallisExtropy\nElectronicEntropy","category":"page"},{"location":"information_measures/#ComplexityMeasures.ShannonExtropy","page":"Information measures (entropies and co.)","title":"ComplexityMeasures.ShannonExtropy","text":"ShannonExtropy <: InformationMeasure\nShannonExtropy(; base = 2)\n\nThe Shannon extropy (Lad et al., 2015), used with information to compute\n\nJ(x) = -sum_i=1^N (1 - pi) log(1 - pi)\n\nfor a probability distribution P = p_1 p_2 ldots p_N, with the log at the given base.\n\n\n\n\n\n","category":"type"},{"location":"information_measures/#ComplexityMeasures.RenyiExtropy","page":"Information measures (entropies and co.)","title":"ComplexityMeasures.RenyiExtropy","text":"RenyiExtropy <: InformationMeasure\nRenyiExtropy(; q = 1.0, base = 2)\n\nThe Rényi extropy (Liu and Xiao, 2023).\n\nDescription\n\nRenyiExtropy is used with information to compute\n\nJ_R(P) = dfrac-(n - 1) log(n - 1) + (n - 1) log left( sum_i=1^N (1 - pi)^q right) q - 1\n\nfor a probability distribution P = p_1 p_2 ldots p_N, with the log at the given base. Alternatively, RenyiExtropy can be used with information_normalized, which ensures that the computed extropy is on the interval 0 1 by normalizing to to the maximal Rényi extropy, given by\n\nJ_R(P) = (N - 1)log left( dfracnn-1 right) \n\n\n\n\n\n","category":"type"},{"location":"information_measures/#ComplexityMeasures.TsallisExtropy","page":"Information measures (entropies and co.)","title":"ComplexityMeasures.TsallisExtropy","text":"TsallisExtropy <: InformationMeasure\nTsallisExtropy(; base = 2)\n\nThe Tsallis extropy (Xue and Deng, 2023).\n\nDescription\n\nTsallisExtropy is used with information to compute\n\nJ_T(P) = k dfracN - 1 - sum_i=1^N ( 1 - pi)^qq - 1\n\nfor a probability distribution P = p_1 p_2 ldots p_N, with the log at the given base. Alternatively, TsallisExtropy can be used with information_normalized, which ensures that the computed extropy is on the interval 0 1 by normalizing to to the maximal Tsallis extropy, given by\n\nJ_T(P) = dfrac(N - 1)N^q - 1 - (N - 1)^q(q - 1)N^q - 1\n\n\n\n\n\n","category":"type"},{"location":"information_measures/#ComplexityMeasures.ElectronicEntropy","page":"Information measures (entropies and co.)","title":"ComplexityMeasures.ElectronicEntropy","text":"ElectronicEntropy <: InformationMeasure\nElectronicEntropy(; h = Shannon(; base = 2), j = ShannonExtropy(; base = 2))\n\nThe \"electronic entropy\" measure is defined in discrete form in Lad et al. (2015) as\n\nH_EL(p) = H_S(p) + J_S(P)\n\nwhere H_S(p) is the Shannon entropy and J_S(p) is the ShannonExtropy extropy of the probability vector p.\n\n\n\n\n\n","category":"type"},{"location":"information_measures/#Discrete-information-estimators","page":"Information measures (entropies and co.)","title":"Discrete information estimators","text":"","category":"section"},{"location":"information_measures/","page":"Information measures (entropies and co.)","title":"Information measures (entropies and co.)","text":"DiscreteInfoEstimator\nPlugIn\nMillerMadow\nSchuermann\nGeneralizedSchuermann\nJackknife\nHorvitzThompson\nChaoShen","category":"page"},{"location":"information_measures/#ComplexityMeasures.DiscreteInfoEstimator","page":"Information measures (entropies and co.)","title":"ComplexityMeasures.DiscreteInfoEstimator","text":"DiscreteInfoEstimator\n\nThe supertype of all discrete information measure estimators, which are used in combination with a ProbabilitiesEstimator as input to  information or related functions.\n\nThe first argument to a discrete estimator is always an InformationMeasure (defaults to Shannon).\n\nDescription\n\nA discrete InformationMeasure is a functional of a probability mass function. To estimate such a measure from data, we must first estimate a probability mass function using a ProbabilitiesEstimator from the (encoded/discretized) input data, and then apply the estimator to the estimated probabilities. For example, the Shannon entropy is typically computed using the RelativeAmount estimator to compute probabilities, which are then given to the PlugIn estimator. Many other estimators exist, not only for Shannon entropy, but other information measures as well.\n\nWe provide a library of both generic estimators such as PlugIn or Jackknife (which can be applied to any measure), as well as dedicated estimators such as MillerMadow, which computes Shannon entropy using the Miller-Madow bias correction. The list below gives a complete overview.\n\nImplementations\n\nThe following estimators are generic and can compute any InformationMeasure.\n\nPlugIn. The default, generic plug-in estimator of any information measure.   It computes the measure exactly as stated in the definition, using the computed   probability mass function.\nJackknife. Uses the a combination of the plug-in estimator and the jackknife   principle to estimate the information measure.\n\nShannon entropy estimators\n\nThe following estimators are dedicated Shannon entropy estimators, which provide improvements over the naive PlugIn estimator.\n\nMillerMadow.\nHorvitzThompson.\nSchuermann.\nGeneralizedSchuermann.\nChaoShen.\n\ninfo: Info\nAny of the implemented DiscreteInfoEstimators can be used in combination with any ProbabilitiesEstimator as input to information. What this means is that every estimator actually comes in many different variants - one for each ProbabilitiesEstimator. For example, the MillerMadow estimator of Shannon entropy is typically calculated with RelativeAmount probabilities. But here, you can use for example the BayesianRegularization or the Shrinkage probabilities estimators instead, i.e. information(MillerMadow(), RelativeAmount(outcome_space), x) and information(MillerMadow(), BayesianRegularization(outcomes_space), x) are distinct estimators. This holds for all DiscreteInfoEstimators. Many of these estimators haven't been explored in the literature before, so feel free to explore, and please cite this software if you use it to explore some new estimator combination!\n\n\n\n\n\n","category":"type"},{"location":"information_measures/#ComplexityMeasures.PlugIn","page":"Information measures (entropies and co.)","title":"ComplexityMeasures.PlugIn","text":"PlugIn(e::InformationMeasure) <: DiscreteInfoEstimator\n\nThe PlugIn estimator is also called the empirical/naive/\"maximum likelihood\" estimator, and is used with information to any discrete InformationMeasure.\n\nIt computes any quantity exactly as given by its formula. When computing an information measure, which here is defined as a probabilities functional, it computes the quantity directly from a probability mass function, which is derived from maximum-likelihood (RelativeAmount estimates of the probabilities.\n\nBias of plug-in estimates\n\nThe plugin-estimator of Shannon entropy underestimates the true entropy, with a bias that grows with the number of distinct outcomes (Arora et al., 2022)(Arora et al., 2022),\n\nbias(H_S^plugin) = -dfracK-12N + o(N^-1)\n\nwhere K is the number of distinct outcomes, and N is the sample size. Many authors have tried to remedy this by proposing alternative Shannon entropy estimators. For example, the MillerMadow estimator is a simple correction to the plug-in estimator that adds back the bias term above. Many other estimators exist; see DiscreteInfoEstimators for an overview.\n\n\n\n\n\n","category":"type"},{"location":"information_measures/#ComplexityMeasures.MillerMadow","page":"Information measures (entropies and co.)","title":"ComplexityMeasures.MillerMadow","text":"MillerMadow <: DiscreteInfoEstimator\nMillerMadow(measure::Shannon = Shannon())\n\nThe MillerMadow estimator is used with information to compute the discrete Shannon entropy according to Miller (1955).\n\nDescription\n\nThe Miller-Madow estimator of Shannon entropy is given by\n\nH_S^MM = H_S^plugin + dfracm - 12N\n\nwhere H_S^plugin is the Shannon entropy estimated using the PlugIn estimator, m is the number of bins with nonzero probability (as defined in Paninski (2003)), and N is the number of observations.\n\n\n\n\n\n","category":"type"},{"location":"information_measures/#ComplexityMeasures.Schuermann","page":"Information measures (entropies and co.)","title":"ComplexityMeasures.Schuermann","text":"Schuermann <: DiscreteInfoEstimator\nSchuermann(definition::Shannon; a = 1.0)\n\nThe Schuermann estimator is used with information to compute the discrete Shannon entropy with the bias-corrected estimator given in Schürmann (2004).\n\nSee detailed description for GeneralizedSchuermann for details.\n\n\n\n\n\n","category":"type"},{"location":"information_measures/#ComplexityMeasures.GeneralizedSchuermann","page":"Information measures (entropies and co.)","title":"ComplexityMeasures.GeneralizedSchuermann","text":"GeneralizedSchuermann <: DiscreteInfoEstimator\nGeneralizedSchuermann(definition = Shannon(); a = 1.0)\n\nThe GeneralizedSchuermann estimator is used with information to compute the discrete Shannon entropy with the bias-corrected estimator given in Grassberger (2022).\n\nThe \"generalized\" part of the name, as opposed to the Schürmann (2004) estimator (Schuermann), is due to the possibility of picking difference parameters a_i for different outcomes. If different parameters are assigned to the different outcomes, a must be a vector of parameters of length length(outcomes), where the outcomes are obtained using outcomes. See Grassberger (2022) for more information. If a is a real number, then a_i = a forall i, and the estimator reduces to the Schuermann estimator.\n\nDescription\n\nFor a set of N observations over M outcomes, the estimator is given by\n\nH_S^opt = varphi(N) - dfrac1N sum_i=1^M n_i G_n_i(a_i)\n\nwhere n_i is the observed frequency of the i-th outcome,\n\nG_n(a) = varphi(n) + (-1)^n int_0^a dfracx^n - 1x + 1 dx\n\nG_n(1) = G_n and G_n(0) = varphi(n), and\n\nG_n = varphi(n) + (-1)^n int_0^1 dfracx^n - 1x + 1 dx\n\n\n\n\n\n","category":"type"},{"location":"information_measures/#ComplexityMeasures.Jackknife","page":"Information measures (entropies and co.)","title":"ComplexityMeasures.Jackknife","text":"Jackknife <: DiscreteInfoEstimator\nJackknife(definition::InformationMeasure = Shannon())\n\nThe Jackknife estimator is used with information to compute any discrete InformationMeasure.\n\nThe Jackknife estimator uses the generic jackknife principle to reduce bias. Zahl (1977) was the first to apply the jaccknife technique in the context of Shannon entropy estimation. Here, we've generalized his estimator to work with any InformationMeasure.\n\nDescription\n\nAs an example of the jackknife technique, here is the formula for a jackknife estimate of Shannon entropy\n\nH_S^J = N H_S^plugin - dfracN-1N sum_i=1^N H_S^plugin^-i\n\nwhere N is the sample size, H_S^plugin is the plugin estimate of Shannon entropy, and H_S^plugin^-i is the plugin estimate, but computed with the i-th sample left out.\n\n\n\n\n\n","category":"type"},{"location":"information_measures/#ComplexityMeasures.HorvitzThompson","page":"Information measures (entropies and co.)","title":"ComplexityMeasures.HorvitzThompson","text":"HorvitzThompson <: DiscreteInfoEstimator\nHorvitzThompson(measure::Shannon = Shannon())\n\nThe HorvitzThompson estimator is used with information to compute the discrete Shannon entropy according to Horvitz and Thompson (1952).\n\nDescription\n\nThe Horvitz-Thompson estimator of Shannon entropy is given by\n\nH_S^HT = -sum_i=1^M dfracp_i log(p_i) 1 - (1 - p_i)^N\n\nwhere N is the sample size and M is the number of outcomes. Given the true probability p_i of the i-th outcome, 1 - (1 - p_i)^N is the probability that the outcome appears at least once in a sample of size N (Arora et al., 2022). Dividing by this inclusion probability is a form of weighting, and compensates for situations where certain outcomes have so low probabilities that they are not often observed in a sample, for example in power-law distributions.\n\n\n\n\n\n","category":"type"},{"location":"information_measures/#ComplexityMeasures.ChaoShen","page":"Information measures (entropies and co.)","title":"ComplexityMeasures.ChaoShen","text":"ChaoShen <: DiscreteInfoEstimator\nChaoShen(definition::Shannon = Shannon())\n\nThe ChaoShen estimator is used with information to compute the discrete Shannon entropy according to Chao and Shen (2003).\n\nDescription\n\nThis estimator is a modification of the HorvitzThompson estimator that multiplies each plugin probability estimate by an estimate of sample coverage. If f_1 is the number of singletons (outcomes that occur only once) in a sample of length N, then the sample coverage is C = 1 - dfracf_1N. The Chao-Shen estimator of Shannon entropy is then\n\nH_S^CS = -sum_i=1^M left( dfracC p_i log(C p_i)1 - (1 - C p_i)^N right)\n\nwhere N is the sample size and M is the number of outcomes. If f_1 = N, then f_1 is set to f_1 = N - 1 to ensure positive entropy (Arora et al., 2022).\n\n\n\n\n\n","category":"type"},{"location":"information_measures/#Differential-information-estimators","page":"Information measures (entropies and co.)","title":"Differential information estimators","text":"","category":"section"},{"location":"information_measures/","page":"Information measures (entropies and co.)","title":"Information measures (entropies and co.)","text":"DifferentialInfoEstimator\nKraskov\nKozachenkoLeonenko\nZhu\nZhuSingh\nGao\nGoria\nLord\nLeonenkoProzantoSavani\nVasicek\nAlizadehArghami\nEbrahimi\nCorrea","category":"page"},{"location":"information_measures/#ComplexityMeasures.DifferentialInfoEstimator","page":"Information measures (entropies and co.)","title":"ComplexityMeasures.DifferentialInfoEstimator","text":"DifferentialInfoEstimator\n\nThe supertype of all differential information measure estimators. These estimators compute an information measure in various ways that do not involve explicitly estimating a probability distribution.\n\nEach DifferentialInfoEstimators uses a specialized technique to approximate relevant densities/integrals, and is often tailored to one or a few types of information measures. For example, Kraskov estimates the Shannon entropy.\n\nSee information for usage.\n\nImplementations\n\nKozachenkoLeonenko.\nKraskov.\nGoria.\nGao.\nZhu\nZhuSingh.\nLord.\nAlizadehArghami.\nCorrea.\nVasicek.\nEbrahimi.\nLeonenkoProzantoSavani.\n\n\n\n\n\n","category":"type"},{"location":"information_measures/#ComplexityMeasures.Kraskov","page":"Information measures (entropies and co.)","title":"ComplexityMeasures.Kraskov","text":"Kraskov <: DifferentialInfoEstimator\nKraskov(definition = Shannon(); k::Int = 1, w::Int = 0)\n\nThe Kraskov estimator computes the Shannon differential information of a multi-dimensional StateSpaceSet using the k-th nearest neighbor searches method from Kraskov et al. (2004), with logarithms to the base specified in definition.\n\nw is the Theiler window, which determines if temporal neighbors are excluded during neighbor searches (defaults to 0, meaning that only the point itself is excluded when searching for neighbours).\n\nDescription\n\nAssume we have samples bfx_1 bfx_2 ldots bfx_N  from a continuous random variable X in mathbbR^d with support mathcalX and density functionf  mathbbR^d to mathbbR. Kraskov estimates the Shannon differential entropy\n\nH(X) = int_mathcalX f(x) log f(x) dx = mathbbE-log(f(X))\n\nSee also: information, KozachenkoLeonenko, DifferentialInfoEstimator.\n\n\n\n\n\n","category":"type"},{"location":"information_measures/#ComplexityMeasures.KozachenkoLeonenko","page":"Information measures (entropies and co.)","title":"ComplexityMeasures.KozachenkoLeonenko","text":"KozachenkoLeonenko <: DifferentialInfoEstimator\nKozachenkoLeonenko(definition = Shannon(); w::Int = 0)\n\nThe KozachenkoLeonenko estimator (Kozachenko and Leonenko, 1987) computes the Shannon differential information of a multi-dimensional StateSpaceSet, with logarithms to the base specified in definition.\n\nDescription\n\nAssume we have samples bfx_1 bfx_2 ldots bfx_N  from a continuous random variable X in mathbbR^d with support mathcalX and density functionf  mathbbR^d to mathbbR. KozachenkoLeonenko estimates the Shannon differential entropy\n\nH(X) = int_mathcalX f(x) log f(x) dx = mathbbE-log(f(X))\n\nusing the nearest neighbor method from Kozachenko and Leonenko (1987), as described in Charzyńska and Gambin (2016).\n\nw is the Theiler window, which determines if temporal neighbors are excluded during neighbor searches (defaults to 0, meaning that only the point itself is excluded when searching for neighbours).\n\nIn contrast to Kraskov, this estimator uses only the closest neighbor.\n\nSee also: information, Kraskov, DifferentialInfoEstimator.\n\n\n\n\n\n","category":"type"},{"location":"information_measures/#ComplexityMeasures.Zhu","page":"Information measures (entropies and co.)","title":"ComplexityMeasures.Zhu","text":"Zhu <: DifferentialInfoEstimator\nZhu(; definition = Shannon(), k = 1, w = 0)\n\nThe Zhu estimator (Zhu et al., 2015) is an extension to KozachenkoLeonenko, and computes the Shannon differential information of a multi-dimensional StateSpaceSet, with logarithms to the base specified in definition.\n\nDescription\n\nAssume we have samples bfx_1 bfx_2 ldots bfx_N  from a continuous random variable X in mathbbR^d with support mathcalX and density functionf  mathbbR^d to mathbbR. Zhu estimates the Shannon differential entropy\n\nH(X) = int_mathcalX f(x) log f(x) dx = mathbbE-log(f(X))\n\nby approximating densities within hyperrectangles surrounding each point xᵢ ∈ x using using k nearest neighbor searches. w is the Theiler window, which determines if temporal neighbors are excluded during neighbor searches (defaults to 0, meaning that only the point itself is excluded when searching for neighbours).\n\nSee also: information, KozachenkoLeonenko, DifferentialInfoEstimator.\n\n\n\n\n\n","category":"type"},{"location":"information_measures/#ComplexityMeasures.ZhuSingh","page":"Information measures (entropies and co.)","title":"ComplexityMeasures.ZhuSingh","text":"ZhuSingh <: DifferentialInfoEstimator\nZhuSingh(definition = Shannon(); k = 1, w = 0)\n\nThe ZhuSingh estimator (Zhu et al., 2015) computes the Shannon differential information of a multi-dimensional StateSpaceSet, with logarithms to the base specified in definition.\n\nDescription\n\nAssume we have samples bfx_1 bfx_2 ldots bfx_N  from a continuous random variable X in mathbbR^d with support mathcalX and density functionf  mathbbR^d to mathbbR. ZhuSingh estimates the Shannon differential entropy\n\nH(X) = int_mathcalX f(x) log f(x) dx = mathbbE-log(f(X))\n\nLike Zhu, this estimator approximates probabilities within hyperrectangles surrounding each point xᵢ ∈ x using using k nearest neighbor searches. However, it also considers the number of neighbors falling on the borders of these hyperrectangles. This estimator is an extension to the entropy estimator in Singh et al. (2003).\n\nw is the Theiler window, which determines if temporal neighbors are excluded during neighbor searches (defaults to 0, meaning that only the point itself is excluded when searching for neighbours).\n\nSee also: information, DifferentialInfoEstimator.\n\n\n\n\n\n","category":"type"},{"location":"information_measures/#ComplexityMeasures.Gao","page":"Information measures (entropies and co.)","title":"ComplexityMeasures.Gao","text":"Gao <: DifferentialInfoEstimator\nGao(definition = Shannon(); k = 1, w = 0, corrected = true)\n\nThe Gao estimator (Gao et al., 09–12 May 2015) computes the Shannon differential information, using a k-th nearest-neighbor approach based on Singh et al. (2003), with logarithms to the base specified in definition.\n\nw is the Theiler window, which determines if temporal neighbors are excluded during neighbor searches (defaults to 0, meaning that only the point itself is excluded when searching for neighbours).\n\nGao et al. (09–12 May 2015) give two variants of this estimator. If corrected == false, then the uncorrected version is used. If corrected == true, then the corrected version is used, which ensures that the estimator is asymptotically unbiased.\n\nDescription\n\nAssume we have samples bfx_1 bfx_2 ldots bfx_N  from a continuous random variable X in mathbbR^d with support mathcalX and density functionf  mathbbR^d to mathbbR. KozachenkoLeonenko estimates the Shannon differential entropy\n\nH(X) = int_mathcalX f(x) log f(x) dx = mathbbE-log(f(X))\n\n\n\n\n\n","category":"type"},{"location":"information_measures/#ComplexityMeasures.Goria","page":"Information measures (entropies and co.)","title":"ComplexityMeasures.Goria","text":"Goria <: DifferentialInfoEstimator\nGoria(measure = Shannon(); k = 1, w = 0)\n\nThe Goria estimator (Goria et al., 2005) computes the Shannon differential information of a multi-dimensional StateSpaceSet, with logarithms to the base specified in definition.\n\nDescription\n\nAssume we have samples bfx_1 bfx_2 ldots bfx_N  from a continuous random variable X in mathbbR^d with support mathcalX and density functionf  mathbbR^d to mathbbR. Goria estimates the Shannon differential entropy\n\nH(X) = int_mathcalX f(x) log f(x) dx = mathbbE-log(f(X))\n\nSpecifically, let bfn_1 bfn_2 ldots bfn_N be the distance of the samples bfx_1 bfx_2 ldots bfx_N  to their k-th nearest neighbors. Next, let the geometric mean of the distances be\n\nhatrho_k = left( prod_i=1^N right)^dfrac1N\n\nGoria et al. (2005)'s estimate of Shannon differential entropy is then\n\nhatH = mhatrho_k + log(N - 1) - psi(k) + log c_1(m)\n\nwhere c_1(m) = dfrac2pi^fracm2m Gamma(m2) and psi is the digamma function.\n\n\n\n\n\n","category":"type"},{"location":"information_measures/#ComplexityMeasures.Lord","page":"Information measures (entropies and co.)","title":"ComplexityMeasures.Lord","text":"Lord <: DifferentialInfoEstimator\nLord(measure = Shannon(); k = 10, w = 0)\n\nThe Lord estimator (Lord et al., 2018) estimates the Shannon differential information using a nearest neighbor approach with a local nonuniformity correction (LNC), with logarithms to the base specified in definition.\n\nw is the Theiler window, which determines if temporal neighbors are excluded during neighbor searches (defaults to 0, meaning that only the point itself is excluded when searching for neighbours).\n\nDescription\n\nAssume we have samples barX = bfx_1 bfx_2 ldots bfx_N  from a continuous random variable X in mathbbR^d with support mathcalX and density function f  mathbbR^d to mathbbR. Lord estimates the Shannon differential entropy\n\nH(X) = int_mathcalX f(x) log f(x) dx = mathbbE-log(f(X))\n\nby using the resubstitution formula\n\nhatbarX k = -mathbbElog(f(X))\napprox sum_i = 1^N log(hatf(bfx_i))\n\nwhere hatf(bfx_i) is an estimate of the density at bfx_i constructed in a manner such that hatf(bfx_i) propto dfrack(x_i)  NV_i, where k(x_i) is the number of points in the neighborhood of bfx_i, and V_i is the volume of that neighborhood.\n\nWhile most nearest-neighbor based differential entropy estimators uses regular volume elements (e.g. hypercubes, hyperrectangles, hyperspheres) for approximating the local densities hatf(bfx_i), the Lord estimator uses hyperellopsoid volume elements. These hyperellipsoids are, for each query point xᵢ, estimated using singular value decomposition (SVD) on the k-th nearest neighbors of xᵢ. Thus, the hyperellipsoids stretch/compress in response to the local geometry around each sample point. This makes Lord a well-suited entropy estimator for a wide range of systems.\n\n\n\n\n\n","category":"type"},{"location":"information_measures/#ComplexityMeasures.LeonenkoProzantoSavani","page":"Information measures (entropies and co.)","title":"ComplexityMeasures.LeonenkoProzantoSavani","text":"LeonenkoProzantoSavani <: DifferentialInfoEstimator\nLeonenkoProzantoSavani(definition = Shannon(); k = 1, w = 0)\n\nThe LeonenkoProzantoSavani estimator (Leonenko et al., 2008) computes the  Shannon, Renyi, or Tsallis differential information of a multi-dimensional StateSpaceSet, with logarithms to the base specified in definition.\n\nDescription\n\nThe estimator uses k-th nearest-neighbor searches.  w is the Theiler window, which determines if temporal neighbors are excluded during neighbor searches (defaults to 0, meaning that only the point itself is excluded when searching for neighbours).\n\nFor details, see Leonenko et al. (2008).\n\n\n\n\n\n","category":"type"},{"location":"information_measures/#ComplexityMeasures.Vasicek","page":"Information measures (entropies and co.)","title":"ComplexityMeasures.Vasicek","text":"Vasicek <: DifferentialInfoEstimator\nVasicek(definition = Shannon(); m::Int = 1)\n\nThe Vasicek estimator computes the Shannon differential information of a timeseries using the method from Vasicek (1976), with logarithms to the base specified in definition.\n\nThe Vasicek estimator belongs to a class of differential entropy estimators based on order statistics, of which Vasicek (1976) was the first. It only works for timeseries input.\n\nDescription\n\nAssume we have samples barX = x_1 x_2 ldots x_N  from a continuous random variable X in mathbbR with support mathcalX and density functionf  mathbbR to mathbbR. Vasicek estimates the Shannon differential entropy\n\nH(X) = int_mathcalX f(x) log f(x) dx = mathbbE-log(f(X))\n\nHowever, instead of estimating the above integral directly, it makes use of the equivalent integral, where F is the distribution function for X,\n\nH(X) = int_0^1 log left(dfracddpF^-1(p) right) dp\n\nThis integral is approximated by first computing the order statistics of barX (the input timeseries), i.e. x_(1) leq x_(2) leq cdots leq x_(n). The Vasicek Shannon differential entropy estimate is then\n\nhatH_V(barX m) =\ndfrac1n\nsum_i = 1^n log left dfracn2m (barX_(i+m) - barX_(i-m)) right\n\nUsage\n\nIn practice, choice of m influences how fast the entropy converges to the true value. For small value of m, convergence is slow, so we recommend to scale m according to the time series length n and use m >= n/100 (this is just a heuristic based on the tests written for this package).\n\nSee also: information, Correa, AlizadehArghami, Ebrahimi, DifferentialInfoEstimator.\n\n\n\n\n\n","category":"type"},{"location":"information_measures/#ComplexityMeasures.AlizadehArghami","page":"Information measures (entropies and co.)","title":"ComplexityMeasures.AlizadehArghami","text":"AlizadehArghami <: DifferentialInfoEstimator\nAlizadehArghami(definition = Shannon(); m::Int = 1)\n\nThe AlizadehArghami estimator computes the Shannon differential information of a timeseries using the method from Alizadeh and Arghami (2010), with logarithms to the base specified in definition.\n\nThe AlizadehArghami estimator belongs to a class of differential entropy estimators based on order statistics. It only works for timeseries input.\n\nDescription\n\nAssume we have samples barX = x_1 x_2 ldots x_N  from a continuous random variable X in mathbbR with support mathcalX and density functionf  mathbbR to mathbbR. AlizadehArghami estimates the Shannon differential entropy\n\nH(X) = int_mathcalX f(x) log f(x) dx = mathbbE-log(f(X))\n\nHowever, instead of estimating the above integral directly, it makes use of the equivalent integral, where F is the distribution function for X:\n\nH(X) = int_0^1 log left(dfracddpF^-1(p) right) dp\n\nThis integral is approximated by first computing the order statistics of barX (the input timeseries), i.e. x_(1) leq x_(2) leq cdots leq x_(n). The AlizadehArghami Shannon differential entropy estimate is then the the Vasicek estimate hatH_V(barX m n), plus a correction factor\n\nhatH_A(barX m n) = hatH_V(barX m n) +\ndfrac2nleft(m log(2) right)\n\nSee also: information, Correa, Ebrahimi, Vasicek, DifferentialInfoEstimator.\n\n\n\n\n\n","category":"type"},{"location":"information_measures/#ComplexityMeasures.Ebrahimi","page":"Information measures (entropies and co.)","title":"ComplexityMeasures.Ebrahimi","text":"Ebrahimi <: DifferentialInfoEstimator\nEbrahimi(definition = Shannon(); m::Int = 1)\n\nThe Ebrahimi estimator computes the Shannon information of a timeseries using the method from Ebrahimi et al. (1994), with logarithms to the base specified in definition.\n\nThe Ebrahimi estimator belongs to a class of differential entropy estimators based on order statistics. It only works for timeseries input.\n\nDescription\n\nAssume we have samples barX = x_1 x_2 ldots x_N  from a continuous random variable X in mathbbR with support mathcalX and density functionf  mathbbR to mathbbR. Ebrahimi estimates the Shannon differential entropy\n\nH(X) = int_mathcalX f(x) log f(x) dx = mathbbE-log(f(X))\n\nHowever, instead of estimating the above integral directly, it makes use of the equivalent integral, where F is the distribution function for X,\n\nH(X) = int_0^1 log left(dfracddpF^-1(p) right) dp\n\nThis integral is approximated by first computing the order statistics of barX (the input timeseries), i.e. x_(1) leq x_(2) leq cdots leq x_(n). The Ebrahimi Shannon differential entropy estimate is then\n\nhatH_E(barX m) =\ndfrac1n sum_i = 1^n log\nleft dfracnc_i m (barX_(i+m) - barX_(i-m)) right\n\nwhere\n\nc_i =\nbegincases\n    1 + fraci - 1m  1 geq i geq m \n    2                     m + 1 geq i geq n - m \n    1 + fracn - im  n - m + 1 geq i geq n\nendcases\n\nSee also: information, Correa, AlizadehArghami, Vasicek, DifferentialInfoEstimator.\n\n\n\n\n\n","category":"type"},{"location":"information_measures/#ComplexityMeasures.Correa","page":"Information measures (entropies and co.)","title":"ComplexityMeasures.Correa","text":"Correa <: DifferentialInfoEstimator\nCorrea(definition = Shannon(); m::Int = 1)\n\nThe Correa estimator computes the Shannon differential information of a timeseries using the method from Correa (1995), with logarithms to the base specified in definition.\n\nThe Correa estimator belongs to a class of differential entropy estimators based on order statistics. It only works for timeseries input.\n\nDescription\n\nAssume we have samples barX = x_1 x_2 ldots x_N  from a continuous random variable X in mathbbR with support mathcalX and density functionf  mathbbR to mathbbR. Correa estimates the Shannon differential entropy\n\nH(X) = int_mathcalX f(x) log f(x) dx = mathbbE-log(f(X))\n\nHowever, instead of estimating the above integral directly, Correa makes use of the equivalent integral, where F is the distribution function for X,\n\nH(X) = int_0^1 log left(dfracddpF^-1(p) right) dp\n\nThis integral is approximated by first computing the order statistics of barX (the input timeseries), i.e. x_(1) leq x_(2) leq cdots leq x_(n), ensuring that end points are included. The Correa estimate of Shannon differential entropy is then\n\nH_C(barX m n) =\ndfrac1n sum_i = 1^n log\nleft dfrac sum_j=i-m^i+m(barX_(j) -\ntildeX_(i))(j - i)n sum_j=i-m^i+m (barX_(j) - tildeX_(i))^2\nright\n\nwhere\n\ntildeX_(i) = dfrac12m + 1 sum_j = i - m^i + m X_(j)\n\nSee also: information, AlizadehArghami, Ebrahimi, Vasicek, DifferentialInfoEstimator.\n\n\n\n\n\n","category":"type"},{"location":"information_measures/#table_diff_ent_est","page":"Information measures (entropies and co.)","title":"Table of differential information measure estimators","text":"","category":"section"},{"location":"information_measures/","page":"Information measures (entropies and co.)","title":"Information measures (entropies and co.)","text":"The following estimators are differential information measure estimators, and can also be used with information.","category":"page"},{"location":"information_measures/","page":"Information measures (entropies and co.)","title":"Information measures (entropies and co.)","text":"Each DifferentialInfoEstimators uses a specialized technique to approximate relevant densities/integrals, and is often tailored to one or a few types of information measures. For example, Kraskov estimates the Shannon entropy.","category":"page"},{"location":"information_measures/","page":"Information measures (entropies and co.)","title":"Information measures (entropies and co.)","text":"Estimator Principle Input data Shannon Renyi Tsallis Kaniadakis Curado StretchedExponential\nKozachenkoLeonenko Nearest neighbors StateSpaceSet ✓ x x x x x\nKraskov Nearest neighbors StateSpaceSet ✓ x x x x x\nZhu Nearest neighbors StateSpaceSet ✓ x x x x x\nZhuSingh Nearest neighbors StateSpaceSet ✓ x x x x x\nGao Nearest neighbors StateSpaceSet ✓ x x x x x\nGoria Nearest neighbors StateSpaceSet ✓ x x x x x\nLord Nearest neighbors StateSpaceSet ✓ x x x x x\nVasicek Order statistics Vector ✓ x x x x x\nEbrahimi Order statistics Vector ✓ x x x x x\nCorrea Order statistics Vector ✓ x x x x x\nAlizadehArghami Order statistics Vector ✓ x x x x x","category":"page"},{"location":"#ComplexityMeasures.jl","page":"ComplexityMeasures.jl","title":"ComplexityMeasures.jl","text":"","category":"section"},{"location":"","page":"ComplexityMeasures.jl","title":"ComplexityMeasures.jl","text":"ComplexityMeasures","category":"page"},{"location":"#ComplexityMeasures","page":"ComplexityMeasures.jl","title":"ComplexityMeasures","text":"ComplexityMeasures.jl\n\n(Image: docsdev) (Image: docsstable) (Image: CI) (Image: codecov) (Image: Package Downloads) (Image: Package Downloads) (Image: DOI)\n\nA Julia package that provides:\n\nA rigorous framework for extracting probabilities from data, based on the mathematical formulation of probability spaces.\nSeveral (12+) outcome spaces, i.e., ways to discretize data into probabilities.\nSeveral estimators for estimating probabilities given an outcome space, which correct theoretically known estimation biases.\nSeveral definitions of information measures, such as various flavours of entropies (Shannon, Tsallis, Curado...), extropies, and probability-based complexity measures, that are used in the context of nonlinear dynamics, nonlinear timeseries analysis, and complex systems.\nSeveral discrete and continuous (differential) estimators for entropies, which correct theoretically known estimation biases.\nEstimators for other complexity measures that are not estimated based on probability functions.\nAn extendable interface and well thought out API accompanied by dedicated developer documentation pages. These makes it trivial to define new outcome spaces, or new estimators for probabilities, information measures, or complexity measures and integrate them with everything else in the software.\n\nComplexityMeasures.jl can be used as a standalone package, or as part of other projects in the JuliaDynamics organization, such as DynamicalSystems.jl or CausalityTools.jl.\n\nTo install it, run import Pkg; Pkg.add(\"ComplexityMeasures\").\n\nAll further information is provided in the documentation, which you can either find online or build locally by running the docs/make.jl file.\n\nPreviously, this package was called Entropies.jl.\n\n\n\n\n\n","category":"module"},{"location":"#Latest-news","page":"ComplexityMeasures.jl","title":"Latest news","text":"","category":"section"},{"location":"","page":"ComplexityMeasures.jl","title":"ComplexityMeasures.jl","text":"ComplexityMeasures.jl has been updated to v3!","category":"page"},{"location":"","page":"ComplexityMeasures.jl","title":"ComplexityMeasures.jl","text":"The software has been massively improved and its core principles were redesigned to be extendable, accessible, and more closely based on the rigorous mathematics of probabilities and entropies.","category":"page"},{"location":"","page":"ComplexityMeasures.jl","title":"ComplexityMeasures.jl","text":"For more details of this new release, please see our announcement post on discourse or the central Tutorial of the v3 documentation.","category":"page"},{"location":"","page":"ComplexityMeasures.jl","title":"ComplexityMeasures.jl","text":"In this v3 many concepts were renamed, but there is no formally breaking change. Everything that changed has been deprecated and is backwards compatible. You can see the CHANGELOG.md for more details!","category":"page"},{"location":"#Documentation-contents","page":"ComplexityMeasures.jl","title":"Documentation contents","text":"","category":"section"},{"location":"","page":"ComplexityMeasures.jl","title":"ComplexityMeasures.jl","text":"Before anything else, we recommend users to go through our overarching Tutorial, which teaches not only central API functions, but also terminology and crucial core concepts:\nProbabilities lists all outcome spaces and probabilities estimators.\nInformation measures lists all implemented information measure definitions and estimators (both discrete and differential).\nComplexity measures lists all implemented complexity measures that are not functionals of probabilities (unlike information measures).\nThe Examples page lists dozens of runnable example code snippets along with their outputs.","category":"page"},{"location":"#input_data","page":"ComplexityMeasures.jl","title":"Input data for ComplexityMeasures.jl","text":"","category":"section"},{"location":"","page":"ComplexityMeasures.jl","title":"ComplexityMeasures.jl","text":"The input data type typically depend on the outcome space chosen. In general though, the standard DynamicalSystems.jl approach is taken and as such we have three types of input data:","category":"page"},{"location":"","page":"ComplexityMeasures.jl","title":"ComplexityMeasures.jl","text":"Timeseries, which are AbstractVector{<:Real}, used in e.g. with WaveletOverlap.\nMulti-variate timeseries, or datasets, or state space sets, which are StateSpaceSets, used e.g. with NaiveKernel. The short syntax SSSet may be used instead of StateSpaceSet.\nSpatial data, which are higher dimensional standard Arrays, used e.g. with  SpatialOrdinalPatterns.","category":"page"},{"location":"","page":"ComplexityMeasures.jl","title":"ComplexityMeasures.jl","text":"StateSpaceSet","category":"page"},{"location":"#StateSpaceSets.StateSpaceSet","page":"ComplexityMeasures.jl","title":"StateSpaceSets.StateSpaceSet","text":"StateSpaceSet{D, T} <: AbstractStateSpaceSet{D,T}\n\nA dedicated interface for sets in a state space. It is an ordered container of equally-sized points of length D. Each point is represented by SVector{D, T}. The data are a standard Julia Vector{SVector}, and can be obtained with vec(ssset::StateSpaceSet). Typically the order of points in the set is the time direction, but it doesn't have to be.\n\nWhen indexed with 1 index, StateSpaceSet is like a vector of points. When indexed with 2 indices it behaves like a matrix that has each of the columns be the timeseries of each of the variables. When iterated over, it iterates over its contained points. See description of indexing below for more.\n\nStateSpaceSet also supports almost all sensible vector operations like append!, push!, hcat, eachrow, among others.\n\nDescription of indexing\n\nIn the following let i, j be integers, typeof(X) <: AbstractStateSpaceSet and v1, v2 be <: AbstractVector{Int} (v1, v2 could also be ranges, and for performance benefits make v2 an SVector{Int}).\n\nX[i] == X[i, :] gives the ith point (returns an SVector)\nX[v1] == X[v1, :], returns a StateSpaceSet with the points in those indices.\nX[:, j] gives the jth variable timeseries (or collection), as Vector\nX[v1, v2], X[:, v2] returns a StateSpaceSet with the appropriate entries (first indices being \"time\"/point index, while second being variables)\nX[i, j] value of the jth variable, at the ith timepoint\n\nUse Matrix(ssset) or StateSpaceSet(matrix) to convert. It is assumed that each column of the matrix is one variable. If you have various timeseries vectors x, y, z, ... pass them like StateSpaceSet(x, y, z, ...). You can use columns(dataset) to obtain the reverse, i.e. all columns of the dataset in a tuple.\n\n\n\n\n\n","category":"type"},{"location":"devdocs/#ComplexityMeasures.jl-Dev-Docs","page":"ComplexityMeasures.jl Dev Docs","title":"ComplexityMeasures.jl Dev Docs","text":"","category":"section"},{"location":"devdocs/","page":"ComplexityMeasures.jl Dev Docs","title":"ComplexityMeasures.jl Dev Docs","text":"Good practices in developing a code base apply in every Pull Request. The Good Scientific Code Workshop is worth checking out for this.","category":"page"},{"location":"devdocs/","page":"ComplexityMeasures.jl Dev Docs","title":"ComplexityMeasures.jl Dev Docs","text":"All PRs contributing new functionality must be well tested and well documented. You only need to add tests for methods that you explicitly extended.","category":"page"},{"location":"devdocs/#Adding-a-new-OutcomeSpace","page":"ComplexityMeasures.jl Dev Docs","title":"Adding a new OutcomeSpace","text":"","category":"section"},{"location":"devdocs/#Mandatory-steps","page":"ComplexityMeasures.jl Dev Docs","title":"Mandatory steps","text":"","category":"section"},{"location":"devdocs/","page":"ComplexityMeasures.jl Dev Docs","title":"ComplexityMeasures.jl Dev Docs","text":"Decide on the outcome space and how the estimator will map probabilities to outcomes.\nDefine your type and make it subtype OutcomeSpace.\nAdd a docstring to your type following the style of the docstrings of other estimators.\nIf suitable, the estimator may be able to operate based on Encodings. If so,  it is preferred to implement an Encoding subtype and extend the methods  encode and decode. This will allow your outcome space to be used  with a larger span of entropy and complexity methods without additional effort.  Have a look at the file defining OrdinalPatterns for an idea of how this  works.","category":"page"},{"location":"devdocs/","page":"ComplexityMeasures.jl Dev Docs","title":"ComplexityMeasures.jl Dev Docs","text":"If your new outcome space is counting-based, then","category":"page"},{"location":"devdocs/","page":"ComplexityMeasures.jl Dev Docs","title":"ComplexityMeasures.jl Dev Docs","text":"Implement dispatch for counts_and_outcomes for your OutcomeSpace  type. If the outcomes do not come for free, then instead you can extend  counts and then explicitly add another method for  counts_and_outcomes that calls counts first and then decodes  the outcomes. Follow existing implementations for guidelines (see for example source  code for Dispersion).\nImplement dispatch for codify. This will ensure that the outcome space  also works automatically with any discrete estimators in the downstream CausalityTools.jl.","category":"page"},{"location":"devdocs/","page":"ComplexityMeasures.jl Dev Docs","title":"ComplexityMeasures.jl Dev Docs","text":"If your new outcome space is not counting-based, then","category":"page"},{"location":"devdocs/","page":"ComplexityMeasures.jl Dev Docs","title":"ComplexityMeasures.jl Dev Docs","text":"Implement dispatch for probabilities_and_outcomes for your  OutcomeSpace type. If the outcomes do not come for free, then instead you  can extend probabilities and then explicitly add another method for  probabilities_and_outcomes that calls probabilities first and  then decodes the outcomes.  Follow existing implementations for guidelines (see for example source code for  NaiveKernel).","category":"page"},{"location":"devdocs/","page":"ComplexityMeasures.jl Dev Docs","title":"ComplexityMeasures.jl Dev Docs","text":"Finally,","category":"page"},{"location":"devdocs/","page":"ComplexityMeasures.jl Dev Docs","title":"ComplexityMeasures.jl Dev Docs","text":"Implement dispatch for outcome_space and your OutcomeSpace type.  The return value of outcome_space must be sorted (as in the default behavior of  sort, in ascending order).\nAdd your outcome space type to the table list in the documentation string of  OutcomeSpace. If you made an encoding, also add it to corresponding table  in the encodings section.","category":"page"},{"location":"devdocs/#Optional-steps","page":"ComplexityMeasures.jl Dev Docs","title":"Optional steps","text":"","category":"section"},{"location":"devdocs/","page":"ComplexityMeasures.jl Dev Docs","title":"ComplexityMeasures.jl Dev Docs","text":"The following methods may be extended for your OutcomeSpace if doing so leads to performance benefits.","category":"page"},{"location":"devdocs/","page":"ComplexityMeasures.jl Dev Docs","title":"ComplexityMeasures.jl Dev Docs","text":"total_outcomes. By default it returns the length of outcome_space.  This is the function that most typically has performance benefits if implemented  explicitly, so most existing estimators extend it by default.","category":"page"},{"location":"devdocs/#Adding-a-new-[ProbabilitiesEstimator](@ref)","page":"ComplexityMeasures.jl Dev Docs","title":"Adding a new ProbabilitiesEstimator","text":"","category":"section"},{"location":"devdocs/#Mandatory-steps-2","page":"ComplexityMeasures.jl Dev Docs","title":"Mandatory steps","text":"","category":"section"},{"location":"devdocs/","page":"ComplexityMeasures.jl Dev Docs","title":"ComplexityMeasures.jl Dev Docs","text":"Define your type and make it subtype ProbabilitiesEstimator.\nAdd a docstring to your type following the style of the docstrings of other  ProbabilitiesEstimators.\nImplement dispatch for probabilities for your  ProbabilitiesEstimator type. You'll then get  probabilities_and_outcomes for free.\nImplement dispatch for allprobabilities_and_outcomes for your  ProbabilitiesEstimator type. \nAdd your new ProbabilitiesEstimator type to the list of probabilities  estimators in the probabilities estimators documentation section.","category":"page"},{"location":"devdocs/#Adding-a-new-InformationMeasureEstimator","page":"ComplexityMeasures.jl Dev Docs","title":"Adding a new InformationMeasureEstimator","text":"","category":"section"},{"location":"devdocs/","page":"ComplexityMeasures.jl Dev Docs","title":"ComplexityMeasures.jl Dev Docs","text":"The type implementation should follow the declared API of InformationMeasureEstimator. If the type is a discrete measure, then extend information(e::YourType, p::Probabilities). If it is a differential measure, then extend information(e::YourType, x::InputData).","category":"page"},{"location":"devdocs/","page":"ComplexityMeasures.jl Dev Docs","title":"ComplexityMeasures.jl Dev Docs","text":"InformationMeasureEstimator","category":"page"},{"location":"devdocs/#ComplexityMeasures.InformationMeasureEstimator","page":"ComplexityMeasures.jl Dev Docs","title":"ComplexityMeasures.InformationMeasureEstimator","text":"InformationMeasureEstimator{I <: InformationMeasure}\n\nThe supertype of all information measure estimators. Its direct subtypes are DiscreteInfoEstimator and DifferentialInfoEstimator.\n\nSince all estimators must reference a measure definition in some way, we made the following interface decisions:\n\nall estimators have as first type parameter I <: InformationMeasure\nall estimators reference the information measure in a definition field\nall estimators are defined using Base.@kwdef so that they may be initialized with the syntax Estimator(; definition = Shannon()) (or any other).\n\nAny concrete subtypes must follow the above, e.g.:\n\nBase.@kwdef struct MyEstimator{I <: InformationMeasure, X} <: DiscreteInfoEstimator{I}\n    definition::I\n    x::X\nend\n\ninfo: Why separate the *definition* of a measure from *estimators* of a measure?\nIn real applications, we generally don't have access to the underlying probability mass functions or densities required to compute the various entropy or extropy definitons. Therefore, these information measures must be estimated from finite data. Estimating a particular measure (e.g. Shannon entropy) can be done in many ways, each with its own own pros and cons. We aim to provide a complete library of literature estimators of the various information measures (PRs are welcome!).\n\n\n\n\n\n","category":"type"},{"location":"devdocs/#Adding-a-new-InformationMeasure","page":"ComplexityMeasures.jl Dev Docs","title":"Adding a new InformationMeasure","text":"","category":"section"},{"location":"devdocs/","page":"ComplexityMeasures.jl Dev Docs","title":"ComplexityMeasures.jl Dev Docs","text":"This amounts to adding a new definition of an information measure, not an estimator. It de-facto means adding a method for the discrete Plug-In estimator.","category":"page"},{"location":"devdocs/#Mandatory-steps-3","page":"ComplexityMeasures.jl Dev Docs","title":"Mandatory steps","text":"","category":"section"},{"location":"devdocs/","page":"ComplexityMeasures.jl Dev Docs","title":"ComplexityMeasures.jl Dev Docs","text":"Define your information measure definition type and make it subtype InformationMeasure.\nImplement dispatch for information(def::YourType, p::Probabilities). This is the Plug-In estimator for the discrete measure.\nAdd a docstring to your type following the style of the docstrings of other information  measure definitions, and should include the mathematical definition of the measure.\nAdd your information measure definition type to the list of definitions in the  docs/src/information_measures.md documentation page.\nAdd a reference to your information measure definition in the docstring for  InformationMeasure.","category":"page"},{"location":"devdocs/#Optional-steps-2","page":"ComplexityMeasures.jl Dev Docs","title":"Optional steps","text":"","category":"section"},{"location":"devdocs/","page":"ComplexityMeasures.jl Dev Docs","title":"ComplexityMeasures.jl Dev Docs","text":"If the maximum value of your information measure type is analytically computable for a  probability distribution with a known number of elements, implementing dispatch for  information_maximum automatically enables information_normalized  for your type.","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"EditURL = \"tutorial.jl\"","category":"page"},{"location":"tutorial/#Tutorial","page":"Tutorial","title":"Tutorial","text":"","category":"section"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"The goal of this tutorial is threefold:","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"To convey the terminology used by ComplexityMeasures.jl: key terms, what they mean, and how they are used within the codebase.\nTo provide a rough overview of the overall features provided by ComplexityMeasures.jl.\nTo introduce the main API functions of ComplexityMeasures.jl in a single, self-contained document: how these functions connect to key terms, what are their main inputs and outputs, and how they are used in realistic scientific scripting.","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"note: Note\nThe documentation and exposition of ComplexityMeasures.jl is inspired by chapter 5 of Nonlinear Dynamics, Datseris & Parlitz, Springer 2022 (Datseris and Parlitz, 2022), and expanded to cover more content.","category":"page"},{"location":"tutorial/#First-things-first:-\"complexity-measures\"","page":"Tutorial","title":"First things first: \"complexity measures\"","text":"","category":"section"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"\"Complexity measure\" is a generic, umbrella term, used extensively in the nonlinear timeseries analysis (NLTS) literature. Roughly speaking, a complexity measure is a quantity extracted from input data that quantifies some dynamical property in the data (often, complexity measures are entropy variants). These complexity measures can highlight some aspects of the dynamics more than others, or distinguish one type of dynamics from another, or classify timeseries into classes with different dynamics, among other things. Typically, more \"complex\" data have higher complexity measure value. ComplexityMeasures.jl implements hundreds such measures, and hence it is named as such. To enable this, ComplexityMeasures.jl is more than a collection \"dynamic statistics\": it is also a framework for rigorously defining probability spaces and estimating probabilities from input data.","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"Within the codebase of ComplexityMeasures.jl we make a separation with the functions information (or its daughter function entropy) and complexity. We use information for complexity measures that are explicit functionals of probability mass or probability density functions, even though these measures might not be labelled as \"information measures\" in the literature. We use complexity for other complexity measures that are not explicit functionals of probabilities. We stress that the separation between information and complexity is purely pragmatic, to establish a generic and extendable software interface within ComplexityMeasures.jl.","category":"page"},{"location":"tutorial/#The-basics:-probabilities-and-outcome-spaces","page":"Tutorial","title":"The basics: probabilities and outcome spaces","text":"","category":"section"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"Information measures and some other complexity measures are computed based on probabilities derived from input data. In order to derive probabilities from data, an outcome space (also called a sample space) needs to be defined: a way to transform data into elements omega of an outcome space omega in Omega, and assign probabilities to each outcome p(omega), such that p(Omega)=1. omega are called outcomes or events. In code, outcome spaces are subtypes of OutcomeSpace. For example, one outcome space is the ValueBinning, which is the most commonly known outcome space, and corresponds to discretizing data by putting the data values into bins of a specific size.","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"using ComplexityMeasures\n\nx = randn(10_000)\nε = 0.1 # bin width\no = ValueBinning(ε)\no isa OutcomeSpace","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"Such outcome spaces may be given to probabilities_and_outcomes to estimate the probabilities and corresponding outcomes from input data:","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"probs, outs = probabilities_and_outcomes(o, x);\nprobs","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"In this example the probabilities are the (normalized) heights of each bin of the histogram. The bins, which are the elements of the outcome space, are shown in the margin, left of the probabilities. They are also returned explicitly as outs above.","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"This convenience printing syntax with outcomes and probabilities is useful for visual inspection of the probabilities data. However, don't let it worry you. Probabilities are returned as a special Probabilities type that behaves identically to a standard Julia numerical Vector. You can obtain the maximum probability","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"maximum(probs)","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"or iterate over the probabilities","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"function total(probs)\n    t = 0.0\n    for p in probs\n        t += p\n    end\n    return t\nend\n\ntotal(probs)","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"Notice that if you use probabilities instead of probabilities_and_outcomes, then outcomes are enumerated generically. This avoids computing outcomes explicitly, and can save some computation time in cases where you don't need the outcomes.","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"probs2 = probabilities(o, x)","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"Of course, if the outcomes are obtained for free while estimating the probabilities, they would be included in the return value.","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"For the ValueBinning example that we use, the outcomes are the left edges of each bin. This allows us to straightforwardly visualize the results.","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"using CairoMakie\nouts = outcomes(probs);\nleft_edges = only.(outs) # convert `Vector{SVector}` into `Vector{Real}`\nbarplot(left_edges, probs; axis = (ylabel = \"probability\",))","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"Naturally, there are other outcome spaces one may use, and one can find the list of implemented ones in OutcomeSpace. A prominent example used in the NLTS literature are ordinal patterns. The outcome space for it is OrdinalPatterns, and can be particularly useful with timeseries that come from nonlinear dynamical systems. For example, let's simulate a logistic map timeseries.","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"using DynamicalSystemsBase\n\nlogistic_rule(u, r, t) = SVector(r*u[1]*(1 - u[1]))\nds = DeterministicIteratedMap(logistic_rule, [0.4], 4.0)\nY, t = trajectory(ds, 10_000; Ttr = 100)\ny = Y[:, 1]\nsummary(y)","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"We can then estimate the probabilities corresponding to the ordinal patterns of a certain length (here we use m = 3).","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"o = OrdinalPatterns{3}()\nprobsy = probabilities(o, y)","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"Comparing these probabilities with those for the purely random timeseries x,","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"probsx = probabilities(o, x)","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"you will notice that the probabilities computing from x has six outcomes, while the probabilities computed from the timeseries y has five outcomes.","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"The reason that there are less outcomes in the y is because one outcome was never encountered in the y data. This is a common theme in ComplexityMeasures.jl: outcomes that are not in the data are skipped. This can save memory for outcome spaces with large numbers of outcomes. To explicitly obtain all outcomes, by assigning 0 probability to not encountered outcomes, use allprobabilities_and_outcomes. For OrdinalPatterns the outcome space does not depend on input data and is always the same. Hence, the corresponding outcomes matching to allprobabilities_and_outcomes, coincide for x and y, and also coincide with the output of the function outcome_space:","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"o = OrdinalPatterns()\nprobsx = allprobabilities(o, x)\nprobsy = allprobabilities(o, y)\noutsx = outsy = outcome_space(o)\n# display all quantities as parallel columns\nhcat(outsx, probsx, probsy)","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"The number of possible outcomes, i.e., the cardinality of the outcome space, can always be found using total_outcomes:","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"total_outcomes(o)","category":"page"},{"location":"tutorial/#Beyond-the-basics:-probabilities-*estimators*","page":"Tutorial","title":"Beyond the basics: probabilities estimators","text":"","category":"section"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"So far we have been estimating probabilities by counting the amount of times each possible outcome was encountered in the data, then normalizing. This is called \"maximum likelihood estimation\" of probabilities. To get the counts themselves, use the counts or counts_and_outcomes function.","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"countsy = counts(o, y)","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"Counts are printed like Probabilities: they display the outcomes they match to on the left marginal, but otherwise can be used as standard Julia numerical Vectors. To go from outcomes to probabilities, we divide with the total:","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"probsy = probabilities(o, y)\noutsy = outcomes(probsy)\nhcat(outsy, countsy, countsy ./ sum(countsy), probsy)","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"By definition, columns 3 and 4 are identical. However, there are other ways to estimate probabilities that may account for biases in counting outcomes from finite data. Alternative estimators for probabilities are subtypes of ProbabilitiesEstimator. ProbabilitiesEstimators  dictate alternative ways to estimate probabilities, given some outcome space and unput data. For example, one could use BayesianRegularization.","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"probsy_bayes = probabilities(BayesianRegularization(), o, y)\n\nprobsy_bayes .- probsy","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"While the corrections of BayesianRegularization are small in this case, they are nevertheless measurable.","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"When calling probabilities only with an outcome space instance and some input data (skipping the ProbabilitiesEstimator), then by default, the RelativeAmount probabilities estimator is used to extract the probabilities.","category":"page"},{"location":"tutorial/#Entropies","page":"Tutorial","title":"Entropies","text":"","category":"section"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"Many compexity measures are a straightforward estimation of Shannon entropy, computed over probabilities estimated from data over some particular outcome space. For example, the well known permutation entropy (Bandt and Pompe, 2002) is exactly the Shannon entropy of the probabilities probsy we computed above based on ordinal patterns. To compute it, we use the entropy function.","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"perm_ent_x = entropy(OrdinalPatterns(), x)\nperm_ent_y = entropy(OrdinalPatterns(), y)\n(perm_ent_x, perm_ent_y)","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"As expected, the permutation entropy of the x signal is higher, because the signal is \"more random\". Moreover, since we have estimated the probabilities already, we could have passed these to the entropy function directly instead of recomputing them as above","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"perm_ent_y_2 = entropy(probsy)","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"We crucially realize here that many quantities in the NLTS literature that are named as entropies, such as \"permutation entropy\", are not really new entropies. They are the good old Shannon entropy (Shannon), but calculated with new outcome spaces that smartly quantify some dynamic property in the data. Nevertheless, we acknowledge that names such as \"permutation entropy\" are commonplace, so in ComplexityMeasures.jl we provide convenience functions like entropy_permutation. More convenience functions can be found in the convenience documentation page.","category":"page"},{"location":"tutorial/#Beyond-Shannon:-more-entropies","page":"Tutorial","title":"Beyond Shannon: more entropies","text":"","category":"section"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"Just like there are different outcome spaces, the same concept applies to entropy. There are many fundamentally different entropies. Shannon entropy is not the only one, just the one used most often. Each entropy is a subtype of InformationMeasure. Another commonly used entropy is the Renyi or generalized entropy. We can use Renyi as an additional first argument to the entropy function to estimate the generalized entropy:","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"perm_ent_y_q2 = entropy(Renyi(q = 2.0), OrdinalPatterns(), y)\n(perm_ent_y_q2, perm_ent_y)","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"In fact, when we called entropy(OrdinalPatterns(), y), this dispatched to the default call of entropy(Shannon(), OrdinalPatterns(), y).","category":"page"},{"location":"tutorial/#Beyond-entropies:-discrete-estimators","page":"Tutorial","title":"Beyond entropies: discrete estimators","text":"","category":"section"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"The estimation of an entropy truly parallelizes the estimation of probabilities: in the latter, we could decide an outcome space and an estimator to estimate probabilities. The same happens for entropy: we can decide an entropy definition and an estimator of how to estimate the entropy. For example, instead of the default PlugIn estimator that we used above implicitly, we could use the Jackknife estimator.","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"ospace = OrdinalPatterns()\nentdef = Renyi(q = 2.0)\nentest = Jackknife(entdef)\nperm_ent_y_q2_jack = entropy(entest, ospace, y)\n\n(perm_ent_y_q2, perm_ent_y_q2_jack)","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"Entropy estimators always reference an entropy definition, even if they only apply to one type of entropy (typically the Shannon one). From here, it is up to the researcher to read the documentation of the plethora of estimators implemented and decide what is most suitable for their data at hand. They all can be found in DiscreteInfoEstimator.","category":"page"},{"location":"tutorial/#Beyond-entropies:-other-information-measures","page":"Tutorial","title":"Beyond entropies: other information measures","text":"","category":"section"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"Recall that at the very beginning of this notebook we mentioned a code separation of information and complexity. We did this because there are other measures, besides entropy, that are explicit functionals of some probability mass function. One example is the Shannon extropy ShannonExtropy, the complementary dual of entropy, which could be computed as follows.","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"extdef = ShannonExtropy()\nperm_ext_y = information(extdef, ospace, y)","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"Just like the Shannon entropy, the extropy could also be estimated with a different estimator such as Jackknife.","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"perm_ext_y_jack = information(Jackknife(extdef), ospace, y)","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"In truth, when we called entropy(e, o, y) it actually calls information(e, o, y), as all \"information measures\" are part of the same function interface. And entropy is an information measure.","category":"page"},{"location":"tutorial/#Beyond-discrete:-differential-or-continuous","page":"Tutorial","title":"Beyond discrete: differential or continuous","text":"","category":"section"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"Discrete information measures are functions of probability mass functions. It is also possible to compute information measures of probability density functions. In ComplexityMeasures.jl, this is done by calling entropy (or the more general information) with a differential information estimator, a subtype of DifferentialInfoEstimator. These estimators are given directly to information without assigning an outcome space, because the probability density is approximated implicitly, not explicitly. For example, the Correa estimator approximates the differential Shannon entropy by utilizing order statistics of the timeseries data:","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"diffest = Correa()\ndiffent = entropy(diffest, x)","category":"page"},{"location":"tutorial/#Beyond-information:-other-complexity-measures","page":"Tutorial","title":"Beyond information: other complexity measures","text":"","category":"section"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"As discussed at the very beginning of this tutorial, there are some complexity measures that are not explicit functionals of probabilities, and hence cannot be straightforwardly related to an outcome space, in the sense of providing an instance of OutcomeSpace to the estimation function. These are estimated with the complexity function, by providing it a subtype of ComplexityEstimator. An example here is the well-known sample entropy (which isn't actually an entropy in the formal mathematical sense). It can be computed as follows.","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"complest = SampleEntropy(r = 0.1)\nsampent = complexity(complest, y)","category":"page"}]
}
