<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Weighted permutation (symbolic) · Entropies.jl</title><link href="https://fonts.googleapis.com/css?family=Lato|Roboto+Mono" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.11.2/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.11.2/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.11.2/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.11.1/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL=".."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../assets/documenter.js"></script><script src="../siteinfo.js"></script><script src="../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-dark.css" data-theme-name="documenter-dark"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../assets/themeswap.js"></script><link href="https://fonts.googleapis.com/css?family=Montserrat|Source+Code+Pro&amp;display=swap" rel="stylesheet" type="text/css"/></head><body><div id="documenter"><nav class="docs-sidebar"><div class="docs-package-name"><span class="docs-autofit">Entropies.jl</span></div><form class="docs-search" action="../search/"><input class="docs-search-query" id="documenter-search-query" name="q" type="text" placeholder="Search docs"/></form><ul class="docs-menu"><li><a class="tocitem" href="../">Documentation</a></li><li><a class="tocitem" href="../generalized_entropy/">Generalized entropy</a></li><li><a class="tocitem" href="../histogram_estimation/">Histrogram estimation</a></li><li><span class="tocitem">Entropy/probability estimators</span><ul><li><a class="tocitem" href="../SymbolicPermutation/">Permutation (symbolic)</a></li><li class="is-active"><a class="tocitem" href>Weighted permutation (symbolic)</a></li><li><a class="tocitem" href="../VisitationFrequency/">Visitation frequency</a></li></ul></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><nav class="breadcrumb"><ul class="is-hidden-mobile"><li><a class="is-disabled">Entropy/probability estimators</a></li><li class="is-active"><a href>Weighted permutation (symbolic)</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>Weighted permutation (symbolic)</a></li></ul></nav><div class="docs-right"><a class="docs-edit-link" href="https://github.com/kahaaga/Entropies.jl/blob/master/docs/src/SymbolicWeightedPermutation.md" title="Edit on GitHub"><span class="docs-icon fab"></span><span class="docs-label is-hidden-touch">Edit on GitHub</span></a><a class="docs-settings-button fas fa-cog" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-sidebar-button fa fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a></div></header><article class="content" id="documenter-page"><h1 id="Weighted-permutation-(symbolic)"><a class="docs-heading-anchor" href="#Weighted-permutation-(symbolic)">Weighted permutation (symbolic)</a><a id="Weighted-permutation-(symbolic)-1"></a><a class="docs-heading-anchor-permalink" href="#Weighted-permutation-(symbolic)" title="Permalink"></a></h1><article class="docstring"><header><a class="docstring-binding" id="Entropies.SymbolicWeightedPermutation" href="#Entropies.SymbolicWeightedPermutation"><code>Entropies.SymbolicWeightedPermutation</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia">SymbolicWeightedPermutation &lt;: PermutationProbabilityEstimator</code></pre><p>A symbolic, weighted permutation based probabilities/entropy estimator.</p><p><strong>Description</strong></p><p>Weighted permutations of a signal preserve not only ordinal patterns (sorting information),  but also encodes amplitude. The implementation here is based on  Fadlallah et al. (2013)<sup class="footnote-reference"><a id="citeref-Fadlallah2013" href="#footnote-Fadlallah2013">[Fadlallah2013]</a></sup>.</p><p><strong>From univariate signals</strong></p><p>Consider the <span>$n$</span>-element univariate time series <span>$\{x(t) = x_1, x_2, \ldots, x_n\}$</span>.  Let <span>$\mathbf{x_i}^{m, \tau} = \{x_j, x_{j+\tau}, \ldots, x_{j+(m-1)\tau}\}$</span> for  <span>$j = 1, 2, \ldots n - (m-1)\tau$</span> be the <span>$i$</span>-th state vector in a delay reconstruction  with embedding dimension <span>$m$</span> and reconstruction lag <span>$\tau$</span>. There are then  <span>$N = n - (m-1)\tau$</span> state vectors. </p><p>For an <span>$m$</span>-dimensional vector, there are <span>$m!$</span> possible ways of sorting it in ascending  order of magnitude. Each such possible sorting ordering is called a <em>motif</em>.  Let <span>$\pi_i^{m, \tau}$</span> denote the motif associated with the <span>$m$</span>-dimensional state  vector <span>$\mathbf{x_i}^{m, \tau}$</span>, and let <span>$R$</span> be the number of distinct motifs that  can be constructed from the <span>$N$</span> state vectors. Then there are at most <span>$R$</span> motifs;  <span>$R = N$</span> precisely when all motifs are unique, and <span>$R = 1$</span> when all motifs are the same.  Each unique motif <span>$\pi_i^{m, \tau}$</span> can be mapped to a unique integer symbol  <span>$0 \leq s_i \leq M!-1$</span>. Let <span>$S(\pi) : \mathbb{R}^m \to \mathbb{N}_0$</span> be the  function that maps the motif <span>$\pi$</span> to its symbol <span>$s$</span>, and let <span>$\Pi$</span> denote the set      of symbols <span>$\Pi = \{ s_i \}_{i\in \{ 1, \ldots, R\}}$</span>.</p><p>Weighted permutation entropy is computed analogously to regular permutation entropy, but  adds weights that encode amplitude information too:</p><div>\[p(\pi_i^{m, \tau}) = \dfrac{\sum_{k=1}^N \mathbf{1}_{u:S(u) = s_i} \left( \mathbf{x}_k^{m, \tau} \right) \, w_k}{\sum_{k=1}^N \mathbf{1}_{u:S(u) \in \Pi} \left( \mathbf{x}_k^{m, \tau} \right) \,w_k} = \dfrac{\sum_{k=1}^N \mathbf{1}_{u:S(u) = s_i} \left( \mathbf{x}_k^{m, \tau} \right) \, w_k}{\sum_{k=1}^N w_k}.\]</div><p>The weighted permutation entropy is equivalent to regular permutation entropy when weights  are positive and identical (<span>$w_j = \beta \,\,\, \forall \,\,\, j \leq N$</span> and  <span>$\beta &gt; 0)$</span>. There are many different choices of weights, but in  Fadlallah et al. (2013)<sup class="footnote-reference"><a id="citeref-Fadlallah2013" href="#footnote-Fadlallah2013">[Fadlallah2013]</a></sup>, weights are dictated by the variance of the  state vectors.</p><p>Let the aritmetic mean of state vector <span>$\mathbf{x}_i$</span> be denoted  by</p><div>\[\mathbf{\hat{x}}_j^{m, \tau} = \frac{1}{m} \sum_{k=1}^m x_{j + (k+1)\tau}.\]</div><p>Weights are then computed as </p><div>\[w_j = \dfrac{1}{m}\sum_{k=1}^m (x_{j+(k+1)\tau} - \mathbf{\hat{x}}_j^{m, \tau})^2.\]</div><p><strong>Difference between original paper and this implementation</strong></p><p><em>Note: in equation 7, section III, of the original paper, the authors write</em></p><div>\[w_j = \dfrac{1}{m}\sum_{k=1}^m (x_{j-(k-1)\tau} - \mathbf{\hat{x}}_j^{m, \tau})^2.\]</div><p><em>But this is <strong>not</strong> the variance of <span>$\mathbf{x}_i$</span>, because the indices are mixed:  <span>$x_{j+(k-1)\tau}$</span> in the weights formula, vs. <span>$x_{j+(k+1)\tau}$</span> in the arithmetic  mean formula. This seems to imply that amplitude information about previous delay vectors  are mixed with mean amplitude information about current vectors. The authors also mix the  terms &quot;vector&quot; and &quot;neighboring vector&quot; (but uses the same notation for both), making it  hard to interpret whether the sign switch is a typo or intended. Here, we use the notation  above, which actually computes the variance for <span>$\mathbf{x}_i$</span></em>.</p><p><strong>Estimation</strong></p><ul><li><p>To compute weighted permutation entropy for a univariate signal <code>x</code>, use the signature    <code>entropy(x::AbstractVector, est::SymbolicWeightedPermutation; τ::Int = 1, m::Int = 3)</code>.</p></li><li><p>The corresponding (unordered) probability distribution of the permutation symbols for a    univariate signal <code>x</code> can be computed using <code>probabilities(x::AbstractVector,    est::SymbolicWeightedPermutation; τ::Int = 1, m::Int = 3)</code>.  </p></li></ul><p><em>Note: by default, embedding dimension <span>$m = 3$</span> with embedding lag <span>$1$</span> is used. You  should probably make a more informed decision about embedding parameters when computing the  permutation entropy of a real dataset. In all cases, <span>$m$</span> must be at least 2</em> (there are  no permutations of a single-element state vector, so need <span>$m \geq 2$</span>).</p><p><strong>From multivariate time series/datasets</strong></p><p>Weighted permutation entropy, just like regular permutation entropy, is can also be  computed for multivariate datasets (either embedded or consisting of multiple time series  variables). This assumes that the mixed symbols described above are actually a typo). </p><p>Then, just skip the delay reconstruction step, compute symbols  directly from the <span>$L$</span> existing state vectors  <span>$\{\mathbf{x}_1, \mathbf{x}_2, \ldots, \mathbf{x_L}\}$</span>, symbolize  each <span>$\mathbf{x_i}$</span> precisely as above, then compute the  quantity </p><div>\[H = - \sum_j p(\pi) \ln p(\pi_j).\]</div><p><strong>Estimation</strong></p><ul><li><p>To compute weighted permutation entropy for a multivariate/embedded dataset <code>x</code>, use the    signature <code>entropy(x::Dataset, est::SymbolicWeightedPermutation)</code>.`</p></li><li><p>To get the probability distribution for a multivariate/embedded dataset <code>x</code>, use    <code>probabilities(x::Dataset, est::SymbolicWeightedPermutation)</code>.</p></li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/kahaaga/Entropies.jl/blob/2eca8e7942db91316c6179b7570e70d5c7774628/src/symbolic/SymbolicWeightedPermutation.jl#L9-L123">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Entropies.entropy" href="#Entropies.entropy"><code>Entropies.entropy</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia">entropy(x::Dataset, est::SymbolicWeightedPermutation, α::Real = 1; m::Int = 3, τ::Int = 1, base = 2) → Real
entropy(x::AbstractVector, est::SymbolicWeightedPermutation, α::Real = 1; m::Int = 3, τ::Int = 1, base = 2) → Real</code></pre><p>Compute the generalized order <code>α</code> entropy based on a weighted permutation  symbolization of <code>x</code>, using symbol size/order <code>m</code> for the permutations.</p><p>If <code>x</code> is a multivariate <code>Dataset</code>, then symbolization is performed directly on the state  vectors. If <code>x</code> is a univariate signal, then a delay reconstruction with embedding lag <code>τ</code>  and embedding dimension <code>m</code> is used to construct state vectors, on which symbolization is  then performed.</p><p><strong>Probability estimation</strong></p><p>An unordered symbol frequency histogram is obtained by symbolizing the points in <code>x</code> by a weighted procedure, using <a href="#Entropies.probabilities-Tuple{Dataset,SymbolicWeightedPermutation}"><code>probabilities(::Dataset, ::SymbolicWeightedPermutation)</code></a>. Sum-normalizing this histogram yields a probability distribution over the weighted symbols.</p><p><strong>Entropy estimation</strong></p><p>After the symbolization histogram/distribution has been obtained, the order <code>α</code> generalized  entropy<sup class="footnote-reference"><a id="citeref-Rényi1960" href="#footnote-Rényi1960">[Rényi1960]</a></sup>, to the given <code>base</code>, is computed from that sum-normalized symbol  distribution, using <a href="../generalized_entropy/#Entropies.genentropy"><code>genentropy</code></a>.</p><p><strong>Notes</strong></p><p><em>Do not confuse the order of the generalized entropy (<code>α</code>) with the order <code>m</code> of the  permutation entropy (<code>m</code>, which controls the symbol size). Permutation entropy is usually  estimated with <code>α = 1</code>, but the implementation here allows the generalized entropy of any  dimension to be computed from the symbol frequency distribution.</em></p><p>See also: <a href="#Entropies.SymbolicWeightedPermutation"><code>SymbolicWeightedPermutation</code></a>, <a href="../generalized_entropy/#Entropies.genentropy"><code>genentropy</code></a>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/kahaaga/Entropies.jl/blob/2eca8e7942db91316c6179b7570e70d5c7774628/src/symbolic/SymbolicWeightedPermutation.jl#L217-L251">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Entropies.probabilities-Tuple{Dataset,SymbolicWeightedPermutation}" href="#Entropies.probabilities-Tuple{Dataset,SymbolicWeightedPermutation}"><code>Entropies.probabilities</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia">probabilities(x::Dataset, est::SymbolicWeightedPermutation) → Vector{&lt;:Real}  
probabilities(x::AbstractVector, est::SymbolicWeightedPermutation; m::Int = 3, τ::Int = 1) → Vector{&lt;:Real}

probabilities!(s::Vector{Int}, x::Dataset, est::SymbolicWeightedPermutation) → Vector{&lt;:Real}  
probabilities!(s::Vector{Int}, x::AbstractVector, est::SymbolicWeightedPermutation; m::Int = 3, τ::Int = 1) → Vector{&lt;:Real}</code></pre><p>Compute the unordered probabilities of the occurrence of weighted symbol sequences  constructed from <code>x</code>. </p><p>If <code>x</code> is a multivariate <code>Dataset</code>, then symbolization is performed directly on the state  vectors. If <code>x</code> is a univariate signal, then a delay reconstruction with embedding lag <code>τ</code>  and embedding dimension <code>m</code> is used to construct state vectors, on which symbolization is  then performed.</p><p>A pre-allocated symbol array <code>s</code> can be provided to save some memory allocations if the  probabilities are to be computed for multiple data sets. If so, it is required that  <code>length(x) == length(s)</code> if <code>x</code> is a <code>Dataset</code>, or  <code>length(s) == length(x) - (m-1)τ</code>  if <code>x</code> is a univariate signal`.</p><p>See also: <a href="#Entropies.SymbolicWeightedPermutation"><code>SymbolicWeightedPermutation</code></a>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/kahaaga/Entropies.jl/blob/2eca8e7942db91316c6179b7570e70d5c7774628/src/symbolic/SymbolicWeightedPermutation.jl#L175-L196">source</a></section></article><section class="footnotes is-size-7"><ul><li class="footnote" id="footnote-Fadlallah2013"><a class="tag is-link" href="#citeref-Fadlallah2013">Fadlallah2013</a>Fadlallah, Bilal, et al. &quot;Weighted-permutation entropy: A complexity  measure for time series incorporating amplitude information.&quot; Physical  Review E 87.2 (2013): 022911.</li><li class="footnote" id="footnote-Rényi1960"><a class="tag is-link" href="#citeref-Rényi1960">Rényi1960</a>A. Rényi, <em>Proceedings of the fourth Berkeley Symposium on Mathematics, Statistics and Probability</em>, pp 547 (1960)</li></ul></section></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../SymbolicPermutation/">« Permutation (symbolic)</a><a class="docs-footer-nextpage" href="../VisitationFrequency/">Visitation frequency »</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> on <span class="colophon-date" title="Saturday 31 October 2020 21:29">Saturday 31 October 2020</span>. Using Julia version 1.5.2.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
