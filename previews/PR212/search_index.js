var documenterSearchIndex = {"docs":
[{"location":"complexity/#Complexity-measures","page":"Complexity measures","title":"Complexity measures","text":"","category":"section"},{"location":"complexity/","page":"Complexity measures","title":"Complexity measures","text":"complexity\ncomplexity_normalized","category":"page"},{"location":"complexity/#Entropies.complexity","page":"Complexity measures","title":"Entropies.complexity","text":"complexity(c::ComplexityMeasure, x)\n\nEstimate the complexity measure c for input data x, where c can be any of the following measures:\n\nReverseDispersion.\nApproximateEntropy.\nSampleEntropy.\nMissingDispersionPatterns.\n\n\n\n\n\n","category":"function"},{"location":"complexity/#Entropies.complexity_normalized","page":"Complexity measures","title":"Entropies.complexity_normalized","text":"complexity_normalized(c::ComplexityMeasure, x) → m ∈ [a, b]\n\nThe same as complexity, but the result is normalized to the interval [a, b], where [a, b] depends on c.\n\n\n\n\n\n","category":"function"},{"location":"complexity/#Approximate-entropy","page":"Complexity measures","title":"Approximate entropy","text":"","category":"section"},{"location":"complexity/","page":"Complexity measures","title":"Complexity measures","text":"ApproximateEntropy","category":"page"},{"location":"complexity/#Entropies.ApproximateEntropy","page":"Complexity measures","title":"Entropies.ApproximateEntropy","text":"ApproximateEntropy([x]; r = 0.2std(x), kwargs...)\n\nAn estimator for the approximate entropy (ApEn; Pincus, 1991)[Pincus1991] complexity measure, used with complexity.\n\nThe keyword argument r is mandatory if an input timeseries x is not provided.\n\nKeyword arguments\n\nr::Real: The radius used when querying for nearest neighbors around points. Its value   should be determined from the input data, for example as some proportion of the   standard deviation of the data.\nm::Int = 2: The embedding dimension.\nτ::Int = 1: The embedding lag.\nbase::Real = MathConstants.e: The base to use for the logarithm. Pincus (1991) uses the   natural logarithm.\nmetric: The metric used to compute distances.\n\nDescription\n\nApproximate entropy is defined as\n\nApEn(m r) = lim_N to infty left phi(x m r) - phi(x m + 1 r) right\n\nApproximate entropy is estimated for a timeseries x, by first embedding x using embedding dimension m and embedding lag τ, then searching for similar vectors within tolerance radius r, using the estimator described below, with logarithms to the given base (natural logarithm is used in Pincus, 1991).\n\nSpecifically, for a finite-length timeseries x, an estimator for ApEn(m r) is\n\nApEn(m r N) = phi(x m r N) -  phi(x m + 1 r N)\n\nwhere N = length(x) and\n\nphi(x k r N) =\ndfrac1N-(k-1)tau sum_i=1^N - (k-1)tau\nlogleft(\n    sum_j = 1^N-(k-1)tau dfractheta(d(bf x_i^m bf x_j^m) leq r)N-(k-1)tau\n    right)\n\nHere, theta(cdot) returns 1 if the argument is true and 0 otherwise,  d(bf x_i bf x_j) returns the Chebyshev distance between vectors  bf x_i and bf x_j, and the k-dimensional embedding vectors are constructed from the input timeseries x(t) as\n\nbf x_i^k = (x(i) x(i+τ) x(i+2τ) ldots x(i+(k-1)tau))\n\nnote: Flexible embedding lag\nIn the original paper, they fix τ = 1. In our implementation, the normalization constant is modified to account for embeddings with τ != 1.\n\n[Pincus1991]: Pincus, S. M. (1991). Approximate entropy as a measure of system complexity. Proceedings of the National Academy of Sciences, 88(6), 2297-2301.\n\n\n\n\n\n","category":"type"},{"location":"complexity/#Sample-entropy","page":"Complexity measures","title":"Sample entropy","text":"","category":"section"},{"location":"complexity/","page":"Complexity measures","title":"Complexity measures","text":"SampleEntropy","category":"page"},{"location":"complexity/#Entropies.SampleEntropy","page":"Complexity measures","title":"Entropies.SampleEntropy","text":"SampleEntropy([x]; r = 0.2std(x), kwargs...)\n\nAn estimator for the sample entropy complexity measure (Richman & Moorman, 2000)[Richman2000], used with complexity and complexity_normalized.\n\nThe keyword argument r is mandatory if an input timeseries x is not provided.\n\nKeyword arguments\n\nr::Real: The radius used when querying for nearest neighbors around points. Its value   should be determined from the input data, for example as some proportion of the   standard deviation of the data.\nm::Int = 1: The embedding dimension.\nτ::Int =: The embedding lag.\nmetric: The metric used to compute distances.\n\nDescription\n\nAn estimator for sample entropy using radius r, embedding dimension m, and embedding lag τ is\n\nSampEn(mr N) = -lndfracA(r N)B(r N)\n\nHere,\n\nbeginaligned\nB(r m N) = sum_i = 1^N-mtau sum_j = 1 j neq i^N-mtau theta(d(bf x_i^m bf x_j^m) leq r) \nA(r m N) = sum_i = 1^N-mtau sum_j = 1 j neq i^N-mtau theta(d(bf x_i^m+1 bf x_j^m+1) leq r) \nendaligned\n\nwhere theta(cdot) returns 1 if the argument is true and 0 otherwise, and d(x y) computes the distance between x and y according to metric (default is Chebyshev), and  bf x_i^m and bf x_i^m+1 are m-dimensional and m+1-dimensional embedding vectors, where k-dimensional embedding vectors are constructed from the input timeseries x(t) as\n\nbf x_i^k = (x(i) x(i+τ) x(i+2τ) ldots x(i+(k-1)tau))\n\nQuoting Richman & Moorman (2002): \"SampEn(m,r,N) will be defined except when B = 0, in which case no regularity has been detected, or when A = 0, which corresponds to a conditional probability of 0 and an infinite value of SampEn(m,r,N)\". In these cases, NaN is returned.\n\nIf computing the normalized measure, then the resulting sample entropy is on [0, 1].\n\nnote: Flexible embedding lag\nThe original algorithm fixes τ = 1. All formulas here are modified to account for any τ.\n\nSee also: sample_entropy.\n\n[Richman2000]: Richman, J. S., & Moorman, J. R. (2000). Physiological time-series analysis using approximate entropy and sample entropy. American Journal of Physiology-Heart and Circulatory Physiology, 278(6), H2039-H2049.\n\n\n\n\n\n","category":"type"},{"location":"complexity/#Missing-dispersion-patterns","page":"Complexity measures","title":"Missing dispersion patterns","text":"","category":"section"},{"location":"complexity/","page":"Complexity measures","title":"Complexity measures","text":"MissingDispersionPatterns","category":"page"},{"location":"complexity/#Entropies.MissingDispersionPatterns","page":"Complexity measures","title":"Entropies.MissingDispersionPatterns","text":"MissingDispersionPatterns <: ComplexityMeasure\nMissingDispersionPatterns(est = Dispersion())\n\nAn estimator for the number of missing dispersion patterns (N_MDP), a complexity measure which can be used to detect nonlinearity in time series (Zhou et al., 2022)[Zhou2022].\n\nUsed with complexity or complexity_normalized, whose implementation uses missing_outcomes.\n\nDescription\n\nIf used with complexity, N_MDP is computed by first symbolising each xᵢ ∈ x, then embedding the resulting symbol sequence using the dispersion pattern estimator est, and computing the quantity\n\nN_MDP = L - N_ODP\n\nwhere L = total_outcomes(est) (i.e. the total number of possible dispersion patterns), and N_ODP is defined as the number of occurring dispersion patterns.\n\nIf used with complexity_normalized, then N_MDP^N = (L - N_ODP)L is computed. The authors recommend that total_outcomes(est.symbolization)^est.m << length(x) - est.m*est.τ + 1 to avoid undersampling.\n\nnote: Encoding\nDispersion's linear mapping from CDFs to integers is based on equidistant partitioning of the interval [0, 1]. This is slightly different from Zhou et al. (2022), which uses the linear mapping s_i = textround(y + 05).\n\nUsage\n\nIn Zhou et al. (2022), MissingDispersionPatterns is used to detect nonlinearity in time series by comparing the N_MDP for a time series x to N_MDP values for an ensemble of surrogates of x. If N_MDP  q_MDP^WIAAFT, where q_MDP^WIAAFT is some q-th quantile of the surrogate ensemble, then it is taken as evidence for nonlinearity.\n\nSee also: Dispersion, ReverseDispersion, total_outcomes.\n\n[Zhou2022]: Zhou, Q., Shang, P., & Zhang, B. (2022). Using missing dispersion patterns to detect determinism and nonlinearity in time series data. Nonlinear Dynamics, 1-20.\n\n\n\n\n\n","category":"type"},{"location":"complexity/#Reverse-dispersion-entropy","page":"Complexity measures","title":"Reverse dispersion entropy","text":"","category":"section"},{"location":"complexity/","page":"Complexity measures","title":"Complexity measures","text":"ReverseDispersion\ndistance_to_whitenoise","category":"page"},{"location":"complexity/#Entropies.ReverseDispersion","page":"Complexity measures","title":"Entropies.ReverseDispersion","text":"ReverseDispersion <: ComplexityMeasure\nReverseDispersion(; c = 3, m = 2, τ = 1, check_unique = true,\n)\n\nEstimator for the reverse dispersion entropy complexity measure (Li et al., 2019)[Li2019].\n\nDescription\n\nLi et al. (2021)[Li2019] defines the reverse dispersion entropy as\n\nH_rde = sum_i = 1^c^m left(p_i - dfrac1c^m right)^2 =\nleft( sum_i=1^c^m p_i^2 right) - dfrac1c^m\n\nwhere the probabilities p_i are obtained precisely as for the Dispersion probability estimator. Relative frequencies of dispersion patterns are computed using the given encoding scheme , which defaults to encoding using the normal cumulative distribution function (NCDF), as implemented by GaussianCDFEncoding, using embedding dimension m and embedding delay τ. Recommended parameter values[Li2018] are m ∈ [2, 3], τ = 1 for the embedding, and c ∈ [3, 4, …, 8] categories for the Gaussian mapping.\n\nIf normalizing, then the reverse dispersion entropy is normalized to [0, 1].\n\nThe minimum value of H_rde is zero and occurs precisely when the dispersion pattern distribution is flat, which occurs when all p_is are equal to 1c^m. Because H_rde geq 0, H_rde can therefore be said to be a measure of how far the dispersion pattern probability distribution is from white noise.\n\nData requirements\n\nThe input must have more than one unique element for the default GaussianEncoding to be well-defined. Li et al. (2018) recommends that x has at least 1000 data points.\n\nIf check_unique == true (default), then it is checked that the input has more than one unique value. If check_unique == false and the input only has one unique element, then a InexactError is thrown when trying to compute probabilities.\n\n[Li2019]: Li, Y., Gao, X., & Wang, L. (2019). Reverse dispersion entropy: a new complexity measure for sensor signal. Sensors, 19(23), 5203.\n\n\n\n\n\n","category":"type"},{"location":"complexity/#Entropies.distance_to_whitenoise","page":"Complexity measures","title":"Entropies.distance_to_whitenoise","text":"distance_to_whitenoise(estimator::ReverseDispersion, p::Probabilities;\n    normalize = false)\n\nCompute the distance of the probability distribution p from a uniform distribution, given the parameters of estimator (which must be known beforehand).\n\nIf normalize == true, then normalize the value to the interval [0, 1] by using the parameters of estimator.\n\nUsed to compute reverse dispersion entropy(ReverseDispersion; Li et al., 2019[Li2019]).\n\n[Li2019]: Li, Y., Gao, X., & Wang, L. (2019). Reverse dispersion entropy: a new complexity measure for sensor signal. Sensors, 19(23), 5203.\n\n\n\n\n\n","category":"function"},{"location":"complexity/#Convenience","page":"Complexity measures","title":"Convenience","text":"","category":"section"},{"location":"complexity/","page":"Complexity measures","title":"Complexity measures","text":"We provide a few convenience functions for widely used \"entropy-like\" complexity measures. Other arbitrary specialized convenience functions can easily be defined in a couple lines of code.","category":"page"},{"location":"complexity/","page":"Complexity measures","title":"Complexity measures","text":"approx_entropy\nsample_entropy","category":"page"},{"location":"complexity/#Entropies.approx_entropy","page":"Complexity measures","title":"Entropies.approx_entropy","text":"approx_entropy(x; m = 2, τ = 1, r = 0.2 * Statistics.std(x), base = MathConstants.e)\n\nConvenience syntax for computing the approximate entropy (Pincus, 1991) for timeseries x.\n\nThis is just a wrapper for complexity(ApproximateEntropy(; m, τ, r, base), x) (see also ApproximateEntropy).\n\n\n\n\n\n","category":"function"},{"location":"complexity/#Entropies.sample_entropy","page":"Complexity measures","title":"Entropies.sample_entropy","text":"sample_entropy(x; r, m = 2, τ = 1, metric = Chebyshev(), normalize = true)\n\nConvenience syntax for estimating the (normalized) sample entropy (Richman & Moorman, 2000) of timeseries x.\n\nThis is just a wrapper for complexity(SampleEntropy(; r, m, τ, base), x).\n\nSee also: SampleEntropy, complexity, complexity_normalized).\n\n\n\n\n\n","category":"function"},{"location":"entropies/#entropies","page":"Entropies","title":"Entropies","text":"","category":"section"},{"location":"entropies/","page":"Entropies","title":"Entropies","text":"entropy\nEntropy","category":"page"},{"location":"entropies/#Entropies.entropy","page":"Entropies","title":"Entropies.entropy","text":"entropy([e::Entropy,] probs::Probabilities)\nentropy([e::Entropy,] est::ProbabilitiesEstimator, x)\nentropy([e::Entropy,] est::EntropyEstimator, x)\n\nCompute h::Real, which is a (generalized) entropy defined by e, in one of three ways:\n\nDirectly from existing Probabilities probs.\nFrom input data x, by first estimating a probability distribution using the provided ProbabilitiesEstimator, then computing entropy from that distribution. In fact, the second method is just a 2-lines-of-code wrapper that calls probabilities and gives the result to the first method.\nFrom input data x, by using a dedicated EntropyEstimator that computes entropy in a way that doesn't involve explicitly computing probabilities first. Usually, this involves computing a differential entropy.\n\nThe entropy definition (first argument) is optional. Explicitly provide e if you need to specify a logarithm base for the entropy. When est is a probability estimator, Shannon(; base = 2) is used by default. When est is a dedicated entropy estimator, the default entropy type is inferred from the estimator (e.g. Kraskov estimates Shannon(; base = 2) differential entropy).\n\nInput data\n\nx is typically an Array or a Dataset, see Input data for Entropies.jl.\n\nMaximum entropy and normalized entropy\n\nAll discrete entropies e have a well defined maximum value for a given probability estimator. To obtain this value one only needs to call the entropy_maximum function with the chosen entropy type and probability estimator. Or, one can use entropy_normalized to obtain the normalized form of the entropy (divided by the maximum).\n\nExamples\n\nDiscrete entropies\n\nx = [rand(Bool) for _ in 1:10000] # coin toss\nps = probabilities(x) # gives about [0.5, 0.5] by definition\nh = entropy(ps) # gives 1, about 1 bit by definition\nh = entropy(Shannon(), ps) # syntactically equivalent to above\nh = entropy(Shannon(), CountOccurrences(), x) # syntactically equivalent to above\nh = entropy(SymbolicPermutation(;m=3), x) # gives about 2, again by definition\nh = entropy(Renyi(2.0), ps) # also gives 1, order `q` doesn't matter for coin toss\n\nDifferential/continuous entropies\n\n# Normal distribution N(0, 1) has differential entropy 0.5*log(2π) + 0.5\nentropy(Shannon(; base = ℯ), Kraskov(k = 5), randn(100000))\n\n\n\n\n\n","category":"function"},{"location":"entropies/#Entropies.Entropy","page":"Entropies","title":"Entropies.Entropy","text":"Entropy\n\nEntropy is the supertype of all (generalized) entropies, and currently implemented entropy types are:\n\nRenyi.\nTsallis.\nShannon, which is a subcase of the above two in the limit q → 1.\nCurado.\nStretchedExponential.\n\nThese entropy types are given as inputs to entropy and [entropy_normalized].\n\nDescription\n\nMathematically speaking, generalized entropies are just nonnegative functions of probability distributions that verify certain (entropy-type-dependent) axioms. Amigó et al.[Amigó2018] summary paper gives a nice overview.\n\n[Amigó2018]:     Amigó, J. M., Balogh, S. G., & Hernández, S. (2018). A brief review of     generalized entropies. Entropy, 20(11), 813.\n\n\n\n\n\n","category":"type"},{"location":"entropies/#Generalized-entropies","page":"Entropies","title":"Generalized entropies","text":"","category":"section"},{"location":"entropies/","page":"Entropies","title":"Entropies","text":"Shannon\nRenyi\nTsallis\nKaniadakis\nCurado\nStretchedExponential","category":"page"},{"location":"entropies/#Entropies.Shannon","page":"Entropies","title":"Entropies.Shannon","text":"Shannon(; base = 2)\n\nThe Shannon[Shannon1948] entropy, used with entropy to compute:\n\nH(p) = - sum_i pi log(pi)\n\nwith the log at the given base.\n\nShannon(base) is syntactically equivalent to Renyi(; base).\n\n[Shannon1948]: C. E. Shannon, Bell Systems Technical Journal 27, pp 379 (1948)\n\n\n\n\n\n","category":"function"},{"location":"entropies/#Entropies.Renyi","page":"Entropies","title":"Entropies.Renyi","text":"Renyi <: Entropy\nRenyi(q, base = 2)\nRenyi(; q = 1.0, base = 2)\n\nThe Rényi[Rényi1960] generalized order-q entropy, used with entropy to compute an entropy with units given by base (typically 2 or MathConstants.e).\n\nDescription\n\nLet p be an array of probabilities (summing to 1). Then the Rényi generalized entropy is\n\nH_q(p) = frac11-q log left(sum_i pi^qright)\n\nand generalizes other known entropies, like e.g. the information entropy (q = 1, see [Shannon1948]), the maximum entropy (q=0, also known as Hartley entropy), or the correlation entropy (q = 2, also known as collision entropy).\n\nIf the probability estimator has known alphabet length L, then the maximum value of the Rényi entropy is log_base(L), which is the entropy of the uniform distribution with given alphabet length.\n\n[Rényi1960]: A. Rényi, Proceedings of the fourth Berkeley Symposium on Mathematics, Statistics and Probability, pp 547 (1960)\n\n[Shannon1948]: C. E. Shannon, Bell Systems Technical Journal 27, pp 379 (1948)\n\n\n\n\n\n","category":"type"},{"location":"entropies/#Entropies.Tsallis","page":"Entropies","title":"Entropies.Tsallis","text":"Tsallis <: Entropy\nTsallis(q; k = 1.0, base = 2)\nTsallis(; q = 1.0, k = 1.0, base = 2)\n\nThe Tsallis[Tsallis1988] generalized order-q entropy, used with entropy to compute an entropy.\n\nbase only applies in the limiting case q == 1, in which the Tsallis entropy reduces to Shannon entropy.\n\nDescription\n\nThe Tsallis entropy is a generalization of the Boltzmann-Gibbs entropy, with k standing for the Boltzmann constant. It is defined as\n\nS_q(p) = frackq - 1left(1 - sum_i pi^qright)\n\nIf the probability estimator has known alphabet length L, then the maximum value of the Tsallis entropy is k(L^1 - q - 1)(1 - q).\n\n[Tsallis1988]: Tsallis, C. (1988). Possible generalization of Boltzmann-Gibbs statistics. Journal of statistical physics, 52(1), 479-487.\n\n\n\n\n\n","category":"type"},{"location":"entropies/#Entropies.Kaniadakis","page":"Entropies","title":"Entropies.Kaniadakis","text":"Kaniadakis <: Entropy\nKaniadakis(; κ = 1.0, base = 2.0)\n\nThe Kaniadakis entropy (Tsallis, 2009)[Tsallis2009], used with entropy to compute\n\nH_K(p) = -sum_i=1^N p_ilog_kappa^K(p_i)\n\nlog_kappa = dfracx^kappa - x^-kappa2kappa\n\nwhere if kappa = 0, regular logarithm to the given base is used, and log(0) = 0.\n\n[Tsallis2009]: Tsallis, C. (2009). Introduction to nonextensive statistical mechanics: approaching a  complex world. Springer, 1(1), 2-1.\n\n\n\n\n\n","category":"type"},{"location":"entropies/#Entropies.Curado","page":"Entropies","title":"Entropies.Curado","text":"Curado <: Entropy\nCurado(; b = 1.0)\n\nThe Curado entropy (Curado & Nobre, 2004)[Curado2004], used with entropy to compute\n\nH_C(p) = left( sum_i=1^N e^-b p_i right) + e^-b - 1\n\nwith b ∈ ℛ, b > 0, where the terms outside the sum ensures that H_C(0) = H_C(1) = 0.\n\n[Curado2004]: Curado, E. M., & Nobre, F. D. (2004). On the stability of analytic entropic forms. Physica A: Statistical Mechanics and its Applications, 335(1-2), 94-106.\n\n\n\n\n\n","category":"type"},{"location":"entropies/#Entropies.StretchedExponential","page":"Entropies","title":"Entropies.StretchedExponential","text":"StretchedExponential <: Entropy\nStretchedExponential(; η = 2.0, base = 2)\n\nThe stretched exponential, or Anteneodo-Plastino, entropy (Anteneodo & Plastino, 1999[Anteneodo1999]), used with entropy to compute\n\nS_eta(p) = sum_i = 1^N\nGamma left( dfraceta + 1eta - log_base(p_i) right) -\np_i Gamma left( dfraceta + 1eta right)\n\nwhere eta geq 0, Gamma(cdot cdot) is the upper incomplete Gamma function, and Gamma(cdot) = Gamma(cdot 0) is the Gamma function. Reduces to Shannon entropy for η = 1.0.\n\n[Anteneodo1999]: Anteneodo, C., & Plastino, A. R. (1999). Maximum entropy approach to stretched exponential probability distributions. Journal of Physics A: Mathematical and General, 32(7), 1089.\n\n\n\n\n\n","category":"type"},{"location":"entropies/#Estimation-(discrete)","page":"Entropies","title":"Estimation (discrete)","text":"","category":"section"},{"location":"entropies/","page":"Entropies","title":"Entropies","text":"Discrete entropies are just simple functions (sums, actually) of probability mass functions (pmf), which you can estimate using ProbabilitiesEstimators.","category":"page"},{"location":"entropies/","page":"Entropies","title":"Entropies","text":"Any ProbabilitiesEstimator may therefore be used as a naive plug-in estimator for discrete entropy. No bias correction is currently applied to any of the discrete estimators.","category":"page"},{"location":"entropies/","page":"Entropies","title":"Entropies","text":"Tables scroll sideways, so are best viewed on a large screen.","category":"page"},{"location":"entropies/","page":"Entropies","title":"Entropies","text":"Estimator Principle Input data Shannon Renyi Tsallis Kaniadakis StretchedExponential Curado\nCountOccurrences Frequencies Vector, Dataset ✅ ✅ ✅ ✅ ✅ ✅\nValueHistogram Binning (histogram) Vector, Dataset ✅ ✅ ✅ ✅ ✅ ✅\nTransferOperator Binning (transfer operator) Vector, Dataset ✅ ✅ ✅ ✅ ✅ ✅\nNaiveKernel Kernel density estimation Dataset ✅ ✅ ✅ ✅ ✅ ✅\nSymbolicPermutation Ordinal patterns Vector ✅ ✅ ✅ ✅ ✅ ✅\nSymbolicWeightedPermutation Ordinal patterns Vector ✅ ✅ ✅ ✅ ✅ ✅\nSymbolicAmplitudeAwarePermutation Ordinal patterns Vector ✅ ✅ ✅ ✅ ✅ ✅\nDispersion Dispersion patterns Vector ✅ ✅ ✅ ✅ ✅ ✅\nDiversity Cosine similarity Vector ✅ ✅ ✅ ✅ ✅ ✅\nWaveletOverlap Wavelet transform Vector ✅ ✅ ✅ ✅ ✅ ✅\nPowerSpectrum Fourier spectra Vector, Dataset ✅ ✅ ✅ ✅ ✅ ✅","category":"page"},{"location":"entropies/#Estimation-(continuous/differential)","page":"Entropies","title":"Estimation (continuous/differential)","text":"","category":"section"},{"location":"entropies/","page":"Entropies","title":"Entropies","text":"The following estimators are differential entropy estimators, and can also be used with entropy. Differential) entropies are functions of integrals, and usually rely on estimating some density functional.","category":"page"},{"location":"entropies/","page":"Entropies","title":"Entropies","text":"Each EntropyEstimators uses a specialized technique to approximating relevant densities/integrals, and is often tailored to one or a few types of generalized entropy. For example, Kraskov estimates the Shannon entropy.","category":"page"},{"location":"entropies/","page":"Entropies","title":"Entropies","text":"Estimator Principle Input data Shannon Renyi Tsallis Kaniadakis Curado StretchedExponential\nKozachenkoLeonenko Nearest neighbors Dataset ✅ x x x x x\nKraskov Nearest neighbors Dataset ✅ x x x x x\nZhu Nearest neighbors Dataset ✅ x x x x x\nZhuSingh Nearest neighbors Dataset ✅ x x x x x\nVasicek Order statistics Vector ✅ x x x x x\nEbrahimi Order statistics Vector ✅ x x x x x\nCorrea Order statistics Vector ✅ x x x x x\nAlizadehArghami Order statistics Vector ✅ x x x x x","category":"page"},{"location":"entropies/","page":"Entropies","title":"Entropies","text":"EntropyEstimator","category":"page"},{"location":"entropies/#Entropies.EntropyEstimator","page":"Entropies","title":"Entropies.EntropyEstimator","text":"EntropyEstimator\n\nThe supertype of all entropy estimators.\n\nThese estimators compute some Entropy in various ways that doesn't involve explicitly estimating a probability distribution. Currently implemented estimators are:\n\nKozachenkoLeonenko\nKraskov\nZhu\nZhuSingh\nVasicek\nEbrahimi\nCorrea\nAlizadehArghami\n\nFor example, entropy(Shannon(), Kraskov(), x) computes the Shannon differential entropy of the input data x using the Kraskov k-th nearest neighbor estimator.\n\n\n\n\n\n","category":"type"},{"location":"entropies/","page":"Entropies","title":"Entropies","text":"Kraskov\nKozachenkoLeonenko\nZhu\nZhuSingh\nVasicek\nAlizadehArghami\nEbrahimi\nCorrea","category":"page"},{"location":"entropies/#Entropies.Kraskov","page":"Entropies","title":"Entropies.Kraskov","text":"Kraskov <: EntropyEstimator\nKraskov(; k::Int = 1, w::Int = 1)\n\nThe Kraskov estimator computes the Shannon differential entropy of x (a multi-dimensional Dataset) using the k-th nearest neighbor searches method from [Kraskov2004].\n\nw is the Theiler window, which determines if temporal neighbors are excluded during neighbor searches (defaults to 0, meaning that only the point itself is excluded when searching for neighbours).\n\nDescription\n\nAssume we have samples bfx_1 bfx_2 ldots bfx_N  from a continuous random variable X in mathbbR^d with support mathcalX and density functionf  mathbbR^d to mathbbR. Kraskov estimates the Shannon differential entropy\n\nH(X) = int_mathcalX f(x) log f(x) dx = mathbbE-log(f(X))\n\nSee also: entropy, KozachenkoLeonenko, EntropyEstimator.\n\n[Kraskov2004]: Kraskov, A., Stögbauer, H., & Grassberger, P. (2004). Estimating mutual information. Physical review E, 69(6), 066138.\n\n\n\n\n\n","category":"type"},{"location":"entropies/#Entropies.KozachenkoLeonenko","page":"Entropies","title":"Entropies.KozachenkoLeonenko","text":"KozachenkoLeonenko <: EntropyEstimator\nKozachenkoLeonenko(; k::Int = 1, w::Int = 1)\n\nThe KozachenkoLeonenko estimator computes the Shannon differential entropy of x (a multi-dimensional Dataset).\n\nDescription\n\nAssume we have samples bfx_1 bfx_2 ldots bfx_N  from a continuous random variable X in mathbbR^d with support mathcalX and density functionf  mathbbR^d to mathbbR. KozachenkoLeonenko estimates the Shannon differential entropy\n\nH(X) = int_mathcalX f(x) log f(x) dx = mathbbE-log(f(X))\n\nusing the nearest neighbor method from Kozachenko & Leonenko (1987)[KozachenkoLeonenko1987], as described in Charzyńska and Gambin[Charzyńska2016].\n\nw is the Theiler window, which determines if temporal neighbors are excluded during neighbor searches (defaults to 0, meaning that only the point itself is excluded when searching for neighbours).\n\nIn contrast to Kraskov, this estimator uses only the closest neighbor.\n\nSee also: entropy, Kraskov, EntropyEstimator.\n\n[Charzyńska2016]: Charzyńska, A., & Gambin, A. (2016). Improvement of the k-NN entropy estimator with applications in systems biology. Entropy, 18(1), 13.\n\n[KozachenkoLeonenko1987]: Kozachenko, L. F., & Leonenko, N. N. (1987). Sample estimate of the entropy of a random vector. Problemy Peredachi Informatsii, 23(2), 9-16.\n\n\n\n\n\n","category":"type"},{"location":"entropies/#Entropies.Zhu","page":"Entropies","title":"Entropies.Zhu","text":"Zhu <: EntropyEstimator\nZhu(k = 1, w = 0)\n\nThe Zhu estimator (Zhu et al., 2015)[Zhu2015] is an extension to KozachenkoLeonenko, and computes the Shannon differential entropy of x (a multi-dimensional Dataset).\n\nDescription\n\nAssume we have samples bfx_1 bfx_2 ldots bfx_N  from a continuous random variable X in mathbbR^d with support mathcalX and density functionf  mathbbR^d to mathbbR. Zhu estimates the Shannon differential entropy\n\nH(X) = int_mathcalX f(x) log f(x) dx = mathbbE-log(f(X))\n\nby approximating densities within hyperrectangles surrounding each point xᵢ ∈ x using using k nearest neighbor searches. w is the Theiler window, which determines if temporal neighbors are excluded during neighbor searches (defaults to 0, meaning that only the point itself is excluded when searching for neighbours).\n\nSee also: entropy, KozachenkoLeonenko, EntropyEstimator.\n\n[Zhu2015]: Zhu, J., Bellanger, J. J., Shu, H., & Le Bouquin Jeannès, R. (2015). Contribution to transfer entropy estimation via the k-nearest-neighbors approach. Entropy, 17(6), 4173-4201.\n\n\n\n\n\n","category":"type"},{"location":"entropies/#Entropies.ZhuSingh","page":"Entropies","title":"Entropies.ZhuSingh","text":"ZhuSingh <: EntropyEstimator\nZhuSingh(k = 1, w = 0)\n\nThe ZhuSingh estimator (Zhu et al., 2015)[Zhu2015] computes the Shannon differential entropy of x (a multi-dimensional Dataset).\n\nDescription\n\nAssume we have samples bfx_1 bfx_2 ldots bfx_N  from a continuous random variable X in mathbbR^d with support mathcalX and density functionf  mathbbR^d to mathbbR. ZhuSingh estimates the Shannon differential entropy\n\nH(X) = int_mathcalX f(x) log f(x) dx = mathbbE-log(f(X))\n\nLike Zhu, this estimator approximates probabilities within hyperrectangles surrounding each point xᵢ ∈ x using using k nearest neighbor searches. However, it also considers the number of neighbors falling on the borders of these hyperrectangles. This estimator is an extension to the entropy estimator in Singh et al. (2003).\n\nw is the Theiler window, which determines if temporal neighbors are excluded during neighbor searches (defaults to 0, meaning that only the point itself is excluded when searching for neighbours).\n\nSee also: entropy.\n\n[Zhu2015]: Zhu, J., Bellanger, J. J., Shu, H., & Le Bouquin Jeannès, R. (2015). Contribution to transfer entropy estimation via the k-nearest-neighbors approach. Entropy, 17(6), 4173-4201.\n\n[Singh2003]: Singh, H., Misra, N., Hnizdo, V., Fedorowicz, A., & Demchuk, E. (2003). Nearest neighbor estimates of entropy. American journal of mathematical and management sciences, 23(3-4), 301-321.\n\n\n\n\n\n","category":"type"},{"location":"entropies/#Entropies.Vasicek","page":"Entropies","title":"Entropies.Vasicek","text":"Vasicek <: EntropyEstimator\nVasicek(; m::Int = 1)\n\nThe Vasicek estimator computes the Shannon differential entropy of x (a multi-dimensional Dataset) using the method from Vasicek (1976)[Vasicek1976].\n\nThe Vasicek estimator belongs to a class of differential entropy estimators based on order statistics, of which Vasicek (1976) was the first. It only works for timeseries input.\n\nDescription\n\nAssume we have samples barX = x_1 x_2 ldots x_N  from a continuous random variable X in mathbbR with support mathcalX and density functionf  mathbbR to mathbbR. Vasicek estimates the Shannon differential entropy\n\nH(X) = int_mathcalX f(x) log f(x) dx = mathbbE-log(f(X))\n\nHowever, instead of estimating the above integral directly, it makes use of the equivalent integral, where F is the distribution function for X,\n\nH(X) = int_0^1 log left(dfracddpF^-1(p) right) dp\n\nThis integral is approximated by first computing the order statistics of barX (the input timeseries), i.e. x_(1) leq x_(2) leq cdots leq x_(n). The Vasicek Shannon differential entropy estimate is then\n\nhatH_V(barX m) =\ndfrac1n\nsum_i = 1^n log left dfracn2m (barX_(i+m) - barX_(i-m)) right\n\nUsage\n\nIn practice, choice of m influences how fast the entropy converges to the true value. For small value of m, convergence is slow, so we recommend to scale m according to the time series length n and use m >= n/100 (this is just a heuristic based on the tests written for this package).\n\n[Vasicek1976]: Vasicek, O. (1976). A test for normality based on sample entropy. Journal of the Royal Statistical Society: Series B (Methodological), 38(1), 54-59.\n\nSee also: entropy, Correa, AlizadehArghami, Ebrahimi, EntropyEstimator.\n\n\n\n\n\n","category":"type"},{"location":"entropies/#Entropies.AlizadehArghami","page":"Entropies","title":"Entropies.AlizadehArghami","text":"AlizadehArghami <: EntropyEstimator\nAlizadehArghami(; m::Int = 1)\n\nThe AlizadehArghamiestimator computes the Shannon differential entropy of x (a multi-dimensional Dataset) using the method from Alizadeh & Arghami (2010)[Alizadeh2010].\n\nThe AlizadehArghami estimator belongs to a class of differential entropy estimators based on order statistics. It only works for timeseries input.\n\nDescription\n\nAssume we have samples barX = x_1 x_2 ldots x_N  from a continuous random variable X in mathbbR with support mathcalX and density functionf  mathbbR to mathbbR. AlizadehArghami estimates the Shannon differential entropy\n\nH(X) = int_mathcalX f(x) log f(x) dx = mathbbE-log(f(X))\n\nHowever, instead of estimating the above integral directly, it makes use of the equivalent integral, where F is the distribution function for X:\n\nH(X) = int_0^1 log left(dfracddpF^-1(p) right) dp\n\nThis integral is approximated by first computing the order statistics of barX (the input timeseries), i.e. x_(1) leq x_(2) leq cdots leq x_(n). The AlizadehArghami Shannon differential entropy estimate is then the the Vasicek estimate hatH_V(barX m n), plus a correction factor\n\nhatH_A(barX m n) = hatH_V(barX m n) +\ndfrac2nleft(m log(2) right)\n\n[Alizadeh2010]: Alizadeh, N. H., & Arghami, N. R. (2010). A new estimator of entropy. Journal of the Iranian Statistical Society (JIRSS).\n\nSee also: entropy, Correa, Ebrahimi, Vasicek, EntropyEstimator.\n\n\n\n\n\n","category":"type"},{"location":"entropies/#Entropies.Ebrahimi","page":"Entropies","title":"Entropies.Ebrahimi","text":"Ebrahimi <: EntropyEstimator\nEbrahimi(; m::Int = 1)\n\nThe Ebrahimi estimator computes the Shannon entropy of x (a multi-dimensional Dataset) using the method from Ebrahimi (1994)[Ebrahimi1994].\n\nThe Ebrahimi estimator belongs to a class of differential entropy estimators based on order statistics. It only works for timeseries input.\n\nDescription\n\nAssume we have samples barX = x_1 x_2 ldots x_N  from a continuous random variable X in mathbbR with support mathcalX and density functionf  mathbbR to mathbbR. Ebrahimi estimates the Shannon differential entropy\n\nH(X) = int_mathcalX f(x) log f(x) dx = mathbbE-log(f(X))\n\nHowever, instead of estimating the above integral directly, it makes use of the equivalent integral, where F is the distribution function for X,\n\nH(X) = int_0^1 log left(dfracddpF^-1(p) right) dp\n\nThis integral is approximated by first computing the order statistics of barX (the input timeseries), i.e. x_(1) leq x_(2) leq cdots leq x_(n). The Ebrahimi Shannon differential entropy estimate is then\n\nhatH_E(barX m) =\ndfrac1n sum_i = 1^n log\nleft dfracnc_i m (barX_(i+m) - barX_(i-m)) right\n\nwhere\n\nc_i =\nbegincases\n    1 + fraci - 1m  1 geq i geq m \n    2                     m + 1 geq i geq n - m \n    1 + fracn - im  n - m + 1 geq i geq n\nendcases\n\n[Ebrahimi1994]: Ebrahimi, N., Pflughoeft, K., & Soofi, E. S. (1994). Two measures of sample entropy. Statistics & Probability Letters, 20(3), 225-234.\n\nSee also: entropy, Correa, AlizadehArghami, Vasicek, EntropyEstimator.\n\n\n\n\n\n","category":"type"},{"location":"entropies/#Entropies.Correa","page":"Entropies","title":"Entropies.Correa","text":"Correa <: EntropyEstimator\nCorrea(; m::Int = 1)\n\nThe Correa estimator computes the Shannon differential entropy of x (a multi-dimensional Dataset) using the method from Correa (1995)[Correa1995].\n\nThe Correa estimator belongs to a class of differential entropy estimators based on order statistics. It only works for timeseries input.\n\nDescription\n\nAssume we have samples barX = x_1 x_2 ldots x_N  from a continuous random variable X in mathbbR with support mathcalX and density functionf  mathbbR to mathbbR. Correa estimates the Shannon differential entropy\n\nH(X) = int_mathcalX f(x) log f(x) dx = mathbbE-log(f(X))\n\nHowever, instead of estimating the above integral directly, Correa makes use of the equivalent integral, where F is the distribution function for X,\n\nH(X) = int_0^1 log left(dfracddpF^-1(p) right) dp\n\nThis integral is approximated by first computing the order statistics of barX (the input timeseries), i.e. x_(1) leq x_(2) leq cdots leq x_(n), ensuring that end points are included. The Correa estimate of Shannon differential entropy is then\n\nH_C(barX m n) =\ndfrac1n sum_i = 1^n log\nleft dfrac sum_j=i-m^i+m(barX_(j) -\ntildeX_(i))(j - i)n sum_j=i-m^i+m (barX_(j) - tildeX_(i))^2\nright\n\nwhere\n\ntildeX_(i) = dfrac12m + 1 sum_j = i - m^i + m X_(j)\n\n[Correa1995]: Correa, J. C. (1995). A new estimator of entropy. Communications in Statistics-Theory and Methods, 24(10), 2439-2449.\n\nSee also: entropy, AlizadehArghami, Ebrahimi, Vasicek, EntropyEstimator.\n\n\n\n\n\n","category":"type"},{"location":"entropies/#Convenience-functions","page":"Entropies","title":"Convenience functions","text":"","category":"section"},{"location":"entropies/","page":"Entropies","title":"Entropies","text":"In this subsection we expand documentation strings of \"entropy names\" that are used commonly in the literature, such as \"permutation entropy\". As we made clear in API & terminology, these are just the existing Shannon entropy with a particularly chosen probability estimator. We have only defined convenience functions for the most used names, and arbitrary more specialized convenience functions can be easily defined in a couple lines of code.","category":"page"},{"location":"entropies/","page":"Entropies","title":"Entropies","text":"entropy_permutation\nentropy_spatial_permutation\nentropy_wavelet\nentropy_dispersion","category":"page"},{"location":"entropies/#Entropies.entropy_permutation","page":"Entropies","title":"Entropies.entropy_permutation","text":"entropy_permutation(x; τ = 1, m = 3, base = 2)\n\nCompute the permutation entropy of x of order m with delay/lag τ. This function is just a convenience call to:\n\nest = SymbolicPermutation(; m, τ)\nentropy(Shannon(base), x, est)\n\nSee SymbolicPermutation for more info. Similarly, one can use SymbolicWeightedPermutation or SymbolicAmplitudeAwarePermutation for the weighted/amplitude-aware versions.\n\n\n\n\n\n","category":"function"},{"location":"entropies/#Entropies.entropy_spatial_permutation","page":"Entropies","title":"Entropies.entropy_spatial_permutation","text":"entropy_spatial_permutation(x, stencil; periodic = true; kwargs...)\n\nCompute the spatial permutation entropy of x given the stencil. Here x must be a matrix or higher dimensional Array containing spatial data. This function is just a convenience call to:\n\nest = SpatialSymbolicPermutation(stencil, x, periodic)\nentropy(Renyi(;kwargs...), est, x)\n\nSee SpatialSymbolicPermutation for more info, or how to encode stencils.\n\n\n\n\n\n","category":"function"},{"location":"entropies/#Entropies.entropy_wavelet","page":"Entropies","title":"Entropies.entropy_wavelet","text":"entropy_wavelet(x; wavelet = Wavelets.WT.Daubechies{12}(), base = 2)\n\nCompute the wavelet entropy. This function is just a convenience call to:\n\nest = WaveletOverlap(wavelet)\nentropy(Shannon(base), est, x)\n\nSee WaveletOverlap for more info.\n\n\n\n\n\n","category":"function"},{"location":"entropies/#Entropies.entropy_dispersion","page":"Entropies","title":"Entropies.entropy_dispersion","text":"entropy_dispersion(x; base = 2, kwargs...)\n\nCompute the dispersion entropy. This function is just a convenience call to:\n\nest = Dispersion(kwargs...)\nentropy(Shannon(base), est, x)\n\nSee Dispersion for more info.\n\n\n\n\n\n","category":"function"},{"location":"probabilities/#probabilities_estimators","page":"Probabilities","title":"Probabilities","text":"","category":"section"},{"location":"probabilities/#Probabilities-API","page":"Probabilities","title":"Probabilities API","text":"","category":"section"},{"location":"probabilities/","page":"Probabilities","title":"Probabilities","text":"The probabilities API is defined by","category":"page"},{"location":"probabilities/","page":"Probabilities","title":"Probabilities","text":"ProbabilitiesEstimator\nprobabilities\nprobabilities_and_outcomes","category":"page"},{"location":"probabilities/","page":"Probabilities","title":"Probabilities","text":"ProbabilitiesEstimator\nprobabilities\nprobabilities!\nProbabilities\nprobabilities_and_outcomes\noutcomes\noutcome_space\ntotal_outcomes\nmissing_outcomes","category":"page"},{"location":"probabilities/#Entropies.ProbabilitiesEstimator","page":"Probabilities","title":"Entropies.ProbabilitiesEstimator","text":"ProbabilitiesEstimator\n\nThe supertype for all probabilities estimators.\n\nIn Entropies.jl, probability distributions are estimated from data by defining a set of possible outcomes Omega = omega_1 omega_2 ldots omega_L , and assigning to each outcome omega_i a probability p(omega_i), such that sum_i=1^N p(omega_i) = 1. It is the role a ProbabilitiesEstimator to\n\nDefine Omega, the \"outcome space\", which is the set of all possible outcomes over  which probabilities are estimated. The cardinality of this set can be obtained using  total_outcomes.\nDefine how probabilities p_i = p(omega_i) are assigned to outcomes\\omega_i``.\n\nIn practice, probability estimation is done by calling probabilities with some input data and one of the following probabilities estimators. The result is a Probabilities p (Vector-like), where each element p[i] is the probability of the outcome ω[i]. Use probabilities_and_outcomes if you need both the probabilities and the outcomes and outcome_space to obtain Omega. The element type of Omega varies between estimators, but it is guranteed to be hashable. This allows for conveniently tracking the probability of a specific event across experimental realizations, by using the outcome as a dictionary key and the probability as the value for that key (or, alternatively, the key remains the outcome and one has a vector of probabilities, one for each experimental realization).\n\nWe have made the design decision that all probabilities estimators have a well defined outcome space when instantiated. For some estimators this means that the input data x must be provided both when instantiating the estimator, but also when computing the probabilities.\n\nAll currently implemented probability estimators are:\n\nCountOccurrences.\nValueHistogram.\nTransferOperator.\nDispersion.\nSpatialDispersion.\nWaveletOverlap.\nPowerSpectrum.\nSymbolicPermutation.\nSymbolicWeightedPermutation.\nSymbolicAmplitudeAwarePermutation.\nSpatialSymbolicPermutation.\nNaiveKernel.\n\n\n\n\n\n","category":"type"},{"location":"probabilities/#Entropies.probabilities","page":"Probabilities","title":"Entropies.probabilities","text":"probabilities(est::ProbabilitiesEstimator, x::Array_or_Dataset) → p::Probabilities\n\nCompute a probability distribution over the set of possible outcomes defined by the probabilities estimator est, given input data x. To obtain the outcomes use outcomes.\n\nThe returned probabilities p may or may not be ordered, and may or may not contain 0s; see the documentation of the individual estimators for more. Configuration options are always given as arguments to the chosen estimator. x is typically an Array or a Dataset; see Input data for Entropies.jl.\n\nprobabilities(x::Array_or_Dataset) → p::Probabilities\n\nEstimate probabilities by directly counting the elements of x, assuming that Ω = sort(unique(x)), i.e. that the outcome space is the unique elements of x. This is mostly useful when x contains categorical or integer data.\n\nSee also: Probabilities, ProbabilitiesEstimator.\n\n\n\n\n\n","category":"function"},{"location":"probabilities/#Entropies.probabilities!","page":"Probabilities","title":"Entropies.probabilities!","text":"probabilities!(s, args...)\n\nSimilar to probabilities(args...), but allows pre-allocation of temporarily used containers s.\n\nOnly works for certain estimators. See for example SymbolicPermutation.\n\n\n\n\n\n","category":"function"},{"location":"probabilities/#Entropies.Probabilities","page":"Probabilities","title":"Entropies.Probabilities","text":"Probabilities <: AbstractVector\nProbabilities(x) → p\n\nProbabilities is a simple wrapper around AbstractVector that ensures its values sum to 1, so that p can be interpreted as probability distribution.\n\n\n\n\n\n","category":"type"},{"location":"probabilities/#Entropies.probabilities_and_outcomes","page":"Probabilities","title":"Entropies.probabilities_and_outcomes","text":"probabilities_and_outcomes(est, x)\n\nReturn probs, outs, where probs = probabilities(x, est) and outs[i] is the outcome with probability probs[i]. The element type of outs depends on the estimator. outs is a subset of the outcome_space of est.\n\nSee also outcomes, total_outcomes.\n\n\n\n\n\n","category":"function"},{"location":"probabilities/#Entropies.outcomes","page":"Probabilities","title":"Entropies.outcomes","text":"outcomes(est::ProbabilitiesEstimator, x)\n\nReturn all (unique) outcomes contained in x according to the given estimator. Equivalent with probabilities_and_outcomes(x, est)[2], but for some estimators it may be explicitly extended for better performance.\n\n\n\n\n\n","category":"function"},{"location":"probabilities/#Entropies.outcome_space","page":"Probabilities","title":"Entropies.outcome_space","text":"outcome_space(est::ProbabilitiesEstimator) → Ω\n\nReturn a container containing all possible outcomes of est.\n\n\n\n\n\n","category":"function"},{"location":"probabilities/#Entropies.total_outcomes","page":"Probabilities","title":"Entropies.total_outcomes","text":"total_outcomes(est::ProbabilitiesEstimator)\n\nReturn the length (cardinality) of the outcome space Omega of est.\n\n\n\n\n\n","category":"function"},{"location":"probabilities/#Entropies.missing_outcomes","page":"Probabilities","title":"Entropies.missing_outcomes","text":"missing_outcomes(est::ProbabilitiesEstimator, x) → n_missing::Int\n\nEstimate a probability distribution for x using the given estimator, then count the number of missing (i.e. zero-probability) outcomes.\n\nSee also: MissingDispersionPatterns.\n\n\n\n\n\n","category":"function"},{"location":"probabilities/#Overview","page":"Probabilities","title":"Overview","text":"","category":"section"},{"location":"probabilities/","page":"Probabilities","title":"Probabilities","text":"Any of the following estimators can be used with probabilities.","category":"page"},{"location":"probabilities/","page":"Probabilities","title":"Probabilities","text":"Estimator Principle Input data\nCountOccurrences Frequencies Vector, Dataset\nValueHistogram Binning (histogram) Vector, Dataset\nTransferOperator Binning (transfer operator) Vector, Dataset\nNaiveKernel Kernel density estimation Dataset\nSymbolicPermutation Ordinal patterns Vector\nSymbolicWeightedPermutation Ordinal patterns Vector\nSymbolicAmplitudeAwarePermutation Ordinal patterns Vector\nDispersion Dispersion patterns Vector\nDiversity Cosine similarity Vector\nWaveletOverlap Wavelet transform Vector\nPowerSpectrum Fourier spectra Vector, Dataset","category":"page"},{"location":"probabilities/#Count-occurrences-(counting)","page":"Probabilities","title":"Count occurrences (counting)","text":"","category":"section"},{"location":"probabilities/","page":"Probabilities","title":"Probabilities","text":"CountOccurrences","category":"page"},{"location":"probabilities/#Entropies.CountOccurrences","page":"Probabilities","title":"Entropies.CountOccurrences","text":"CountOccurrences(x)\n\nA probabilities/entropy estimator based on straight-forward counting of distinct elements in a univariate time series or multivariate dataset. This is the same as giving no estimator to probabilities.\n\nOutcome space\n\nThe outcome space is the unique sorted values of the input. Hence, input x is needed for a well-defined outcome space.\n\n\n\n\n\n","category":"type"},{"location":"probabilities/#Visitation-frequency-(histograms)","page":"Probabilities","title":"Visitation frequency (histograms)","text":"","category":"section"},{"location":"probabilities/","page":"Probabilities","title":"Probabilities","text":"ValueHistogram\nRectangularBinning\nFixedRectangularBinning","category":"page"},{"location":"probabilities/#Entropies.ValueHistogram","page":"Probabilities","title":"Entropies.ValueHistogram","text":"ValueHistogram(x, b::RectangularBinning) <: ProbabilitiesEstimator\nValueHistogram(b::FixedRectangularBinning) <: ProbabilitiesEstimator\n\nA probability estimator based on binning the values of the data as dictated by the binning scheme b and formally computing their histogram, i.e., the frequencies of points in the bins. An alias to this is VisitationFrequency. Available binnings are:\n\nRectangularBinning\nFixedRectangularBinning\n\nNotice that if not using the fixed binning, x (the input data) must also be given to the estimator, as it is not possible to deduce histogram size only from the binning.\n\nThe ValueHistogram estimator has a linearithmic time complexity (n log(n) for n = length(x)) and a linear space complexity (l for l = dimension(x)). This allows computation of probabilities (histograms) of high-dimensional datasets and with small box sizes ε without memory overflow and with maximum performance. For performance reasons, the probabilities returned never contain 0s and are arbitrarily ordered.\n\nValueHistogram(x, ϵ::Union{Real,Vector})\n\nA convenience method that accepts same input as RectangularBinning and initializes this binning directly.\n\nOutcomes\n\nThe outcome space for ValueHistogram is the unique bins constructed from b. Each bin is identified by its left (lowest-value) corner. The bins are in data units, not integer (cartesian indices units), and are returned as SVectors.\n\nSee also: RectangularBinning.\n\n\n\n\n\n","category":"type"},{"location":"probabilities/#Entropies.RectangularBinning","page":"Probabilities","title":"Entropies.RectangularBinning","text":"RectangularBinning(ϵ) <: AbstractBinning\n\nRectangular box partition of state space using the scheme ϵ, deducing the coordinates of the grid axis minima from the input data. Generally it is preferred to use FixedRectangularBinning instead, as it has a well defined outcome space without knowledge of input data.\n\nBinning instructions are deduced from the type of ϵ as follows:\n\nϵ::Int divides each coordinate axis into ϵ equal-length intervals  that cover all data.\nϵ::Float64 divides each coordinate axis into intervals of fixed size ϵ, starting  from the axis minima until the data is completely covered by boxes.\nϵ::Vector{Int} divides the i-th coordinate axis into ϵ[i] equal-length  intervals that cover all data.\nϵ::Vector{Float64} divides the i-th coordinate axis into intervals of fixed size  ϵ[i], starting from the axis minima until the data is completely covered by boxes.\n\n\n\n\n\n","category":"type"},{"location":"probabilities/#Entropies.FixedRectangularBinning","page":"Probabilities","title":"Entropies.FixedRectangularBinning","text":"FixedRectangularBinning <: AbstractBinning\nFixedRectangularBinning(ϵmin::NTuple, ϵmax::NTuple, N::Int)\n\nRectangular box partition of state space where the extent of the grid is explicitly specified by ϵmin and emax, and along each dimension, the grid is subdivided into N subintervals. Points falling outside the partition do not attribute to probabilities. This binning type leads to a well-defined outcome space without knowledge of input, see ValueHistogram.\n\nϵmin/emax must be NTuple{D, <:Real} for input of D-dimensional data.\n\nFixedRectangularBinning(ϵmin::Real, ϵmax::Real, N::Int, D::Int = 1)\n\nThis is a convenience method where each dimension of the binning has the same extent and the input data are D dimensional, which defaults to 1 (timeseries).\n\n\n\n\n\n","category":"type"},{"location":"probabilities/#Permutation-(symbolic)","page":"Probabilities","title":"Permutation (symbolic)","text":"","category":"section"},{"location":"probabilities/","page":"Probabilities","title":"Probabilities","text":"SymbolicPermutation\nSymbolicWeightedPermutation\nSymbolicAmplitudeAwarePermutation\nSpatialSymbolicPermutation","category":"page"},{"location":"probabilities/#Entropies.SymbolicPermutation","page":"Probabilities","title":"Entropies.SymbolicPermutation","text":"SymbolicPermutation <: ProbabilitiesEstimator\nSymbolicPermutation(; m = 3, τ = 1, lt::Function = Entropies.isless_rand)\n\nA probabilities estimator based on ordinal permutation patterns.\n\nThe quantity computed depends on the input data:\n\nUnivariate data. If applied to a univariate time series, then the time series   is first embedded using embedding delay τ and dimension m, resulting in embedding   vectors  bfx_i _i=1^N-(m-1)tau. Then, for each bfx_i,   we find its permutation pattern pi_i, which we internally encode a an integer   s_i in mathbbN^+ for efficient computation (integer symbols are obtained by   using encode with OrdinalPatternEncoding).   Probabilities are then   estimated as naive frequencies over the encoded permutation symbols    s_i _i=1^N-(m-1)tau by using CountOccurrences.   The resulting probabilities can be used to compute permutation entropy (PE;   Bandt & Pompe, 2002[BandtPompe2002]).\nMultivariate data. If applied to a an D-dimensional Dataset,   then it is assumed that the input data represents N observations of a multivariate   system  bfx_i _i=1^N, and no embedding is constructed.   For each bfx_i in mathbbR^D, we direct find its permutation pattern   pi_i and encode it as s_i in mathbbN^+ (i.e. est.τ and est.m are   ignored, and we set m = D instead). Finally, probabilities are estimated as relative   frequencies of occurrences of the encoded permutation symbols.   The resulting probabilities can be used to compute multivariate permutation   entropy (MvPE; He et al., 2016[He2016]), but here we don't perform any subdivision   of the permutation patterns (see Figure 3 in He et al., 2016).\n\nOutcome space\n\nThe outcome space Ω for SymbolicPermutation is the set of length-m ordinal patterns (i.e. permutations) that can be formed by the integers 1, 2, …, m, ordered lexicographically. There are factorial(m) such patterns.\n\nIn-place symbolization\n\nSymbolicPermutation also implements the in-place entropy! and probabilities!. The length of the pre-allocated symbol vector must match the length of the embedding: N - (m-1)τ for univariate time series, and M for length-M Datasets), i.e.\n\nusing DelayEmbeddings, Entropies\nm, τ, N = 2, 1, 100\nest = SymbolicPermutation(; m, τ)\n\n# For a time series\nx_ts = rand(N)\nπs_ts = zeros(Int, N - (m - 1)*τ)\np = probabilities!(πs_ts, est, x_ts)\nh = entropy!(πs_ts, Renyi(), est, x_ts)\n\n# For a pre-discretized `Dataset`\nx_symb = outcomes(x_ts, OrdinalPatternEncoding(m = 2, τ = 1))\nx_d = genembed(x_symb, (0, -1, -2))\nπs_d = zeros(Int, length(x_d))\np = probabilities!(πs_d, est, x_d)\nh = entropy!(πs_d, Renyi(), est, x_d)\n\nSee SymbolicWeightedPermutation and SymbolicAmplitudeAwarePermutation for estimators that not only consider ordinal (sorting) patterns, but also incorporate information about within-state-vector amplitudes.\n\nnote: Handling equal values in ordinal patterns\nIn Bandt & Pompe (2002), equal values are ordered after their order of appearance, but this can lead to erroneous temporal correlations, especially for data with low-amplitude resolution [Zunino2017]. Here, by default, if two values are equal, then one of the is randomly assigned as \"the largest\", using lt = Entropies.isless_rand. To get the behaviour from Bandt and Pompe (2002), use lt = Base.isless).\n\n[BandtPompe2002]: Bandt, Christoph, and Bernd Pompe. \"Permutation entropy: a natural complexity measure for time series.\" Physical review letters 88.17 (2002): 174102.\n\n[Zunino2017]: Zunino, L., Olivares, F., Scholkmann, F., & Rosso, O. A. (2017). Permutation entropy based time series analysis: Equalities in the input signal can lead to false conclusions. Physics Letters A, 381(22), 1883-1892.\n\n[He2016]: He, S., Sun, K., & Wang, H. (2016). Multivariate permutation entropy and its application for complexity analysis of chaotic systems. Physica A: Statistical Mechanics and its Applications, 461, 812-823.\n\n\n\n\n\n","category":"type"},{"location":"probabilities/#Entropies.SymbolicWeightedPermutation","page":"Probabilities","title":"Entropies.SymbolicWeightedPermutation","text":"SymbolicWeightedPermutation <: ProbabilitiesEstimator\nSymbolicWeightedPermutation(; τ = 1, m = 3, lt = Entropies.isless_rand)\n\nA variant of SymbolicPermutation that also incorporates amplitude information, based on the weighted permutation entropy (Fadlallah et al., 2013).\n\nOutcome space\n\nLike for SymbolicPermutation, the outcome space Ω for SymbolicWeightedPermutation is the lexiographically ordered set of length-m ordinal patterns (i.e. permutations) that can be formed by the integers 1, 2, …, m. There are factorial(m) such patterns.\n\nDescription\n\nProbabilities are computed as\n\np(pi_i^m tau) = dfracsum_k=1^N mathbf1_uS(u) = s_i\nleft( mathbfx_k^m tau right)\n w_ksum_k=1^N mathbf1_uS(u) in Pi\nleft( mathbfx_k^m tau right) w_k = dfracsum_k=1^N\nmathbf1_uS(u) = s_i\nleft( mathbfx_k^m tau right)  w_ksum_k=1^N w_k\n\nwhere weights are computed based on the variance of the state vectors as\n\nw_j = dfrac1msum_k=1^m (x_j+(k+1)tau - mathbfhatx_j^m tau)^2\n\nand mathbfx_i is the aritmetic mean of state vector:\n\nmathbfhatx_j^m tau = frac1m sum_k=1^m x_j + (k+1)tau\n\nThe weighted permutation entropy is equivalent to regular permutation entropy when weights are positive and identical (w_j = beta  forall  j leq N and beta  0).\n\nSee SymbolicPermutation for an estimator that only incorporates ordinal/sorting information and disregards amplitudes, and SymbolicAmplitudeAwarePermutation for another estimator that incorporates amplitude information.\n\nnote: An implementation note\nNote: in equation 7, section III, of the original paper, the authors writew_j = dfrac1msum_k=1^m (x_j-(k-1)tau - mathbfhatx_j^m tau)^2But given the formula they give for the arithmetic mean, this is not the variance of mathbfx_i, because the indices are mixed: x_j+(k-1)tau in the weights formula, vs. x_j+(k+1)tau in the arithmetic mean formula. This seems to imply that amplitude information about previous delay vectors are mixed with mean amplitude information about current vectors. The authors also mix the terms \"vector\" and \"neighboring vector\" (but uses the same notation for both), making it hard to interpret whether the sign switch is a typo or intended. Here, we use the notation above, which actually computes the variance for mathbfx_i.\n\n[Fadlallah2013]: Fadlallah, Bilal, et al. \"Weighted-permutation entropy: A complexity measure for time series incorporating amplitude information.\" Physical Review E 87.2 (2013): 022911.\n\n\n\n\n\n","category":"type"},{"location":"probabilities/#Entropies.SymbolicAmplitudeAwarePermutation","page":"Probabilities","title":"Entropies.SymbolicAmplitudeAwarePermutation","text":"SymbolicAmplitudeAwarePermutation <: ProbabilitiesEstimator\nSymbolicAmplitudeAwarePermutation(; τ = 1, m = 3, A = 0.5, lt = Entropies.isless_rand)\n\nA variant of SymbolicPermutation that also incorporates amplitude information, based on the amplitude-aware permutation entropy (Azami & Escudero, 2016).\n\nOutcome space\n\nLike for SymbolicPermutation, the outcome space Ω for SymbolicAmplitudeAwarePermutation is the lexiographically ordered set of length-m ordinal patterns (i.e. permutations) that can be formed by the integers 1, 2, …, m. There are factorial(m) such patterns.\n\nDescription\n\nProbabilities are computed as\n\np(pi_i^m tau) =\ndfracsum_k=1^N\nmathbf1_uS(u) = s_i left( mathbfx_k^m tau right)  a_ksum_k=1^N\nmathbf1_uS(u) in Pi left( mathbfx_k^m tau right) a_k =\ndfracsum_k=1^N mathbf1_uS(u) = s_i\nleft( mathbfx_k^m tau right)  a_ksum_k=1^N a_k\n\nThe weights encoding amplitude information about state vector mathbfx_i = (x_1^i x_2^i ldots x_m^i) are\n\na_i = dfracAm sum_k=1^m x_k^i  + dfrac1-Ad-1\nsum_k=2^d x_k^i - x_k-1^i\n\nwith 0 leq A leq 1. When A=0 , only internal differences between the elements of mathbfx_i are weighted. Only mean amplitude of the state vector elements are weighted when A=1. With, 0A1, a combined weighting is used.\n\nSee SymbolicPermutation for an estimator that only incorporates ordinal/sorting information and disregards amplitudes, and SymbolicWeightedPermutation for another estimator that incorporates amplitude information.\n\n[Azami2016]: Azami, H., & Escudero, J. (2016). Amplitude-aware permutation entropy: Illustration in spike detection and signal segmentation. Computer methods and programs in biomedicine, 128, 40-51.\n\n\n\n\n\n","category":"type"},{"location":"probabilities/#Entropies.SpatialSymbolicPermutation","page":"Probabilities","title":"Entropies.SpatialSymbolicPermutation","text":"SpatialSymbolicPermutation <: ProbabilitiesEstimator\nSpatialSymbolicPermutation(stencil, x; periodic = true)\n\nA symbolic, permutation-based probabilities estimator for spatiotemporal systems that generalises SymbolicPermutation to high-dimensional arrays.\n\nSpatialSymbolicPermutation is based on the 2D and 3D spatiotemporal permutation entropy estimators by by Ribeiro et al. (2012)[Ribeiro2012] and Schlemmer et al. (2018)[Schlemmer2018]), respectively, but is here implemented as a pure probabilities probabilities estimator that is generalized for N-dimensional input data x, with arbitrary neighborhood regions (stencils) and periodic boundary conditions.\n\nIn combination with entropy and entropy_normalized, this probabilities estimator can be used to compute (normalized) generalized spatiotemporal permutation Entropy of any type.\n\nArguments\n\nstencil. Defines what local area (hyperrectangle), or which points within this area,   to include around each hypervoxel (i.e. pixel in 2D). The examples below demonstrate   different ways of specifying stencils. For details, see   SpatialSymbolicPermutation.\nx::AbstractArray. The input data. Must be provided because we need to know its size   for optimization and bound checking.\n\nKeyword arguments\n\nperiodic::Bool. If periodic == true, then the stencil should wrap around at the   end of the array. If periodic = false, then pixels whose stencil exceeds the array   bounds are skipped.\n\nStencils\n\nStencils are passed in one of the following three ways:\n\nAs vectors of CartesianIndex which encode the pixels to include in the  stencil, with respect to the current pixel, or integer arrays of the same dimensionality  as the data. For example stencil = CartesianIndex.([(0,0), (0,1), (1,1), (1,0)]).  Don't forget to include the zero offset index if you want to include the point itself,  which is almost always the case.  Here the stencil creates a 2x2 square extending to the bottom and right of the pixel  (directions here correspond to the way Julia prints matrices by default).  When passing a stencil as a vector of CartesianIndex, m = length(stencil).\nAs a D-dimensional array (where D matches the dimensionality of the input data)  containing 0s and 1s, where if stencil[index] == 1, the corresponding pixel is  included, and if stencil[index] == 0, it is not included.  To generate the same estimator as in 1., use stencil = [1 1; 1 1].  When passing a stencil as a D-dimensional array, m = sum(stencil)\nAs a Tuple containing two Tuples, both of length D, for D-dimensional data.  The first tuple specifies the extent of the stencil, where extent[i]  dictates the number of pixels to be included along the ith axis and lag[i]  the separation of pixels along the same axis.  This method can only generate (hyper)rectangular stencils. To create the same estimator as  in the previous examples, use here stencil = ((2, 2), (1, 1)).  When passing a stencil using extent and lag, m = prod(extent)!.\n\nExample: spatiotemporal entropy for time series\n\nUsage is simple. First, define a SpatialSymbolicPermutation estimator by specifying a stencil and giving some input data (a matrix with the same dimensions as the data as you're going to analyse). Then simply call entropy with the estimator.\n\nusing Entropies\nx = rand(50, 50) # first \"time slice\" of a spatial system evolution\nstencil = [1 1; 0 1] # or one of the other ways of specifying stencils\nest = SpatialSymbolicPermutation(stencil, x)\nh = entropy(est, x)\n\nTo apply this to timeseries of spatial data, simply loop over the call, e.g.:\n\ndata = [rand(50, 50) for i in 1:50]\nest = SpatialSymbolicPermutation(stencil, first(data))\nh_vs_t = [entropy(est, d) for d in data]\n\nComputing generalized spatiotemporal permutation entropy is trivial, e.g. with Renyi:\n\nx = reshape(repeat(1:5, 500) .+ 0.1*rand(500*5), 50, 50)\nest = SpatialSymbolicPermutation(stencil, x)\nentropy(Renyi(q = 2), est, x)\n\nOutcome space\n\nThe outcome space Ω for SpatialSymbolicPermutation is the set of length-m ordinal patterns (i.e. permutations) that can be formed by the integers 1, 2, …, m, ordered lexicographically. There are factorial(m) such patterns. Here, m refers to the number of points included by stencil.\n\n[Ribeiro2012]: Ribeiro et al. (2012). Complexity-entropy causality plane as a complexity measure for two-dimensional patterns. https://doi.org/10.1371/journal.pone.0040689\n\n[Schlemmer2018]: Schlemmer et al. (2018). Spatiotemporal Permutation Entropy as a Measure for Complexity of Cardiac Arrhythmia. https://doi.org/10.3389/fphy.2018.00039\n\n\n\n\n\n","category":"type"},{"location":"probabilities/#Dispersion-(symbolic)","page":"Probabilities","title":"Dispersion (symbolic)","text":"","category":"section"},{"location":"probabilities/","page":"Probabilities","title":"Probabilities","text":"Dispersion\nSpatialDispersion","category":"page"},{"location":"probabilities/#Entropies.Dispersion","page":"Probabilities","title":"Entropies.Dispersion","text":"Dispersion(; c = 5, m = 2, τ = 1, check_unique = true)\n\nA probability estimator based on dispersion patterns, originally used by Rostaghi & Azami, 2016[Rostaghi2016] to compute the \"dispersion entropy\", which characterizes the complexity and irregularity of a time series.\n\nRecommended parameter values[Li2018] are m ∈ [2, 3], τ = 1 for the embedding, and c ∈ [3, 4, …, 8] categories for the Gaussian symbol mapping.\n\nDescription\n\nAssume we have a univariate time series X = x_i_i=1^N. First, this time series is discretized a \"Gaussian encoding\", which uses the normal cumulative distribution function (CDF) to encode a timeseries x as integers like so:\n\nEach x_i is mapped to a new real number y_i in 0 1 by using the normal   cumulative distribution function (CDF), x_i to y_i  y_i = dfrac1 sigma   sqrt2 pi int_-infty^x_i e^(-(x_i - mu)^2)(2 sigma^2) dx,   where mu and sigma are the empirical mean and standard deviation of X.   Other choices of CDFs are also possible, but currently only Gaussian is implemented.\nNext, each y_i is linearly mapped to an integer   z_i in 1 2 ldots c using the map   y_i to z_i  z_i = textfloor(y_i  (1  c)) + 1 (note: this mapping differs   slightly from the linear mapping in the original paper). This procedure subdivides the   interval 0 1 into c   different subintervals that form a covering of 0 1, and assigns each y_i to one   of these subintervals. The original time series X is thus transformed to a symbol time   series S =  s_i _i=1^N, where s_i in 1 2 ldots c.\nNext, the symbol time series S is embedded into an   m-dimensional time series, using an embedding lag of tau = 1, which yields a   total of N - (m - 1)tau points, or \"dispersion patterns\". Because each z_i can   take on c different values, and each embedding point has m values, there   are c^m possible dispersion patterns. This number is used for normalization when   computing dispersion entropy.\n\nComputing dispersion probabilities and entropy\n\nA probability distribution P = p_i _i=1^c^m, where sum_i^c^m p_i = 1, can then be estimated by counting and sum-normalising the distribution of dispersion patterns among the embedding vectors. Note that dispersion patterns that are not present are not counted. Therefore, you'll always get non-zero probabilities using the Dispersion probability estimator.\n\nOutcome space\n\nThe outcome space for Dispersion is the unique delay vectors whose elements are the the symbols (integers) encoded by the Gaussian CDF. Hence, the outcome space is all m-dimensional delay vectors whose elements are all possible values in 1:c. There are c ^ m such vectors.\n\nData requirements and parameters\n\nThe input must have more than one unique element for the Gaussian mapping to be well-defined. Li et al. (2018) recommends that x has at least 1000 data points.\n\nIf check_unique == true (default), then it is checked that the input has more than one unique value. If check_unique == false and the input only has one unique element, then a InexactError is thrown when trying to compute probabilities.\n\nnote: Why 'dispersion patterns'?\nEach embedding vector is called a \"dispersion pattern\". Why? Let's consider the case when m = 5 and c = 3, and use some very imprecise terminology for illustration:When c = 3, values clustering far below mean are in one group, values clustered around the mean are in one group, and values clustering far above the mean are in a third group. Then the embedding vector 2 2 2 2 2 consists of values that are relatively close together (close to the mean), so it represents a set of numbers that are not very spread out (less dispersed). The embedding vector 1 1 2 3 3, however, represents numbers that are much more spread out (more dispersed), because the categories representing \"outliers\" both above and below the mean are represented, not only values close to the mean.\n\n[Rostaghi2016]: Rostaghi, M., & Azami, H. (2016). Dispersion entropy: A measure for time-series analysis. IEEE Signal Processing Letters, 23(5), 610-614.\n\n[Li2018]: Li, G., Guan, Q., & Yang, H. (2018). Noise reduction method of underwater acoustic signals based on CEEMDAN, effort-to-compress complexity, refined composite multiscale dispersion entropy and wavelet threshold denoising. Entropy, 21(1), 11.\n\n\n\n\n\n","category":"type"},{"location":"probabilities/#Entropies.SpatialDispersion","page":"Probabilities","title":"Entropies.SpatialDispersion","text":"SpatialDispersion <: ProbabilitiesEstimator\nSpatialDispersion(stencil, x::AbstractArray;\n    periodic = true,\n    c = 5,\n    skip_encoding = false,\n    L = nothing,\n)\n\nA dispersion-based probabilities estimator that generalises Dispersion for input data that are high-dimensional arrays.\n\nSpatialDispersion is based on Azami et al. (2019)[Azami2019]'s 2D square dispersion (Shannon) entropy estimator, but is here implemented as a pure probabilities probabilities estimator that is generalized for N-dimensional input data x, with arbitrary neighborhood regions (stencils) and (optionally) periodic boundary conditions.\n\nIn combination with entropy and entropy_normalized, this probabilities estimator can be used to compute (normalized) generalized spatiotemporal dispersion Entropy of any type.\n\nArguments\n\nstencil. Defines what local area (hyperrectangle), or which points within this area,   to include around each hypervoxel (i.e. pixel in 2D). The examples below demonstrate   different ways of specifying stencils. For details, see   SpatialSymbolicPermutation. See SpatialSymbolicPermutation for   more information about stencils.\nx::AbstractArray. The input data. Must be provided because we need to know its size   for optimization and bound checking.\n\nKeyword arguments\n\nperiodic::Bool. If periodic == true, then the stencil should wrap around at the   end of the array. If periodic = false, then pixels whose stencil exceeds the array   bounds are skipped.\nc::Int. Determines how many discrete categories to use for the Gaussian encoding.\nskip_encoding. If skip_encoding == true, encoding is ignored, and dispersion   patterns are computed directly from x, under the assumption that L is the alphabet   length for x (useful for categorical or integer data). Thus, if   skip_encoding == true, then L must also be specified. This is useful for   categorical or integer-valued data.\nL. If L == nothing (default), then the number of total outcomes is inferred from   stencil and encoding. If L is set to an integer, then the data is considered   pre-encoded and the number of total outcomes is set to L.\n\nOutcome space\n\nThe outcome space for SpatialDispersion is the unique delay vectors whose elements are the the symbols (integers) encoded by the Gaussian CDF. Hence, the outcome space is all m-dimensional delay vectors whose elements are all possible values in 1:c. There are c ^ m such vectors.\n\nDescription\n\nEstimating probabilities/entropies from higher-dimensional data is conceptually simple.\n\nDiscretize each value (hypervoxel) in x relative to all other values xᵢ ∈ x using the  provided encoding scheme.\nUse stencil to extract relevant (discretized) points around each hypervoxel.\nConstruct a symbol these points.\nTake the sum-normalized histogram of the symbol as a probability distribution.\nOptionally, compute entropy or entropy_normalized from this  probability distribution.\n\nUsage\n\nHere's how to compute spatial dispersion entropy using the three different ways of specifying stencils.\n\nx = rand(50, 50) # first \"time slice\" of a spatial system evolution\n\n# Cartesian stencil\nstencil_cartesian = CartesianIndex.([(0,0), (1,0), (1,1), (0,1)])\nest = SpatialDispersion(stencil_cartesian, x)\nentropy_normalized(est, x)\n\n# Extent/lag stencil\nextent = (2, 2); lag = (1, 1); stencil_ext_lag = (extent, lag)\nest = SpatialDispersion(stencil_ext_lag, x)\nentropy_normalized(est, x)\n\n# Matrix stencil\nstencil_matrix = [1 1; 1 1]\nest = SpatialDispersion(stencil_matrix, x)\nentropy_normalized(est, x)\n\nTo apply this to timeseries of spatial data, simply loop over the call (broadcast), e.g.:\n\nimgs = [rand(50, 50) for i = 1:100]; # one image per second over 100 seconds\nstencil = ((2, 2), (1, 1)) # a 2x2 stencil (i.e. dispersion patterns of length 4)\nest = SpatialDispersion(stencil, first(imgs))\nh_vs_t = entropy_normalized.(Ref(est), imgs)\n\nComputing generalized spatiotemporal dispersion entropy is trivial, e.g. with Renyi:\n\nx = reshape(repeat(1:5, 500) .+ 0.1*rand(500*5), 50, 50)\nest = SpatialDispersion(stencil, x)\nentropy(Renyi(q = 2), est, x)\n\nSee also: SpatialSymbolicPermutation, GaussianCDFEncoding, symbolize.\n\n[Azami2019]: Azami, H., da Silva, L. E. V., Omoto, A. C. M., & Humeau-Heurtier, A. (2019). Two-dimensional dispersion entropy: An information-theoretic method for irregularity analysis of images. Signal Processing: Image Communication, 75, 178-187.\n\n\n\n\n\n","category":"type"},{"location":"probabilities/#Transfer-operator-(binning)","page":"Probabilities","title":"Transfer operator (binning)","text":"","category":"section"},{"location":"probabilities/","page":"Probabilities","title":"Probabilities","text":"TransferOperator","category":"page"},{"location":"probabilities/#Entropies.TransferOperator","page":"Probabilities","title":"Entropies.TransferOperator","text":"TransferOperator <: <ProbabilitiesEstimator\nTransferOperator(b::RectangularBinning)\n\nA probability estimator based on binning data into rectangular boxes dictated by the given binning scheme b, then approximating the transfer (Perron-Frobenius) operator over the bins, then taking the invariant measure associated with that transfer operator as the bin probabilities. Assumes that the input data are sequential (time-ordered).\n\nThis implementation follows the grid estimator approach in Diego et al. (2019)[Diego2019].\n\nOutcome space\n\nThe outcome space for TransferOperator is the set of unique bins constructed from b. Bins are identified by their left (lowest-value) corners, are given in data units, and are returned as SVectors.\n\nBin ordering\n\nBins returned by probabilities_and_outcomes are ordered according to first appearance (i.e. the first time the input (multivariate) timeseries visits the bin). Thus, if\n\nb = RectangularBinning(4)\nest = TransferOperator(b)\nprobs, outcomes = probabilities_and_outcomes(x, est) # x is some timeseries\n\nthen probs[i] is the invariant measure (probability) of the bin outcomes[i], which is the i-th bin visited by the timeseries with nonzero measure.\n\nDescription\n\nThe transfer operator P^Nis computed as an N-by-N matrix of transition probabilities between the states defined by the partition elements, where N is the number of boxes in the partition that is visited by the orbit/points.\n\nIf  x_t^(D) _n=1^L are the L different D-dimensional points over which the transfer operator is approximated,  C_k=1^N  are the N different partition elements (as dictated by ϵ) that gets visited by the points, and  phi(x_t) = x_t+1, then\n\nP_ij = dfrac\n x_n  phi(x_n) in C_j cap x_n in C_i \n x_m  x_m in C_i \n\nwhere  denotes the cardinal. The element P_ij thus indicates how many points that are initially in box C_i end up in box C_j when the points in C_i are projected one step forward in time. Thus, the row P_ik^N where k in 1 2 ldots N  gives the probability of jumping from the state defined by box C_i to any of the other N states. It follows that sum_k=1^N P_ik = 1 for all i. Thus, P^N is a row/right stochastic matrix.\n\nInvariant measure estimation from transfer operator\n\nThe left invariant distribution mathbfrho^N is a row vector, where mathbfrho^N P^N = mathbfrho^N. Hence, mathbfrho^N is a row eigenvector of the transfer matrix P^N associated with eigenvalue 1. The distribution mathbfrho^N approximates the invariant density of the system subject to binning, and can be taken as a probability distribution over the partition elements.\n\nIn practice, the invariant measure mathbfrho^N is computed using invariantmeasure, which also approximates the transfer matrix. The invariant distribution is initialized as a length-N random distribution which is then applied to P^N. The resulting length-N distribution is then applied to P^N again. This process repeats until the difference between the distributions over consecutive iterations is below some threshold.\n\nSee also: RectangularBinning, invariantmeasure.\n\n[Diego2019]: Diego, D., Haaga, K. A., & Hannisdal, B. (2019). Transfer entropy computation using the Perron-Frobenius operator. Physical Review E, 99(4), 042212.\n\n\n\n\n\n","category":"type"},{"location":"probabilities/#Utility-methods/types","page":"Probabilities","title":"Utility methods/types","text":"","category":"section"},{"location":"probabilities/","page":"Probabilities","title":"Probabilities","text":"InvariantMeasure\ninvariantmeasure\ntransfermatrix","category":"page"},{"location":"probabilities/#Entropies.InvariantMeasure","page":"Probabilities","title":"Entropies.InvariantMeasure","text":"InvariantMeasure(to, ρ)\n\nMinimal return struct for invariantmeasure that contains the estimated invariant measure ρ, as well as the transfer operator to from which it is computed (including bin information).\n\nSee also: invariantmeasure.\n\n\n\n\n\n","category":"type"},{"location":"probabilities/#Entropies.invariantmeasure","page":"Probabilities","title":"Entropies.invariantmeasure","text":"invariantmeasure(x::AbstractDataset, binning::RectangularBinning) → iv::InvariantMeasure\n\nEstimate an invariant measure over the points in x based on binning the data into rectangular boxes dictated by the binning, then approximate the transfer (Perron-Frobenius) operator over the bins. From the approximation to the transfer operator, compute an invariant distribution over the bins. Assumes that the input data are sequential.\n\nDetails on the estimation procedure is found the TransferOperator docstring.\n\nExample\n\nusing DynamicalSystems, Plots, Entropies\nD = 4\nds = Systems.lorenz96(D; F = 32.0)\nN, dt = 20000, 0.1\norbit = trajectory(ds, N*dt; dt = dt, Ttr = 10.0)\n\n# Estimate the invariant measure over some coarse graining of the orbit.\niv = invariantmeasure(orbit, RectangularBinning(15))\n\n# Get the probabilities and bins\ninvariantmeasure(iv)\n\nProbabilities and bin information\n\ninvariantmeasure(iv::InvariantMeasure) → (ρ::Probabilities, bins::Vector{<:SVector})\n\nFrom a pre-computed invariant measure, return the probabilities and associated bins. The element ρ[i] is the probability of visitation to the box bins[i]. Analogous to binhist.\n\nhint: Transfer operator approach vs. naive histogram approach\nWhy bother with the transfer operator instead of using regular histograms to obtain probabilities?In fact, the naive histogram approach and the transfer operator approach are equivalent in the limit of long enough time series (as n to intfy), which is guaranteed by the ergodic theorem. There is a crucial difference, however:The naive histogram approach only gives the long-term probabilities that orbits visit a certain region of the state space. The transfer operator encodes that information too, but comes with the added benefit of knowing the transition probabilities between states (see transfermatrix).\n\nSee also: InvariantMeasure.\n\n\n\n\n\n","category":"function"},{"location":"probabilities/#Entropies.transfermatrix","page":"Probabilities","title":"Entropies.transfermatrix","text":"transfermatrix(iv::InvariantMeasure) → (M::AbstractArray{<:Real, 2}, bins::Vector{<:SVector})\n\nReturn the transfer matrix/operator and corresponding bins. Here, bins[i] corresponds to the i-th row/column of the transfer matrix. Thus, the entry M[i, j] is the probability of jumping from the state defined by bins[i] to the state defined by bins[j].\n\nSee also: TransferOperator.\n\n\n\n\n\n","category":"function"},{"location":"probabilities/#Kernel-density","page":"Probabilities","title":"Kernel density","text":"","category":"section"},{"location":"probabilities/","page":"Probabilities","title":"Probabilities","text":"NaiveKernel","category":"page"},{"location":"probabilities/#Entropies.NaiveKernel","page":"Probabilities","title":"Entropies.NaiveKernel","text":"NaiveKernel(x, ϵ::Real; method = KDTree, w = 0, metric = Euclidean()) <: ProbabilitiesEstimator\n\nEstimate probabilities/entropy using a \"naive\" kernel density estimation approach (KDE), as discussed in Prichard and Theiler (1995) [PrichardTheiler1995].\n\nProbabilities P(mathbfx epsilon) are assigned to every point mathbfx by counting how many other points occupy the space spanned by a hypersphere of radius ϵ around mathbfx, according to:\n\nP_i( X epsilon) approx dfrac1N sum_s B(X_i - X_j  epsilon)\n\nwhere B gives 1 if the argument is true. Probabilities are then normalized.\n\nKeyword arguments\n\nmethod = KDTree: the search structure supported by Neighborhood.jl. Specifically, use KDTree to use a tree-based neighbor search, or BruteForce for the direct distances between all points. KDTrees heavily outperform direct distances when the dimensionality of the data is much smaller than the data length.\nw = 0: the Theiler window, which excludes indices s that are within i - s  w from the given point x_i.\nmetric = Euclidean(): the distance metric.\n\nOutcome space\n\nThe outcome space Ω for NaiveKernel are the indices of the input data, eachindex(x). The reason to not return the data points themselves is because duplicate data points may not have same probabilities (due to having different neighbors).\n\n[PrichardTheiler1995]: Prichard, D., & Theiler, J. (1995). Generalized redundancies for time series analysis. Physica D: Nonlinear Phenomena, 84(3-4), 476-493.\n\n\n\n\n\n","category":"type"},{"location":"probabilities/#Timescales","page":"Probabilities","title":"Timescales","text":"","category":"section"},{"location":"probabilities/","page":"Probabilities","title":"Probabilities","text":"WaveletOverlap\nPowerSpectrum","category":"page"},{"location":"probabilities/#Entropies.WaveletOverlap","page":"Probabilities","title":"Entropies.WaveletOverlap","text":"WaveletOverlap(x [,wavelet]) <: ProbabilitiesEstimator\n\nApply the maximal overlap discrete wavelet transform (MODWT) to a signal, then compute probabilities as the (normalized) energies at different wavelet scales. These probabilities are used to compute the wavelet entropy, according to Rosso et al. (2001)[Rosso2001]. Input timeseries x is needed for a well-defined outcome space.\n\nBy default the wavelet Wavelets.WT.Daubechies{12}() is used. Otherwise, you may choose a wavelet from the Wavelets package (it must subtype OrthoWaveletClass).\n\nOutcome space\n\nThe outcome space for WaveletOverlap are the integers 1, 2, …, N enumerating the wavelet scales. To obtain a better understanding of what these mean, we prepared a notebook you can view online. As such, this estimator only works for timeseries input.\n\n[Rosso2001]: Rosso et al. (2001). Wavelet entropy: a new tool for analysis of short duration brain electrical signals. Journal of neuroscience methods, 105(1), 65-75.\n\n\n\n\n\n","category":"type"},{"location":"probabilities/#Entropies.PowerSpectrum","page":"Probabilities","title":"Entropies.PowerSpectrum","text":"PowerSpectrum(x_or_length(x)) <: ProbabilitiesEstimator\n\nCalculate the power spectrum of a timeseries (amplitude square of its Fourier transform), and return the spectrum normalized to sum = 1 as probabilities. The Shannon entropy of these probabilities is typically referred in the literature as spectral entropy, e.g. [Llanos2016],[Tian2017].\n\nThe closer the spectrum is to flat, i.e., white noise, the higher the entropy. However, you can't compare entropies of timeseries with different length, because the binning in spectral space depends on the length of the input.\n\nOutcome space\n\nThe outcome space Ω for PowerSpectrum is the set of frequencies in Fourier space. They should be multiplied with the sampling rate of the signal, which is assumed to be 1. The length of the input is therefore required for this estimator to have a well-defined outcome space.\n\n[Llanos2016]: Llanos et al., Power spectral entropy as an information-theoretic correlate of manner of articulation in American English, The Journal of the Acoustical Society of America 141, EL127 (2017)\n\n[Tian2017]: Tian et al, Spectral Entropy Can Predict Changes of Working Memory Performance Reduced by Short-Time Training in the Delayed-Match-to-Sample Task, Front. Hum. Neurosci.\n\n\n\n\n\n","category":"type"},{"location":"probabilities/#Diversity","page":"Probabilities","title":"Diversity","text":"","category":"section"},{"location":"probabilities/","page":"Probabilities","title":"Probabilities","text":"Diversity","category":"page"},{"location":"probabilities/#Entropies.Diversity","page":"Probabilities","title":"Entropies.Diversity","text":"Diversity(; m::Int, τ::Int, nbins::Int)\n\nA ProbabilitiesEstimator based on the cosine similarity. It can be used with entropy to compute the diversity entropy of an input timeseries[Wang2020].\n\nThe implementation here allows for τ != 1, which was not considered in the original paper.\n\nDescription\n\nDiversity probabilities are computed as follows.\n\nFrom the input time series x, using embedding lag τ and embedding dimension m,  construct the embedding  Y = bf x_i  = (x_i x_i+tau x_i+2tau ldots x_i+mtau - 1_i = 1^N-mτ.\nCompute D = d(bf x_t bf x_t+1) _t=1^N-mτ-1,  where d(cdot cdot) is the cosine similarity between two m-dimensional  vectors in the embedding.\nDivide the interval [-1, 1] into nbins equally sized subintervals.\nConstruct a histogram of cosine similarities d in D over those subintervals.\nSum-normalize the histogram to obtain probabilities.\n\nOutcome space\n\nThe outcome space for Diversity is the bins of the [-1, 1] interval, and the return configuration is the same as in ValueHistogram (left bin edge).\n\n[Wang2020]: Wang, X., Si, S., & Li, Y. (2020). Multiscale diversity entropy: A novel dynamical measure for fault diagnosis of rotating machinery. IEEE Transactions on Industrial Informatics, 17(8), 5419-5429.\n\n\n\n\n\n","category":"type"},{"location":"examples/#Entropies.jl-Examples","page":"Entropies.jl Examples","title":"Entropies.jl Examples","text":"","category":"section"},{"location":"examples/#Probabilities:-kernel-density","page":"Entropies.jl Examples","title":"Probabilities: kernel density","text":"","category":"section"},{"location":"examples/","page":"Entropies.jl Examples","title":"Entropies.jl Examples","text":"Here, we draw some random points from a 2D normal distribution. Then, we use kernel density estimation to associate a probability to each point p, measured by how many points are within radius 1.5 of p. Plotting the actual points, along with their associated probabilities estimated by the KDE procedure, we get the following surface plot.","category":"page"},{"location":"examples/","page":"Entropies.jl Examples","title":"Entropies.jl Examples","text":"using Entropies\nusing StateSpaceSets\nusing DynamicalSystemsBase\nusing CairoMakie\nusing Distributions: MvNormal\n\n𝒩 = MvNormal([1, -4], 2)\nN = 500\nD = Dataset(sort([rand(𝒩) for i = 1:N]))\nx, y = columns(D)\np = probabilities(NaiveKernel(D, 1.5), D)\nfig, ax = scatter(D[:, 1], D[:, 2], zeros(N);\n    markersize=8, axis=(type = Axis3,)\n)\nsurface!(ax, x, y, p.p)\nax.zlabel = \"P\"\nax.zticklabelsvisible = false\nfig","category":"page"},{"location":"examples/#Differential-entropy:-estimator-comparison","page":"Entropies.jl Examples","title":"Differential entropy: estimator comparison","text":"","category":"section"},{"location":"examples/","page":"Entropies.jl Examples","title":"Entropies.jl Examples","text":"Here, we compare how the nearest neighbor differential entropy estimators (Kraskov, KozachenkoLeonenko, Zhu and ZhuSingh) converge towards the true entropy value for increasing time series length.","category":"page"},{"location":"examples/","page":"Entropies.jl Examples","title":"Entropies.jl Examples","text":"Entropies.jl also provides entropy estimators based on order statistics. These estimators are only defined for scalar-valued vectors, in this example, so we compute these estimates separately, and add these estimators (Vasicek, Ebrahimi, AlizadehArghami and Correa) to the comparison.","category":"page"},{"location":"examples/","page":"Entropies.jl Examples","title":"Entropies.jl Examples","text":"Input data are from a normal 1D distribution mathcalN(0 1), for which the true entropy is 0.5*log(2π) + 0.5 nats when using natural logarithms.","category":"page"},{"location":"examples/","page":"Entropies.jl Examples","title":"Entropies.jl Examples","text":"using Entropies\nusing DynamicalSystemsBase, CairoMakie, Statistics\nnreps = 30\nNs = [100:100:500; 1000:1000:10000]\ne = Shannon(; base = MathConstants.e)\n\n# --------------------------\n# kNN estimators\n# --------------------------\nw = 0 # Theiler window of 0 (only exclude the point itself during neighbor searches)\nknn_estimators = [\n    # with k = 1, Kraskov is virtually identical to\n    # Kozachenko-Leonenko, so pick a higher number of neighbors for Kraskov\n    Kraskov(; k = 3, w),\n    KozachenkoLeonenko(; w),\n    Zhu(; k = 3, w),\n    ZhuSingh(; k = 3, w),\n]\n\n# Test each estimator `nreps` times over time series of varying length.\nHs_uniform_knn = [[zeros(nreps) for N in Ns] for e in knn_estimators]\nfor (i, est) in enumerate(knn_estimators)\n    for j = 1:nreps\n        pts = randn(maximum(Ns)) |> Dataset\n        for (k, N) in enumerate(Ns)\n            Hs_uniform_knn[i][k][j] = entropy(e, est, pts[1:N])\n        end\n    end\nend\n\n# --------------------------\n# Order statistic estimators\n# --------------------------\n\n# Just provide types here, they are instantiated inside the loop\nestimators_os = [Vasicek, Ebrahimi, AlizadehArghami, Correa]\nHs_uniform_os = [[zeros(nreps) for N in Ns] for e in estimators_os]\nfor (i, est_os) in enumerate(estimators_os)\n    for j = 1:nreps\n        pts = randn(maximum(Ns)) # raw timeseries, not a `Dataset`\n        for (k, N) in enumerate(Ns)\n            m = floor(Int, N / 100) # Scale `m` to timeseries length\n            est = est_os(; m) # Instantiate estimator with current `m`\n            Hs_uniform_os[i][k][j] = entropy(e, est, pts[1:N])\n        end\n    end\nend\n\n# -------------\n# Plot results\n# -------------\nfig = Figure(resolution = (700, 8 * 200))\nlabels_knn = [\"KozachenkoLeonenko\", \"Kraskov\", \"Zhu\", \"ZhuSingh\"]\nlabels_os = [\"Vasicek\", \"Ebrahimi\", \"AlizadehArghami\", \"Correa\"]\n\nfor (i, e) in enumerate(knn_estimators)\n    Hs = Hs_uniform_knn[i]\n    ax = Axis(fig[i,1]; ylabel = \"h (nats)\")\n    lines!(ax, Ns, mean.(Hs); color = Cycled(i), label = labels_knn[i])\n    band!(ax, Ns, mean.(Hs) .+ std.(Hs), mean.(Hs) .- std.(Hs); alpha = 0.5,\n        color = (Main.COLORS[i], 0.5))\n    hlines!(ax, [(0.5*log(2π) + 0.5)], color = :black, lw = 5, linestyle = :dash)\n\n    ylims!(1.2, 1.6)\n    axislegend()\nend\n\nfor (i, e) in enumerate(estimators_os)\n    Hs = Hs_uniform_os[i]\n    ax = Axis(fig[i + length(knn_estimators),1]; ylabel = \"h (nats)\")\n    lines!(ax, Ns, mean.(Hs); color = Cycled(i), label = labels_os[i])\n    band!(ax, Ns, mean.(Hs) .+ std.(Hs), mean.(Hs) .- std.(Hs), alpha = 0.5,\n        color = (Main.COLORS[i], 0.5))\n    hlines!(ax, [(0.5*log(2π) + 0.5)], color = :black, lw = 5, linestyle = :dash)\n    ylims!(1.2, 1.6)\n    axislegend()\nend\n\nfig","category":"page"},{"location":"examples/","page":"Entropies.jl Examples","title":"Entropies.jl Examples","text":"All estimators approach the true differential entropy, but those based on order statistics are negatively biased for small sample sizes.","category":"page"},{"location":"examples/#Discrete-entropy:-permutation-entropy","page":"Entropies.jl Examples","title":"Discrete entropy: permutation entropy","text":"","category":"section"},{"location":"examples/","page":"Entropies.jl Examples","title":"Entropies.jl Examples","text":"This example reproduces an example from Bandt and Pompe (2002), where the permutation entropy is compared with the largest Lyapunov exponents from time series of the chaotic logistic map. Entropy estimates using SymbolicWeightedPermutation and SymbolicAmplitudeAwarePermutation are added here for comparison.","category":"page"},{"location":"examples/","page":"Entropies.jl Examples","title":"Entropies.jl Examples","text":"using DynamicalSystemsBase, CairoMakie, ChaosTools\n\nds = Systems.logistic()\nrs = 3.4:0.001:4\nN_lyap, N_ent = 100000, 10000\nm, τ = 6, 1 # Symbol size/dimension and embedding lag\n\n# Generate one time series for each value of the logistic parameter r\nlyaps, hs_perm, hs_wtperm, hs_ampperm = [zeros(length(rs)) for _ in 1:4]\n\nfor (i, r) in enumerate(rs)\n    ds.p[1] = r\n    lyaps[i] = lyapunov(ds, N_lyap)\n\n    x = trajectory(ds, N_ent) # time series\n    hperm = entropy(SymbolicPermutation(; m, τ), x)\n    hwtperm = entropy(SymbolicWeightedPermutation(; m, τ), x)\n    hampperm = entropy(SymbolicAmplitudeAwarePermutation(; m, τ), x)\n\n    hs_perm[i] = hperm; hs_wtperm[i] = hwtperm; hs_ampperm[i] = hampperm\nend\n\nfig = Figure()\na1 = Axis(fig[1,1]; ylabel = L\"\\lambda\")\nlines!(a1, rs, lyaps); ylims!(a1, (-2, log(2)))\na2 = Axis(fig[2,1]; ylabel = L\"h_6 (SP)\")\nlines!(a2, rs, hs_perm; color = Cycled(2))\na3 = Axis(fig[3,1]; ylabel = L\"h_6 (WT)\")\nlines!(a3, rs, hs_wtperm; color = Cycled(3))\na4 = Axis(fig[4,1]; ylabel = L\"h_6 (SAAP)\")\nlines!(a4, rs, hs_ampperm; color = Cycled(4))\na4.xlabel = L\"r\"\n\nfor a in (a1,a2,a3)\n    hidexdecorations!(a, grid = false)\nend\nfig","category":"page"},{"location":"examples/#Discrete-entropy:-wavelet-entropy","page":"Entropies.jl Examples","title":"Discrete entropy: wavelet entropy","text":"","category":"section"},{"location":"examples/","page":"Entropies.jl Examples","title":"Entropies.jl Examples","text":"The scale-resolved wavelet entropy should be lower for very regular signals (most of the energy is contained at one scale) and higher for very irregular signals (energy spread more out across scales).","category":"page"},{"location":"examples/","page":"Entropies.jl Examples","title":"Entropies.jl Examples","text":"using DynamicalSystemsBase, CairoMakie\nN, a = 1000, 10\nt = LinRange(0, 2*a*π, N)\n\nx = sin.(t);\ny = sin.(t .+ cos.(t/0.5));\nz = sin.(rand(1:15, N) ./ rand(1:10, N))\n\nh_x = entropy_wavelet(x)\nh_y = entropy_wavelet(y)\nh_z = entropy_wavelet(z)\n\nfig = Figure()\nax = Axis(fig[1,1]; ylabel = \"x\")\nlines!(ax, t, x; color = Cycled(1), label = \"h=$(h=round(h_x, sigdigits = 5))\");\nay = Axis(fig[2,1]; ylabel = \"y\")\nlines!(ay, t, y; color = Cycled(2), label = \"h=$(h=round(h_y, sigdigits = 5))\");\naz = Axis(fig[3,1]; ylabel = \"z\", xlabel = \"time\")\nlines!(az, t, z; color = Cycled(3), label = \"h=$(h=round(h_z, sigdigits = 5))\");\nfor a in (ax, ay, az); axislegend(a); end\nfor a in (ax, ay); hidexdecorations!(a; grid=false); end\nfig","category":"page"},{"location":"examples/#Discrete-entropies:-properties","page":"Entropies.jl Examples","title":"Discrete entropies: properties","text":"","category":"section"},{"location":"examples/","page":"Entropies.jl Examples","title":"Entropies.jl Examples","text":"Here, we show the sensitivity of the various entropies to variations in their parameters.","category":"page"},{"location":"examples/#Curado-entropy","page":"Entropies.jl Examples","title":"Curado entropy","text":"","category":"section"},{"location":"examples/","page":"Entropies.jl Examples","title":"Entropies.jl Examples","text":"Here, we reproduce Figure 2 from Curado & Nobre (2004)[Curado2004], showing how the Curado entropy changes as function of the parameter a for a range of two-element probability distributions given by Probabilities([p, 1 - p] for p in 1:0.0:0.01:1.0).","category":"page"},{"location":"examples/","page":"Entropies.jl Examples","title":"Entropies.jl Examples","text":"using Entropies, CairoMakie\nbs = [1.0, 1.5, 2.0, 3.0, 4.0, 10.0]\nps = [Probabilities([p, 1 - p]) for p = 0.0:0.01:1.0]\nhs = [[entropy(Curado(; b = b), p) for p in ps] for b in bs]\nfig = Figure()\nax = Axis(fig[1,1]; xlabel = \"p\", ylabel = \"H(p)\")\npp = [p[1] for p in ps]\nfor (i, b) in enumerate(bs)\n    lines!(ax, pp, hs[i], label = \"b=$b\", color = Cycled(i))\nend\naxislegend(ax)\nfig","category":"page"},{"location":"examples/","page":"Entropies.jl Examples","title":"Entropies.jl Examples","text":"[Curado2004]: Curado, E. M., & Nobre, F. D. (2004). On the stability of analytic entropic forms. Physica A: Statistical Mechanics and its Applications, 335(1-2), 94-106.","category":"page"},{"location":"examples/#Kaniadakis-entropy","page":"Entropies.jl Examples","title":"Kaniadakis entropy","text":"","category":"section"},{"location":"examples/","page":"Entropies.jl Examples","title":"Entropies.jl Examples","text":"Here, we show how Kaniadakis entropy changes as function of the parameter a for  a range of two-element probability distributions given by Probabilities([p, 1 - p] for p in 1:0.0:0.01:1.0).","category":"page"},{"location":"examples/","page":"Entropies.jl Examples","title":"Entropies.jl Examples","text":"using Entropies\nusing CairoMakie\n\nprobs = [Probabilities([p, 1-p]) for p in 0.0:0.01:1.0]\nps = collect(0.0:0.01:1.0);\nκs = [-0.99, -0.66, -0.33, 0, 0.33, 0.66, 0.99];\nHs = [[entropy(Kaniadakis(κ = κ), p) for p in probs] for κ in κs];\n\nfig = Figure()\nax = Axis(fig[1, 1], xlabel = \"p\", ylabel = \"H(p)\")\n\nfor (i, H) in enumerate(Hs)\n    lines!(ax, ps, H, label = \"$(κs[i])\")\nend\n\naxislegend()\n\nfig","category":"page"},{"location":"examples/#Stretched-exponential-entropy","page":"Entropies.jl Examples","title":"Stretched exponential entropy","text":"","category":"section"},{"location":"examples/","page":"Entropies.jl Examples","title":"Entropies.jl Examples","text":"Here, we reproduce the example from Anteneodo & Plastino (1999)[Anteneodo1999], showing how the stretched exponential entropy changes as function of the parameter η for a range of two-element probability distributions given by Probabilities([p, 1 - p] for p in 1:0.0:0.01:1.0).","category":"page"},{"location":"examples/","page":"Entropies.jl Examples","title":"Entropies.jl Examples","text":"using Entropies, SpecialFunctions, CairoMakie\nηs = [0.01, 0.2, 0.3, 0.5, 0.7, 1.0, 1.5, 3.0]\nps = [Probabilities([p, 1 - p]) for p = 0.0:0.01:1.0]\n\nhs_norm = [[entropy(StretchedExponential( η = η), p) / gamma((η + 1)/η) for p in ps] for η in ηs]\nfig = Figure()\nax = Axis(fig[1,1]; xlabel = \"p\", ylabel = \"H(p)\")\npp = [p[1] for p in ps]\n\nfor (i, η) in enumerate(ηs)\n    lines!(ax, pp, hs_norm[i], label = \"η=$η\")\nend\naxislegend(ax)\nfig","category":"page"},{"location":"examples/","page":"Entropies.jl Examples","title":"Entropies.jl Examples","text":"[Anteneodo1999]: Anteneodo, C., & Plastino, A. R. (1999). Maximum entropy approach to stretched exponential probability distributions. Journal of Physics A: Mathematical and General, 32(7), 1089.","category":"page"},{"location":"examples/#dispersion_example","page":"Entropies.jl Examples","title":"Discrete entropy: dispersion entropy","text":"","category":"section"},{"location":"examples/","page":"Entropies.jl Examples","title":"Entropies.jl Examples","text":"Here we compute dispersion entropy (Rostaghi et al. 2016)[Rostaghi2016], using the use the Dispersion probabilities estimator, for a time series consisting of normally distributed noise with a single spike in the middle of the signal. We compute the entropies over a range subsets of the data, using a sliding window consisting of 70 data points, stepping the window 10 time steps at a time. This example is adapted from Li et al. (2021)[Li2019].","category":"page"},{"location":"examples/","page":"Entropies.jl Examples","title":"Entropies.jl Examples","text":"using Entropies\nusing DynamicalSystemsBase\nusing Random\nusing CairoMakie\nusing Distributions: Normal\n\nn = 1000\nts = 1:n\nx = [i == n ÷ 2 ? 50.0 : 0.0 for i in ts]\nrng = Random.default_rng()\ns = rand(rng, Normal(0, 1), n)\ny = x .+ s\n\nws = 70\nwindows = [t:t+ws for t in 1:10:n-ws]\nrdes = zeros(length(windows))\ndes = zeros(length(windows))\npes = zeros(length(windows))\n\nm, c = 2, 6\nest_de = Dispersion(c = c, m = m, τ = 1)\nfor (i, window) in enumerate(windows)\n    des[i] = entropy_normalized(Renyi(), est_de, y[window])\nend\n\nfig = Figure()\na1 = Axis(fig[1,1]; xlabel = \"Time step\", ylabel = \"Value\")\nlines!(a1, ts, y)\ndisplay(fig)\na2 = Axis(fig[2, 1]; xlabel = \"Time step\", ylabel = \"Value\")\np_de = scatterlines!([first(w) for w in windows], des,\n    label = \"Dispersion entropy\",\n    color = :red,\n    markercolor = :red, marker = '●', markersize = 20)\n\naxislegend(position = :rc)\nylims!(0, max(maximum(pes), 1))\nfig","category":"page"},{"location":"examples/","page":"Entropies.jl Examples","title":"Entropies.jl Examples","text":"[Rostaghi2016]: Rostaghi, M., & Azami, H. (2016). Dispersion entropy: A measure for time-series analysis. IEEE Signal Processing Letters, 23(5), 610-614.","category":"page"},{"location":"examples/","page":"Entropies.jl Examples","title":"Entropies.jl Examples","text":"[Li2019]: Li, Y., Gao, X., & Wang, L. (2019). Reverse dispersion entropy: a new complexity measure for sensor signal. Sensors, 19(23), 5203.","category":"page"},{"location":"examples/#Discrete-entropy:-normalized-entropy-for-comparing-different-signals","page":"Entropies.jl Examples","title":"Discrete entropy: normalized entropy for comparing different signals","text":"","category":"section"},{"location":"examples/","page":"Entropies.jl Examples","title":"Entropies.jl Examples","text":"When comparing different signals or signals that have different length, it is best to normalize entropies so that the \"complexity\" or \"disorder\" quantification is directly comparable between signals. Here is an example based on the Wavelet entropy example (where we use the spectral entropy instead of the wavelet entropy):","category":"page"},{"location":"examples/","page":"Entropies.jl Examples","title":"Entropies.jl Examples","text":"using DynamicalSystemsBase\nN1, N2, a = 101, 100001, 10\n\nfor N in (N1, N2)\n    local t = LinRange(0, 2*a*π, N)\n    local x = sin.(t) # periodic\n    local y = sin.(t .+ cos.(t/0.5)) # periodic, complex spectrum\n    local z = sin.(rand(1:15, N) ./ rand(1:10, N)) # random\n    local w = trajectory(Systems.lorenz(), N÷10; Δt = 0.1, Ttr = 100)[:, 1] # chaotic\n\n    for q in (x, y, z, w)\n        h = entropy(PowerSpectrum(), q)\n        n = entropy_normalized(PowerSpectrum(), q)\n        println(\"entropy: $(h), normalized: $(n).\")\n    end\nend","category":"page"},{"location":"examples/","page":"Entropies.jl Examples","title":"Entropies.jl Examples","text":"You see that while the direct entropy values of the chaotic and noisy signals change massively with N but they are almost the same for the normalized version. For the regular signals, the entropy decreases nevertheless because the noise contribution of the Fourier computation becomes less significant.","category":"page"},{"location":"examples/#Spatial-discrete-entropy:-Fabio","page":"Entropies.jl Examples","title":"Spatial discrete entropy: Fabio","text":"","category":"section"},{"location":"examples/","page":"Entropies.jl Examples","title":"Entropies.jl Examples","text":"Let's see how the normalized permutation and dispersion entropies increase for an image that gets progressively more noise added to it.","category":"page"},{"location":"examples/","page":"Entropies.jl Examples","title":"Entropies.jl Examples","text":"using Entropies\nusing Distributions\nusing CairoMakie\nusing Statistics\nusing TestImages, ImageTransformations, CoordinateTransformations, Rotations\n\nimg = testimage(\"fabio_grey_256\")\nrot = warp(img, recenter(RotMatrix(-3pi/2), center(img));)\noriginal = Float32.(rot)\nnoise_levels = collect(0.0:0.25:1.0) .* std(original) * 5 # % of 1 standard deviation\n\nnoisy_imgs = [i == 1 ? original : original .+ rand(Uniform(0, nL), size(original)) \n    for (i, nL) in enumerate(noise_levels)]\n\n# a 2x2 stencil (i.e. dispersion/permutation patterns of length 4)\nstencil = ((2, 2), (1, 1)) \n\nest_disp = SpatialDispersion(stencil, original; c = 5, periodic = false)\nest_perm = SpatialSymbolicPermutation(stencil, original; periodic = false)\nhs_disp = [entropy_normalized(est_disp, img) for img in noisy_imgs]\nhs_perm = [entropy_normalized(est_perm, img) for img in noisy_imgs]\n\n# Plot the results\nfig = Figure(size = (800, 1000))\nax = Axis(fig[1, 1:length(noise_levels)], \n    xlabel = \"Noise level\", \n    ylabel = \"Normalized entropy\")\nscatterlines!(ax, noise_levels, hs_disp, label = \"Dispersion\")\nscatterlines!(ax, noise_levels, hs_perm, label = \"Permutation\")\nylims!(ax, 0, 1.05)\naxislegend(position = :rb)\nfor (i, nl) in enumerate(noise_levels)\n    ax_i = Axis(fig[2, i])\n    image!(ax_i, Float32.(noisy_imgs[i]), label = \"$nl\")\n    hidedecorations!(ax_i)  # hides ticks, grid and lables\n    hidespines!(ax_i)  # hide the frame\nend\nfig","category":"page"},{"location":"examples/","page":"Entropies.jl Examples","title":"Entropies.jl Examples","text":"While the normalized SpatialSymbolicPermutation entropy quickly approaches its maximum value, the normalized SpatialDispersion entropy much better resolves the increase in entropy as the image gets noiser. This can probably be explained by the fact that the number of possible states (or total_outcomes) for any given stencil is larger for SpatialDispersion than for SpatialSymbolicPermutation, so the dispersion approach is much less sensitive to noise addition (i.e. noise saturation over the possible states is slower for SpatialDispersion).","category":"page"},{"location":"examples/#Complexity:-reverse-dispersion-entropy","page":"Entropies.jl Examples","title":"Complexity: reverse dispersion entropy","text":"","category":"section"},{"location":"examples/","page":"Entropies.jl Examples","title":"Entropies.jl Examples","text":"Here, we compare regular dispersion entropy (Rostaghi et al., 2016)[Rostaghi2016], and reverse dispersion entropy Li et al. (2021)[Li2019] for a time series consisting of normally distributed noise with a single spike in the middle of the signal. We compute the entropies over a range subsets of the data, using a sliding window consisting of 70 data points, stepping the window 10 time steps at a time. This example reproduces parts of figure 3 in Li et al. (2021), but results here are not exactly the same as in the original paper, because their examples are based on randomly generated numbers and do not provide code that specify random number seeds.","category":"page"},{"location":"examples/","page":"Entropies.jl Examples","title":"Entropies.jl Examples","text":"using Entropies\nusing Entropies\nusing DynamicalSystemsBase\nusing Random\nusing CairoMakie\nusing Distributions\n\nn = 1000\nts = 1:n\nx = [i == n ÷ 2 ? 50.0 : 0.0 for i in ts]\nrng = Random.default_rng()\ns = rand(rng, Normal(0, 1), n)\ny = x .+ s\n\nws = 70\nwindows = [t:t+ws for t in 1:10:n-ws]\nrdes = zeros(length(windows))\ndes = zeros(length(windows))\npes = zeros(length(windows))\n\nm, c = 2, 6\nest_rd = ReverseDispersion(; c, m, τ = 1)\nest_de = Dispersion(; c, m, τ = 1)\n\nfor (i, window) in enumerate(windows)\n    rdes[i] = complexity_normalized(est_rd, y[window])\n    des[i] = entropy_normalized(Renyi(), est_de, y[window])\nend\n\nfig = Figure()\n\na1 = Axis(fig[1,1]; xlabel = \"Time step\", ylabel = \"Value\")\nlines!(a1, ts, y)\ndisplay(fig)\n\na2 = Axis(fig[2, 1]; xlabel = \"Time step\", ylabel = \"Value\")\np_rde = scatterlines!([first(w) for w in windows], rdes,\n    label = \"Reverse dispersion entropy\",\n    color = :black,\n    markercolor = :black, marker = '●')\np_de = scatterlines!([first(w) for w in windows], des,\n    label = \"Dispersion entropy\",\n    color = :red,\n    markercolor = :red, marker = 'x', markersize = 20)\n\naxislegend(position = :rc)\nylims!(0, max(maximum(pes), 1))\nfig","category":"page"},{"location":"examples/","page":"Entropies.jl Examples","title":"Entropies.jl Examples","text":"[Rostaghi2016]: Rostaghi, M., & Azami, H. (2016). Dispersion entropy: A measure for time-series analysis. IEEE Signal Processing Letters, 23(5), 610-614.","category":"page"},{"location":"examples/","page":"Entropies.jl Examples","title":"Entropies.jl Examples","text":"[Li2019]: Li, Y., Gao, X., & Wang, L. (2019). Reverse dispersion entropy: a new complexity measure for sensor signal. Sensors, 19(23), 5203.","category":"page"},{"location":"examples/#Complexity:-missing-dispersion-patterns","page":"Entropies.jl Examples","title":"Complexity: missing dispersion patterns","text":"","category":"section"},{"location":"examples/","page":"Entropies.jl Examples","title":"Entropies.jl Examples","text":"using Entropies\nusing CairoMakie\nusing DynamicalSystemsBase\nusing TimeseriesSurrogates\n\nest = MissingDispersionPatterns(Dispersion(m = 3, c = 7))\nsys = Systems.logistic(0.6; r = 4.0)\nLs = collect(100:100:1000)\nnL = length(Ls)\nnreps = 50\nmethod = WLS(IAAFT(), rescale = true)\n\nr_det, r_noise = zeros(length(Ls)), zeros(length(Ls))\nr_det_surr, r_noise_surr = [zeros(nreps) for L in Ls], [zeros(nreps) for L in Ls]\ny = rand(maximum(Ls))\n\nfor (i, L) in enumerate(Ls)\n    # Deterministic time series\n    x = trajectory(sys, L - 1, Ttr = 5000)\n    sx = surrogenerator(x, method)\n    r_det[i] = complexity_normalized(est, x)\n    r_det_surr[i][:] = [complexity_normalized(est, sx()) for j = 1:nreps]\n\n    # Random time series\n    r_noise[i] = complexity_normalized(est, y[1:L])\n    sy = surrogenerator(y[1:L], method)\n    r_noise_surr[i][:] = [complexity_normalized(est, sy()) for j = 1:nreps]\nend\n\nfig = Figure()\nax = Axis(fig[1, 1],\n    xlabel = \"Time series length (L)\",\n    ylabel = \"# missing dispersion patterns (normalized)\"\n)\n\nlines!(ax, Ls, r_det, label = \"logistic(x0 = 0.6; r = 4.0)\", color = :black)\nlines!(ax, Ls, r_noise, label = \"Uniform noise\", color = :red)\nfor i = 1:nL\n    if i == 1\n        boxplot!(ax, fill(Ls[i], nL), r_det_surr[i]; width = 50, color = :black,\n            label = \"WIAAFT surrogates (logistic)\")\n         boxplot!(ax, fill(Ls[i], nL), r_noise_surr[i]; width = 50, color = :red,\n            label = \"WIAAFT surrogates (noise)\")\n    else\n        boxplot!(ax, fill(Ls[i], nL), r_det_surr[i]; width = 50, color = :black)\n        boxplot!(ax, fill(Ls[i], nL), r_noise_surr[i]; width = 50, color = :red)\n    end\nend\naxislegend(position = :rc)\nylims!(0, 1.1)\n\nfig","category":"page"},{"location":"examples/","page":"Entropies.jl Examples","title":"Entropies.jl Examples","text":"We don't need to actually to compute the quantiles here to see that for the logistic map, across all time series lengths, the N_MDP values are above the extremal values of the N_MDP values for the surrogate ensembles. Thus, we conclude that the logistic map time series has nonlinearity (well, of course).","category":"page"},{"location":"examples/","page":"Entropies.jl Examples","title":"Entropies.jl Examples","text":"For the univariate noise time series, there is considerable overlap between N_MDP for the surrogate distributions and the original signal, so we can't claim nonlinearity for this signal.","category":"page"},{"location":"examples/","page":"Entropies.jl Examples","title":"Entropies.jl Examples","text":"Of course, to robustly reject the null hypothesis, we'd need to generate a sufficient number of surrogate realizations, and actually compute quantiles to compare with.","category":"page"},{"location":"examples/","page":"Entropies.jl Examples","title":"Entropies.jl Examples","text":"[Zhou2022]: Zhou, Q., Shang, P., & Zhang, B. (2022). Using missing dispersion patterns to detect determinism and nonlinearity in time series data. Nonlinear Dynamics, 1-20.","category":"page"},{"location":"examples/#Complexity:-approximate-entropy","page":"Entropies.jl Examples","title":"Complexity: approximate entropy","text":"","category":"section"},{"location":"examples/","page":"Entropies.jl Examples","title":"Entropies.jl Examples","text":"Here, we reproduce the Henon map example with R=08 from Pincus (1991), comparing our values with relevant values from table 1 in Pincus (1991).","category":"page"},{"location":"examples/","page":"Entropies.jl Examples","title":"Entropies.jl Examples","text":"We use DiscreteDynamicalSystem from DynamicalSystemsBase to represent the map, and use the trajectory function from the same package to iterate the map for different initial conditions, for multiple time series lengths.","category":"page"},{"location":"examples/","page":"Entropies.jl Examples","title":"Entropies.jl Examples","text":"Finally, we summarize our results in box plots and compare the values to those obtained by Pincus (1991).","category":"page"},{"location":"examples/","page":"Entropies.jl Examples","title":"Entropies.jl Examples","text":"using Entropies\nusing DynamicalSystemsBase\nusing DelayEmbeddings\nusing CairoMakie\n\n# Equation 13 in Pincus (1991)\nfunction eom_henon(u, p, n)\n    R = p[1]\n    x, y = u\n    dx = R*y + 1 - 1.4*x^2\n    dy = 0.3*R*x\n\n    return SVector{2}(dx, dy)\nend\n\nfunction henon(; u₀ = rand(2), R = 0.8)\n    DiscreteDynamicalSystem(eom_henon, u₀, [R])\nend\n\nts_lengths = [300, 1000, 2000, 3000]\nnreps = 100\napens_08 = [zeros(nreps) for i = 1:length(ts_lengths)]\n\n# For some initial conditions, the Henon map as specified here blows up,\n# so we need to check for infinite values.\ncontainsinf(x) = any(isinf.(x))\n\nc = ApproximateEntropy(r = 0.05, m = 2)\n\nfor (i, L) in enumerate(ts_lengths)\n    k = 1\n    while k <= nreps\n        sys = henon(u₀ = rand(2), R = 0.8)\n        t = trajectory(sys, L, Ttr = 5000)\n\n        if !any([containsinf(tᵢ) for tᵢ in t])\n            x, y = columns(t)\n            apens_08[i][k] = complexity(c, x)\n            k += 1\n        end\n    end\nend\n\nfig = Figure()\n\n# Example time series\na1 = Axis(fig[1,1]; xlabel = \"Time (t)\", ylabel = \"Value\")\nsys = henon(u₀ = [0.5, 0.1], R = 0.8)\nx, y = columns(trajectory(sys, 100, Ttr = 500))\nlines!(a1, 1:length(x), x, label = \"x\")\nlines!(a1, 1:length(y), y, label = \"y\")\n\n# Approximate entropy values, compared to those of the original paper (black dots).\na2 = Axis(fig[2, 1];\n    xlabel = \"Time series length (L)\",\n    ylabel = \"ApEn(m = 2, r = 0.05)\")\n\n# hacky boxplot, but this seems to be how it's done in Makie at the moment\nn = length(ts_lengths)\nfor i = 1:n\n    boxplot!(a2, fill(ts_lengths[i], n), apens_08[i];\n        width = 200)\nend\n\nscatter!(a2, ts_lengths, [0.337, 0.385, NaN, 0.394];\n    label = \"Pincus (1991)\", color = :black)\nfig","category":"page"},{"location":"examples/#Complexity:-sample-entropy","page":"Entropies.jl Examples","title":"Complexity: sample entropy","text":"","category":"section"},{"location":"examples/","page":"Entropies.jl Examples","title":"Entropies.jl Examples","text":"Completely regular signals should have sample entropy approaching zero, while less regular signals should have higher sample entropy.","category":"page"},{"location":"examples/","page":"Entropies.jl Examples","title":"Entropies.jl Examples","text":"using Entropies\nusing DynamicalSystemsBase\nusing CairoMakie\nN, a = 2000, 10\nt = LinRange(0, 2*a*π, N)\n\nx = repeat([-5:5 |> collect; 4:-1:-4 |> collect], N ÷ 20);\ny = sin.(t .+ cos.(t/0.5));\nz = rand(N)\n\nh_x, h_y, h_z = map(t -> complexity(SampleEntropy(t), t), (x, y, z))\n\nfig = Figure()\nax = Axis(fig[1,1]; ylabel = \"x\")\nlines!(ax, t, x; color = Cycled(1), label = \"h=$(h=round(h_x, sigdigits = 5))\");\nay = Axis(fig[2,1]; ylabel = \"y\")\nlines!(ay, t, y; color = Cycled(2), label = \"h=$(h=round(h_y, sigdigits = 5))\");\naz = Axis(fig[3,1]; ylabel = \"z\", xlabel = \"time\")\nlines!(az, t, z; color = Cycled(3), label = \"h=$(h=round(h_z, sigdigits = 5))\");\nfor a in (ax, ay, az); axislegend(a); end\nfor a in (ax, ay); hidexdecorations!(a; grid=false); end\nfig","category":"page"},{"location":"examples/","page":"Entropies.jl Examples","title":"Entropies.jl Examples","text":"Next, we compare the sample entropy obtained for different values of the radius r for uniform noise, normally distributed noise, and a periodic signal.","category":"page"},{"location":"examples/","page":"Entropies.jl Examples","title":"Entropies.jl Examples","text":"using Entropies\nusing CairoMakie\nusing Statistics\nusing Distributions: Normal\nN = 2000\nx_U = rand(N)\nx_N = rand(Normal(0, 3), N)\nx_periodic = repeat(rand(20), N ÷ 20)\n\nx_U .= (x_U .- mean(x_U)) ./ std(x_U)\nx_N .= (x_N .- mean(x_N)) ./ std(x_N)\nx_periodic .= (x_periodic .- mean(x_periodic)) ./ std(x_periodic)\n\nrs = 10 .^ range(-1, 0, length = 30)\nbase = 2\nm = 2\nhs_U = [complexity_normalized(SampleEntropy(m = m, r = r), x_U) for r in rs]\nhs_N = [complexity_normalized(SampleEntropy(m = m, r = r), x_N) for r in rs]\nhs_periodic = [complexity_normalized(SampleEntropy(m = m, r = r), x_periodic) for r in rs]\n\nfig = Figure()\n# Time series\na1 = Axis(fig[1,1]; xlabel = \"r\", ylabel = \"Sample entropy\")\nlines!(a1, rs, hs_U, label = \"Uniform noise, U(0, 1)\")\nlines!(a1, rs, hs_N, label = \"Gaussian noise, N(0, 1)\")\nlines!(a1, rs, hs_periodic, label = \"Periodic signal\")\naxislegend()\nfig","category":"page"},{"location":"examples/#Complexity:-multiscale","page":"Entropies.jl Examples","title":"Complexity: multiscale","text":"","category":"section"},{"location":"examples/","page":"Entropies.jl Examples","title":"Entropies.jl Examples","text":"using Entropies\nusing CairoMakie\n\nN, a = 2000, 20\nt = LinRange(0, 2*a*π, N)\n\nx = repeat([-5:5 |> collect; 4:-1:-4 |> collect], N ÷ 20);\ny = sin.(t .+ cos.(t/0.5)) .+ 0.2 .* x\nmaxscale = 10\nhs = multiscale_normalized(Regular(), SampleEntropy(y), y; maxscale)\n\nfig = Figure()\nax1 = Axis(fig[1,1]; ylabel = \"y\")\nlines!(ax1, t, y; color = Cycled(1));\nax2 = Axis(fig[2, 1]; ylabel = \"Sample entropy (h)\", xlabel = \"Scale\")\nscatterlines!(ax2, 1:maxscale |> collect, hs; color = Cycled(1));\nfig","category":"page"},{"location":"multiscale/#Multiscale","page":"Multiscale","title":"Multiscale","text":"","category":"section"},{"location":"multiscale/#Multiscale-API","page":"Multiscale","title":"Multiscale API","text":"","category":"section"},{"location":"multiscale/","page":"Multiscale","title":"Multiscale","text":"The multiscale API is defined by the functions","category":"page"},{"location":"multiscale/","page":"Multiscale","title":"Multiscale","text":"multiscale\nmultiscale_normalized\ndownsample","category":"page"},{"location":"multiscale/","page":"Multiscale","title":"Multiscale","text":"which dispatch any of the MultiScaleAlgorithms listed below.","category":"page"},{"location":"multiscale/","page":"Multiscale","title":"Multiscale","text":"MultiScaleAlgorithm\nRegular\nComposite\ndownsample\nmultiscale\nmultiscale_normalized","category":"page"},{"location":"multiscale/#Entropies.MultiScaleAlgorithm","page":"Multiscale","title":"Entropies.MultiScaleAlgorithm","text":"MultiScaleAlgorithm\n\nThe supertype for all multiscale algorithms.\n\n\n\n\n\n","category":"type"},{"location":"multiscale/#Entropies.Regular","page":"Multiscale","title":"Entropies.Regular","text":"Regular <: MultiScaleAlgorithm\nRegular(; f::Function = Statistics.mean)\n\nThe original multi-scale algorithm for multiscale entropy analysis (Costa et al., 2022)[Costa2002], which yields a single downsampled time series per scale s.\n\nDescription\n\nGiven a scalar-valued input time series x, the Regular multiscale algorithm downsamples and coarse-grains x by splitting it into non-overlapping windows of length s, and then constructing a new downsampled time series D_t(s f) by applying the function f to each of the resulting length-s windows.\n\nThe downsampled time series D_t(s) with t ∈ [1, 2, …, L], where L = floor(N / s), is given by:\n\n D_t(s f)  _t = 1^L = left f left( bf x_t right) right_t = 1^L =\nleft\n    fleft( (x_i)_i = (t - 1)s + 1^ts right)\nright_t = 1^L\n\nwhere f is some summary statistic applied to the length-ts-((t - 1)s + 1) tuples xₖ. Different choices of f have yield different multiscale methods appearing in the literature. For example:\n\nf == Statistics.mean yields the original first-moment multiscale sample entropy (Costa   et al., 2002)[Costa2002].\nf == Statistics.var yields the generalized multiscale sample entropy (Costa &   Goldberger, 2015)[Costa2015], which uses the second-moment (variance) instead of the   mean.\n\nSee also: Composite.\n\n[Costa2002]: Costa, M., Goldberger, A. L., & Peng, C. K. (2002). Multiscale entropy analysis of complex physiologic time series. Physical review letters, 89(6), 068102.\n\n[Costa2015]: Costa, M. D., & Goldberger, A. L. (2015). Generalized multiscale entropy analysis: Application to quantifying the complex volatility of human heartbeat time series. Entropy, 17(3), 1197-1203.\n\n\n\n\n\n","category":"type"},{"location":"multiscale/#Entropies.Composite","page":"Multiscale","title":"Entropies.Composite","text":"Composite <: MultiScaleAlgorithm\nComposite(; f::Function = Statistics.mean)\n\nComposite multi-scale algorithm for multiscale entropy analysis (Wu et al., 2013)[Wu2013], used, with multiscale to compute, for example, composite multiscale entropy (CMSE).\n\nDescription\n\nGiven a scalar-valued input time series x, the composite multiscale algorithm, like Regular, downsamples and coarse-grains x by splitting it into non-overlapping windows of length s, and then constructing downsampled time series by applying the function f to each of the resulting length-s windows.\n\nHowever, Wu et al. (2013)[Wu2013] realized that for each scale s, there are actually s different ways of selecting windows, depending on where indexing starts/ends. These s different downsampled time series D_t(s, f) at each scale s are constructed as follows:\n\n D_k(s)  =  D_t k(s) _t = 1^L =  f left( bf x_t k right)  =\nleft\n    fleft( (x_i)_i = (t - 1)s + k^ts + k - 1 right)\nright_t = 1^L\n\nwhere L = floor((N - s + 1) / s) and 1 ≤ k ≤ s, such that D_i k(s) is the i-th element of the k-th downsampled time series at scale s.\n\nFinally, compute dfrac1s sum_k = 1^s g(D_k(s)), where g is some summary function, for example entropy or complexity.\n\nnote: Relation to Regular\nThe downsampled time series D_t 1(s) constructed using the composite multiscale method is equivalent to the downsampled time series D_t(s) constructed using the Regular method, for which k == 1 is fixed, such that only a single time series is returned.\n\nSee also: Regular.\n\n[Wu2013]: Wu, S. D., Wu, C. W., Lin, S. G., Wang, C. C., & Lee, K. Y. (2013). Time series analysis using composite multiscale entropy. Entropy, 15(3), 1069-1084.\n\n\n\n\n\n","category":"type"},{"location":"multiscale/#Entropies.downsample","page":"Multiscale","title":"Entropies.downsample","text":"downsample(algorithm::MultiScaleAlgorithm, s::Int, x)\n\nDownsample and coarse-grain x to scale s according to the given multiscale algorithm.\n\nPositional arguments args and keyword arguments kwargs are propagated to relevant functions in algorithm, if applicable.\n\nThe return type depends on algorithm. For example:\n\nRegular yields a single Vector per scale.\nComposite yields a Vector{Vector} per scale.\n\n\n\n\n\n","category":"function"},{"location":"multiscale/#Entropies.multiscale","page":"Multiscale","title":"Entropies.multiscale","text":"multiscale(alg::MultiScaleAlgorithm, e::Entropy, est::EntropyEstimator, x; kwargs...)\nmultiscale(alg::MultiScaleAlgorithm, e::Entropy, est::ProbabilitiesEstimator, x; kwargs...)\nmultiscale(alg::MultiScaleAlgorithm, c::ComplexityMeasure, x::AbstractVector; kwargs...)\n\nCompute the multi-scale entropy e with estimator est, or the complexity measure c, for timeseries x.\n\nThe first signature estimates differential/continuous multiscale entropy. The second signature estimates discrete multiscale entropy. The third signature estimate some other multiscale complexity measure that is not, strictly speaking, an entropy.\n\nThis function generalizes all multi-scale entropy estimators, as long as a relevant MultiScaleAlgorithm, a downsample method and an estimator is defined. It also generalizes all multi-scale complexity measures, provided a relevant ComplexityMeasre is defined.\n\nDescription\n\nUtilizes downsample to compute the entropy/complexity of coarse-grained, downsampled versions of x for scale factors 1:maxscale. If N = length(x), then the length of the most severely downsampled version of x is N ÷ maxscale, while for scale factor 1, the original time series is considered.\n\nArguments\n\ne::Entropy. A valid entropy type, i.e. Shannon() or Renyi().\nc::ComplexityMeasure. A valid complexity measure, i.e. SampleEntropy,   or ApproximateEntropy.\nalg::MultiScaleAlgorithm. A valid multiscale algorithm,   i.e. Regular() or Composite(), which determines how down-sampling/coarse-graining   is performed.\nx. The input data. Usually a timeseries.\nest. For discrete entropy, est is a ProbabilitiesEstimator, which determines   how probabilities are estimated for each sampled time series. Alternatively,for   continuous/differential entropy, est can be a EntropyEstimator,   which dictates the entropy estimation method for each downsampled time series.\n\nKeyword Arguments\n\nmaxscale::Int. The maximum number of scales (i.e. levels of downsampling). The actual   maximum scale level is length(x) ÷ 2, but to avoid applying the method to time   series that are extremely short, maybe consider limiting maxscale (e.g.   maxscale = length(x) ÷ 5).\n\n[Costa2002]: Costa, M., Goldberger, A. L., & Peng, C. K. (2002). Multiscale entropy analysis of complex physiologic time series. Physical review letters, 89(6), 068102.\n\n\n\n\n\n","category":"function"},{"location":"multiscale/#Entropies.multiscale_normalized","page":"Multiscale","title":"Entropies.multiscale_normalized","text":"multiscale_normalized(alg::MultiScaleAlgorithm, e::Entropy,\n    est::ProbabilitiesEstimator, x)\n\nThe same as multiscale, but normalizes the entropy if entropy_maximum is implemented for e.\n\nNote: this doesn't work if est is an EntropyEstimator.\n\n\n\n\n\n","category":"function"},{"location":"multiscale/#Available-literature-methods","page":"Multiscale","title":"Available literature methods","text":"","category":"section"},{"location":"multiscale/","page":"Multiscale","title":"Multiscale","text":"A non-exhaustive list of literature methods, and the syntax to compute them, are listed below. Please open an issue or make a pull-request to Entropies.jl if you find a literature method missing from this list, or if you publish a paper based on some new multiscale combination.","category":"page"},{"location":"multiscale/","page":"Multiscale","title":"Multiscale","text":"Method Syntax Reference\nRefined composite multiscale dispersion entropy multiscale(Composite(), Dispersion(), est, x, normalized = true) Azami et al. (2017)[Azami2017]\nMultiscale sample entropy (first moment) multiscale(Regular(f = mean), SampleEntropy(), x) Costa et al. (2002)[Costa2002]\nGeneralized multiscale sample entropy (second moment) multiscale(Regular(f = std), SampleEntropy(),  x) Costa et al. (2015)[Costa2015]","category":"page"},{"location":"multiscale/","page":"Multiscale","title":"Multiscale","text":"[Azami2017]: Azami, H., Rostaghi, M., Abásolo, D., & Escudero, J. (2017). Refined composite multiscale dispersion entropy and its application to biomedical signals. IEEE Transactions on Biomedical Engineering, 64(12), 2872-2879.","category":"page"},{"location":"multiscale/","page":"Multiscale","title":"Multiscale","text":"[Costa2002]: Costa, M., Goldberger, A. L., & Peng, C. K. (2002). Multiscale entropy analysis of complex physiologic time series. Physical review letters, 89(6), 068102.","category":"page"},{"location":"multiscale/","page":"Multiscale","title":"Multiscale","text":"[Costa2015]: Costa, M. D., & Goldberger, A. L. (2015). Generalized multiscale entropy analysis: Application to quantifying the complex volatility of human heartbeat time series. Entropy, 17(3), 1197-1203.","category":"page"},{"location":"#Entropies.jl","page":"Entropies.jl","title":"Entropies.jl","text":"","category":"section"},{"location":"","page":"Entropies.jl","title":"Entropies.jl","text":"Entropies","category":"page"},{"location":"#Entropies","page":"Entropies.jl","title":"Entropies","text":"A Julia package that provides estimators for probabilities and entropies for nonlinear dynamics, nonlinear timeseries analysis, and complex systems. It can be used as a standalone package, or as part of several projects in the JuliaDynamics organization, such as DynamicalSystems.jl or CausalityTools.jl.\n\nTo install it, run import Pkg; Pkg.add(\"Entropies\").\n\n\n\n\n\n","category":"module"},{"location":"","page":"Entropies.jl","title":"Entropies.jl","text":"info: Info\nYou are reading the development version of the documentation of Entropies.jl, that will become version 2.0.","category":"page"},{"location":"#API-and-terminology","page":"Entropies.jl","title":"API & terminology","text":"","category":"section"},{"location":"","page":"Entropies.jl","title":"Entropies.jl","text":"note: Note\nThe documentation here follows (loosely) chapter 5 of Nonlinear Dynamics, Datseris & Parlitz, Springer 2022.","category":"page"},{"location":"","page":"Entropies.jl","title":"Entropies.jl","text":"In the literature, the term \"entropy\" is used (and abused) in multiple contexts. The API and documentation of Entropies.jl aim to clarify some aspects of its usage, and to provide a simple way to obtain probabilities, entropies, or other complexity measures.","category":"page"},{"location":"#Probabilities","page":"Entropies.jl","title":"Probabilities","text":"","category":"section"},{"location":"","page":"Entropies.jl","title":"Entropies.jl","text":"Entropies and other complexity measures are typically computed based on probability distributions. These are obtained from Input data for Entropies.jl in a plethora of different ways. The central API function that returns a probability distribution (in fact, just a vector of probabilities) is probabilities, which takes in a subtype of ProbabilitiesEstimator to specify how the probabilities are computed. All estimators available in Entropies.jl can be found in the estimators page.","category":"page"},{"location":"#Entropies","page":"Entropies.jl","title":"Entropies","text":"","category":"section"},{"location":"","page":"Entropies.jl","title":"Entropies.jl","text":"Entropy is an established concept in statistics, information theory, and nonlinear dynamics. However it is also an umbrella term that may mean several computationally different quantities. In Entropies.jl, we provide the generic function entropy that tries to both clarify the disparate \"entropy concepts\", while unifying them under a common interface that highlights the modular nature of the word \"entropy\".","category":"page"},{"location":"","page":"Entropies.jl","title":"Entropies.jl","text":"Most of the time, computing an entropy boils down to two simple steps: first estimating a probability distribution, and then applying one of the so-called \"generalized entropy\" formulas to the distributions. Thus, any of the implemented probabilities estimators can be used to compute generalized entropies.","category":"page"},{"location":"","page":"Entropies.jl","title":"Entropies.jl","text":"tip: There aren't many entropies, really.\nA crucial thing to clarify is that many quantities that are named as entropies (e.g., permutation entropy entropy_permutation, wavelet entropy entropy_wavelet, etc.), are not really new entropies. They are new probabilities estimators. They simply devise a new way to calculate probabilities from data, and then plug those probabilities into formal entropy formulas such as the Shannon entropy. The probabilities estimators are smartly created so that they elegantly highlight important aspects of the data relevant to complexity.These names are commonplace, and so in Entropies.jl we provide convenience functions like entropy_wavelet. However, it should be noted that these functions really aren't anything more than 2-lines-of-code wrappers that call entropy with the appropriate ProbabilitiesEstimator.In addition to ProbabilitiesEstimators, we also provide EntropyEstimators,  which compute entropies via alternate means, without explicitly computing some  probability distribution. Differential/continuous entropy, for example, is computed using a dedicated EntropyEstimator. For example, the Kraskov  estimator computes Shannon differential entropy via a nearest neighbor algorithm, while  the Zhu estimator computes Shannon differential entropy using order statistics.","category":"page"},{"location":"#Other-complexity-measures","page":"Entropies.jl","title":"Other complexity measures","text":"","category":"section"},{"location":"","page":"Entropies.jl","title":"Entropies.jl","text":"Other complexity measures, which strictly speaking don't compute entropies, and may or may not explicitly compute probability distributions, are found in Complexity.jl package. This includes measures like sample entropy and approximate entropy.","category":"page"},{"location":"#input_data","page":"Entropies.jl","title":"Input data for Entropies.jl","text":"","category":"section"},{"location":"","page":"Entropies.jl","title":"Entropies.jl","text":"The input data type typically depend on the probability estimator chosen. In general though, the standard DynamicalSystems.jl approach is taken and as such we have three types of input data:","category":"page"},{"location":"","page":"Entropies.jl","title":"Entropies.jl","text":"Timeseries, which are AbstractVector{<:Real}, used in e.g. with WaveletOverlap.\nMulti-dimensional timeseries, or datasets, or state space sets, which are Dataset, used e.g. with NaiveKernel.\nSpatial data, which are higher dimensional standard Arrays, used e.g. with  SpatialSymbolicPermutation.","category":"page"},{"location":"devdocs/#Entropies.jl-Dev-Docs","page":"Entropies.jl Dev Docs","title":"Entropies.jl Dev Docs","text":"","category":"section"},{"location":"devdocs/","page":"Entropies.jl Dev Docs","title":"Entropies.jl Dev Docs","text":"Good practices in developing a code base apply in every Pull Request. The Good Scientific Code Workshop is worth checking out for this.","category":"page"},{"location":"devdocs/#Adding-a-new-ProbabilitiesEstimator","page":"Entropies.jl Dev Docs","title":"Adding a new ProbabilitiesEstimator","text":"","category":"section"},{"location":"devdocs/#Mandatory-steps","page":"Entropies.jl Dev Docs","title":"Mandatory steps","text":"","category":"section"},{"location":"devdocs/","page":"Entropies.jl Dev Docs","title":"Entropies.jl Dev Docs","text":"Decide on the outcome space and how the estimator will map probabilities to outcomes.\nDefine your type and make it subtype ProbabilitiesEstimator.\nAdd a docstring to your type following the style of the docstrings of other estimators.\nIf suitable, the estimator may be able to operate based on [Encoding]s. If so, it is preferred to implement an Encoding subtype and extend the methods encode and decode. This will allow your probabilities estimator to be used with a larger span of entropy and complexity methods without additional effort.\nImplement dispatch for probabilities_and_outcomes and your probabilities estimator type.\nImplement dispatch for outcome_space and your probabilities estimator type.\nAdd your probabilities estimator type to the list in the docstring of ProbabilitiyEstimator, and if you also made an encoding, add it to the Encoding docstring.","category":"page"},{"location":"devdocs/#Optional-steps","page":"Entropies.jl Dev Docs","title":"Optional steps","text":"","category":"section"},{"location":"devdocs/","page":"Entropies.jl Dev Docs","title":"Entropies.jl Dev Docs","text":"You may extend any of the following functions if there are potential performance benefits in doing so:","category":"page"},{"location":"devdocs/","page":"Entropies.jl Dev Docs","title":"Entropies.jl Dev Docs","text":"probabilities. By default it calls probabilities_and_outcomes and returns the first value.\noutcomes. By default calls probabilities_and_outcomes and returns the second value.\ntotal_outcomes. By default it returns the length of outcome_space. This is the function that most typically has performance benefits if implemented explicitly, so most existing estimators extend it by default.","category":"page"},{"location":"devdocs/#Tests","page":"Entropies.jl Dev Docs","title":"Tests","text":"","category":"section"},{"location":"devdocs/","page":"Entropies.jl Dev Docs","title":"Entropies.jl Dev Docs","text":"You also need to add tests for all functions that you explicitly extended. Not extended functions do not need to be tested.","category":"page"}]
}
