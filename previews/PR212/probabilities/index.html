<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Probabilities · Entropies.jl</title><script data-outdated-warner src="../assets/warner.js"></script><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.045/juliamono.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.24/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL=".."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../assets/documenter.js"></script><script src="../siteinfo.js"></script><script src="../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../assets/themeswap.js"></script><link href="https://fonts.googleapis.com/css?family=Montserrat|Source+Code+Pro&amp;display=swap" rel="stylesheet" type="text/css"/></head><body><div id="documenter"><nav class="docs-sidebar"><div class="docs-package-name"><span class="docs-autofit"><a href="../">Entropies.jl</a></span></div><form class="docs-search" action="../search/"><input class="docs-search-query" id="documenter-search-query" name="q" type="text" placeholder="Search docs"/></form><ul class="docs-menu"><li><a class="tocitem" href="../">Entropies.jl</a></li><li class="is-active"><a class="tocitem" href>Probabilities</a><ul class="internal"><li><a class="tocitem" href="#Probabilities-API"><span>Probabilities API</span></a></li><li><a class="tocitem" href="#Overview"><span>Overview</span></a></li><li><a class="tocitem" href="#Count-occurrences-(counting)"><span>Count occurrences (counting)</span></a></li><li><a class="tocitem" href="#Visitation-frequency-(histograms)"><span>Visitation frequency (histograms)</span></a></li><li><a class="tocitem" href="#Permutation-(symbolic)"><span>Permutation (symbolic)</span></a></li><li><a class="tocitem" href="#Dispersion-(symbolic)"><span>Dispersion (symbolic)</span></a></li><li><a class="tocitem" href="#Transfer-operator-(binning)"><span>Transfer operator (binning)</span></a></li><li><a class="tocitem" href="#Kernel-density"><span>Kernel density</span></a></li><li><a class="tocitem" href="#Timescales"><span>Timescales</span></a></li><li><a class="tocitem" href="#Diversity"><span>Diversity</span></a></li></ul></li><li><a class="tocitem" href="../entropies/">Entropies</a></li><li><a class="tocitem" href="../complexity/">Complexity measures</a></li><li><a class="tocitem" href="../multiscale/">Multiscale</a></li><li><a class="tocitem" href="../examples/">Entropies.jl Examples</a></li><li><a class="tocitem" href="../devdocs/">Entropies.jl Dev Docs</a></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><nav class="breadcrumb"><ul class="is-hidden-mobile"><li class="is-active"><a href>Probabilities</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>Probabilities</a></li></ul></nav><div class="docs-right"><a class="docs-edit-link" href="https://github.com/JuliaDynamics/Entropies.jl/blob/main/docs/src/probabilities.md" title="Edit on GitHub"><span class="docs-icon fab"></span><span class="docs-label is-hidden-touch">Edit on GitHub</span></a><a class="docs-settings-button fas fa-cog" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-sidebar-button fa fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a></div></header><article class="content" id="documenter-page"><h1 id="probabilities_estimators"><a class="docs-heading-anchor" href="#probabilities_estimators">Probabilities</a><a id="probabilities_estimators-1"></a><a class="docs-heading-anchor-permalink" href="#probabilities_estimators" title="Permalink"></a></h1><h2 id="Probabilities-API"><a class="docs-heading-anchor" href="#Probabilities-API">Probabilities API</a><a id="Probabilities-API-1"></a><a class="docs-heading-anchor-permalink" href="#Probabilities-API" title="Permalink"></a></h2><p>The probabilities API is defined by</p><ul><li><a href="#Entropies.ProbabilitiesEstimator"><code>ProbabilitiesEstimator</code></a></li><li><a href="#Entropies.probabilities"><code>probabilities</code></a></li><li><a href="#Entropies.probabilities_and_outcomes"><code>probabilities_and_outcomes</code></a></li></ul><article class="docstring"><header><a class="docstring-binding" id="Entropies.ProbabilitiesEstimator" href="#Entropies.ProbabilitiesEstimator"><code>Entropies.ProbabilitiesEstimator</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">ProbabilitiesEstimator</code></pre><p>The supertype for all probabilities estimators.</p><p>In Entropies.jl, probability distributions are estimated from data by defining a set of possible outcomes <span>$\Omega = \{\omega_1, \omega_2, \ldots, \omega_L \}$</span>, and assigning to each outcome <span>$\omega_i$</span> a probability <span>$p(\omega_i)$</span>, such that <span>$\sum_{i=1}^N p(\omega_i) = 1$</span>. It is the role a <a href="#Entropies.ProbabilitiesEstimator"><code>ProbabilitiesEstimator</code></a> to</p><ol><li>Define <span>$\Omega$</span>, the &quot;outcome space&quot;, which is the set of all possible outcomes over  which probabilities are estimated. The cardinality of this set can be obtained using  <a href="#Entropies.total_outcomes"><code>total_outcomes</code></a>.</li><li>Define how probabilities <span>$p_i = p(\omega_i)` are assigned to outcomes$</span>\omega_i``.</li></ol><p>In practice, probability estimation is done by calling <a href="#Entropies.probabilities"><code>probabilities</code></a> with some input data and one of the following probabilities estimators. The result is a <a href="#Entropies.Probabilities"><code>Probabilities</code></a> <code>p</code> (<code>Vector</code>-like), where each element <code>p[i]</code> is the probability of the outcome <code>ω[i]</code>. Use <a href="#Entropies.probabilities_and_outcomes"><code>probabilities_and_outcomes</code></a> if you need both the probabilities and the outcomes and <a href="#Entropies.outcome_space"><code>outcome_space</code></a> to obtain <span>$\Omega$</span>. The element type of <span>$\Omega$</span> varies between estimators, but it is guranteed to be <em>hashable</em>. This allows for conveniently tracking the probability of a specific event across experimental realizations, by using the outcome as a dictionary key and the probability as the value for that key (or, alternatively, the key remains the outcome and one has a vector of probabilities, one for each experimental realization).</p><p>We have made the design decision that all probabilities estimators have a well defined outcome space when instantiated. For some estimators this means that the input data <code>x</code> must be provided both when instantiating the estimator, but also when computing the probabilities.</p><p>All currently implemented probability estimators are:</p><ul><li><a href="#Entropies.CountOccurrences"><code>CountOccurrences</code></a>.</li><li><a href="#Entropies.ValueHistogram"><code>ValueHistogram</code></a>.</li><li><a href="#Entropies.TransferOperator"><code>TransferOperator</code></a>.</li><li><a href="#Entropies.Dispersion"><code>Dispersion</code></a>.</li><li><a href="#Entropies.SpatialDispersion"><code>SpatialDispersion</code></a>.</li><li><a href="#Entropies.WaveletOverlap"><code>WaveletOverlap</code></a>.</li><li><a href="#Entropies.PowerSpectrum"><code>PowerSpectrum</code></a>.</li><li><a href="#Entropies.SymbolicPermutation"><code>SymbolicPermutation</code></a>.</li><li><a href="#Entropies.SymbolicWeightedPermutation"><code>SymbolicWeightedPermutation</code></a>.</li><li><a href="#Entropies.SymbolicAmplitudeAwarePermutation"><code>SymbolicAmplitudeAwarePermutation</code></a>.</li><li><a href="#Entropies.SpatialSymbolicPermutation"><code>SpatialSymbolicPermutation</code></a>.</li><li><a href="#Entropies.NaiveKernel"><code>NaiveKernel</code></a>.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaDynamics/Entropies.jl/blob/c24e9c9119474abb81242c1e1abb894ee7ba58ca/src/probabilities.jl#L44-L89">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Entropies.probabilities" href="#Entropies.probabilities"><code>Entropies.probabilities</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">probabilities(est::ProbabilitiesEstimator, x::Array_or_Dataset) → p::Probabilities</code></pre><p>Compute a probability distribution over the set of possible outcomes defined by the probabilities estimator <code>est</code>, given input data <code>x</code>. To obtain the outcomes use <a href="#Entropies.outcomes"><code>outcomes</code></a>.</p><p>The returned probabilities <code>p</code> may or may not be ordered, and may or may not contain 0s; see the documentation of the individual estimators for more. Configuration options are always given as arguments to the chosen estimator. <code>x</code> is typically an <code>Array</code> or a <code>Dataset</code>; see <a href="@ref">Input data for Entropies.jl</a>.</p><pre><code class="nohighlight hljs">probabilities(x::Array_or_Dataset) → p::Probabilities</code></pre><p>Estimate probabilities by directly counting the elements of <code>x</code>, assuming that <code>Ω = sort(unique(x))</code>, i.e. that the outcome space is the unique elements of <code>x</code>. This is mostly useful when <code>x</code> contains categorical or integer data.</p><p>See also: <a href="#Entropies.Probabilities"><code>Probabilities</code></a>, <a href="#Entropies.ProbabilitiesEstimator"><code>ProbabilitiesEstimator</code></a>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaDynamics/Entropies.jl/blob/c24e9c9119474abb81242c1e1abb894ee7ba58ca/src/probabilities.jl#L95-L114">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Entropies.probabilities!" href="#Entropies.probabilities!"><code>Entropies.probabilities!</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">probabilities!(s, args...)</code></pre><p>Similar to <code>probabilities(args...)</code>, but allows pre-allocation of temporarily used containers <code>s</code>.</p><p>Only works for certain estimators. See for example <a href="#Entropies.SymbolicPermutation"><code>SymbolicPermutation</code></a>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaDynamics/Entropies.jl/blob/c24e9c9119474abb81242c1e1abb894ee7ba58ca/src/probabilities.jl#L133-L140">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Entropies.Probabilities" href="#Entropies.Probabilities"><code>Entropies.Probabilities</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">Probabilities &lt;: AbstractVector
Probabilities(x) → p</code></pre><p><code>Probabilities</code> is a simple wrapper around <code>AbstractVector</code> that ensures its values sum to 1, so that <code>p</code> can be interpreted as probability distribution.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaDynamics/Entropies.jl/blob/c24e9c9119474abb81242c1e1abb894ee7ba58ca/src/probabilities.jl#L11-L17">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Entropies.probabilities_and_outcomes" href="#Entropies.probabilities_and_outcomes"><code>Entropies.probabilities_and_outcomes</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">probabilities_and_outcomes(est, x)</code></pre><p>Return <code>probs, outs</code>, where <code>probs = probabilities(x, est)</code> and <code>outs[i]</code> is the outcome with probability <code>probs[i]</code>. The element type of <code>outs</code> depends on the estimator. <code>outs</code> is a subset of the <a href="#Entropies.outcome_space"><code>outcome_space</code></a> of <code>est</code>.</p><p>See also <a href="#Entropies.outcomes"><code>outcomes</code></a>, <a href="#Entropies.total_outcomes"><code>total_outcomes</code></a>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaDynamics/Entropies.jl/blob/c24e9c9119474abb81242c1e1abb894ee7ba58ca/src/probabilities.jl#L119-L128">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Entropies.outcomes" href="#Entropies.outcomes"><code>Entropies.outcomes</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">outcomes(est::ProbabilitiesEstimator, x)</code></pre><p>Return all (unique) outcomes contained in <code>x</code> according to the given estimator. Equivalent with <code>probabilities_and_outcomes(x, est)[2]</code>, but for some estimators it may be explicitly extended for better performance.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaDynamics/Entropies.jl/blob/c24e9c9119474abb81242c1e1abb894ee7ba58ca/src/probabilities.jl#L177-L182">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Entropies.outcome_space" href="#Entropies.outcome_space"><code>Entropies.outcome_space</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">outcome_space(est::ProbabilitiesEstimator) → Ω</code></pre><p>Return a container containing all <em>possible</em> outcomes of <code>est</code>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaDynamics/Entropies.jl/blob/c24e9c9119474abb81242c1e1abb894ee7ba58ca/src/probabilities.jl#L146-L150">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Entropies.total_outcomes" href="#Entropies.total_outcomes"><code>Entropies.total_outcomes</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">total_outcomes(est::ProbabilitiesEstimator)</code></pre><p>Return the length (cardinality) of the outcome space <span>$\Omega$</span> of <code>est</code>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaDynamics/Entropies.jl/blob/c24e9c9119474abb81242c1e1abb894ee7ba58ca/src/probabilities.jl#L155-L159">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Entropies.missing_outcomes" href="#Entropies.missing_outcomes"><code>Entropies.missing_outcomes</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">missing_outcomes(est::ProbabilitiesEstimator, x) → n_missing::Int</code></pre><p>Estimate a probability distribution for <code>x</code> using the given estimator, then count the number of missing (i.e. zero-probability) outcomes.</p><p>See also: <a href="../complexity/#Entropies.MissingDispersionPatterns"><code>MissingDispersionPatterns</code></a>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaDynamics/Entropies.jl/blob/c24e9c9119474abb81242c1e1abb894ee7ba58ca/src/probabilities.jl#L162-L169">source</a></section></article><h2 id="Overview"><a class="docs-heading-anchor" href="#Overview">Overview</a><a id="Overview-1"></a><a class="docs-heading-anchor-permalink" href="#Overview" title="Permalink"></a></h2><p>Any of the following estimators can be used with <a href="#Entropies.probabilities"><code>probabilities</code></a>.</p><table><tr><th style="text-align: right">Estimator</th><th style="text-align: right">Principle</th><th style="text-align: right">Input data</th></tr><tr><td style="text-align: right"><a href="#Entropies.CountOccurrences"><code>CountOccurrences</code></a></td><td style="text-align: right">Frequencies</td><td style="text-align: right"><code>Vector</code>, <code>Dataset</code></td></tr><tr><td style="text-align: right"><a href="#Entropies.ValueHistogram"><code>ValueHistogram</code></a></td><td style="text-align: right">Binning (histogram)</td><td style="text-align: right"><code>Vector</code>, <code>Dataset</code></td></tr><tr><td style="text-align: right"><a href="#Entropies.TransferOperator"><code>TransferOperator</code></a></td><td style="text-align: right">Binning (transfer operator)</td><td style="text-align: right"><code>Vector</code>, <code>Dataset</code></td></tr><tr><td style="text-align: right"><a href="#Entropies.NaiveKernel"><code>NaiveKernel</code></a></td><td style="text-align: right">Kernel density estimation</td><td style="text-align: right"><code>Dataset</code></td></tr><tr><td style="text-align: right"><a href="#Entropies.SymbolicPermutation"><code>SymbolicPermutation</code></a></td><td style="text-align: right">Ordinal patterns</td><td style="text-align: right"><code>Vector</code></td></tr><tr><td style="text-align: right"><a href="#Entropies.SymbolicWeightedPermutation"><code>SymbolicWeightedPermutation</code></a></td><td style="text-align: right">Ordinal patterns</td><td style="text-align: right"><code>Vector</code></td></tr><tr><td style="text-align: right"><a href="#Entropies.SymbolicAmplitudeAwarePermutation"><code>SymbolicAmplitudeAwarePermutation</code></a></td><td style="text-align: right">Ordinal patterns</td><td style="text-align: right"><code>Vector</code></td></tr><tr><td style="text-align: right"><a href="#Entropies.Dispersion"><code>Dispersion</code></a></td><td style="text-align: right">Dispersion patterns</td><td style="text-align: right"><code>Vector</code></td></tr><tr><td style="text-align: right"><a href="#Entropies.Diversity"><code>Diversity</code></a></td><td style="text-align: right">Cosine similarity</td><td style="text-align: right"><code>Vector</code></td></tr><tr><td style="text-align: right"><a href="#Entropies.WaveletOverlap"><code>WaveletOverlap</code></a></td><td style="text-align: right">Wavelet transform</td><td style="text-align: right"><code>Vector</code></td></tr><tr><td style="text-align: right"><a href="#Entropies.PowerSpectrum"><code>PowerSpectrum</code></a></td><td style="text-align: right">Fourier spectra</td><td style="text-align: right"><code>Vector</code>, <code>Dataset</code></td></tr></table><h2 id="Count-occurrences-(counting)"><a class="docs-heading-anchor" href="#Count-occurrences-(counting)">Count occurrences (counting)</a><a id="Count-occurrences-(counting)-1"></a><a class="docs-heading-anchor-permalink" href="#Count-occurrences-(counting)" title="Permalink"></a></h2><article class="docstring"><header><a class="docstring-binding" id="Entropies.CountOccurrences" href="#Entropies.CountOccurrences"><code>Entropies.CountOccurrences</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">CountOccurrences(x)</code></pre><p>A probabilities/entropy estimator based on straight-forward counting of distinct elements in a univariate time series or multivariate dataset. This is the same as giving no estimator to <a href="#Entropies.probabilities"><code>probabilities</code></a>.</p><p><strong>Outcome space</strong></p><p>The outcome space is the unique sorted values of the input. Hence, input <code>x</code> is needed for a well-defined outcome space.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaDynamics/Entropies.jl/blob/c24e9c9119474abb81242c1e1abb894ee7ba58ca/src/probabilities_estimators/counting/count_occurences.jl#L3-L13">source</a></section></article><h2 id="Visitation-frequency-(histograms)"><a class="docs-heading-anchor" href="#Visitation-frequency-(histograms)">Visitation frequency (histograms)</a><a id="Visitation-frequency-(histograms)-1"></a><a class="docs-heading-anchor-permalink" href="#Visitation-frequency-(histograms)" title="Permalink"></a></h2><article class="docstring"><header><a class="docstring-binding" id="Entropies.ValueHistogram" href="#Entropies.ValueHistogram"><code>Entropies.ValueHistogram</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">ValueHistogram(x, b::RectangularBinning) &lt;: ProbabilitiesEstimator
ValueHistogram(b::FixedRectangularBinning) &lt;: ProbabilitiesEstimator</code></pre><p>A probability estimator based on binning the values of the data as dictated by the binning scheme <code>b</code> and formally computing their histogram, i.e., the frequencies of points in the bins. An alias to this is <code>VisitationFrequency</code>. Available binnings are:</p><ul><li><a href="#Entropies.RectangularBinning"><code>RectangularBinning</code></a></li><li><a href="#Entropies.FixedRectangularBinning"><code>FixedRectangularBinning</code></a></li></ul><p>Notice that if not using the fixed binning, <code>x</code> (the input data) must also be given to the estimator, as it is not possible to deduce histogram size only from the binning.</p><p>The <code>ValueHistogram</code> estimator has a linearithmic time complexity (<code>n log(n)</code> for <code>n = length(x)</code>) and a linear space complexity (<code>l</code> for <code>l = dimension(x)</code>). This allows computation of probabilities (histograms) of high-dimensional datasets and with small box sizes <code>ε</code> without memory overflow and with maximum performance. For performance reasons, the probabilities returned never contain 0s and are arbitrarily ordered.</p><pre><code class="nohighlight hljs">ValueHistogram(x, ϵ::Union{Real,Vector})</code></pre><p>A convenience method that accepts same input as <a href="#Entropies.RectangularBinning"><code>RectangularBinning</code></a> and initializes this binning directly.</p><p><strong>Outcomes</strong></p><p>The outcome space for <code>ValueHistogram</code> is the unique bins constructed from <code>b</code>. Each bin is identified by its left (lowest-value) corner. The bins are in data units, not integer (cartesian indices units), and are returned as <code>SVector</code>s.</p><p>See also: <a href="#Entropies.RectangularBinning"><code>RectangularBinning</code></a>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaDynamics/Entropies.jl/blob/c24e9c9119474abb81242c1e1abb894ee7ba58ca/src/probabilities_estimators/histograms/value_histogram.jl#L6-L40">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Entropies.RectangularBinning" href="#Entropies.RectangularBinning"><code>Entropies.RectangularBinning</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">RectangularBinning(ϵ) &lt;: AbstractBinning</code></pre><p>Rectangular box partition of state space using the scheme <code>ϵ</code>, deducing the coordinates of the grid axis minima from the input data. Generally it is preferred to use <a href="#Entropies.FixedRectangularBinning"><code>FixedRectangularBinning</code></a> instead, as it has a well defined outcome space without knowledge of input data.</p><p>Binning instructions are deduced from the type of <code>ϵ</code> as follows:</p><ol><li><code>ϵ::Int</code> divides each coordinate axis into <code>ϵ</code> equal-length intervals  that cover all data.</li><li><code>ϵ::Float64</code> divides each coordinate axis into intervals of fixed size <code>ϵ</code>, starting  from the axis minima until the data is completely covered by boxes.</li><li><code>ϵ::Vector{Int}</code> divides the i-th coordinate axis into <code>ϵ[i]</code> equal-length  intervals that cover all data.</li><li><code>ϵ::Vector{Float64}</code> divides the i-th coordinate axis into intervals of fixed size  <code>ϵ[i]</code>, starting from the axis minima until the data is completely covered by boxes.</li></ol></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaDynamics/Entropies.jl/blob/c24e9c9119474abb81242c1e1abb894ee7ba58ca/src/probabilities_estimators/histograms/rectangular_binning.jl#L15-L33">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Entropies.FixedRectangularBinning" href="#Entropies.FixedRectangularBinning"><code>Entropies.FixedRectangularBinning</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">FixedRectangularBinning &lt;: AbstractBinning
FixedRectangularBinning(ϵmin::NTuple, ϵmax::NTuple, N::Int)</code></pre><p>Rectangular box partition of state space where the extent of the grid is explicitly specified by <code>ϵmin</code> and <code>emax</code>, and along each dimension, the grid is subdivided into <code>N</code> subintervals. Points falling outside the partition do not attribute to probabilities. This binning type leads to a well-defined outcome space without knowledge of input, see <a href="#Entropies.ValueHistogram"><code>ValueHistogram</code></a>.</p><p><code>ϵmin</code>/<code>emax</code> must be <code>NTuple{D, &lt;:Real}</code> for input of <code>D</code>-dimensional data.</p><pre><code class="nohighlight hljs">FixedRectangularBinning(ϵmin::Real, ϵmax::Real, N::Int, D::Int = 1)</code></pre><p>This is a convenience method where each dimension of the binning has the same extent and the input data are <code>D</code> dimensional, which defaults to 1 (timeseries).</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaDynamics/Entropies.jl/blob/c24e9c9119474abb81242c1e1abb894ee7ba58ca/src/probabilities_estimators/histograms/rectangular_binning.jl#L40-L56">source</a></section></article><h2 id="Permutation-(symbolic)"><a class="docs-heading-anchor" href="#Permutation-(symbolic)">Permutation (symbolic)</a><a id="Permutation-(symbolic)-1"></a><a class="docs-heading-anchor-permalink" href="#Permutation-(symbolic)" title="Permalink"></a></h2><article class="docstring"><header><a class="docstring-binding" id="Entropies.SymbolicPermutation" href="#Entropies.SymbolicPermutation"><code>Entropies.SymbolicPermutation</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">SymbolicPermutation &lt;: ProbabilitiesEstimator
SymbolicPermutation(; m = 3, τ = 1, lt::Function = Entropies.isless_rand)</code></pre><p>A probabilities estimator based on ordinal permutation patterns.</p><p>The quantity computed depends on the input data:</p><ul><li><strong>Univariate data</strong>. If applied to a univariate time series, then the time series   is first embedded using embedding delay <code>τ</code> and dimension <code>m</code>, resulting in embedding   vectors <span>$\{ \bf{x}_i \}_{i=1}^{N-(m-1)\tau}$</span>. Then, for each <span>$\bf{x}_i$</span>,   we find its permutation pattern <span>$\pi_{i}$</span>, which we internally encode a an integer   <span>$s_i \in \mathbb{N}^+$</span> for efficient computation (integer symbols are obtained by   using <a href="@ref"><code>encode</code></a> with <a href="@ref"><code>OrdinalPatternEncoding</code></a>).   Probabilities are then   estimated as naive frequencies over the encoded permutation symbols   <span>$\{ s_i \}_{i=1}^{N-(m-1)\tau}$</span> by using <a href="#Entropies.CountOccurrences"><code>CountOccurrences</code></a>.   The resulting probabilities can be used to compute permutation entropy (PE;   Bandt &amp; Pompe, 2002<sup class="footnote-reference"><a id="citeref-BandtPompe2002" href="#footnote-BandtPompe2002">[BandtPompe2002]</a></sup>).</li><li><strong>Multivariate data</strong>. If applied to a an <code>D</code>-dimensional <code>Dataset</code>,   then it is assumed that the input data represents <span>$N$</span> observations of a multivariate   system <span>$\{ \bf{x}_i \}_{i=1}^N$</span>, and no embedding is constructed.   For each <span>$\bf{x}_i \in \mathbb{R}^D$</span>, we direct find its permutation pattern   <span>$\pi_{i}$</span> and encode it as <span>$s_i \in \mathbb{N}^+$</span> (i.e. <code>est.τ</code> and <code>est.m</code> are   ignored, and we set <code>m = D</code> instead). Finally, probabilities are estimated as relative   frequencies of occurrences of the encoded permutation symbols.   The resulting probabilities can be used to compute multivariate permutation   entropy (MvPE; He et al., 2016<sup class="footnote-reference"><a id="citeref-He2016" href="#footnote-He2016">[He2016]</a></sup>), but here we don&#39;t perform any subdivision   of the permutation patterns (see Figure 3 in He et al., 2016).</li></ul><p><strong>Outcome space</strong></p><p>The outcome space <code>Ω</code> for <code>SymbolicPermutation</code> is the set of length-<code>m</code> ordinal patterns (i.e. permutations) that can be formed by the integers <code>1, 2, …, m</code>, ordered lexicographically. There are <code>factorial(m)</code> such patterns.</p><p><strong>In-place symbolization</strong></p><p><code>SymbolicPermutation</code> also implements the in-place <a href="@ref"><code>entropy!</code></a> and <a href="#Entropies.probabilities!"><code>probabilities!</code></a>. The length of the pre-allocated symbol vector must match the length of the embedding: <code>N - (m-1)τ</code> for univariate time series, and <code>M</code> for length-<code>M</code> <code>Dataset</code>s), i.e.</p><pre><code class="language-julia hljs">using DelayEmbeddings, Entropies
m, τ, N = 2, 1, 100
est = SymbolicPermutation(; m, τ)

# For a time series
x_ts = rand(N)
πs_ts = zeros(Int, N - (m - 1)*τ)
p = probabilities!(πs_ts, est, x_ts)
h = entropy!(πs_ts, Renyi(), est, x_ts)

# For a pre-discretized `Dataset`
x_symb = outcomes(x_ts, OrdinalPatternEncoding(m = 2, τ = 1))
x_d = genembed(x_symb, (0, -1, -2))
πs_d = zeros(Int, length(x_d))
p = probabilities!(πs_d, est, x_d)
h = entropy!(πs_d, Renyi(), est, x_d)</code></pre><p>See <a href="#Entropies.SymbolicWeightedPermutation"><code>SymbolicWeightedPermutation</code></a> and <a href="#Entropies.SymbolicAmplitudeAwarePermutation"><code>SymbolicAmplitudeAwarePermutation</code></a> for estimators that not only consider ordinal (sorting) patterns, but also incorporate information about within-state-vector amplitudes.</p><div class="admonition is-info"><header class="admonition-header">Handling equal values in ordinal patterns</header><div class="admonition-body"><p>In Bandt &amp; Pompe (2002), equal values are ordered after their order of appearance, but this can lead to erroneous temporal correlations, especially for data with low-amplitude resolution <sup class="footnote-reference"><a id="citeref-Zunino2017" href="#footnote-Zunino2017">[Zunino2017]</a></sup>. Here, by default, if two values are equal, then one of the is randomly assigned as &quot;the largest&quot;, using <code>lt = Entropies.isless_rand</code>. To get the behaviour from Bandt and Pompe (2002), use <code>lt = Base.isless</code>).</p></div></div></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaDynamics/Entropies.jl/blob/c24e9c9119474abb81242c1e1abb894ee7ba58ca/src/probabilities_estimators/permutation_ordinal/SymbolicPermutation.jl#L4-L87">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Entropies.SymbolicWeightedPermutation" href="#Entropies.SymbolicWeightedPermutation"><code>Entropies.SymbolicWeightedPermutation</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">SymbolicWeightedPermutation &lt;: ProbabilitiesEstimator
SymbolicWeightedPermutation(; τ = 1, m = 3, lt = Entropies.isless_rand)</code></pre><p>A variant of <a href="#Entropies.SymbolicPermutation"><code>SymbolicPermutation</code></a> that also incorporates amplitude information, based on the weighted permutation entropy (Fadlallah et al., 2013).</p><p><strong>Outcome space</strong></p><p>Like for <a href="#Entropies.SymbolicPermutation"><code>SymbolicPermutation</code></a>, the outcome space <code>Ω</code> for <code>SymbolicWeightedPermutation</code> is the lexiographically ordered set of length-<code>m</code> ordinal patterns (i.e. permutations) that can be formed by the integers <code>1, 2, …, m</code>. There are <code>factorial(m)</code> such patterns.</p><p><strong>Description</strong></p><p>Probabilities are computed as</p><p class="math-container">\[p(\pi_i^{m, \tau}) = \dfrac{\sum_{k=1}^N \mathbf{1}_{u:S(u) = s_i}
\left( \mathbf{x}_k^{m, \tau} \right)
\, w_k}{\sum_{k=1}^N \mathbf{1}_{u:S(u) \in \Pi}
\left( \mathbf{x}_k^{m, \tau} \right) \,w_k} = \dfrac{\sum_{k=1}^N
\mathbf{1}_{u:S(u) = s_i}
\left( \mathbf{x}_k^{m, \tau} \right) \, w_k}{\sum_{k=1}^N w_k},\]</p><p>where weights are computed based on the variance of the state vectors as</p><p class="math-container">\[w_j = \dfrac{1}{m}\sum_{k=1}^m (x_{j+(k+1)\tau} - \mathbf{\hat{x}}_j^{m, \tau})^2,\]</p><p>and <span>$\mathbf{x}_i$</span> is the aritmetic mean of state vector:</p><p class="math-container">\[\mathbf{\hat{x}}_j^{m, \tau} = \frac{1}{m} \sum_{k=1}^m x_{j + (k+1)\tau}.\]</p><p>The weighted permutation entropy is equivalent to regular permutation entropy when weights are positive and identical (<span>$w_j = \beta \,\,\, \forall \,\,\, j \leq N$</span> and <span>$\beta &gt; 0)$</span>.</p><p>See <a href="#Entropies.SymbolicPermutation"><code>SymbolicPermutation</code></a> for an estimator that only incorporates ordinal/sorting information and disregards amplitudes, and <a href="#Entropies.SymbolicAmplitudeAwarePermutation"><code>SymbolicAmplitudeAwarePermutation</code></a> for another estimator that incorporates amplitude information.</p><div class="admonition is-info"><header class="admonition-header">An implementation note</header><div class="admonition-body"><p><em>Note: in equation 7, section III, of the original paper, the authors write</em></p><p class="math-container">\[w_j = \dfrac{1}{m}\sum_{k=1}^m (x_{j-(k-1)\tau} - \mathbf{\hat{x}}_j^{m, \tau})^2.\]</p><p><em>But given the formula they give for the arithmetic mean, this is <strong>not</strong> the variance of <span>$\mathbf{x}_i$</span>, because the indices are mixed: <span>$x_{j+(k-1)\tau}$</span> in the weights formula, vs. <span>$x_{j+(k+1)\tau}$</span> in the arithmetic mean formula. This seems to imply that amplitude information about previous delay vectors are mixed with mean amplitude information about current vectors. The authors also mix the terms &quot;vector&quot; and &quot;neighboring vector&quot; (but uses the same notation for both), making it hard to interpret whether the sign switch is a typo or intended. Here, we use the notation above, which actually computes the variance for <span>$\mathbf{x}_i$</span></em>.</p></div></div></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaDynamics/Entropies.jl/blob/c24e9c9119474abb81242c1e1abb894ee7ba58ca/src/probabilities_estimators/permutation_ordinal/SymbolicWeightedPermutation.jl#L6-L73">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Entropies.SymbolicAmplitudeAwarePermutation" href="#Entropies.SymbolicAmplitudeAwarePermutation"><code>Entropies.SymbolicAmplitudeAwarePermutation</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">SymbolicAmplitudeAwarePermutation &lt;: ProbabilitiesEstimator
SymbolicAmplitudeAwarePermutation(; τ = 1, m = 3, A = 0.5, lt = Entropies.isless_rand)</code></pre><p>A variant of <a href="#Entropies.SymbolicPermutation"><code>SymbolicPermutation</code></a> that also incorporates amplitude information, based on the amplitude-aware permutation entropy (Azami &amp; Escudero, 2016).</p><p><strong>Outcome space</strong></p><p>Like for <a href="#Entropies.SymbolicPermutation"><code>SymbolicPermutation</code></a>, the outcome space <code>Ω</code> for <code>SymbolicAmplitudeAwarePermutation</code> is the lexiographically ordered set of length-<code>m</code> ordinal patterns (i.e. permutations) that can be formed by the integers <code>1, 2, …, m</code>. There are <code>factorial(m)</code> such patterns.</p><p><strong>Description</strong></p><p>Probabilities are computed as</p><p class="math-container">\[p(\pi_i^{m, \tau}) =
\dfrac{\sum_{k=1}^N
\mathbf{1}_{u:S(u) = s_i} \left( \mathbf{x}_k^{m, \tau} \right) \, a_k}{\sum_{k=1}^N
\mathbf{1}_{u:S(u) \in \Pi} \left( \mathbf{x}_k^{m, \tau} \right) \,a_k} =
\dfrac{\sum_{k=1}^N \mathbf{1}_{u:S(u) = s_i}
\left( \mathbf{x}_k^{m, \tau} \right) \, a_k}{\sum_{k=1}^N a_k}.\]</p><p>The weights encoding amplitude information about state vector <span>$\mathbf{x}_i = (x_1^i, x_2^i, \ldots, x_m^i)$</span> are</p><p class="math-container">\[a_i = \dfrac{A}{m} \sum_{k=1}^m |x_k^i | + \dfrac{1-A}{d-1}
\sum_{k=2}^d |x_{k}^i - x_{k-1}^i|,\]</p><p>with <span>$0 \leq A \leq 1$</span>. When <span>$A=0$</span> , only internal differences between the elements of <span>$\mathbf{x}_i$</span> are weighted. Only mean amplitude of the state vector elements are weighted when <span>$A=1$</span>. With, <span>$0&lt;A&lt;1$</span>, a combined weighting is used.</p><p>See <a href="#Entropies.SymbolicPermutation"><code>SymbolicPermutation</code></a> for an estimator that only incorporates ordinal/sorting information and disregards amplitudes, and <a href="#Entropies.SymbolicWeightedPermutation"><code>SymbolicWeightedPermutation</code></a> for another estimator that incorporates amplitude information.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaDynamics/Entropies.jl/blob/c24e9c9119474abb81242c1e1abb894ee7ba58ca/src/probabilities_estimators/permutation_ordinal/SymbolicAmplitudeAware.jl#L3-L50">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Entropies.SpatialSymbolicPermutation" href="#Entropies.SpatialSymbolicPermutation"><code>Entropies.SpatialSymbolicPermutation</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">SpatialSymbolicPermutation &lt;: ProbabilitiesEstimator
SpatialSymbolicPermutation(stencil, x; periodic = true)</code></pre><p>A symbolic, permutation-based probabilities estimator for spatiotemporal systems that generalises <a href="#Entropies.SymbolicPermutation"><code>SymbolicPermutation</code></a> to high-dimensional arrays.</p><p><code>SpatialSymbolicPermutation</code> is based on the 2D and 3D <em>spatiotemporal permutation entropy</em> estimators by by Ribeiro et al. (2012)<sup class="footnote-reference"><a id="citeref-Ribeiro2012" href="#footnote-Ribeiro2012">[Ribeiro2012]</a></sup> and Schlemmer et al. (2018)<sup class="footnote-reference"><a id="citeref-Schlemmer2018" href="#footnote-Schlemmer2018">[Schlemmer2018]</a></sup>), respectively, but is here implemented as a pure probabilities probabilities estimator that is generalized for <code>N</code>-dimensional input data <code>x</code>, with arbitrary neighborhood regions (stencils) and periodic boundary conditions.</p><p>In combination with <a href="../entropies/#Entropies.entropy"><code>entropy</code></a> and <a href="@ref"><code>entropy_normalized</code></a>, this probabilities estimator can be used to compute (normalized) generalized spatiotemporal permutation <a href="../entropies/#Entropies.Entropy"><code>Entropy</code></a> of any type.</p><p><strong>Arguments</strong></p><ul><li><code>stencil</code>. Defines what local area (hyperrectangle), or which points within this area,   to include around each hypervoxel (i.e. pixel in 2D). The examples below demonstrate   different ways of specifying stencils. For details, see   <a href="#Entropies.SpatialSymbolicPermutation"><code>SpatialSymbolicPermutation</code></a>.</li><li><code>x::AbstractArray</code>. The input data. Must be provided because we need to know its size   for optimization and bound checking.</li></ul><p><strong>Keyword arguments</strong></p><ul><li><code>periodic::Bool</code>. If <code>periodic == true</code>, then the stencil should wrap around at the   end of the array. If <code>periodic = false</code>, then pixels whose stencil exceeds the array   bounds are skipped.</li></ul><p><strong>Stencils</strong></p><p>Stencils are passed in one of the following three ways:</p><ol><li><p>As vectors of <code>CartesianIndex</code> which encode the pixels to include in the  stencil, with respect to the current pixel, or integer arrays of the same dimensionality  as the data. For example <code>stencil = CartesianIndex.([(0,0), (0,1), (1,1), (1,0)])</code>.  Don&#39;t forget to include the zero offset index if you want to include the point itself,  which is almost always the case.  Here the stencil creates a 2x2 square extending to the bottom and right of the pixel  (directions here correspond to the way Julia prints matrices by default).  When passing a stencil as a vector of <code>CartesianIndex</code>, <code>m = length(stencil)</code>.</p></li><li><p>As a <code>D</code>-dimensional array (where <code>D</code> matches the dimensionality of the input data)  containing <code>0</code>s and <code>1</code>s, where if <code>stencil[index] == 1</code>, the corresponding pixel is  included, and if <code>stencil[index] == 0</code>, it is not included.  To generate the same estimator as in 1., use <code>stencil = [1 1; 1 1]</code>.  When passing a stencil as a <code>D</code>-dimensional array, <code>m = sum(stencil)</code></p></li><li><p>As a <code>Tuple</code> containing two <code>Tuple</code>s, both of length <code>D</code>, for <code>D</code>-dimensional data.  The first tuple specifies the <code>extent</code> of the stencil, where <code>extent[i]</code>  dictates the number of pixels to be included along the <code>i</code>th axis and <code>lag[i]</code>  the separation of pixels along the same axis.  This method can only generate (hyper)rectangular stencils. To create the same estimator as  in the previous examples, use here <code>stencil = ((2, 2), (1, 1))</code>.  When passing a stencil using <code>extent</code> and <code>lag</code>, <code>m = prod(extent)!</code>.</p></li></ol><p><strong>Example: spatiotemporal entropy for time series</strong></p><p>Usage is simple. First, define a <code>SpatialSymbolicPermutation</code> estimator by specifying a stencil and giving some input data (a matrix with the same dimensions as the data as you&#39;re going to analyse). Then simply call <a href="../entropies/#Entropies.entropy"><code>entropy</code></a> with the estimator.</p><pre><code class="language-julia hljs">using Entropies
x = rand(50, 50) # first &quot;time slice&quot; of a spatial system evolution
stencil = [1 1; 0 1] # or one of the other ways of specifying stencils
est = SpatialSymbolicPermutation(stencil, x)
h = entropy(est, x)</code></pre><p>To apply this to timeseries of spatial data, simply loop over the call, e.g.:</p><pre><code class="language-julia hljs">data = [rand(50, 50) for i in 1:50]
est = SpatialSymbolicPermutation(stencil, first(data))
h_vs_t = [entropy(est, d) for d in data]</code></pre><p>Computing generalized spatiotemporal permutation entropy is trivial, e.g. with <a href="../entropies/#Entropies.Renyi"><code>Renyi</code></a>:</p><pre><code class="language-julia hljs">x = reshape(repeat(1:5, 500) .+ 0.1*rand(500*5), 50, 50)
est = SpatialSymbolicPermutation(stencil, x)
entropy(Renyi(q = 2), est, x)</code></pre><p><strong>Outcome space</strong></p><p>The outcome space <code>Ω</code> for <code>SpatialSymbolicPermutation</code> is the set of length-<code>m</code> ordinal patterns (i.e. permutations) that can be formed by the integers <code>1, 2, …, m</code>, ordered lexicographically. There are <code>factorial(m)</code> such patterns. Here, <code>m</code> refers to the number of points included by <code>stencil</code>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaDynamics/Entropies.jl/blob/c24e9c9119474abb81242c1e1abb894ee7ba58ca/src/probabilities_estimators/spatial/spatial_permutation/SpatialSymbolicPermutation.jl#L8-L112">source</a></section></article><h2 id="Dispersion-(symbolic)"><a class="docs-heading-anchor" href="#Dispersion-(symbolic)">Dispersion (symbolic)</a><a id="Dispersion-(symbolic)-1"></a><a class="docs-heading-anchor-permalink" href="#Dispersion-(symbolic)" title="Permalink"></a></h2><article class="docstring"><header><a class="docstring-binding" id="Entropies.Dispersion" href="#Entropies.Dispersion"><code>Entropies.Dispersion</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">Dispersion(; c = 5, m = 2, τ = 1, check_unique = true)</code></pre><p>A probability estimator based on dispersion patterns, originally used by Rostaghi &amp; Azami, 2016<sup class="footnote-reference"><a id="citeref-Rostaghi2016" href="#footnote-Rostaghi2016">[Rostaghi2016]</a></sup> to compute the &quot;dispersion entropy&quot;, which characterizes the complexity and irregularity of a time series.</p><p>Recommended parameter values<sup class="footnote-reference"><a id="citeref-Li2018" href="#footnote-Li2018">[Li2018]</a></sup> are <code>m ∈ [2, 3]</code>, <code>τ = 1</code> for the embedding, and <code>c ∈ [3, 4, …, 8]</code> categories for the Gaussian symbol mapping.</p><p><strong>Description</strong></p><p>Assume we have a univariate time series <span>$X = \{x_i\}_{i=1}^N$</span>. First, this time series is discretized a &quot;Gaussian encoding&quot;, which uses the normal cumulative distribution function (CDF) to encode a timeseries <span>$x$</span> as integers like so:</p><ul><li>Each <span>$x_i$</span> is mapped to a new real number <span>$y_i \in [0, 1]$</span> by using the normal   cumulative distribution function (CDF), <span>$x_i \to y_i : y_i = \dfrac{1}{ \sigma   \sqrt{2 \pi}} \int_{-\infty}^{x_i} e^{(-(x_i - \mu)^2)/(2 \sigma^2)} dx$</span>,   where <span>$\mu$</span> and <span>$\sigma$</span> are the empirical mean and standard deviation of <span>$X$</span>.   Other choices of CDFs are also possible, but currently only Gaussian is implemented.</li><li>Next, each <span>$y_i$</span> is linearly mapped to an integer   <span>$z_i \in [1, 2, \ldots, c]$</span> using the map   <span>$y_i \to z_i : z_i = \text{floor}(y_i ./ (1 / c)) + 1$</span> (<em>note: this mapping differs   slightly from the linear mapping in the original paper</em>). This procedure subdivides the   interval <span>$[0, 1]$</span> into <span>$c$</span>   different subintervals that form a covering of <span>$[0, 1]$</span>, and assigns each <span>$y_i$</span> to one   of these subintervals. The original time series <span>$X$</span> is thus transformed to a symbol time   series <span>$S = \{ s_i \}_{i=1}^N$</span>, where <span>$s_i \in [1, 2, \ldots, c]$</span>.</li><li>Next, the symbol time series <span>$S$</span> is embedded into an   <span>$m$</span>-dimensional time series, using an embedding lag of <span>$\tau = 1$</span>, which yields a   total of <span>$N - (m - 1)\tau$</span> points, or &quot;dispersion patterns&quot;. Because each <span>$z_i$</span> can   take on <span>$c$</span> different values, and each embedding point has <span>$m$</span> values, there   are <span>$c^m$</span> possible dispersion patterns. This number is used for normalization when   computing dispersion entropy.</li></ul><p><strong>Computing dispersion probabilities and entropy</strong></p><p>A probability distribution <span>$P = \{p_i \}_{i=1}^{c^m}$</span>, where <span>$\sum_i^{c^m} p_i = 1$</span>, can then be estimated by counting and sum-normalising the distribution of dispersion patterns among the embedding vectors. Note that dispersion patterns that are not present are not counted. Therefore, you&#39;ll always get non-zero probabilities using the <code>Dispersion</code> probability estimator.</p><p><strong>Outcome space</strong></p><p>The outcome space for <code>Dispersion</code> is the unique delay vectors whose elements are the the symbols (integers) encoded by the Gaussian CDF. Hence, the outcome space is all <code>m</code>-dimensional delay vectors whose elements are all possible values in <code>1:c</code>. There are <code>c ^ m</code> such vectors.</p><p><strong>Data requirements and parameters</strong></p><p>The input must have more than one unique element for the Gaussian mapping to be well-defined. Li et al. (2018) recommends that <code>x</code> has at least 1000 data points.</p><p>If <code>check_unique == true</code> (default), then it is checked that the input has more than one unique value. If <code>check_unique == false</code> and the input only has one unique element, then a <code>InexactError</code> is thrown when trying to compute probabilities.</p><div class="admonition is-info"><header class="admonition-header">Why &#39;dispersion patterns&#39;?</header><div class="admonition-body"><p>Each embedding vector is called a &quot;dispersion pattern&quot;. Why? Let&#39;s consider the case when <span>$m = 5$</span> and <span>$c = 3$</span>, and use some very imprecise terminology for illustration:</p><p>When <span>$c = 3$</span>, values clustering far below mean are in one group, values clustered around the mean are in one group, and values clustering far above the mean are in a third group. Then the embedding vector <span>$[2, 2, 2, 2, 2]$</span> consists of values that are relatively close together (close to the mean), so it represents a set of numbers that are not very spread out (less dispersed). The embedding vector <span>$[1, 1, 2, 3, 3]$</span>, however, represents numbers that are much more spread out (more dispersed), because the categories representing &quot;outliers&quot; both above and below the mean are represented, not only values close to the mean.</p></div></div></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaDynamics/Entropies.jl/blob/c24e9c9119474abb81242c1e1abb894ee7ba58ca/src/probabilities_estimators/dispersion/dispersion.jl#L5-L81">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Entropies.SpatialDispersion" href="#Entropies.SpatialDispersion"><code>Entropies.SpatialDispersion</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">SpatialDispersion &lt;: ProbabilitiesEstimator
SpatialDispersion(stencil, x::AbstractArray;
    periodic = true,
    c = 5,
    skip_encoding = false,
    L = nothing,
)</code></pre><p>A dispersion-based probabilities estimator that generalises <a href="#Entropies.Dispersion"><code>Dispersion</code></a> for input data that are high-dimensional arrays.</p><p><code>SpatialDispersion</code> is based on Azami et al. (2019)<sup class="footnote-reference"><a id="citeref-Azami2019" href="#footnote-Azami2019">[Azami2019]</a></sup>&#39;s 2D square dispersion (Shannon) entropy estimator, but is here implemented as a pure probabilities probabilities estimator that is generalized for <code>N</code>-dimensional input data <code>x</code>, with arbitrary neighborhood regions (stencils) and (optionally) periodic boundary conditions.</p><p>In combination with <a href="../entropies/#Entropies.entropy"><code>entropy</code></a> and <a href="@ref"><code>entropy_normalized</code></a>, this probabilities estimator can be used to compute (normalized) generalized spatiotemporal dispersion <a href="../entropies/#Entropies.Entropy"><code>Entropy</code></a> of any type.</p><p><strong>Arguments</strong></p><ul><li><code>stencil</code>. Defines what local area (hyperrectangle), or which points within this area,   to include around each hypervoxel (i.e. pixel in 2D). The examples below demonstrate   different ways of specifying stencils. For details, see   <a href="#Entropies.SpatialSymbolicPermutation"><code>SpatialSymbolicPermutation</code></a>. See <a href="#Entropies.SpatialSymbolicPermutation"><code>SpatialSymbolicPermutation</code></a> for   more information about stencils.</li><li><code>x::AbstractArray</code>. The input data. Must be provided because we need to know its size   for optimization and bound checking.</li></ul><p><strong>Keyword arguments</strong></p><ul><li><code>periodic::Bool</code>. If <code>periodic == true</code>, then the stencil should wrap around at the   end of the array. If <code>periodic = false</code>, then pixels whose stencil exceeds the array   bounds are skipped.</li><li><code>c::Int</code>. Determines how many discrete categories to use for the Gaussian encoding.</li><li><code>skip_encoding</code>. If <code>skip_encoding == true</code>, <code>encoding</code> is ignored, and dispersion   patterns are computed directly from <code>x</code>, under the assumption that <code>L</code> is the alphabet   length for <code>x</code> (useful for categorical or integer data). Thus, if   <code>skip_encoding == true</code>, then <code>L</code> must also be specified. This is useful for   categorical or integer-valued data.</li><li><code>L</code>. If <code>L == nothing</code> (default), then the number of total outcomes is inferred from   <code>stencil</code> and <code>encoding</code>. If <code>L</code> is set to an integer, then the data is considered   pre-encoded and the number of total outcomes is set to <code>L</code>.</li></ul><p><strong>Outcome space</strong></p><p>The outcome space for <code>SpatialDispersion</code> is the unique delay vectors whose elements are the the symbols (integers) encoded by the Gaussian CDF. Hence, the outcome space is all <code>m</code>-dimensional delay vectors whose elements are all possible values in <code>1:c</code>. There are <code>c ^ m</code> such vectors.</p><p><strong>Description</strong></p><p>Estimating probabilities/entropies from higher-dimensional data is conceptually simple.</p><ol><li>Discretize each value (hypervoxel) in <code>x</code> relative to all other values <code>xᵢ ∈ x</code> using the  provided <code>encoding</code> scheme.</li><li>Use <code>stencil</code> to extract relevant (discretized) points around each hypervoxel.</li><li>Construct a symbol these points.</li><li>Take the sum-normalized histogram of the symbol as a probability distribution.</li><li>Optionally, compute <a href="../entropies/#Entropies.entropy"><code>entropy</code></a> or <a href="@ref"><code>entropy_normalized</code></a> from this  probability distribution.</li></ol><p><strong>Usage</strong></p><p>Here&#39;s how to compute spatial dispersion entropy using the three different ways of specifying stencils.</p><pre><code class="language-julia hljs">x = rand(50, 50) # first &quot;time slice&quot; of a spatial system evolution

# Cartesian stencil
stencil_cartesian = CartesianIndex.([(0,0), (1,0), (1,1), (0,1)])
est = SpatialDispersion(stencil_cartesian, x)
entropy_normalized(est, x)

# Extent/lag stencil
extent = (2, 2); lag = (1, 1); stencil_ext_lag = (extent, lag)
est = SpatialDispersion(stencil_ext_lag, x)
entropy_normalized(est, x)

# Matrix stencil
stencil_matrix = [1 1; 1 1]
est = SpatialDispersion(stencil_matrix, x)
entropy_normalized(est, x)</code></pre><p>To apply this to timeseries of spatial data, simply loop over the call (broadcast), e.g.:</p><pre><code class="language-julia hljs">imgs = [rand(50, 50) for i = 1:100]; # one image per second over 100 seconds
stencil = ((2, 2), (1, 1)) # a 2x2 stencil (i.e. dispersion patterns of length 4)
est = SpatialDispersion(stencil, first(imgs))
h_vs_t = entropy_normalized.(Ref(est), imgs)</code></pre><p>Computing generalized spatiotemporal dispersion entropy is trivial, e.g. with <a href="../entropies/#Entropies.Renyi"><code>Renyi</code></a>:</p><pre><code class="language-julia hljs">x = reshape(repeat(1:5, 500) .+ 0.1*rand(500*5), 50, 50)
est = SpatialDispersion(stencil, x)
entropy(Renyi(q = 2), est, x)</code></pre><p>See also: <a href="#Entropies.SpatialSymbolicPermutation"><code>SpatialSymbolicPermutation</code></a>, <a href="@ref"><code>GaussianCDFEncoding</code></a>, <a href="@ref"><code>symbolize</code></a>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaDynamics/Entropies.jl/blob/c24e9c9119474abb81242c1e1abb894ee7ba58ca/src/probabilities_estimators/spatial/spatial_dispersion/SpatialDispersion.jl#L5-L120">source</a></section></article><h2 id="Transfer-operator-(binning)"><a class="docs-heading-anchor" href="#Transfer-operator-(binning)">Transfer operator (binning)</a><a id="Transfer-operator-(binning)-1"></a><a class="docs-heading-anchor-permalink" href="#Transfer-operator-(binning)" title="Permalink"></a></h2><article class="docstring"><header><a class="docstring-binding" id="Entropies.TransferOperator" href="#Entropies.TransferOperator"><code>Entropies.TransferOperator</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">TransferOperator &lt;: &lt;ProbabilitiesEstimator
TransferOperator(b::RectangularBinning)</code></pre><p>A probability estimator based on binning data into rectangular boxes dictated by the given binning scheme <code>b</code>, then approximating the transfer (Perron-Frobenius) operator over the bins, then taking the invariant measure associated with that transfer operator as the bin probabilities. Assumes that the input data are sequential (time-ordered).</p><p>This implementation follows the grid estimator approach in Diego et al. (2019)<sup class="footnote-reference"><a id="citeref-Diego2019" href="#footnote-Diego2019">[Diego2019]</a></sup>.</p><p><strong>Outcome space</strong></p><p>The outcome space for <code>TransferOperator</code> is the set of unique bins constructed from <code>b</code>. Bins are identified by their left (lowest-value) corners, are given in data units, and are returned as <code>SVector</code>s.</p><p><strong>Bin ordering</strong></p><p>Bins returned by <a href="#Entropies.probabilities_and_outcomes"><code>probabilities_and_outcomes</code></a> are ordered according to first appearance (i.e. the first time the input (multivariate) timeseries visits the bin). Thus, if</p><pre><code class="language-julia hljs">b = RectangularBinning(4)
est = TransferOperator(b)
probs, outcomes = probabilities_and_outcomes(x, est) # x is some timeseries</code></pre><p>then <code>probs[i]</code> is the invariant measure (probability) of the bin <code>outcomes[i]</code>, which is the <code>i</code>-th bin visited by the timeseries with nonzero measure.</p><p><strong>Description</strong></p><p>The transfer operator <span>$P^{N}$</span>is computed as an <code>N</code>-by-<code>N</code> matrix of transition probabilities between the states defined by the partition elements, where <code>N</code> is the number of boxes in the partition that is visited by the orbit/points.</p><p>If  <span>$\{x_t^{(D)} \}_{n=1}^L$</span> are the <span>$L$</span> different <span>$D$</span>-dimensional points over which the transfer operator is approximated, <span>$\{ C_{k=1}^N \}$</span> are the <span>$N$</span> different partition elements (as dictated by <code>ϵ</code>) that gets visited by the points, and  <span>$\phi(x_t) = x_{t+1}$</span>, then</p><p class="math-container">\[P_{ij} = \dfrac
{\#\{ x_n | \phi(x_n) \in C_j \cap x_n \in C_i \}}
{\#\{ x_m | x_m \in C_i \}},\]</p><p>where <span>$\#$</span> denotes the cardinal. The element <span>$P_{ij}$</span> thus indicates how many points that are initially in box <span>$C_i$</span> end up in box <span>$C_j$</span> when the points in <span>$C_i$</span> are projected one step forward in time. Thus, the row <span>$P_{ik}^N$</span> where <span>$k \in \{1, 2, \ldots, N \}$</span> gives the probability of jumping from the state defined by box <span>$C_i$</span> to any of the other <span>$N$</span> states. It follows that <span>$\sum_{k=1}^{N} P_{ik} = 1$</span> for all <span>$i$</span>. Thus, <span>$P^N$</span> is a row/right stochastic matrix.</p><p><strong>Invariant measure estimation from transfer operator</strong></p><p>The left invariant distribution <span>$\mathbf{\rho}^N$</span> is a row vector, where <span>$\mathbf{\rho}^N P^{N} = \mathbf{\rho}^N$</span>. Hence, <span>$\mathbf{\rho}^N$</span> is a row eigenvector of the transfer matrix <span>$P^{N}$</span> associated with eigenvalue 1. The distribution <span>$\mathbf{\rho}^N$</span> approximates the invariant density of the system subject to <code>binning</code>, and can be taken as a probability distribution over the partition elements.</p><p>In practice, the invariant measure <span>$\mathbf{\rho}^N$</span> is computed using <a href="#Entropies.invariantmeasure"><code>invariantmeasure</code></a>, which also approximates the transfer matrix. The invariant distribution is initialized as a length-<code>N</code> random distribution which is then applied to <span>$P^{N}$</span>. The resulting length-<code>N</code> distribution is then applied to <span>$P^{N}$</span> again. This process repeats until the difference between the distributions over consecutive iterations is below some threshold.</p><p>See also: <a href="#Entropies.RectangularBinning"><code>RectangularBinning</code></a>, <a href="#Entropies.invariantmeasure"><code>invariantmeasure</code></a>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaDynamics/Entropies.jl/blob/c24e9c9119474abb81242c1e1abb894ee7ba58ca/src/probabilities_estimators/transfer_operator/transfer_operator.jl#L11-L89">source</a></section></article><h3 id="Utility-methods/types"><a class="docs-heading-anchor" href="#Utility-methods/types">Utility methods/types</a><a id="Utility-methods/types-1"></a><a class="docs-heading-anchor-permalink" href="#Utility-methods/types" title="Permalink"></a></h3><article class="docstring"><header><a class="docstring-binding" id="Entropies.InvariantMeasure" href="#Entropies.InvariantMeasure"><code>Entropies.InvariantMeasure</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">InvariantMeasure(to, ρ)</code></pre><p>Minimal return struct for <a href="#Entropies.invariantmeasure"><code>invariantmeasure</code></a> that contains the estimated invariant measure <code>ρ</code>, as well as the transfer operator <code>to</code> from which it is computed (including bin information).</p><p>See also: <a href="#Entropies.invariantmeasure"><code>invariantmeasure</code></a>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaDynamics/Entropies.jl/blob/c24e9c9119474abb81242c1e1abb894ee7ba58ca/src/probabilities_estimators/transfer_operator/transfer_operator.jl#L324-L332">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Entropies.invariantmeasure" href="#Entropies.invariantmeasure"><code>Entropies.invariantmeasure</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">invariantmeasure(x::AbstractDataset, binning::RectangularBinning) → iv::InvariantMeasure</code></pre><p>Estimate an invariant measure over the points in <code>x</code> based on binning the data into rectangular boxes dictated by the <code>binning</code>, then approximate the transfer (Perron-Frobenius) operator over the bins. From the approximation to the transfer operator, compute an invariant distribution over the bins. Assumes that the input data are sequential.</p><p>Details on the estimation procedure is found the <a href="#Entropies.TransferOperator"><code>TransferOperator</code></a> docstring.</p><p><strong>Example</strong></p><pre><code class="language-julia hljs">using DynamicalSystems, Plots, Entropies
D = 4
ds = Systems.lorenz96(D; F = 32.0)
N, dt = 20000, 0.1
orbit = trajectory(ds, N*dt; dt = dt, Ttr = 10.0)

# Estimate the invariant measure over some coarse graining of the orbit.
iv = invariantmeasure(orbit, RectangularBinning(15))

# Get the probabilities and bins
invariantmeasure(iv)</code></pre><p><strong>Probabilities and bin information</strong></p><pre><code class="nohighlight hljs">invariantmeasure(iv::InvariantMeasure) → (ρ::Probabilities, bins::Vector{&lt;:SVector})</code></pre><p>From a pre-computed invariant measure, return the probabilities and associated bins. The element <code>ρ[i]</code> is the probability of visitation to the box <code>bins[i]</code>. Analogous to <a href="@ref"><code>binhist</code></a>.</p><div class="admonition is-category-hint"><header class="admonition-header">Transfer operator approach vs. naive histogram approach</header><div class="admonition-body"><p>Why bother with the transfer operator instead of using regular histograms to obtain probabilities?</p><p>In fact, the naive histogram approach and the transfer operator approach are equivalent in the limit of long enough time series (as <span>$n \to \intfy$</span>), which is guaranteed by the ergodic theorem. There is a crucial difference, however:</p><p>The naive histogram approach only gives the long-term probabilities that orbits visit a certain region of the state space. The transfer operator encodes that information too, but comes with the added benefit of knowing the <em>transition probabilities</em> between states (see <a href="#Entropies.transfermatrix"><code>transfermatrix</code></a>).</p></div></div><p>See also: <a href="#Entropies.InvariantMeasure"><code>InvariantMeasure</code></a>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaDynamics/Entropies.jl/blob/c24e9c9119474abb81242c1e1abb894ee7ba58ca/src/probabilities_estimators/transfer_operator/transfer_operator.jl#L344-L395">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Entropies.transfermatrix" href="#Entropies.transfermatrix"><code>Entropies.transfermatrix</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">transfermatrix(iv::InvariantMeasure) → (M::AbstractArray{&lt;:Real, 2}, bins::Vector{&lt;:SVector})</code></pre><p>Return the transfer matrix/operator and corresponding bins. Here, <code>bins[i]</code> corresponds to the i-th row/column of the transfer matrix. Thus, the entry <code>M[i, j]</code> is the probability of jumping from the state defined by <code>bins[i]</code> to the state defined by <code>bins[j]</code>.</p><p>See also: <a href="#Entropies.TransferOperator"><code>TransferOperator</code></a>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaDynamics/Entropies.jl/blob/c24e9c9119474abb81242c1e1abb894ee7ba58ca/src/probabilities_estimators/transfer_operator/transfer_operator.jl#L467-L476">source</a></section></article><h2 id="Kernel-density"><a class="docs-heading-anchor" href="#Kernel-density">Kernel density</a><a id="Kernel-density-1"></a><a class="docs-heading-anchor-permalink" href="#Kernel-density" title="Permalink"></a></h2><article class="docstring"><header><a class="docstring-binding" id="Entropies.NaiveKernel" href="#Entropies.NaiveKernel"><code>Entropies.NaiveKernel</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">NaiveKernel(x, ϵ::Real; method = KDTree, w = 0, metric = Euclidean()) &lt;: ProbabilitiesEstimator</code></pre><p>Estimate probabilities/entropy using a &quot;naive&quot; kernel density estimation approach (KDE), as discussed in Prichard and Theiler (1995) <sup class="footnote-reference"><a id="citeref-PrichardTheiler1995" href="#footnote-PrichardTheiler1995">[PrichardTheiler1995]</a></sup>.</p><p>Probabilities <span>$P(\mathbf{x}, \epsilon)$</span> are assigned to every point <span>$\mathbf{x}$</span> by counting how many other points occupy the space spanned by a hypersphere of radius <code>ϵ</code> around <span>$\mathbf{x}$</span>, according to:</p><p class="math-container">\[P_i( X, \epsilon) \approx \dfrac{1}{N} \sum_{s} B(||X_i - X_j|| &lt; \epsilon),\]</p><p>where <span>$B$</span> gives 1 if the argument is <code>true</code>. Probabilities are then normalized.</p><p><strong>Keyword arguments</strong></p><ul><li><code>method = KDTree</code>: the search structure supported by Neighborhood.jl. Specifically, use <code>KDTree</code> to use a tree-based neighbor search, or <code>BruteForce</code> for the direct distances between all points. KDTrees heavily outperform direct distances when the dimensionality of the data is much smaller than the data length.</li><li><code>w = 0</code>: the Theiler window, which excludes indices <span>$s$</span> that are within <span>$|i - s| ≤ w$</span> from the given point <span>$x_i$</span>.</li><li><code>metric = Euclidean()</code>: the distance metric.</li></ul><p><strong>Outcome space</strong></p><p>The outcome space <code>Ω</code> for <code>NaiveKernel</code> are the indices of the input data, <code>eachindex(x)</code>. The reason to not return the data points themselves is because duplicate data points may not have same probabilities (due to having different neighbors).</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaDynamics/Entropies.jl/blob/c24e9c9119474abb81242c1e1abb894ee7ba58ca/src/probabilities_estimators/kernel/kernel_density.jl#L5-L38">source</a></section></article><h2 id="Timescales"><a class="docs-heading-anchor" href="#Timescales">Timescales</a><a id="Timescales-1"></a><a class="docs-heading-anchor-permalink" href="#Timescales" title="Permalink"></a></h2><article class="docstring"><header><a class="docstring-binding" id="Entropies.WaveletOverlap" href="#Entropies.WaveletOverlap"><code>Entropies.WaveletOverlap</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">WaveletOverlap(x [,wavelet]) &lt;: ProbabilitiesEstimator</code></pre><p>Apply the maximal overlap discrete wavelet transform (MODWT) to a signal, then compute probabilities as the (normalized) energies at different wavelet scales. These probabilities are used to compute the wavelet entropy, according to Rosso et al. (2001)<sup class="footnote-reference"><a id="citeref-Rosso2001" href="#footnote-Rosso2001">[Rosso2001]</a></sup>. Input timeseries <code>x</code> is needed for a well-defined outcome space.</p><p>By default the wavelet <code>Wavelets.WT.Daubechies{12}()</code> is used. Otherwise, you may choose a wavelet from the <code>Wavelets</code> package (it must subtype <code>OrthoWaveletClass</code>).</p><p><strong>Outcome space</strong></p><p>The outcome space for <code>WaveletOverlap</code> are the integers <code>1, 2, …, N</code> enumerating the wavelet scales. To obtain a better understanding of what these mean, we prepared a notebook you can <a href=" https:/github.com/kahaaga/waveletentropy_example/blob/main/wavelet_entropy_example.ipynb">view online</a>. As such, this estimator only works for timeseries input.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaDynamics/Entropies.jl/blob/c24e9c9119474abb81242c1e1abb894ee7ba58ca/src/probabilities_estimators/timescales/wavelet_overlap.jl#L4-L28">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Entropies.PowerSpectrum" href="#Entropies.PowerSpectrum"><code>Entropies.PowerSpectrum</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">PowerSpectrum(x_or_length(x)) &lt;: ProbabilitiesEstimator</code></pre><p>Calculate the power spectrum of a timeseries (amplitude square of its Fourier transform), and return the spectrum normalized to sum = 1 as probabilities. The Shannon entropy of these probabilities is typically referred in the literature as <em>spectral entropy</em>, e.g. <sup class="footnote-reference"><a id="citeref-Llanos2016" href="#footnote-Llanos2016">[Llanos2016]</a></sup>,<sup class="footnote-reference"><a id="citeref-Tian2017" href="#footnote-Tian2017">[Tian2017]</a></sup>.</p><p>The closer the spectrum is to flat, i.e., white noise, the higher the entropy. However, you can&#39;t compare entropies of timeseries with different length, because the binning in spectral space depends on the length of the input.</p><p><strong>Outcome space</strong></p><p>The outcome space <code>Ω</code> for <code>PowerSpectrum</code> is the set of frequencies in Fourier space. They should be multiplied with the sampling rate of the signal, which is assumed to be <code>1</code>. The length of the input is therefore required for this estimator to have a well-defined outcome space.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaDynamics/Entropies.jl/blob/c24e9c9119474abb81242c1e1abb894ee7ba58ca/src/probabilities_estimators/timescales/power_spectrum.jl#L4-L32">source</a></section></article><h2 id="Diversity"><a class="docs-heading-anchor" href="#Diversity">Diversity</a><a id="Diversity-1"></a><a class="docs-heading-anchor-permalink" href="#Diversity" title="Permalink"></a></h2><article class="docstring"><header><a class="docstring-binding" id="Entropies.Diversity" href="#Entropies.Diversity"><code>Entropies.Diversity</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">Diversity(; m::Int, τ::Int, nbins::Int)</code></pre><p>A <a href="#Entropies.ProbabilitiesEstimator"><code>ProbabilitiesEstimator</code></a> based on the cosine similarity. It can be used with <a href="../entropies/#Entropies.entropy"><code>entropy</code></a> to compute the diversity entropy of an input timeseries<sup class="footnote-reference"><a id="citeref-Wang2020" href="#footnote-Wang2020">[Wang2020]</a></sup>.</p><p>The implementation here allows for <code>τ != 1</code>, which was not considered in the original paper.</p><p><strong>Description</strong></p><p>Diversity probabilities are computed as follows.</p><ol><li>From the input time series <code>x</code>, using embedding lag <code>τ</code> and embedding dimension <code>m</code>,  construct the embedding  <span>$Y = \{\bf x_i \} = \{(x_{i}, x_{i+\tau}, x_{i+2\tau}, \ldots, x_{i+m\tau - 1}\}_{i = 1}^{N-mτ}$</span>.</li><li>Compute <span>$D = \{d(\bf x_t, \bf x_{t+1}) \}_{t=1}^{N-mτ-1}$</span>,  where <span>$d(\cdot, \cdot)$</span> is the cosine similarity between two <code>m</code>-dimensional  vectors in the embedding.</li><li>Divide the interval <code>[-1, 1]</code> into <code>nbins</code> equally sized subintervals.</li><li>Construct a histogram of cosine similarities <span>$d \in D$</span> over those subintervals.</li><li>Sum-normalize the histogram to obtain probabilities.</li></ol><p><strong>Outcome space</strong></p><p>The outcome space for <code>Diversity</code> is the bins of the <code>[-1, 1]</code> interval, and the return configuration is the same as in <a href="#Entropies.ValueHistogram"><code>ValueHistogram</code></a> (left bin edge).</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaDynamics/Entropies.jl/blob/c24e9c9119474abb81242c1e1abb894ee7ba58ca/src/probabilities_estimators/diversity/diversity.jl#L5-L36">source</a></section></article><section class="footnotes is-size-7"><ul><li class="footnote" id="footnote-BandtPompe2002"><a class="tag is-link" href="#citeref-BandtPompe2002">BandtPompe2002</a>Bandt, Christoph, and Bernd Pompe. &quot;Permutation entropy: a natural complexity measure for time series.&quot; Physical review letters 88.17 (2002): 174102.</li><li class="footnote" id="footnote-Zunino2017"><a class="tag is-link" href="#citeref-Zunino2017">Zunino2017</a>Zunino, L., Olivares, F., Scholkmann, F., &amp; Rosso, O. A. (2017). Permutation entropy based time series analysis: Equalities in the input signal can lead to false conclusions. Physics Letters A, 381(22), 1883-1892.</li><li class="footnote" id="footnote-He2016"><a class="tag is-link" href="#citeref-He2016">He2016</a>He, S., Sun, K., &amp; Wang, H. (2016). Multivariate permutation entropy and its application for complexity analysis of chaotic systems. Physica A: Statistical Mechanics and its Applications, 461, 812-823.</li><li class="footnote" id="footnote-Fadlallah2013"><a class="tag is-link" href="#citeref-Fadlallah2013">Fadlallah2013</a>Fadlallah, Bilal, et al. &quot;Weighted-permutation entropy: A complexity measure for time series incorporating amplitude information.&quot; Physical Review E 87.2 (2013): 022911.</li><li class="footnote" id="footnote-Azami2016"><a class="tag is-link" href="#citeref-Azami2016">Azami2016</a>Azami, H., &amp; Escudero, J. (2016). Amplitude-aware permutation entropy: Illustration in spike detection and signal segmentation. Computer methods and programs in biomedicine, 128, 40-51.</li><li class="footnote" id="footnote-Ribeiro2012"><a class="tag is-link" href="#citeref-Ribeiro2012">Ribeiro2012</a>Ribeiro et al. (2012). Complexity-entropy causality plane as a complexity measure for two-dimensional patterns. https://doi.org/10.1371/journal.pone.0040689</li><li class="footnote" id="footnote-Schlemmer2018"><a class="tag is-link" href="#citeref-Schlemmer2018">Schlemmer2018</a>Schlemmer et al. (2018). Spatiotemporal Permutation Entropy as a Measure for Complexity of Cardiac Arrhythmia. https://doi.org/10.3389/fphy.2018.00039</li><li class="footnote" id="footnote-Rostaghi2016"><a class="tag is-link" href="#citeref-Rostaghi2016">Rostaghi2016</a>Rostaghi, M., &amp; Azami, H. (2016). Dispersion entropy: A measure for time-series analysis. IEEE Signal Processing Letters, 23(5), 610-614.</li><li class="footnote" id="footnote-Li2018"><a class="tag is-link" href="#citeref-Li2018">Li2018</a>Li, G., Guan, Q., &amp; Yang, H. (2018). Noise reduction method of underwater acoustic signals based on CEEMDAN, effort-to-compress complexity, refined composite multiscale dispersion entropy and wavelet threshold denoising. Entropy, 21(1), 11.</li><li class="footnote" id="footnote-Azami2019"><a class="tag is-link" href="#citeref-Azami2019">Azami2019</a>Azami, H., da Silva, L. E. V., Omoto, A. C. M., &amp; Humeau-Heurtier, A. (2019). Two-dimensional dispersion entropy: An information-theoretic method for irregularity analysis of images. Signal Processing: Image Communication, 75, 178-187.</li><li class="footnote" id="footnote-Diego2019"><a class="tag is-link" href="#citeref-Diego2019">Diego2019</a>Diego, D., Haaga, K. A., &amp; Hannisdal, B. (2019). Transfer entropy computation using the Perron-Frobenius operator. Physical Review E, 99(4), 042212.</li><li class="footnote" id="footnote-PrichardTheiler1995"><a class="tag is-link" href="#citeref-PrichardTheiler1995">PrichardTheiler1995</a>Prichard, D., &amp; Theiler, J. (1995). Generalized redundancies for time series analysis. Physica D: Nonlinear Phenomena, 84(3-4), 476-493.</li><li class="footnote" id="footnote-Rosso2001"><a class="tag is-link" href="#citeref-Rosso2001">Rosso2001</a>Rosso et al. (2001). Wavelet entropy: a new tool for analysis of short duration brain electrical signals. Journal of neuroscience methods, 105(1), 65-75.</li><li class="footnote" id="footnote-Llanos2016"><a class="tag is-link" href="#citeref-Llanos2016">Llanos2016</a>Llanos et al., <em>Power spectral entropy as an information-theoretic correlate of manner of articulation in American English</em>, <a href="https://doi.org/10.1121/1.4976109">The Journal of the Acoustical Society of America 141, EL127 (2017)</a></li><li class="footnote" id="footnote-Tian2017"><a class="tag is-link" href="#citeref-Tian2017">Tian2017</a>Tian et al, <em>Spectral Entropy Can Predict Changes of Working Memory Performance Reduced by Short-Time Training in the Delayed-Match-to-Sample Task</em>, <a href="https://doi.org/10.3389/fnhum.2017.00437">Front. Hum. Neurosci.</a></li><li class="footnote" id="footnote-Wang2020"><a class="tag is-link" href="#citeref-Wang2020">Wang2020</a>Wang, X., Si, S., &amp; Li, Y. (2020). Multiscale diversity entropy: A novel dynamical measure for fault diagnosis of rotating machinery. IEEE Transactions on Industrial Informatics, 17(8), 5419-5429.</li></ul></section></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../">« Entropies.jl</a><a class="docs-footer-nextpage" href="../entropies/">Entropies »</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 0.27.23 on <span class="colophon-date" title="Wednesday 21 December 2022 21:27">Wednesday 21 December 2022</span>. Using Julia version 1.8.3.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
