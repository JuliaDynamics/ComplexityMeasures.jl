<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>References · ComplexityMeasures.jl</title><script data-outdated-warner src="../assets/warner.js"></script><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.045/juliamono.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.24/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL=".."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../assets/documenter.js"></script><script src="../siteinfo.js"></script><script src="../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../assets/themeswap.js"></script><link href="https://fonts.googleapis.com/css?family=Montserrat|Source+Code+Pro&amp;display=swap" rel="stylesheet" type="text/css"/></head><body><div id="documenter"><nav class="docs-sidebar"><div class="docs-package-name"><span class="docs-autofit"><a href="../">ComplexityMeasures.jl</a></span></div><form class="docs-search" action="../search/"><input class="docs-search-query" id="documenter-search-query" name="q" type="text" placeholder="Search docs"/></form><ul class="docs-menu"><li><a class="tocitem" href="../">ComplexityMeasures.jl</a></li><li><a class="tocitem" href="../tutorial/">Tutorial</a></li><li><a class="tocitem" href="../probabilities/">Probabilities</a></li><li><a class="tocitem" href="../information_measures/">Information measures (entropies and co.)</a></li><li><a class="tocitem" href="../complexity/">Complexity measures</a></li><li><a class="tocitem" href="../convenience/">Convenience functions</a></li><li><a class="tocitem" href="../examples/">ComplexityMeasures.jl Examples</a></li><li><a class="tocitem" href="../devdocs/">ComplexityMeasures.jl Dev Docs</a></li><li class="is-active"><a class="tocitem" href>References</a></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><nav class="breadcrumb"><ul class="is-hidden-mobile"><li class="is-active"><a href>References</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>References</a></li></ul></nav><div class="docs-right"><a class="docs-edit-link" href="https://github.com/JuliaDynamics/ComplexityMeasures.jl/blob/main/docs/src/references.md" title="Edit on GitHub"><span class="docs-icon fab"></span><span class="docs-label is-hidden-touch">Edit on GitHub</span></a><a class="docs-settings-button fas fa-cog" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-sidebar-button fa fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a></div></header><article class="content" id="documenter-page"><h1 id="References"><a class="docs-heading-anchor" href="#References">References</a><a id="References-1"></a><a class="docs-heading-anchor-permalink" href="#References" title="Permalink"></a></h1><div class="citation canonical"><ul><li>
<div id="Alizadeh2010">Alizadeh, N. H. and Arghami, N. R. (2010). <a href='http://jirss.irstat.ir/article-1-81-en.pdf'><i>A new estimator of entropy</i></a>. Journal of the Iranian Statistical Society (JIRSS).</div>
</li><li>
<div id="Amigó2018">Amigó, J. M.; Balogh, S. G. and Hernández, S. (2018). <i>A brief review of generalized entropies</i>. Entropy <b>20</b>, 813.</div>
</li><li>
<div id="Amigó2004">Amigó, J. M.; Szczepański, J.; Wajnryb, E. and Sanchez-Vives, M. V. (2004). <a href='https://doi.org/10.1162/089976604322860677'><i>Estimating the Entropy Rate of Spike Trains via Lempel-Ziv Complexity</i></a>. <a href='https://doi.org/https://doi.org/10.1162/089976604322860677'>Neural Computation <b>16</b>, 717-736</a>, <a href='https://arxiv.org/abs/https://direct.mit.edu/neco/article-pdf/16/4/717/815838/089976604322860677.pdf'>arXiv:https://direct.mit.edu/neco/article-pdf/16/4/717/815838/089976604322860677.pdf</a>.</div>
</li><li>
<div id="Anteneodo1999">Anteneodo, C. and Plastino, A. R. (1999). <a href='https://dx.doi.org/10.1088/0305-4470/32/7/002'><i>Maximum entropy approach to stretched exponential probability distributions</i></a>. <a href='https://doi.org/https://doi.org/10.1088/0305-4470/32/7/002'>Journal of Physics A: Mathematical and General <b>32</b>, 1089</a>.</div>
</li><li>
<div id="Arora2022">Arora, A.; Meister, C. and Cotterell, R. (2022). <a href='https://arxiv.org/abs/2204.01469'><i>Estimating the Entropy of Linguistic Distributions</i></a>, <a href='https://arxiv.org/abs/2204.01469'>arXiv:2204.01469 [cs.CL]</a>.</div>
</li><li>
<div id="Azami2016">Azami, H. and Escudero, J. (2016). <a href='https://www.sciencedirect.com/science/article/pii/S0169260715301152'><i>Amplitude-aware permutation entropy: Illustration in spike detection and signal segmentation</i></a>. <a href='https://doi.org/https://doi.org/10.1016/j.cmpb.2016.02.008'>Computer Methods and Programs in Biomedicine <b>128</b>, 40-51</a>.</div>
</li><li>
<div id="Azami2017">Azami, H.; Rostaghi, M.; Abásolo, D. and Escudero, J. (2017). <i>Refined Composite Multiscale Dispersion Entropy and its Application to Biomedical Signals</i>. <a href='https://doi.org/https://doi.org/10.1109/TBME.2017.2679136'>IEEE Transactions on Biomedical Engineering <b>64</b>, 2872-2879</a>.</div>
</li><li>
<div id="Azami2019">Azami, H.; da Silva, L. E.; Omoto, A. C. and Humeau-Heurtier, A. (2019). <a href='https://www.sciencedirect.com/science/article/pii/S0923596519300682'><i>Two-dimensional dispersion entropy: An information-theoretic method for irregularity analysis of images</i></a>. <a href='https://doi.org/https://doi.org/10.1016/j.image.2019.04.013'>Signal Processing: Image Communication <b>75</b>, 178-187</a>.</div>
</li><li>
<div id="BandtPompe2002">Bandt, C. and Pompe, B. (2002). <a href='https://link.aps.org/doi/10.1103/PhysRevLett.88.174102'><i>Permutation Entropy: A Natural Complexity Measure for Time Series</i></a>. <a href='https://doi.org/https://doi.org/10.1103/PhysRevLett.88.174102'>Phys. Rev. Lett. <b>88</b>, 174102</a>.</div>
</li><li>
<div id="Berger2019">Berger, S.; Kravtsiv, A.; Schneider, G. and Jordan, D. (2019). <a href='https://www.mdpi.com/1099-4300/21/10/1023'><i>Teaching Ordinal Patterns to a Computer: Efficient Encoding Algorithms Based on the Lehmer Code</i></a>. <a href='https://doi.org/https://doi.org/10.3390/e21101023'>Entropy <b>21</b></a>.</div>
</li><li>
<div id="Chao2003">Chao, A. and Shen, T.-J. (2003). <a href='https://doi.org/10.1023/A:1026096204727'><i>Nonparametric estimation of Shannon's index of diversity when there are unseen species in sample</i></a>. <a href='https://doi.org/https://doi.org/10.1023/A:1026096204727'>Environmental and Ecological Statistics <b>10</b>, 429–443</a>.</div>
</li><li>
<div id="Charzyńska2015">Charzyńska, A. and Gambin, A. (2016). <a href='https://www.mdpi.com/1099-4300/18/1/13'><i>Improvement of the k-nn Entropy Estimator with Applications in Systems Biology</i></a>. <a href='https://doi.org/10.3390/e18010013'>Entropy <b>18</b></a>.</div>
</li><li>
<div id="Correa1995">Correa, J. C. (1995). <a href='https://doi.org/10.1080/03610929508831626'><i>A new estimator of entropy</i></a>. <a href='https://doi.org/https://doi.org/10.1080/03610929508831626'>Communications in Statistics - Theory and Methods <b>24</b>, 2439-2449</a>, <a href='https://arxiv.org/abs/https://doi.org/10.1080/03610929508831626'>arXiv:https://doi.org/10.1080/03610929508831626</a>.</div>
</li><li>
<div id="Costa2002">Costa, M.; Goldberger, A. L. and Peng, C.-K. (2002). <a href='https://link.aps.org/doi/10.1103/PhysRevLett.89.068102'><i>Multiscale Entropy Analysis of Complex Physiologic Time Series</i></a>. <a href='https://doi.org/https://doi.org/10.1103/PhysRevLett.89.068102'>Phys. Rev. Lett. <b>89</b>, 068102</a>.</div>
</li><li>
<div id="Costa2015">Costa, M. D. and Goldberger, A. L. (2015). <a href='https://www.mdpi.com/1099-4300/17/3/1197'><i>Generalized Multiscale Entropy Analysis: Application to Quantifying the Complex Volatility of Human Heartbeat Time Series</i></a>. <a href='https://doi.org/10.3390/e17031197'>Entropy <b>17</b>, 1197–1203</a>.</div>
</li><li>
<div id="Curado2004">Curado, E. M. and Nobre, F. D. (2004). <a href='https://www.sciencedirect.com/science/article/pii/S0378437103011889'><i>On the stability of analytic entropic forms</i></a>. <a href='https://doi.org/https://doi.org/10.1016/j.physa.2003.12.026'>Physica A: Statistical Mechanics and its Applications <b>335</b>, 94-106</a>.</div>
</li><li>
<div id="Datseris2022">Datseris, G. and Parlitz, U. (2022). <a href='https://link.springer.com/book/10.1007/978-3-030-91032-7'><i>Nonlinear dynamics: a concise introduction interlaced with code</i></a>. <a href='https://doi.org/https://doi.org/10.1007/978-3-030-91032-7'>Springer Nature</a>.</div>
</li><li>
<div id="Diego2019">Diego, D.; Haaga, K. A. and Hannisdal, B. (2019). <a href='https://link.aps.org/doi/10.1103/PhysRevE.99.042212'><i>Transfer entropy computation using the Perron-Frobenius operator</i></a>. <a href='https://doi.org/https://doi.org/10.1103/PhysRevE.99.042212'>Phys. Rev. E <b>99</b>, 042212</a>.</div>
</li><li>
<div id="Ebrahimi1994">Ebrahimi, N.; Pflughoeft, K. and Soofi, E. S. (1994). <a href='https://www.sciencedirect.com/science/article/pii/0167715294900469'><i>Two measures of sample entropy</i></a>. <a href='https://doi.org/https://doi.org/10.1016/0167-7152(94)90046-9'>Statistics & Probability Letters <b>20</b>, 225-234</a>.</div>
</li><li>
<div id="Fadlallah2013">Fadlallah, B.; Chen, B.; Keil, A. and Prı́ncipe, J. (2013). <a href='https://link.aps.org/doi/10.1103/PhysRevE.87.022911'><i>Weighted-permutation entropy: A complexity measure for time series incorporating amplitude information</i></a>. <a href='https://doi.org/https://doi.org/10.1103/PhysRevE.87.022911'>Phys. Rev. E <b>87</b>, 022911</a>.</div>
</li><li>
<div id="Gao2015">Gao, S.; Ver Steeg, G. and Galstyan, A. (2015). <a href='https://proceedings.mlr.press/v38/gao15.html'><i>Efficient Estimation of Mutual Information for Strongly Dependent Variables</i></a>. In: Proceedings of the Eighteenth International Conference on Artificial Intelligence and Statistics, Proceedings of Machine Learning Research, 277–286, San Diego, California, USA, PMLR.</div>
</li><li>
<div id="Goria2005">Goria, M. N.; Leonenko, N. N.; Mergel, V. V. and Inverardi, P. L. (2005). <a href='https://doi.org/10.1080/104852504200026815'><i>A new class of random vector entropy estimators and its applications in testing statistical hypotheses</i></a>. <a href='https://doi.org/https://doi.org/10.1080/104852504200026815'>Journal of Nonparametric Statistics <b>17</b>, 277-297</a>, <a href='https://arxiv.org/abs/https://doi.org/10.1080/104852504200026815'>arXiv:https://doi.org/10.1080/104852504200026815</a>.</div>
</li><li>
<div id="Grassberger2022">Grassberger, P. (2022). <a href='https://www.mdpi.com/1099-4300/24/5/680'><i>On Generalized Schürmann Entropy Estimators</i></a>. <a href='https://doi.org/10.3390/e24050680'>Entropy <b>24</b></a>.</div>
</li><li>
<div id="Hausser2009">Hausser, J. and Strimmer, K. (2009). <a href='https://jmlr.csail.mit.edu/papers/volume10/hausser09a/hausser09a.pdf'><i>Entropy inference and the James-Stein estimator, with application to nonlinear gene association networks.</i></a> Journal of Machine Learning Research <b>10</b>.</div>
</li><li>
<div id="He2016">He, S.; Sun, K. and Wang, H. (2016). <a href='https://www.sciencedirect.com/science/article/pii/S0378437116302801'><i>Multivariate permutation entropy and its application for complexity analysis of chaotic systems</i></a>. <a href='https://doi.org/https://doi.org/10.1016/j.physa.2016.06.012'>Physica A: Statistical Mechanics and its Applications <b>461</b>, 812-823</a>.</div>
</li><li>
<div id="Horvitz1952">Horvitz, D. G. and Thompson, D. J. (1952). <a href='https://www.tandfonline.com/doi/abs/10.1080/01621459.1952.10483446'><i>A Generalization of Sampling Without Replacement from a Finite Universe</i></a>. <a href='https://doi.org/https://doi.org/10.1080/01621459.1952.10483446'>Journal of the American Statistical Association <b>47</b>, 663-685</a>, <a href='https://arxiv.org/abs/https://www.tandfonline.com/doi/pdf/10.1080/01621459.1952.10483446'>arXiv:https://www.tandfonline.com/doi/pdf/10.1080/01621459.1952.10483446</a>.</div>
</li><li>
<div id="JamesStein1992">James, W. and Stein, C. (1992). <a href='https://link.springer.com/chapter/10.1007/978-1-4612-0919-5_30'><i>Estimation with quadratic loss</i></a>. In: Breakthroughs in statistics: Foundations and basic theory, editors, 443–460. Springer.</div>
</li><li>
<div id="KozachenkoLeonenko1987">Kozachenko, L. F. and Leonenko, N. N. (1987). <a href='https://www.mathnet.ru/php/archive.phtml?wshow=paper&jrnid=ppi&paperid=797&option_lang=eng'><i>Sample estimate of the entropy of a random vector</i></a>. Problemy Peredachi Informatsii <b>23</b>, 9–16.</div>
</li><li>
<div id="Kraskov2004">Kraskov, A.; Stögbauer, H. and Grassberger, P. (2004). <a href='https://link.aps.org/doi/10.1103/PhysRevE.69.066138'><i>Estimating mutual information</i></a>. <a href='https://doi.org/https://doi.org/10.1103/PhysRevE.69.066138'>Phys. Rev. E <b>69</b>, 066138</a>.</div>
</li><li>
<div id="Lad2015">Lad, F.; Sanfilippo, G. and Agrò, G. (2015). <a href='https://doi.org/10.1214/14-STS430'><i>Extropy: Complementary Dual of Entropy</i></a>. <a href='https://doi.org/https://doi.org/10.1214/14-STS430'>Statistical Science <b>30</b>, 40 – 58</a>.</div>
</li><li>
<div id="LempelZiv1976">Lempel, A. and Ziv, J. (1976). <i>On the Complexity of Finite Sequences</i>. <a href='https://doi.org/https://doi.org/10.1109/TIT.1976.1055501'>IEEE Transactions on Information Theory <b>22</b>, 75-81</a>.</div>
</li><li>
<div id="LeonenkoProzantoSavani2008">Leonenko, N.; Pronzato, L. and Savani, V. (2008). <a href='https://doi.org/10.1214/07-AOS539'><i>A class of Rényi information estimators for multidimensional densities</i></a>. <a href='https://doi.org/https://doi.org/10.1214/07-AOS539'>The Annals of Statistics <b>36</b>, 2153 – 2182</a>.</div>
</li><li>
<div id="Li2018">Li, G.; Guan, Q. and Yang, H. (2019). <a href='https://www.mdpi.com/1099-4300/21/1/11'><i>Noise Reduction Method of Underwater Acoustic Signals Based on CEEMDAN, Effort-To-Compress Complexity, Refined Composite Multiscale Dispersion Entropy and Wavelet Threshold Denoising</i></a>. <a href='https://doi.org/10.3390/e21010011'>Entropy <b>21</b></a>.</div>
</li><li>
<div id="Li2019">Li, Y.; Gao, X. and Wang, L. (2019). <a href='https://www.mdpi.com/1424-8220/19/23/5203'><i>Reverse Dispersion Entropy: A New Complexity Measure for Sensor Signal</i></a>. <a href='https://doi.org/10.3390/s19235203'>Sensors <b>19</b></a>.</div>
</li><li>
<div id="Liu2023">Liu, J. and Xiao, F. (2023). <i>Renyi extropy</i>. <a href='https://doi.org/https://doi.org/10.1080/03610926.2021.2020843'>Communications in Statistics, Theory and Methods <b>52</b>, 5836-5847</a>.</div>
</li><li>
<div id="Llanos2017">Llanos, F.; Alexander, J. M.; Stilp, C. E. and Kluender, K. R. (2017). <a href='https://pubmed.ncbi.nlm.nih.gov/28253693/'><i>Power spectral entropy as an information-theoretic correlate of manner of articulation in American English</i></a>. <a href='https://doi.org/https://doi.org/10.1121/1.4976109'>The Journal of the Acoustical Society of America <b>141</b>, EL127–EL133</a>.</div>
</li><li>
<div id="Lord2018">Lord, W. M.; Sun, J. and Bollt, E. M. (2018). <a href='https://pubs.aip.org/aip/cha/article/28/3/033114/685022'><i>Geometric k-nearest neighbor estimation of entropy and mutual information</i></a>. <a href='https://doi.org/https://doi.org/10.1063/1.5011683'>Chaos: An Interdisciplinary Journal of Nonlinear Science <b>28</b></a>.</div>
</li><li>
<div id="Miller1955">Miller, G. (1955). <i>Note on the bias of information estimates</i>. Information theory in psychology: Problems and methods.</div>
</li><li>
<div id="Paninski2003">Paninski, L. (2003). <a href='https://ieeexplore.ieee.org/abstract/document/6790247'><i>Estimation of entropy and mutual information</i></a>. <a href='https://doi.org/https://doi.org/10.1162/089976603321780272'>Neural computation <b>15</b>, 1191–1253</a>.</div>
</li><li>
<div id="Pincus1991">Pincus, S. M. (1991). <i>Approximate entropy as a measure of system complexity.</i> <a href='https://doi.org/https://doi.org/10.1073/pnas.88.6.2297'>Proceedings of the National Academy of Sciences <b>88</b>, 2297–2301</a>.</div>
</li><li>
<div id="PrichardTheiler1995">Prichard, D. and Theiler, J. (1995). <i>Generalized redundancies for time series analysis</i>. <a href='https://doi.org/https://doi.org/10.1016/0167-2789(95)00041-2'>Physica D: Nonlinear Phenomena <b>84</b>, 476–493</a>.</div>
</li><li>
<div id="Ribeiro2012">Ribeiro, H. V.; Zunino, L.; Lenzi, E. K.; Santoro, P. A. and Mendes, R. S. (2012). <a href='https://doi.org/10.1371/journal.pone.0040689'><i>Complexity-Entropy Causality Plane as a Complexity Measure for Two-Dimensional Patterns</i></a>. <a href='https://doi.org/https://doi.org/10.1371/journal.pone.0040689'>PLOS ONE <b>7</b>, 1-9</a>.</div>
</li><li>
<div id="Richman2000">Richman, J. S. and Moorman, J. R. (2000). <i>Physiological time-series analysis using approximate entropy and sample entropy</i>. <a href='https://doi.org/https://doi.org/10.1152/ajpheart.2000.278.6.H2039'>American journal of physiology-heart and circulatory physiology <b>278</b>, H2039–H2049</a>.</div>
</li><li>
<div id="Rosso2001">Rosso, O. A.; Blanco, S.; Yordanova, J.; Kolev, V.; Figliola, A.; Schürmann, M. and Başar, E. (2001). <a href='https://www.sciencedirect.com/science/article/pii/S0165027000003563'><i>Wavelet entropy: a new tool for analysis of short duration brain electrical signals</i></a>. <a href='https://doi.org/https://doi.org/10.1016/S0165-0270(00)00356-3'>Journal of Neuroscience Methods <b>105</b>, 65-75</a>.</div>
</li><li>
<div id="Rosso2007">Rosso, O. A.; Larrondo, H.; Martin, M. T.; Plastino, A. and Fuentes, M. A. (2007). <i>Distinguishing noise from chaos</i>. <a href='https://doi.org/https://doi.org/10.1103/PhysRevLett.99.154102'>Physical review letters <b>99</b>, 154102</a>.</div>
</li><li>
<div id="Rosso2013">Rosso, O. A.; Martín, M.; Larrondo, H. A.; Kowalski, A. and Plastino, A. (2013). <a href='https://www.researchgate.net/profile/Osvaldo-Rosso-3/publication/260145202_Generalized_Statistical_Complexity_a_new_tool_for_dynamical_systems/links/02e7e52fbe7122d461000000/Generalized-Statistical-Complexity-a-new-tool-for-dynamical-systems.pdf'><i>Generalized statistical complexity: A new tool for dynamical systems</i></a>. Concepts and recent advances in generalized information measures and statistics, 169–215.</div>
</li><li>
<div id="Rostaghi2016">Rostaghi, M. and Azami, H. (2016). <i>Dispersion entropy: A measure for time-series analysis</i>. <a href='https://doi.org/https://doi.org/10.1109/LSP.2016.2542881'>IEEE Signal Processing Letters <b>23</b>, 610–614</a>.</div>
</li><li>
<div id="Rényi1961">Rényi, A. (1961). <a href='https://projecteuclid.org/ebook/Download?urlid=bsmsp/1200512181&isFullBook=false'><i>On measures of entropy and information</i></a>. In: Proceedings of the Fourth Berkeley Symposium on Mathematical Statistics and Probability, Volume 1: Contributions to the Theory of Statistics, 547–562.</div>
</li><li>
<div id="Schlemmer2018">Schlemmer, A.; Berg, S.; Lilienkamp, T.; Luther, S. and Parlitz, U. (2018). <i>Spatiotemporal permutation entropy as a measure for complexity of cardiac arrhythmia</i>. <a href='https://doi.org/https://doi.org/10.3389/fphy.2018.00039'>Frontiers in Physics <b>6</b>, 39</a>.</div>
</li><li>
<div id="Schurmann2004">Schürmann, T. (2004). <i>Bias analysis in entropy estimation</i>. <a href='https://doi.org/https://doi.org/10.1088/0305-4470/37/27/L02'>Journal of Physics A: Mathematical and General <b>37</b>, L295</a>.</div>
</li><li>
<div id="Shannon1948">Shannon, C. E. (1948). <i>A mathematical theory of communication</i>. <a href='https://doi.org/https://doi.org/10.1002/j.1538-7305.1948.tb01338.x'>The Bell system technical journal <b>27</b>, 379–423</a>.</div>
</li><li>
<div id="Singh2003">Singh, H.; Misra, N.; Hnizdo, V.; Fedorowicz, A. and Demchuk, E. (2003). <i>Nearest neighbor estimates of entropy</i>. <a href='https://doi.org/https://doi.org/10.1080/01966324.2003.10737616'>American journal of mathematical and management sciences <b>23</b>, 301–321</a>.</div>
</li><li>
<div id="Sippel2016">Sippel, S.; Lange, H. and Gans, F. (2016), <a href='https://cran.r-project.org/web/packages/statcomp/index.html'><i>statcomp: Statistical Complexity and Information measures for time series analysis</i></a>. R package version.</div>
</li><li>
<div id="Tian2017">Tian, Y.; Zhang, H.; Xu, W.; Zhang, H.; Yang, L.; Zheng, S. and Shi, Y. (2017). <i>Spectral entropy can predict changes of working memory performance reduced by short-time training in the delayed-match-to-sample task</i>. <a href='https://doi.org/https://doi.org/10.3389/fnhum.2017.00437'>Frontiers in human neuroscience <b>11</b>, 437</a>.</div>
</li><li>
<div id="Tsallis1988">Tsallis, C. (1988). <i>Possible generalization of Boltzmann-Gibbs statistics</i>. <a href='https://doi.org/https://doi.org/10.1007/BF01016429'>Journal of statistical physics <b>52</b>, 479–487</a>.</div>
</li><li>
<div id="Tsallis2009">Tsallis, C. (2009). <a href='https://link.springer.com/book/10.1007/978-0-387-85359-8'><i>Introduction to nonextensive statistical mechanics: approaching a complex world</i></a>. Springer.</div>
</li><li>
<div id="Vasicek1976">Vasicek, O. (1976). <i>A test for normality based on sample entropy</i>. <a href='https://doi.org/https://doi.org/10.1111/j.2517-6161.1976.tb01566.x'>Journal of the Royal Statistical Society Series B: Statistical Methodology <b>38</b>, 54–59</a>.</div>
</li><li>
<div id="Wang2020">Wang, X.; Si, S. and Li, Y. (2020). <i>Multiscale diversity entropy: A novel dynamical measure for fault diagnosis of rotating machinery</i>. <a href='https://doi.org/https://doi.org/10.1109/TII.2020.3022369'>IEEE Transactions on Industrial Informatics <b>17</b>, 5419–5429</a>.</div>
</li><li>
<div id="Wu2013">Wu, S.-D.; Wu, C.-W.; Lin, S.-G.; Wang, C.-C. and Lee, K.-Y. (2013). <i>Time series analysis using composite multiscale entropy</i>. <a href='https://doi.org/https://doi.org/10.3390/e15031069'>Entropy <b>15</b>, 1069–1084</a>.</div>
</li><li>
<div id="Xue2023">Xue, Y. and Deng, Y. (2023). <i>Tsallis extropy</i>. <a href='https://doi.org/https://doi.org/10.1080/03610926.2021.1921804'>Communications in Statistics-Theory and Methods <b>52</b>, 751–762</a>.</div>
</li><li>
<div id="Zahl1977">Zahl, S. (1977). <i>Jackknifing an index of diversity</i>. <a href='https://doi.org/https://doi.org/10.2307/1936227'>Ecology <b>58</b>, 907–913</a>.</div>
</li><li>
<div id="Zhou2023">Zhou, Q.; Shang, P. and Zhang, B. (2023). <i>Using missing dispersion patterns to detect determinism and nonlinearity in time series data</i>. <a href='https://doi.org/https://doi.org/10.1007/s11071-022-07835-3'>Nonlinear Dynamics <b>111</b>, 439–458</a>.</div>
</li><li>
<div id="Zhu2015">Zhu, J.; Bellanger, J.-J.; Shu, H. and Le Bouquin Jeannès, R. (2015). <i>Contribution to transfer entropy estimation via the k-nearest-neighbors approach</i>. <a href='https://doi.org/https://doi.org/10.3390/e17064173'>Entropy <b>17</b>, 4173–4201</a>.</div>
</li><li>
<div id="Zunino2017">Zunino, L.; Olivares, F.; Scholkmann, F. and Rosso, O. A. (2017). <i>Permutation entropy based time series analysis: Equalities in the input signal can lead to false conclusions</i>. <a href='https://doi.org/https://doi.org/10.1016/j.physleta.2017.03.052'>Physics Letters A <b>381</b>, 1883–1892</a>.</div>
</li>
</ul></div></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../devdocs/">« ComplexityMeasures.jl Dev Docs</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 0.27.25 on <span class="colophon-date" title="Friday 20 October 2023 09:55">Friday 20 October 2023</span>. Using Julia version 1.9.3.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
