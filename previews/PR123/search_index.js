var documenterSearchIndex = {"docs":
[{"location":"entropies/#Entropy-API","page":"Entropies","title":"Entropy API","text":"","category":"section"},{"location":"entropies/","page":"Entropies","title":"Entropies","text":"entropy\nentropy!","category":"page"},{"location":"entropies/#Entropies.entropy","page":"Entropies","title":"Entropies.entropy","text":"entropy([e::Entropy,] x, est::ProbabilitiesEstimator) → h::Real\nentropy([e::Entropy,] probs::Probabilities) → h::Real\n\nCompute a (generalized) entropy h from x according to the specified entropy type e and the given probability estimator est.\n\nAlternatively compute the entropy directly from the existing probabilities probs. In fact, the first method is a 2-lines-of-code wrapper that calls probabilities and gives the result to the second method.\n\nx is typically an Array or a Dataset, see Input data for Entropies.jl.\n\nThe entropy types that support this interface are \"direct\" entropies. They always yield an entropy value given a probability distribution. Such entropies are theoretically well-founded and are typically called \"generalized entropies\". Currently implemented types are:\n\nRenyi.\nTsallis.\nShannon, which is a subcase of the above two in the limit q → 1.\nCurado.\nStretchedExponential.\n\nThe entropy (first argument) is optional: if not given, Shannon() is used instead.\n\nThese entropies also have a well defined maximum value for a given probability estimator. To obtain this value one only needs to call the maximum function with the chosen entropy type and probability estimator. Or, one can use entropy_normalized to obtain the normalized form of the entropy (divided by the maximum).\n\nExamples\n\nx = [rand(Bool) for _ in 1:10000] # coin toss\nps = probabilities(x) # gives about [0.5, 0.5] by definition\nh = entropy(ps) # gives 1, about 1 bit by definition\nh = entropy(Shannon(), ps) # syntactically equivalent to above\nh = entropy(Shannon(), x, CountOccurrences()) # syntactically equivalent to above\nh = entropy(x, SymbolicPermutation(;m=3)) # gives about 2, again by definition\nh = entropy(Renyi(2.0), ps) # also gives 1, order `q` doesn't matter for coin toss\n\n\n\n\n\nentropy(e::IndirectEntropy, x) → h::Real\n\nCompute the entropy of x, here named h, according to the specified indirect entropy estimator e.\n\nIn contrast to the \"typical\" way one obtains entropies in the above methods, indirect entropy estimators compute Shannon entropies via alternate means, without explicitly computing probability distributions. The available indirect entropies are:\n\nKraskov.\nKozachenkoLeonenko.\n\n\n\n\n\n","category":"function"},{"location":"entropies/#Entropies.entropy!","page":"Entropies","title":"Entropies.entropy!","text":"entropy!(s, [e::Entropy,] x, est::ProbabilitiesEstimator)\n\nSimilar to probabilities!, this is an in-place version of entropy that allows pre-allocation of temporarily used containers.\n\nThe entropy (second argument) is optional: if not given, Shannon() is used instead.\n\nOnly works for certain estimators. See for example SymbolicPermutation.\n\n\n\n\n\n","category":"function"},{"location":"entropies/#Rényi-(generalized)-entropy","page":"Entropies","title":"Rényi (generalized) entropy","text":"","category":"section"},{"location":"entropies/","page":"Entropies","title":"Entropies","text":"Renyi","category":"page"},{"location":"entropies/#Entropies.Renyi","page":"Entropies","title":"Entropies.Renyi","text":"Renyi <: Entropy\nRenyi(q, base = 2)\nRenyi(; q = 1.0, base = 2)\n\nThe Rényi[Rényi1960] generalized order-q entropy, used with entropy to compute an entropy with units given by base (typically 2 or MathConstants.e).\n\nDescription\n\nLet p be an array of probabilities (summing to 1). Then the Rényi generalized entropy is\n\nH_q(p) = frac11-q log left(sum_i pi^qright)\n\nand generalizes other known entropies, like e.g. the information entropy (q = 1, see [Shannon1948]), the maximum entropy (q=0, also known as Hartley entropy), or the correlation entropy (q = 2, also known as collision entropy).\n\nIf the probability estimator has known alphabet length L, then the maximum value of the Rényi entropy is log_base(L), which is the entropy of the uniform distribution with given alphabet length.\n\n[Rényi1960]: A. Rényi, Proceedings of the fourth Berkeley Symposium on Mathematics, Statistics and Probability, pp 547 (1960)\n\n[Shannon1948]: C. E. Shannon, Bell Systems Technical Journal 27, pp 379 (1948)\n\n\n\n\n\n","category":"type"},{"location":"entropies/#Tsallis-(generalized)-entropy","page":"Entropies","title":"Tsallis (generalized) entropy","text":"","category":"section"},{"location":"entropies/","page":"Entropies","title":"Entropies","text":"Tsallis","category":"page"},{"location":"entropies/#Entropies.Tsallis","page":"Entropies","title":"Entropies.Tsallis","text":"Tsallis <: Entropy\nTsallis(q; k = 1.0, base = 2)\nTsallis(; q = 1.0, k = 1.0, base = 2)\n\nThe Tsallis[Tsallis1988] generalized order-q entropy, used with entropy to compute an entropy.\n\nbase only applies in the limiting case q == 1, in which the Tsallis entropy reduces to Shannon entropy.\n\nDescription\n\nThe Tsallis entropy is a generalization of the Boltzmann-Gibbs entropy, with k standing for the Boltzmann constant. It is defined as\n\nS_q(p) = frackq - 1left(1 - sum_i pi^qright)\n\nIf the probability estimator has known alphabet length L, then the maximum value of the Tsallis entropy is k(L^1 - q - 1)(1 - q).\n\n[Tsallis1988]: Tsallis, C. (1988). Possible generalization of Boltzmann-Gibbs statistics. Journal of statistical physics, 52(1), 479-487.\n\n\n\n\n\n","category":"type"},{"location":"entropies/#Shannon-entropy-(convenience)","page":"Entropies","title":"Shannon entropy (convenience)","text":"","category":"section"},{"location":"entropies/","page":"Entropies","title":"Entropies","text":"Shannon","category":"page"},{"location":"entropies/#Entropies.Shannon","page":"Entropies","title":"Entropies.Shannon","text":"Shannon(; base = 2)\n\nThe Shannon[Shannon1948] entropy, used with entropy to compute:\n\nH(p) = - sum_i pi log(pi)\n\nwith the log at the given base.\n\nShannon(base) is syntactically equivalent to Renyi(; base).\n\n[Shannon1948]: C. E. Shannon, Bell Systems Technical Journal 27, pp 379 (1948)\n\n\n\n\n\n","category":"function"},{"location":"entropies/#Curado-entropy","page":"Entropies","title":"Curado entropy","text":"","category":"section"},{"location":"entropies/","page":"Entropies","title":"Entropies","text":"Curado","category":"page"},{"location":"entropies/#Entropies.Curado","page":"Entropies","title":"Entropies.Curado","text":"Curado <: Entropy\nCurado(; b = 1.0)\n\nThe Curado entropy (Curado & Nobre, 2004)[Curado2004], used with entropy to compute\n\nH_C(p) = left( sum_i=1^N e^-b p_i right) + e^-b - 1\n\nwith b ∈ ℛ, b > 0, where the terms outside the sum ensures that H_C(0) = H_C(1) = 0.\n\n[Curado2004]: Curado, E. M., & Nobre, F. D. (2004). On the stability of analytic entropic forms. Physica A: Statistical Mechanics and its Applications, 335(1-2), 94-106.\n\n\n\n\n\n","category":"type"},{"location":"entropies/#Stretched-exponental-entropy","page":"Entropies","title":"Stretched exponental entropy","text":"","category":"section"},{"location":"entropies/","page":"Entropies","title":"Entropies","text":"StretchedExponential","category":"page"},{"location":"entropies/#Entropies.StretchedExponential","page":"Entropies","title":"Entropies.StretchedExponential","text":"StretchedExponential <: Entropy\nStretchedExponential(; η = 2.0, base = 2)\n\nThe stretched exponential, or Anteneodo-Plastino, entropy (Anteneodo & Plastino, 1999[Anteneodo1999]), used with entropy to compute\n\nS_eta(p) = sum_i = 1^N\nGamma left( dfraceta + 1eta - log_base(p_i) right) -\np_i Gamma left( dfraceta + 1eta right)\n\nwhere eta geq 0, Gamma(cdot cdot) is the upper incomplete Gamma function, and Gamma(cdot) = Gamma(cdot 0) is the Gamma function. Reduces to Shannon entropy for η = 1.0.\n\n[Anteneodo1999]: Anteneodo, C., & Plastino, A. R. (1999). Maximum entropy approach to stretched exponential probability distributions. Journal of Physics A: Mathematical and General, 32(7), 1089.\n\n\n\n\n\n","category":"type"},{"location":"entropies/#Normalized-entropies","page":"Entropies","title":"Normalized entropies","text":"","category":"section"},{"location":"entropies/","page":"Entropies","title":"Entropies","text":"maximum(::Entropy, ::Any, ::ProbabilitiesEstimator)\nentropy_normalized","category":"page"},{"location":"entropies/#Base.maximum-Tuple{Entropy, Any, ProbabilitiesEstimator}","page":"Entropies","title":"Base.maximum","text":"maximum(e::Entropy, x, est::ProbabilitiesEstimator) → m::Real\n\nReturn the maximum value m of the given entropy type based on the given estimator and the given input x (whose values are not important, but layout and type are).\n\nThis function only works if the maximum value is dedicable, which is possible only when the estimator has a known alphabet_length.\n\nmaximum(e::Entropy, L::Int) → m::Real\n\nAlternatively, compute the maximum entropy from the alphabet length L directly.\n\n\n\n\n\n","category":"method"},{"location":"entropies/#Entropies.entropy_normalized","page":"Entropies","title":"Entropies.entropy_normalized","text":"entropy_normalized([e::Entropy,] x, est::ProbabilitiesEstimator) → h̃ ∈ [0, 1]\n\nReturn the normalized entropy of x, i.e., the value of entropy divided by the maximum value for e, according to the given probability estimator. If e is not given, it defaults to Shannon().\n\nNotice that unlike for entropy, here there is no method entropy_normalized(e::Entropy, probs::Probabilities) because there is no way to know the amount of possible events (i.e., the alphabet_length) from probs.\n\n\n\n\n\n","category":"function"},{"location":"entropies/#Indirect-entropies","page":"Entropies","title":"Indirect entropies","text":"","category":"section"},{"location":"entropies/","page":"Entropies","title":"Entropies","text":"Here we list functions which compute Shannon entropies via alternate means, without explicitly computing some probability distributions and then using the Shannon formula.","category":"page"},{"location":"entropies/#Nearest-neighbors-entropy","page":"Entropies","title":"Nearest neighbors entropy","text":"","category":"section"},{"location":"entropies/","page":"Entropies","title":"Entropies","text":"Kraskov\nKozachenkoLeonenko","category":"page"},{"location":"entropies/#Entropies.Kraskov","page":"Entropies","title":"Entropies.Kraskov","text":"Kraskov <: IndirectEntropy\nKraskov(; k::Int = 1, w::Int = 1, base = 2)\n\nAn indirect entropy used in entropy(Kraskov(), x) to estimate the Shannon entropy of x (a multi-dimensional Dataset) to the given base using k-th nearest neighbor searches as in [Kraskov2004].\n\nw is the Theiler window, which determines if temporal neighbors are excluded during neighbor searches (defaults to 0, meaning that only the point itself is excluded when searching for neighbours).\n\nSee also: KozachenkoLeonenko.\n\n[Kraskov2004]: Kraskov, A., Stögbauer, H., & Grassberger, P. (2004). Estimating mutual information. Physical review E, 69(6), 066138.\n\n\n\n\n\n","category":"type"},{"location":"entropies/#Entropies.KozachenkoLeonenko","page":"Entropies","title":"Entropies.KozachenkoLeonenko","text":"KozachenkoLeonenko <: IndirectEntropy\nKozachenkoLeonenko(; k::Int = 1, w::Int = 1, base = 2)\n\nAn indirect entropy estimator used in entropy(KozachenkoLeonenko(), x) to estimate the Shannon entropy of x (a multi-dimensional Dataset) to the given base using k-th nearest neighbor searches using the method from Kozachenko & Leonenko[KozachenkoLeonenko1987], as described in Charzyńska and Gambin[Charzyńska2016].\n\nw is the Theiler window, which determines if temporal neighbors are excluded during neighbor searches (defaults to 0, meaning that only the point itself is excluded when searching for neighbours).\n\nSee also: Kraskov.\n\n[Charzyńska2016]: Charzyńska, A., & Gambin, A. (2016). Improvement of the k-NN entropy estimator with applications in systems biology. Entropy, 18(1), 13.\n\n[KozachenkoLeonenko1987]: Kozachenko, L. F., & Leonenko, N. N. (1987). Sample estimate of the entropy of a random vector. Problemy Peredachi Informatsii, 23(2), 9-16.\n\n\n\n\n\n","category":"type"},{"location":"entropies/#Convenience-functions","page":"Entropies","title":"Convenience functions","text":"","category":"section"},{"location":"entropies/","page":"Entropies","title":"Entropies","text":"In this subsection we expand documentation strings of \"entropy names\" that are used commonly in the literature, such as \"permutation entropy\". As we made clear in API & terminology, these are just the existing Shannon entropy with a particularly chosen probability estimator. We have only defined convenience functions for the most used names, and arbitrary more specialized convenience functions can be easily defined in a couple lines of code.","category":"page"},{"location":"entropies/","page":"Entropies","title":"Entropies","text":"entropy_permutation\nentropy_spatial_permutation\nentropy_wavelet\nentropy_dispersion","category":"page"},{"location":"entropies/#Entropies.entropy_permutation","page":"Entropies","title":"Entropies.entropy_permutation","text":"entropy_permutation(x; τ = 1, m = 3, base = 2)\n\nCompute the permutation entropy of x of order m with delay/lag τ. This function is just a convenience call to:\n\nest = SymbolicPermutation(; m, τ)\nentropy(Shannon(base), x, est)\n\nSee SymbolicPermutation for more info. Similarly, one can use SymbolicWeightedPermutation or SymbolicAmplitudeAwarePermutation for the weighted/amplitude-aware versions.\n\n\n\n\n\n","category":"function"},{"location":"entropies/#Entropies.entropy_spatial_permutation","page":"Entropies","title":"Entropies.entropy_spatial_permutation","text":"entropy_spatial_permutation(x, stencil, periodic = true; kwargs...)\n\nCompute the spatial permutation entropy of x given the stencil. Here x must be a matrix or higher dimensional Array containing spatial data. This function is just a convenience call to:\n\nest = SpatialSymbolicPermutation(stencil, x, periodic)\nentropy(Renyi(;kwargs...), x, est)\n\nSee SpatialSymbolicPermutation for more info, or how to encode stencils.\n\n\n\n\n\n","category":"function"},{"location":"entropies/#Entropies.entropy_wavelet","page":"Entropies","title":"Entropies.entropy_wavelet","text":"entropy_wavelet(x; wavelet = Wavelets.WT.Daubechies{12}(), base = 2)\n\nCompute the wavelet entropy. This function is just a convenience call to:\n\nest = WaveletOverlap(wavelet)\nentropy(Shannon(base), x, est)\n\nSee WaveletOverlap for more info.\n\n\n\n\n\n","category":"function"},{"location":"entropies/#Entropies.entropy_dispersion","page":"Entropies","title":"Entropies.entropy_dispersion","text":"entropy_dispersion(x; base = 2, kwargs...)\n\nCompute the dispersion entropy. This function is just a convenience call to:\n\nest = Dispersion(kwargs...)\nentropy(Shannon(base), x, est)\n\nSee Dispersion for more info.\n\n\n\n\n\n","category":"function"},{"location":"probabilities/#probabilities_estimators","page":"Probabilities","title":"Probabilities","text":"","category":"section"},{"location":"probabilities/","page":"Probabilities","title":"Probabilities","text":"probabilities\nprobabilities!\nProbabilities\nProbabilitiesEstimator","category":"page"},{"location":"probabilities/#Entropies.probabilities","page":"Probabilities","title":"Entropies.probabilities","text":"probabilities(x::Array_or_Dataset) → p::Probabilities\n\nDirectly count probabilities from the elements of x without any discretization, binning, symbolizing, or any other common processing. This is mostly useful when x contains categorical or integer data.\n\nprobabilities always returns a Probabilities container (Vector-like).\n\nx is typically an Array or a Dataset, see Input data for Entropies.jl.\n\nprobabilities(x::Array_or_Dataset, est::ProbabilitiesEstimator) → p::Probabilities\n\nCalculate probabilities representing x based on the provided estimator. The probabilities may, or may not be ordered, and may, or may not contain 0s, see the documentation of the individual estimators for more. Configuration options are always given as arguments to the chosen estimator.\n\nprobabilities(x::Array_or_Dataset, ε::AbstractFloat) → p::Probabilities\n\nConvenience syntax which provides probabilities for x based on rectangular binning (i.e. performing a histogram). In short, the state space is divided into boxes of length ε, and formally we use est = ValueHistogram(RectangularBinning(ε)) as an estimator, see ValueHistogram.\n\nprobabilities(x::Array_or_Dataset, n::Int) → p::Probabilities\n\nSame as the above method, but now each dimension of the data is binned into n equal sized bins instead of bins of length ε::AbstractFloat.\n\n\n\n\n\n","category":"function"},{"location":"probabilities/#Entropies.probabilities!","page":"Probabilities","title":"Entropies.probabilities!","text":"probabilities!(s, args...)\n\nSimilar to probabilities(args...), but allows pre-allocation of temporarily used containers s.\n\nOnly works for certain estimators. See for example SymbolicPermutation.\n\n\n\n\n\n","category":"function"},{"location":"probabilities/#Entropies.Probabilities","page":"Probabilities","title":"Entropies.Probabilities","text":"Probabilities(x) → p\n\nA simple wrapper type around an x::AbstractVector which ensures that p sums to 1. Behaves identically to Vector.\n\n\n\n\n\n","category":"type"},{"location":"probabilities/#Entropies.ProbabilitiesEstimator","page":"Probabilities","title":"Entropies.ProbabilitiesEstimator","text":"An abstract type for probabilities estimators.\n\n\n\n\n\n","category":"type"},{"location":"probabilities/#Count-occurrences-(counting)","page":"Probabilities","title":"Count occurrences (counting)","text":"","category":"section"},{"location":"probabilities/","page":"Probabilities","title":"Probabilities","text":"CountOccurrences","category":"page"},{"location":"probabilities/#Entropies.CountOccurrences","page":"Probabilities","title":"Entropies.CountOccurrences","text":"CountOccurrences()\n\nA probabilities/entropy estimator based on straight-forward counting of distinct elements in a univariate time series or multivariate dataset. This is the same as giving no estimator to probabilities.\n\nThe events in probabilities_and_events are the sorted unique values of the input.\n\n\n\n\n\n","category":"type"},{"location":"probabilities/#Visitation-frequency-(histograms)","page":"Probabilities","title":"Visitation frequency (histograms)","text":"","category":"section"},{"location":"probabilities/","page":"Probabilities","title":"Probabilities","text":"ValueHistogram\nRectangularBinning","category":"page"},{"location":"probabilities/#Entropies.ValueHistogram","page":"Probabilities","title":"Entropies.ValueHistogram","text":"ValueHistogram(b::AbstractBinning) <: ProbabilitiesEstimator\n\nA probability estimator based on binning the values of the data as dictated by the binning scheme b and formally computing their histogram, i.e., the frequencies of points in the bins. Alias to this is VisitationFrequency.\n\nThis method has a linearithmic time complexity (n log(n) for n = length(x)) and a linear space complexity (l for l = dimension(x)). This allows computation of probabilities (histograms) of high-dimensional datasets and with small box sizes ε without memory overflow and with maximum performance.\n\nTo obtain the bin information along with the probabilities, use probabilities_and_events. The events correspond to the bin corners.\n\nSee also: RectangularBinning.\n\n\n\n\n\n","category":"type"},{"location":"probabilities/#Entropies.RectangularBinning","page":"Probabilities","title":"Entropies.RectangularBinning","text":"RectangularBinning(ϵ) <: AbstractBinning\n\nRectangular box partition of state space using the scheme ϵ. Binning instructions are deduced from the type of ϵ as follows:\n\nϵ::Int divides each coordinate axis into ϵ equal-length intervals  that cover all data.\nϵ::Float64 divides each coordinate axis into intervals of fixed size ϵ, starting  from the axis minima until the data is completely covered by boxes.\nϵ::Vector{Int} divides the i-th coordinate axis into ϵ[i] equal-length  intervals that cover all data.\nϵ::Vector{Float64} divides the i-th coordinate axis into intervals of fixed size  ϵ[i], starting from the axis minima until the data is completely covered by boxes.\n\n\n\n\n\n","category":"type"},{"location":"probabilities/#Permutation-(symbolic)","page":"Probabilities","title":"Permutation (symbolic)","text":"","category":"section"},{"location":"probabilities/","page":"Probabilities","title":"Probabilities","text":"SymbolicPermutation\nSymbolicWeightedPermutation\nSymbolicAmplitudeAwarePermutation\nSpatialSymbolicPermutation","category":"page"},{"location":"probabilities/#Entropies.SymbolicPermutation","page":"Probabilities","title":"Entropies.SymbolicPermutation","text":"SymbolicPermutation(; m = 3, τ = 1, lt::Function = Entropies.isless_rand)\n\nA probabilities estimator based on ordinal permutation patterns, originally used by Bandt & Pompe (2002)[BandtPompe2002] to compute permutation entropy.\n\nIf applied to a univariate time series, then the time series is first embedded using embedding delay τ and dimension m, and then converted to a symbol time series using symbolize with OrdinalPattern, from which probabilities are estimated. If applied to a Dataset, then τ and m are ignored, and probabilities are computed directly from the state vectors.\n\nnote: Handling equal values in ordinal patterns\nIn Bandt & Pompe (2002), equal values are ordered after their order of appearance, but this can lead to erroneous temporal correlations, especially for data with low-amplitude resolution [Zunino2017]. Here, by default, if two values are equal, then one of the is randomly assigned as \"the largest\", using lt = Entropies.isless_rand. To get the behaviour from Bandt and Pompe (2002), use lt = Base.isless).\n\nIn-place symbolization\n\nSymbolicPermutation also implements the in-place entropy! and probabilities!. The length of the pre-allocated symbol vector must match the length of the embedding: N - (m-1)τ for univariate time series, and M for length-M Datasets), i.e.\n\nusing DelayEmbeddings, Entropies\nm, τ, N = 2, 1, 100\nest = SymbolicPermutation(; m, τ)\n\n# For a time series\nx_ts = rand(N)\ns_ts = zeros(Int, N - (m - 1)*τ)\np = probabilities!(s_ts, x_ts, est)\nh = entropy!(s_ts, Renyi(),  x_ts, est)\n\n# For a pre-symbolized `Dataset`\nx_symb = symbolize(x_ts, OrdinalPattern(m = 2, τ = 1))\nx_d = genembed(x_symb, (0, -1, -2))\ns_d = zeros(Int, length(x_d))\np = probabilities!(s_d, x_d, est)\nh = entropy!(s_d, Renyi(), x_d, est)\n\nSee SymbolicWeightedPermutation and SymbolicAmplitudeAwarePermutation for estimators that not only consider ordinal (sorting) patterns, but also incorporate information about within-state-vector amplitudes.\n\n[BandtPompe2002]: Bandt, Christoph, and Bernd Pompe. \"Permutation entropy: a natural complexity measure for time series.\" Physical review letters 88.17 (2002): 174102.\n\n[Zunino2017]: Zunino, L., Olivares, F., Scholkmann, F., & Rosso, O. A. (2017). Permutation entropy based time series analysis: Equalities in the input signal can lead to false conclusions. Physics Letters A, 381(22), 1883-1892.\n\n\n\n\n\n","category":"type"},{"location":"probabilities/#Entropies.SymbolicWeightedPermutation","page":"Probabilities","title":"Entropies.SymbolicWeightedPermutation","text":"SymbolicWeightedPermutation(; τ = 1, m = 3, lt = Entropies.isless_rand)\n\nA variant of SymbolicPermutation that also incorporates amplitude information, based on the weighted permutation entropy (Fadlallah et al., 2013).\n\nProbabilities are computed as\n\np(pi_i^m tau) = dfracsum_k=1^N mathbf1_uS(u) = s_i\nleft( mathbfx_k^m tau right)\n w_ksum_k=1^N mathbf1_uS(u) in Pi\nleft( mathbfx_k^m tau right) w_k = dfracsum_k=1^N\nmathbf1_uS(u) = s_i\nleft( mathbfx_k^m tau right)  w_ksum_k=1^N w_k\n\nwhere weights are computed based on the variance of the state vectors as\n\nw_j = dfrac1msum_k=1^m (x_j+(k+1)tau - mathbfhatx_j^m tau)^2\n\nand mathbfx_i is the aritmetic mean of state vector:\n\nmathbfhatx_j^m tau = frac1m sum_k=1^m x_j + (k+1)tau\n\nThe weighted permutation entropy is equivalent to regular permutation entropy when weights are positive and identical (w_j = beta  forall  j leq N and beta  0).\n\nnote: An implementation note\nNote: in equation 7, section III, of the original paper, the authors writew_j = dfrac1msum_k=1^m (x_j-(k-1)tau - mathbfhatx_j^m tau)^2But given the formula they give for the arithmetic mean, this is not the variance of mathbfx_i, because the indices are mixed: x_j+(k-1)tau in the weights formula, vs. x_j+(k+1)tau in the arithmetic mean formula. This seems to imply that amplitude information about previous delay vectors are mixed with mean amplitude information about current vectors. The authors also mix the terms \"vector\" and \"neighboring vector\" (but uses the same notation for both), making it hard to interpret whether the sign switch is a typo or intended. Here, we use the notation above, which actually computes the variance for mathbfx_i.\n\nSee SymbolicPermutation for an estimator that only incorporates ordinal/sorting information and disregards amplitudes, and SymbolicAmplitudeAwarePermutation for another estimator that incorporates amplitude information.\n\n[Fadlallah2013]: Fadlallah, Bilal, et al. \"Weighted-permutation entropy: A complexity measure for time series incorporating amplitude information.\" Physical Review E 87.2 (2013): 022911.\n\n\n\n\n\n","category":"type"},{"location":"probabilities/#Entropies.SymbolicAmplitudeAwarePermutation","page":"Probabilities","title":"Entropies.SymbolicAmplitudeAwarePermutation","text":"SymbolicAmplitudeAwarePermutation(; τ = 1, m = 3, A = 0.5, lt = Entropies.isless_rand)\n\nA variant of SymbolicPermutation that also incorporates amplitude information, based on the amplitude-aware permutation entropy (Azami & Escudero, 2016).\n\nProbabilities are computed as\n\np(pi_i^m tau) =\ndfracsum_k=1^N\nmathbf1_uS(u) = s_i left( mathbfx_k^m tau right)  a_ksum_k=1^N\nmathbf1_uS(u) in Pi left( mathbfx_k^m tau right) a_k =\ndfracsum_k=1^N mathbf1_uS(u) = s_i\nleft( mathbfx_k^m tau right)  a_ksum_k=1^N a_k\n\nThe weights encoding amplitude information about state vector mathbfx_i = (x_1^i x_2^i ldots x_m^i) are\n\na_i = dfracAm sum_k=1^m x_k^i  + dfrac1-Ad-1\nsum_k=2^d x_k^i - x_k-1^i\n\nwith 0 leq A leq 1. When A=0 , only internal differences between the elements of mathbfx_i are weighted. Only mean amplitude of the state vector elements are weighted when A=1. With, 0A1, a combined weighting is used.\n\nSee SymbolicPermutation for an estimator that only incorporates ordinal/sorting information and disregards amplitudes, and SymbolicWeightedPermutation for another estimator that incorporates amplitude information.\n\n[Azami2016]: Azami, H., & Escudero, J. (2016). Amplitude-aware permutation entropy: Illustration in spike detection and signal segmentation. Computer methods and programs in biomedicine, 128, 40-51.\n\n\n\n\n\n","category":"type"},{"location":"probabilities/#Entropies.SpatialSymbolicPermutation","page":"Probabilities","title":"Entropies.SpatialSymbolicPermutation","text":"SpatialSymbolicPermutation(stencil, x, periodic = true)\n\nA symbolic, permutation-based probabilities/entropy estimator for spatiotemporal systems.\n\nThe input data x are high-dimensional arrays, for example 2D arrays [Ribeiro2012] or 3D arrays [Schlemmer2018]. This approach is also known as spatiotemporal permutation entropy. x is given because we need to know its size for optimization and bound checking.\n\nA stencil defines what local area around each pixel to consider, and compute the ordinal pattern within the stencil. Stencils are given as vectors of CartesianIndex which encode the offsets of the pixes to include in the stencil, with respect to the current pixel. For example\n\ndata = [rand(50, 50) for _ in 1:50]\nx = data[1] # first \"time slice\" of a spatial system evolution\nstencil = CartesianIndex.([(0,1), (1,1), (1,0)])\nest = SpatialSymbolicPermutation(stencil, x)\n\nHere the stencil creates a 2x2 square extending to the bottom and right of the pixel (directions here correspond to the way Julia prints matrices by default). Notice that no offset (meaning the pixel itself) is always included automatically. The length of the stencil decides the order of the permutation entropy, and the ordering within the stencil dictates the order that pixels are compared with. The pixel without any offset is always first in the order.\n\nAfter having defined est, one calculates the spatial permutation entropy by calling entropy with est, and with the array data. To apply this to timeseries of spatial data, simply loop over the call, e.g.:\n\nh = entropy(x, est)\nh_vs_t = entropy.(data, est) # broadcasting with `.`\n\nThe argument periodic decides whether the stencil should wrap around at the end of the array. If periodic = false, pixels whose stencil exceeds the array bounds are skipped.\n\n[Ribeiro2012]: Ribeiro et al. (2012). Complexity-entropy causality plane as a complexity measure for two-dimensional patterns. https://doi.org/10.1371/journal.pone.0040689\n\n[Schlemmer2018]: Schlemmer et al. (2018). Spatiotemporal Permutation Entropy as a Measure for Complexity of Cardiac Arrhythmia. https://doi.org/10.3389/fphy.2018.00039\n\n\n\n\n\n","category":"type"},{"location":"probabilities/#Dispersion-(symbolic)","page":"Probabilities","title":"Dispersion (symbolic)","text":"","category":"section"},{"location":"probabilities/","page":"Probabilities","title":"Probabilities","text":"Dispersion","category":"page"},{"location":"probabilities/#Entropies.Dispersion","page":"Probabilities","title":"Entropies.Dispersion","text":"Dispersion(; symbolization = GaussianSymbolization(c = 5), m = 2, τ = 1,\n    check_unique = true)\n\nA probability estimator based on dispersion patterns, originally used by Rostaghi & Azami, 2016[Rostaghi2016] to compute the \"dispersion entropy\", which characterizes the complexity and irregularity of a time series.\n\nRelative frequencies of dispersion patterns are computed using the symbolization scheme s with embedding dimension m and embedding delay τ. Recommended parameter values[Li2018] are m ∈ [2, 3], τ = 1 for the embedding, and c ∈ [3, 4, …, 8] categories for the Gaussian symbol mapping.\n\nDescription\n\nAssume we have a univariate time series X = x_i_i=1^N. First, this time series is symbolized using symbolization, which default to GaussianSymbolization, which uses the normal cumulative distribution function (CDF) for symbolization. Other choices of CDFs are also possible, but Entropies.jl currently only implements GaussianSymbolization, which was used in Rostaghi & Azami (2016). This step results in an integer-valued symbol time series S =  s_i _i=1^N, where s_i in 1 2 ldots c.\n\nNext, the symbol time series S is embedded into an m-dimensional time series, using an embedding lag of tau = 1, which yields a total of N - (m - 1)tau points, or \"dispersion patterns\". Because each z_i can take on c different values, and each embedding point has m values, there are c^m possible dispersion patterns. This number is used for normalization when computing dispersion entropy.\n\nComputing dispersion probabilities and entropy\n\nA probability distribution P = p_i _i=1^c^m, where sum_i^c^m p_i = 1, can then be estimated by counting and sum-normalising the distribution of dispersion patterns among the embedding vectors. Note that dispersion patterns that are not present are not counted. Therefore, you'll always get non-zero probabilities using the Dispersion probability estimator.\n\nTo compute dispersion entropy of order q to a given base on the univariate input time series x, do:\n\nentropy_renyi(x, Dispersion(), base = 2, q = 1)\n\nData requirements and parameters\n\nThe input must have more than one unique element for the Gaussian mapping to be well-defined. Li et al. (2018) recommends that x has at least 1000 data points.\n\nIf check_unique == true (default), then it is checked that the input has more than one unique value. If check_unique == false and the input only has one unique element, then a InexactError is thrown when trying to compute probabilities.\n\nSee also: entropy_dispersion, GaussianSymbolization.\n\nnote: Why 'dispersion patterns'?\nEach embedding vector is called a \"dispersion pattern\". Why? Let's consider the case when m = 5 and c = 3, and use some very imprecise terminology for illustration:When c = 3, values clustering far below mean are in one group, values clustered around the mean are in one group, and values clustering far above the mean are in a third group. Then the embedding vector 2 2 2 2 2 consists of values that are relatively close together (close to the mean), so it represents a set of numbers that are not very spread out (less dispersed). The embedding vector 1 1 2 3 3, however, represents numbers that are much more spread out (more dispersed), because the categories representing \"outliers\" both above and below the mean are represented, not only values close to the mean.\n\n[Rostaghi2016]: Rostaghi, M., & Azami, H. (2016). Dispersion entropy: A measure for time-series analysis. IEEE Signal Processing Letters, 23(5), 610-614.\n\n[Li2018]: Li, G., Guan, Q., & Yang, H. (2018). Noise reduction method of underwater acoustic signals based on CEEMDAN, effort-to-compress complexity, refined composite multiscale dispersion entropy and wavelet threshold denoising. Entropy, 21(1), 11.\n\n\n\n\n\n","category":"type"},{"location":"probabilities/#Transfer-operator-(binning)","page":"Probabilities","title":"Transfer operator (binning)","text":"","category":"section"},{"location":"probabilities/","page":"Probabilities","title":"Probabilities","text":"TransferOperator","category":"page"},{"location":"probabilities/#Entropies.TransferOperator","page":"Probabilities","title":"Entropies.TransferOperator","text":"TransferOperator(ϵ::RectangularBinning) <: ProbabilitiesEstimator\n\nA probability estimator based on binning data into rectangular boxes dictated by the binning scheme ϵ, then approxmating the transfer (Perron-Frobenius) operator over the bins, then taking the invariant measure associated with that transfer operator as the bin probabilities. Assumes that the input data are sequential (time-ordered).\n\nThis implementation follows the grid estimator approach in Diego et al. (2019)[Diego2019].\n\nDescription\n\nThe transfer operator P^Nis computed as an N-by-N matrix of transition probabilities between the states defined by the partition elements, where N is the number of boxes in the partition that is visited by the orbit/points.\n\nIf  x_t^(D) _n=1^L are the L different D-dimensional points over which the transfer operator is approximated,  C_k=1^N  are the N different partition elements (as dictated by ϵ) that gets visited by the points, and  phi(x_t) = x_t+1, then\n\nP_ij = dfrac\n x_n  phi(x_n) in C_j cap x_n in C_i \n x_m  x_m in C_i \n\nwhere  denotes the cardinal. The element P_ij thus indicates how many points that are initially in box C_i end up in box C_j when the points in C_i are projected one step forward in time. Thus, the row P_ik^N where k in 1 2 ldots N  gives the probability of jumping from the state defined by box C_i to any of the other N states. It follows that sum_k=1^N P_ik = 1 for all i. Thus, P^N is a row/right stochastic matrix.\n\nInvariant measure estimation from transfer operator\n\nThe left invariant distribution mathbfrho^N is a row vector, where mathbfrho^N P^N = mathbfrho^N. Hence, mathbfrho^N is a row eigenvector of the transfer matrix P^N associated with eigenvalue 1. The distribution mathbfrho^N approximates the invariant density of the system subject to the partition ϵ, and can be taken as a probability distribution over the partition elements.\n\nIn practice, the invariant measure mathbfrho^N is computed using invariantmeasure, which also approximates the transfer matrix. The invariant distribution is initialized as a length-N random distribution which is then applied to P^N. The resulting length-N distribution is then applied to P^N again. This process repeats until the difference between the distributions over consecutive iterations is below some threshold.\n\nProbability and entropy estimation\n\nprobabilities(x::AbstractDataset, est::TransferOperator{RectangularBinning}) estimates   probabilities for the bins defined by the provided binning (est.ϵ)\nentropy_renyi(x::AbstractDataset, est::TransferOperator{RectangularBinning}) does the same,   but computes generalized entropy using the probabilities.\n\nSee also: RectangularBinning, invariantmeasure.\n\n[Diego2019]: Diego, D., Haaga, K. A., & Hannisdal, B. (2019). Transfer entropy computation using the Perron-Frobenius operator. Physical Review E, 99(4), 042212.\n\n\n\n\n\n","category":"type"},{"location":"probabilities/#Utility-methods/types","page":"Probabilities","title":"Utility methods/types","text":"","category":"section"},{"location":"probabilities/","page":"Probabilities","title":"Probabilities","text":"InvariantMeasure\ninvariantmeasure\ntransfermatrix","category":"page"},{"location":"probabilities/#Entropies.InvariantMeasure","page":"Probabilities","title":"Entropies.InvariantMeasure","text":"InvariantMeasure(to, ρ)\n\nMinimal return struct for invariantmeasure that contains the estimated invariant measure ρ, as well as the transfer operator to from which it is computed (including bin information).\n\nSee also: invariantmeasure.\n\n\n\n\n\n","category":"type"},{"location":"probabilities/#Entropies.invariantmeasure","page":"Probabilities","title":"Entropies.invariantmeasure","text":"invariantmeasure(x::AbstractDataset, ϵ::RectangularBinning) → iv::InvariantMeasure\n\nEstimate an invariant measure over the points in x based on binning the data into rectangular boxes dictated by the binning scheme ϵ, then approximate the transfer (Perron-Frobenius) operator over the bins. From the approximation to the transfer operator, compute an invariant distribution over the bins. Assumes that the input data are sequential.\n\nDetails on the estimation procedure is found the TransferOperator docstring.\n\nExample\n\nusing DynamicalSystems, Plots, Entropies\nD = 4\nds = Systems.lorenz96(D; F = 32.0)\nN, dt = 20000, 0.1\norbit = trajectory(ds, N*dt; dt = dt, Ttr = 10.0)\n\n# Estimate the invariant measure over some coarse graining of the orbit.\niv = invariantmeasure(orbit, RectangularBinning(15))\n\n# Get the probabilities and bins\ninvariantmeasure(iv)\n\nProbabilities and bin information\n\ninvariantmeasure(iv::InvariantMeasure) → (ρ::Probabilities, bins::Vector{<:SVector})\n\nFrom a pre-computed invariant measure, return the probabilities and associated bins. The element ρ[i] is the probability of visitation to the box bins[i]. Analogous to binhist.\n\nhint: Transfer operator approach vs. naive histogram approach\nWhy bother with the transfer operator instead of using regular histograms to obtain probabilities?In fact, the naive histogram approach and the transfer operator approach are equivalent in the limit of long enough time series (as n to intfy), which is guaranteed by the ergodic theorem. There is a crucial difference, however:The naive histogram approach only gives the long-term probabilities that orbits visit a certain region of the state space. The transfer operator encodes that information too, but comes with the added benefit of knowing the transition probabilities between states (see transfermatrix).\n\nSee also: InvariantMeasure.\n\n\n\n\n\n","category":"function"},{"location":"probabilities/#Entropies.transfermatrix","page":"Probabilities","title":"Entropies.transfermatrix","text":"transfermatrix(iv::InvariantMeasure) → (M::AbstractArray{<:Real, 2}, bins::Vector{<:SVector})\n\nReturn the transfer matrix/operator and corresponding bins. Here, bins[i] corresponds to the i-th row/column of the transfer matrix. Thus, the entry M[i, j] is the probability of jumping from the state defined by bins[i] to the state defined by bins[j].\n\nSee also: TransferOperator.\n\n\n\n\n\n","category":"function"},{"location":"probabilities/#Kernel-density","page":"Probabilities","title":"Kernel density","text":"","category":"section"},{"location":"probabilities/","page":"Probabilities","title":"Probabilities","text":"NaiveKernel","category":"page"},{"location":"probabilities/#Entropies.NaiveKernel","page":"Probabilities","title":"Entropies.NaiveKernel","text":"NaiveKernel(ϵ::Real, ss = KDTree; w = 0, metric = Euclidean()) <: ProbabilitiesEstimator\n\nEstimate probabilities/entropy using a \"naive\" kernel density estimation approach (KDE), as discussed in Prichard and Theiler (1995) [PrichardTheiler1995].\n\nProbabilities P(mathbfx epsilon) are assigned to every point mathbfx by counting how many other points occupy the space spanned by a hypersphere of radius ϵ around mathbfx, according to:\n\nP_i( X epsilon) approx dfrac1N sum_s B(X_i - X_j  epsilon)\n\nwhere B gives 1 if the argument is true. Probabilities are then normalized.\n\nThe search structure ss is any search structure supported by Neighborhood.jl. Specifically, use KDTree to use a tree-based neighbor search, or BruteForce for the direct distances between all points. KDTrees heavily outperform direct distances when the dimensionality of the data is much smaller than the data length.\n\nThe keyword w stands for the Theiler window, and excludes indices s that are within i - s  w from the given point X_i.\n\nThe events for probabilities_and_events are the input data themselves.\n\n[PrichardTheiler1995]: Prichard, D., & Theiler, J. (1995). Generalized redundancies for time series analysis. Physica D: Nonlinear Phenomena, 84(3-4), 476-493.\n\n\n\n\n\n","category":"type"},{"location":"probabilities/#Timescales","page":"Probabilities","title":"Timescales","text":"","category":"section"},{"location":"probabilities/","page":"Probabilities","title":"Probabilities","text":"WaveletOverlap\nPowerSpectrum","category":"page"},{"location":"probabilities/#Entropies.WaveletOverlap","page":"Probabilities","title":"Entropies.WaveletOverlap","text":"WaveletOverlap([wavelet]) <: ProbabilitiesEstimator\n\nApply the maximal overlap discrete wavelet transform (MODWT) to a signal, then compute probabilities/entropy from the energies at different wavelet scales. These probabilities are used to compute the wavelet entropy, according to Rosso et al. (2001)[Rosso2001].\n\nThe probability p[i] is the relative energy for the i-th wavelet scale. To obtain a better understanding of what these probabilities mean, we prepared a notebook you can view online. As such, this estimator only works for timeseries input.\n\nBy default the wavelet Wavelets.WT.Daubechies{12}() is used. Otherwise, you may choose a wavelet from the Wavelets package (it must subtype OrthoWaveletClass).\n\n[Rosso2001]: Rosso et al. (2001). Wavelet entropy: a new tool for analysis of short duration brain electrical signals. Journal of neuroscience methods, 105(1), 65-75.\n\n\n\n\n\n","category":"type"},{"location":"probabilities/#Entropies.PowerSpectrum","page":"Probabilities","title":"Entropies.PowerSpectrum","text":"PowerSpectrum() <: ProbabilitiesEstimator\n\nCalculate the power spectrum of a timeseries (amplitude square of its Fourier transform), and return the spectrum normalized to sum = 1 as probabilities. The Shannon entropy of these probabilities is typically referred in the literature as spectral entropy, e.g. [Llanos2016],[Tian2017].\n\nThe closer the spectrum is to flat, i.e., white noise, the higher the entropy. However, you can't compare entropies of timeseries with different length, because the binning in spectral space depends on the length of the input.\n\nThe events in probabilities_and_events are just the frequencies in Fourier space. They should be multiplied with the sampling rate of the signal, which is assumed to be 1.\n\n[Llanos2016]: Llanos et al., Power spectral entropy as an information-theoretic correlate of manner of articulation in American English, The Journal of the Acoustical Society of America 141, EL127 (2017)\n\n[Tian2017]: Tian et al, Spectral Entropy Can Predict Changes of Working Memory Performance Reduced by Short-Time Training in the Delayed-Match-to-Sample Task, Front. Hum. Neurosci.\n\n\n\n\n\n","category":"type"},{"location":"examples/#Examples","page":"Entropies.jl examples","title":"Examples","text":"","category":"section"},{"location":"examples/#Nearest-neighbor-direct-entropy-example","page":"Entropies.jl examples","title":"Nearest neighbor direct entropy example","text":"","category":"section"},{"location":"examples/","page":"Entropies.jl examples","title":"Entropies.jl examples","text":"This example reproduces Figure in Charzyńska & Gambin (2016)[Charzyńska2016]. Both estimators nicely converge to the \"true\" entropy with increasing time series length. For a uniform 1D distribution U(0 1), the true entropy is 0.","category":"page"},{"location":"examples/","page":"Entropies.jl examples","title":"Entropies.jl examples","text":"using DynamicalSystems, CairoMakie, Statistics\nusing Distributions: Uniform, Normal\n\nNs = [100:100:500; 1000:1000:10000]\nEkl = Vector{Vector{Float64}}(undef, 0)\nEkr = Vector{Vector{Float64}}(undef, 0)\n\nnreps = 50\nfor N in Ns\n    kl = Float64[]\n    kr = Float64[]\n    for i = 1:nreps\n        pts = Dataset([rand(Uniform(0, 1), 1) for i = 1:N]);\n\n        push!(kl, entropy(KozachenkoLeonenko(w = 0, k = 1, base = MathConstants.e), pts))\n        # with k = 1, Kraskov is virtually identical to\n        # Kozachenko-Leonenko, so pick a higher number of neighbors\n        push!(kr, entropy(Kraskov(w = 0, k = 3, base = MathConstants.e), pts))\n    end\n    push!(Ekl, kl)\n    push!(Ekr, kr)\nend\n\nfig = Figure()\nax = Axis(fig[1,1]; ylabel = \"entropy (nats)\", title = \"Kozachenko-Leonenko\")\nlines!(ax, Ns, mean.(Ekl); color = Cycled(1))\nband!(ax, Ns, mean.(Ekl) .+ std.(Ekl), mean.(Ekl) .- std.(Ekl);\ncolor = (Main.COLORS[1], 0.5))\n\nay = Axis(fig[2,1]; xlabel = \"time step\", ylabel = \"entropy (nats)\", title = \"Kraskov\")\nlines!(ay, Ns, mean.(Ekr); color = Cycled(2))\nband!(ay, Ns, mean.(Ekr) .+ std.(Ekr), mean.(Ekr) .- std.(Ekr);\ncolor = (Main.COLORS[2], 0.5))\n\nfig","category":"page"},{"location":"examples/","page":"Entropies.jl examples","title":"Entropies.jl examples","text":"[Charzyńska2016]: Charzyńska, A., & Gambin, A. (2016). Improvement of the k-NN entropy estimator with applications in systems biology. Entropy, 18(1), 13.","category":"page"},{"location":"examples/#Permutation-entropy-example","page":"Entropies.jl examples","title":"Permutation entropy example","text":"","category":"section"},{"location":"examples/","page":"Entropies.jl examples","title":"Entropies.jl examples","text":"This example reproduces an example from Bandt and Pompe (2002), where the permutation entropy is compared with the largest Lyapunov exponents from time series of the chaotic logistic map. Entropy estimates using SymbolicWeightedPermutation and SymbolicAmplitudeAwarePermutation are added here for comparison.","category":"page"},{"location":"examples/","page":"Entropies.jl examples","title":"Entropies.jl examples","text":"using DynamicalSystems, CairoMakie\n\nds = Systems.logistic()\nrs = 3.4:0.001:4\nN_lyap, N_ent = 100000, 10000\nm, τ = 6, 1 # Symbol size/dimension and embedding lag\n\n# Generate one time series for each value of the logistic parameter r\nlyaps, hs_perm, hs_wtperm, hs_ampperm = [zeros(length(rs)) for _ in 1:4]\n\nfor (i, r) in enumerate(rs)\n    ds.p[1] = r\n    lyaps[i] = lyapunov(ds, N_lyap)\n\n    x = trajectory(ds, N_ent) # time series\n    hperm = entropy(x, SymbolicPermutation(; m, τ))\n    hwtperm = entropy(x, SymbolicWeightedPermutation(; m, τ))\n    hampperm = entropy(x, SymbolicAmplitudeAwarePermutation(; m, τ))\n\n    hs_perm[i] = hperm; hs_wtperm[i] = hwtperm; hs_ampperm[i] = hampperm\nend\n\nfig = Figure()\na1 = Axis(fig[1,1]; ylabel = L\"\\lambda\")\nlines!(a1, rs, lyaps); ylims!(a1, (-2, log(2)))\na2 = Axis(fig[2,1]; ylabel = L\"h_6 (SP)\")\nlines!(a2, rs, hs_perm; color = Cycled(2))\na3 = Axis(fig[3,1]; ylabel = L\"h_6 (WT)\")\nlines!(a3, rs, hs_wtperm; color = Cycled(3))\na4 = Axis(fig[4,1]; ylabel = L\"h_6 (SAAP)\")\nlines!(a4, rs, hs_ampperm; color = Cycled(4))\na4.xlabel = L\"r\"\n\nfor a in (a1,a2,a3)\n    hidexdecorations!(a, grid = false)\nend\nfig","category":"page"},{"location":"examples/#Kernel-density-example","page":"Entropies.jl examples","title":"Kernel density example","text":"","category":"section"},{"location":"examples/","page":"Entropies.jl examples","title":"Entropies.jl examples","text":"Here, we draw some random points from a 2D normal distribution. Then, we use kernel density estimation to associate a probability to each point p, measured by how many points are within radius 1.5 of p. Plotting the actual points, along with their associated probabilities estimated by the KDE procedure, we get the following surface plot.","category":"page"},{"location":"examples/","page":"Entropies.jl examples","title":"Entropies.jl examples","text":"using DynamicalSystems, CairoMakie, Distributions\n𝒩 = MvNormal([1, -4], 2)\nN = 500\nD = Dataset(sort([rand(𝒩) for i = 1:N]))\nx, y = columns(D)\np = probabilities(D, NaiveKernel(1.5))\nfig, ax = scatter(D[:, 1], D[:, 2], zeros(N);\n    markersize=8, axis=(type = Axis3,)\n)\nsurface!(ax, x, y, p.p)\nax.zlabel = \"P\"\nax.zticklabelsvisible = false\nfig","category":"page"},{"location":"examples/#Wavelet-entropy-example","page":"Entropies.jl examples","title":"Wavelet entropy example","text":"","category":"section"},{"location":"examples/","page":"Entropies.jl examples","title":"Entropies.jl examples","text":"The scale-resolved wavelet entropy should be lower for very regular signals (most of the energy is contained at one scale) and higher for very irregular signals (energy spread more out across scales).","category":"page"},{"location":"examples/","page":"Entropies.jl examples","title":"Entropies.jl examples","text":"using DynamicalSystems, CairoMakie\nN, a = 1000, 10\nt = LinRange(0, 2*a*π, N)\n\nx = sin.(t);\ny = sin.(t .+ cos.(t/0.5));\nz = sin.(rand(1:15, N) ./ rand(1:10, N))\n\nh_x = entropy_wavelet(x)\nh_y = entropy_wavelet(y)\nh_z = entropy_wavelet(z)\n\nfig = Figure()\nax = Axis(fig[1,1]; ylabel = \"x\")\nlines!(ax, t, x; color = Cycled(1), label = \"h=$(h=round(h_x, sigdigits = 5))\");\nay = Axis(fig[2,1]; ylabel = \"y\")\nlines!(ay, t, y; color = Cycled(2), label = \"h=$(h=round(h_y, sigdigits = 5))\");\naz = Axis(fig[3,1]; ylabel = \"z\", xlabel = \"time\")\nlines!(az, t, z; color = Cycled(3), label = \"h=$(h=round(h_z, sigdigits = 5))\");\nfor a in (ax, ay, az); axislegend(a); end\nfor a in (ax, ay); hidexdecorations!(a; grid=false); end\nfig","category":"page"},{"location":"examples/#Properties-of-different-entropies","page":"Entropies.jl examples","title":"Properties of different entropies","text":"","category":"section"},{"location":"examples/","page":"Entropies.jl examples","title":"Entropies.jl examples","text":"Here, we show the sensitivity of the various entropies to variations in their parameters.","category":"page"},{"location":"examples/#Curado-entropy","page":"Entropies.jl examples","title":"Curado entropy","text":"","category":"section"},{"location":"examples/","page":"Entropies.jl examples","title":"Entropies.jl examples","text":"Here, we reproduce Figure 2 from Curado & Nobre (2004)[Curado2004], showing how the Curado entropy changes as function of the parameter a for a range of two-element probability distributions given by Probabilities([p, 1 - p] for p in 1:0.0:0.01:1.0).","category":"page"},{"location":"examples/","page":"Entropies.jl examples","title":"Entropies.jl examples","text":"using Entropies, CairoMakie\nbs = [1.0, 1.5, 2.0, 3.0, 4.0, 10.0]\nps = [Probabilities([p, 1 - p]) for p = 0.0:0.01:1.0]\nhs = [[entropy(Curado(; b = b), p) for p in ps] for b in bs]\nfig = Figure()\nax = Axis(fig[1,1]; xlabel = \"p\", ylabel = \"H(p)\")\npp = [p[1] for p in ps]\nfor (i, b) in enumerate(bs)\n    lines!(ax, pp, hs[i], label = \"b=$b\", color = Cycled(i))\nend\naxislegend(ax)\nfig","category":"page"},{"location":"examples/","page":"Entropies.jl examples","title":"Entropies.jl examples","text":"[Curado2004]: Curado, E. M., & Nobre, F. D. (2004). On the stability of analytic entropic forms. Physica A: Statistical Mechanics and its Applications, 335(1-2), 94-106.","category":"page"},{"location":"examples/#Stretched-exponential-entropy","page":"Entropies.jl examples","title":"Stretched exponential entropy","text":"","category":"section"},{"location":"examples/","page":"Entropies.jl examples","title":"Entropies.jl examples","text":"Here, we reproduce the example from Anteneodo & Plastino (1999)[Anteneodo1999], showing how the stretched exponential entropy changes as function of the parameter η for a range of two-element probability distributions given by Probabilities([p, 1 - p] for p in 1:0.0:0.01:1.0).","category":"page"},{"location":"examples/","page":"Entropies.jl examples","title":"Entropies.jl examples","text":"using Entropies, SpecialFunctions, CairoMakie\nηs = [0.01, 0.2, 0.3, 0.5, 0.7, 1.0, 1.5, 3.0]\nps = [Probabilities([p, 1 - p]) for p = 0.0:0.01:1.0]\n\nhs_norm = [[entropy(StretchedExponential( η = η), p) / gamma((η + 1)/η) for p in ps] for η in ηs]\nfig = Figure()\nax = Axis(fig[1,1]; xlabel = \"p\", ylabel = \"H(p)\")\npp = [p[1] for p in ps]\n\nfor (i, η) in enumerate(ηs)\n    lines!(ax, pp, hs_norm[i], label = \"η=$η\")\nend\naxislegend(ax)\nfig","category":"page"},{"location":"examples/","page":"Entropies.jl examples","title":"Entropies.jl examples","text":"[Anteneodo1999]: Anteneodo, C., & Plastino, A. R. (1999). Maximum entropy approach to stretched exponential probability distributions. Journal of Physics A: Mathematical and General, 32(7), 1089.","category":"page"},{"location":"examples/#dispersion_examples","page":"Entropies.jl examples","title":"Dispersion and reverse dispersion entropy","text":"","category":"section"},{"location":"examples/","page":"Entropies.jl examples","title":"Entropies.jl examples","text":"Here we reproduce parts of figure 3 in Li et al. (2019), computing reverse and regular dispersion entropy for a time series consisting of normally distributed noise with a single spike in the middle of the signal. We compute the entropies over a range subsets of the data, using a sliding window consisting of 70 data points, stepping the window 10 time steps at a time.","category":"page"},{"location":"examples/","page":"Entropies.jl examples","title":"Entropies.jl examples","text":"Note: the results here are not exactly the same as in the original paper, because Li et al. (2019) base their examples on randomly generated numbers and do not provide code that specify random number seeds.","category":"page"},{"location":"examples/","page":"Entropies.jl examples","title":"Entropies.jl examples","text":"using Entropies, DynamicalSystems, Random, CairoMakie, Distributions\n\nn = 1000\nts = 1:n\nx = [i == n ÷ 2 ? 50.0 : 0.0 for i in ts]\nrng = Random.default_rng()\ns = rand(rng, Normal(0, 1), n)\ny = x .+ s\n\nws = 70\nwindows = [t:t+ws for t in 1:10:n-ws]\nrdes = zeros(length(windows))\ndes = zeros(length(windows))\npes = zeros(length(windows))\n\nm, c = 2, 6\nest_de = Dispersion(symbolization = GaussianSymbolization(c), m = m, τ = 1)\n\nfor (i, window) in enumerate(windows)\n    rdes[i] = reverse_dispersion(y[window], est_de; normalize = true)\n    des[i] = entropy_normalized(Renyi(), y[window], est_de)\nend\n\nfig = Figure()\n\na1 = Axis(fig[1,1]; xlabel = \"Time step\", ylabel = \"Value\")\nlines!(a1, ts, y)\ndisplay(fig)\n\na2 = Axis(fig[2, 1]; xlabel = \"Time step\", ylabel = \"Value\")\np_rde = scatterlines!([first(w) for w in windows], rdes,\n    label = \"Reverse dispersion entropy\",\n    color = :black,\n    markercolor = :black, marker = '●')\np_de = scatterlines!([first(w) for w in windows], des,\n    label = \"Dispersion entropy\",\n    color = :red,\n    markercolor = :red, marker = 'x', markersize = 20)\n\naxislegend(position = :rc)\nylims!(0, max(maximum(pes), 1))\nfig","category":"page"},{"location":"examples/","page":"Entropies.jl examples","title":"Entropies.jl examples","text":"[Rostaghi2016]: Rostaghi, M., & Azami, H. (2016). Dispersion entropy: A measure for time-series analysis. IEEE Signal Processing Letters, 23(5), 610-614.","category":"page"},{"location":"examples/","page":"Entropies.jl examples","title":"Entropies.jl examples","text":"[Li2019]: Li, Y., Gao, X., & Wang, L. (2019). Reverse dispersion entropy: a new complexity measure for sensor signal. Sensors, 19(23), 5203.","category":"page"},{"location":"examples/#Normalized-entropy-for-comparing-different-signals","page":"Entropies.jl examples","title":"Normalized entropy for comparing different signals","text":"","category":"section"},{"location":"examples/","page":"Entropies.jl examples","title":"Entropies.jl examples","text":"When comparing different signals or signals that have different length, it is best to normalize entropies so that the \"complexity\" or \"disorder\" quantification is directly comparable between signals. Here is an example based on the Wavelet entropy example (where we use the spectral entropy instead of the wavelet entropy):","category":"page"},{"location":"examples/","page":"Entropies.jl examples","title":"Entropies.jl examples","text":"using DynamicalSystems\nN1, N2, a = 101, 100001, 10\n\nfor N in (N1, N2)\n    local t = LinRange(0, 2*a*π, N)\n    local x = sin.(t) # periodic\n    local y = sin.(t .+ cos.(t/0.5)) # periodic, complex spectrum\n    local z = sin.(rand(1:15, N) ./ rand(1:10, N)) # random\n    local w = trajectory(Systems.lorenz(), N÷10; Δt = 0.1, Ttr = 100)[:, 1] # chaotic\n\n    for q in (x, y, z, w)\n        h = entropy(q, PowerSpectrum())\n        n = entropy_normalized(q, PowerSpectrum())\n        println(\"entropy: $(h), normalized: $(n).\")\n    end\nend","category":"page"},{"location":"examples/","page":"Entropies.jl examples","title":"Entropies.jl examples","text":"You see that while the direct entropy values of the chaotic and noisy signals change massively with N but they are almost the same for the normalized version. For the regular signals, the entropy decreases nevertheless because the noise contribution of the Fourier computation becomes less significant.","category":"page"},{"location":"complexity_measures/#complexity_measures","page":"Complexity measures","title":"Complexity measures","text":"","category":"section"},{"location":"complexity_measures/#Reverse-dispersion-entropy","page":"Complexity measures","title":"Reverse dispersion entropy","text":"","category":"section"},{"location":"complexity_measures/","page":"Complexity measures","title":"Complexity measures","text":"reverse_dispersion\ndistance_to_whitenoise","category":"page"},{"location":"complexity_measures/#Entropies.reverse_dispersion","page":"Complexity measures","title":"Entropies.reverse_dispersion","text":"reverse_dispersion(x::AbstractVector{T}, est::Dispersion = Dispersion();\n    normalize = true) where T <: Real\n\nCompute the reverse dispersion entropy complexity measure (Li et al., 2019)[Li2019].\n\nDescription\n\nLi et al. (2021)[Li2019] defines the reverse dispersion entropy as\n\nH_rde = sum_i = 1^c^m left(p_i - dfrac1c^m right)^2 =\nleft( sum_i=1^c^m p_i^2 right) - dfrac1c^m\n\nwhere the probabilities p_i are obtained precisely as for the Dispersion probability estimator. Relative frequencies of dispersion patterns are computed using the given symbolization scheme , which defaults to symbolization using the normal cumulative distribution function (NCDF), as implemented by GaussianSymbolization, using embedding dimension m and embedding delay τ. Recommended parameter values[Li2018] are m ∈ [2, 3], τ = 1 for the embedding, and c ∈ [3, 4, …, 8] categories for the Gaussian mapping.\n\nIf normalize == true, then the reverse dispersion entropy is normalized to [0, 1].\n\nThe minimum value of H_rde is zero and occurs precisely when the dispersion pattern distribution is flat, which occurs when all p_is are equal to 1c^m. Because H_rde geq 0, H_rde can therefore be said to be a measure of how far the dispersion pattern probability distribution is from white noise.\n\n[Li2019]: Li, Y., Gao, X., & Wang, L. (2019). Reverse dispersion entropy: a new complexity measure for sensor signal. Sensors, 19(23), 5203.\n\n\n\n\n\n","category":"function"},{"location":"complexity_measures/#Entropies.distance_to_whitenoise","page":"Complexity measures","title":"Entropies.distance_to_whitenoise","text":"distance_to_whitenoise(p::Probabilities, estimator::Dispersion; normalize = false)\n\nCompute the distance of the probability distribution p from a uniform distribution, given the parameters of estimator (which must be known beforehand).\n\nIf normalize == true, then normalize the value to the interval [0, 1] by using the parameters of estimator.\n\nUsed to compute reverse dispersion entropy(reverse_dispersion; Li et al., 2019[Li2019]).\n\n[Li2019]: Li, Y., Gao, X., & Wang, L. (2019). Reverse dispersion entropy: a new complexity measure for sensor signal. Sensors, 19(23), 5203.\n\n\n\n\n\n","category":"function"},{"location":"#Entropies.jl","page":"Entropies.jl","title":"Entropies.jl","text":"","category":"section"},{"location":"","page":"Entropies.jl","title":"Entropies.jl","text":"Entropies","category":"page"},{"location":"#Entropies","page":"Entropies.jl","title":"Entropies","text":"A Julia package that provides estimators for probabilities, entropies, and complexity measures for nonlinear dynamics, nonlinear timeseries analysis, and complex systems. It can be used as a standalone package, or as part of several projects in the JuliaDynamics organization, such as DynamicalSystems.jl or CausalityTools.jl.\n\nTo install it, run import Pkg; Pkg.add(\"Entropies\").\n\n\n\n\n\n","category":"module"},{"location":"#API-and-terminology","page":"Entropies.jl","title":"API & terminology","text":"","category":"section"},{"location":"","page":"Entropies.jl","title":"Entropies.jl","text":"note: Note\nThe documentation here follows (loosely) chapter 5 of Nonlinear Dynamics, Datseris & Parlitz, Springer 2022.","category":"page"},{"location":"","page":"Entropies.jl","title":"Entropies.jl","text":"In the literature, the term \"entropy\" is used (and abused) in multiple contexts. The API and documentation of Entropies.jl aim to clarify some aspects of its usage, and to provide a simple way to obtain probabilities, entropies, or other complexity measures.","category":"page"},{"location":"#Probabilities","page":"Entropies.jl","title":"Probabilities","text":"","category":"section"},{"location":"","page":"Entropies.jl","title":"Entropies.jl","text":"Entropies and other complexity measures are typically computed based on probability distributions. These are obtained from Input data for Entropies.jl in a plethora of different ways. The central API function that returns a probability distribution (in fact, just a vector of probabilities) is probabilities, which takes in a subtype of ProbabilitiesEstimator to specify how the probabilities are computed. All estimators available in Entropies.jl can be found in the estimators page.","category":"page"},{"location":"#Entropies","page":"Entropies.jl","title":"Entropies","text":"","category":"section"},{"location":"","page":"Entropies.jl","title":"Entropies.jl","text":"Entropy is an established concept in statistics, information theory, and nonlinear dynamics. However it is also an umbrella term that may mean several computationally different quantities. In Entropies.jl, we provide the generic function entropy that tries to both clarify the disparate \"entropy concepts\", while unifying them under a common interface that highlights the modular nature of the word \"entropy\".","category":"page"},{"location":"","page":"Entropies.jl","title":"Entropies.jl","text":"Most of the time, computing an entropy boils down to two simple steps: first estimating a probability distribution, and then applying one of the so-called \"generalized entropy\" formulas to the distributions. Thus, any of the implemented probabilities estimators can be used to compute generalized entropies.","category":"page"},{"location":"","page":"Entropies.jl","title":"Entropies.jl","text":"tip: There aren't many entropies, really.\nA crucial thing to clarify is that many quantities that are named as entropies (e.g., permutation entropy entropy_permutation, wavelet entropy entropy_wavelet, etc.), are not really new entropies. They are new probabilities estimators. They simply devise a new way to calculate probabilities from data, and then plug those probabilities into formal entropy formulas such as the Shannon entropy. The probabilities estimators are smartly created so that they elegantly highlight important aspects of the data relevant to complexity.These names are common place, and so in Entropies.jl we provide convenience functions like entropy_wavelet. However, it should be noted that these functions really aren't anything more than 2-lines-of-code wrappers that call entropy with the appropriate ProbabilitiesEstimator.There are only a few exceptions to this rule, which are quantities that are able to compute Shannon entropies via alternate means, without explicitly computing some probability distributions. These are IndirectEntropy instances, such as Kraskov.","category":"page"},{"location":"#Complexity-measures","page":"Entropies.jl","title":"Complexity measures","text":"","category":"section"},{"location":"","page":"Entropies.jl","title":"Entropies.jl","text":"Other complexity measures, which strictly speaking don't compute entropies, and may or may not explicitly compute probability distributions, appear in the Complexity measures section.","category":"page"},{"location":"#Input-data-for-Entropies.jl","page":"Entropies.jl","title":"Input data for Entropies.jl","text":"","category":"section"},{"location":"","page":"Entropies.jl","title":"Entropies.jl","text":"The input data type typically depend on the probability estimator chosen. In general though, the standard DynamicalSystems.jl approach is taken and as such we have three types of input data:","category":"page"},{"location":"","page":"Entropies.jl","title":"Entropies.jl","text":"Timeseries, which are AbstractVector{<:Real}, used in e.g. with WaveletOverlap.\nMulti-dimensional timeseries, or datasets, or state space sets, which are Dataset, used e.g. with NaiveKernel.\nSpatial data, which are higher dimensional standard Arrays, used e.g. with  SpatialSymbolicPermutation.","category":"page"},{"location":"utils/#Utility-methods","page":"Utility methods","title":"Utility methods","text":"","category":"section"},{"location":"utils/#Fast-histograms","page":"Utility methods","title":"Fast histograms","text":"","category":"section"},{"location":"utils/","page":"Utility methods","title":"Utility methods","text":"Entropies.binhist","category":"page"},{"location":"utils/#Symbolization","page":"Utility methods","title":"Symbolization","text":"","category":"section"},{"location":"utils/","page":"Utility methods","title":"Utility methods","text":"symbolize","category":"page"},{"location":"utils/#Entropies.symbolize","page":"Utility methods","title":"Entropies.symbolize","text":"symbolize(x, scheme::SymbolizationScheme) → Vector{Int}\nsymbolize!(s, x, scheme::SymbolizationScheme) → Vector{Int}\n\nSymbolize x using the provided symbolization scheme, optionally writing symbols into the pre-allocated symbol vector s if the scheme allows for it. For usage examples, see individual symbolization scheme docstrings.\n\nThe following symbolization schemes are currently implemented:\n\nOrdinalPattern.\nGaussianSymbolization.\nRectangularBinEncoder.\n\n\n\n\n\n","category":"function"},{"location":"utils/#Symbolization-schemes","page":"Utility methods","title":"Symbolization schemes","text":"","category":"section"},{"location":"utils/","page":"Utility methods","title":"Utility methods","text":"GaussianSymbolization\nOrdinalPattern\nRectangularBinEncoder","category":"page"},{"location":"utils/#Entropies.GaussianSymbolization","page":"Utility methods","title":"Entropies.GaussianSymbolization","text":"GaussianSymbolization(; c::Int = 3)\n\nA symbolization scheme where the elements of x are symbolized into c distinct integer categories using the normal cumulative distribution function (NCDF).\n\nAlgorithm\n\nAssume we have a univariate time series X = x_i_i=1^N. GaussianSymbolization first maps each x_i to a new real number y_i in 0 1 by using the normal cumulative distribution function (CDF), x_i to y_i  y_i = dfrac1 sigma     sqrt2 pi int_-infty^x_i e^(-(x_i - mu)^2)(2 sigma^2) dx, where mu and sigma are the empirical mean and standard deviation of X.\n\nNext, each y_i is linearly mapped to an integer z_i in 1 2 ldots c using the map y_i to z_i  z_i = R(y_i(c-1) + 05), where R indicates rounding up to the nearest integer. This procedure subdivides the interval 0 1 into c different subintervals that form a covering of 0 1, and assigns each y_i to one of these subintervals. The original time series X is thus transformed to a symbol time series S =  s_i _i=1^N, where s_i in 1 2 ldots c.\n\nUsage\n\nsymbolize(x::AbstractVector, s::GaussianSymbolization)\n\nMap the elements of x to a symbol time series according to the Gaussian symbolization scheme s.\n\nExamples\n\njulia> x = [0.1, 0.4, 0.7, -2.1, 8.0, 0.9, -5.2];\n\njulia> Entropies.symbolize(x, GaussianSymbolization(c = 5))\n7-element Vector{Int64}:\n 3\n 3\n 3\n 2\n 5\n 3\n 1\n\nSee also: symbolize.\n\n\n\n\n\n","category":"type"},{"location":"utils/#Entropies.OrdinalPattern","page":"Utility methods","title":"Entropies.OrdinalPattern","text":"OrdinalPattern(m = 3, τ = 1; lt = est.lt)\n\nA symbolization scheme that converts the input time series to ordinal patterns, which are then encoded to integers using encode_motif.\n\nnote: Note\nOrdinalPattern is intended for symbolizing time series. If providing a short vector, say x = [2, 5, 2, 1, 3, 4], then symbolize(x, OrdinalPattern(m = 2, τ = 1) will first embed x, then encode/symbolize each resulting state vector, not the original input. For symbolizing a single vector, use sortperm on it and use encode_motif on the resulting permutation indices.\n\nUsage\n\nsymbolize(x, scheme::OrdinalPattern) → Vector{Int}\nsymbolize!(s, x, scheme::OrdinalPattern) → Vector{Int}\n\nIf applied to an m-dimensional Dataset x, then m and τ are ignored, and m-dimensional permutation patterns are obtained directly for each xᵢ ∈ x. Permutation patterns are then encoded as integers using encode_motif. Optionally, symbols can be written directly into a pre-allocated integer vector s, where length(s) == length(x) using symbolize!.\n\nIf applied to a univariate vector x, then x is first converted to a delay reconstruction using embedding dimension m and lag τ. Permutation patterns are then computed for each of the resulting m-dimensional xᵢ ∈ x, and each permutation is then encoded as an integer using encode_motif. If using the in-place variant with univariate input, s must obey length(s) == length(x)-(est.m-1)*est.τ.\n\nExamples\n\nusing DelayEmbeddings, Entropies\nD = Dataset([rand(7) for i = 1:1000])\ns = symbolize(D, OrdinalPattern())\n\nSee also: symbolize.\n\n\n\n\n\n","category":"type"},{"location":"utils/#Entropies.RectangularBinEncoder","page":"Utility methods","title":"Entropies.RectangularBinEncoder","text":"RectangularBinEncoder(x, binning::RectangularBinning) <: SymbolizationScheme\n\nFind the minima along each dimension, and compute appropriate edge lengths for each dimension of x given a rectangular binning. Put them in an RectangularBinEncoder that can be then used to map points into bins via symbolize.\n\n\n\n\n\n","category":"type"},{"location":"utils/#Encoding-ordinal-patterns","page":"Utility methods","title":"Encoding ordinal patterns","text":"","category":"section"},{"location":"utils/","page":"Utility methods","title":"Utility methods","text":"Entropies.encode_motif","category":"page"},{"location":"utils/#Entropies.encode_motif","page":"Utility methods","title":"Entropies.encode_motif","text":"encode_motif(x, m::Int = length(x)) → s::Int\n\nEncode the length-m motif x (a vector of indices that would sort some vector v in ascending order) into its unique integer symbol s in 1 2 ldots m - 1 , using Algorithm 1 in Berger et al. (2019)[Berger2019].\n\nExample\n\nv = rand(5)\n\n# The indices that would sort `v` in ascending order. This is now a permutation\n# of the index permutation (1, 2, ..., 5)\nx = sortperm(v)\n\n# Encode this permutation as an integer.\nencode_motif(x)\n\n[Berger2019]: Berger, Sebastian, et al. \"Teaching Ordinal Patterns to a Computer: Efficient Encoding Algorithms Based on the Lehmer Code.\" Entropy 21.10 (2019): 1023.\n\n\n\n\n\n","category":"function"},{"location":"utils/#Alphabet-length","page":"Utility methods","title":"Alphabet length","text":"","category":"section"},{"location":"utils/","page":"Utility methods","title":"Utility methods","text":"alphabet_length","category":"page"},{"location":"utils/#Entropies.alphabet_length","page":"Utility methods","title":"Entropies.alphabet_length","text":"alphabet_length(x::Array_or_Dataset, est::ProbabilitiesEstimator) → Int\n\nReturn the total number of possible symbols/states implied by estimator for a given x. For some estimators, this total number is independent of x, in which case the input x is ignored and the method alphabet_length(est) is called.\n\nIf the total number of states cannot be known a priori, an error is thrown. Primarily used in entropy_normalized.\n\nExamples\n\njulia> est = SymbolicPermutation(m = 4);\n\njulia> alphabet_length(rand(42), est) # same as `factorial(m)` for any `x`\n24\n\n\n\n\n\n","category":"function"}]
}
