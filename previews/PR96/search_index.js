var documenterSearchIndex = {"docs":
[{"location":"entropies/#Entropies","page":"Entropies","title":"Entropies","text":"","category":"section"},{"location":"entropies/#Rényi-(generalized)-entropy","page":"Entropies","title":"Rényi (generalized) entropy","text":"","category":"section"},{"location":"entropies/","page":"Entropies","title":"Entropies","text":"entropy_renyi","category":"page"},{"location":"entropies/#Entropies.entropy_renyi","page":"Entropies","title":"Entropies.entropy_renyi","text":"entropy_renyi(p::Probabilities; q = 1.0, base = MathConstants.e)\n\nCompute the Rényi[Rényi1960] generalized order-q entropy of some probabilities (typically returned by the probabilities function).\n\nentropy_renyi(x::Array_or_Dataset, est; q = 1.0, base)\n\nA convenience syntax, which calls first probabilities(x, est) and then calculates the entropy of the result (and thus est can be anything the probabilities function accepts).\n\nDescription\n\nLet p be an array of probabilities (summing to 1). Then the Rényi generalized entropy is\n\nH_q(p) = frac11-q log left(sum_i pi^qright)\n\nand generalizes other known entropies, like e.g. the information entropy (q = 1, see [Shannon1948]), the maximum entropy (q=0, also known as Hartley entropy), or the correlation entropy (q = 2, also known as collision entropy).\n\n[Rényi1960]: A. Rényi, Proceedings of the fourth Berkeley Symposium on Mathematics, Statistics and Probability, pp 547 (1960)\n\n[Kumar1986]: Kumar, U., Kumar, V., & Kapur, J. N. (1986). Normalized measures of entropy. International Journal Of General System, 12(1), 55-69.\n\n[Shannon1948]: C. E. Shannon, Bell Systems Technical Journal 27, pp 379 (1948)\n\nSee also: maxentropy_renyi.\n\n\n\n\n\n","category":"function"},{"location":"entropies/#Tsallis-(generalized)-entropy","page":"Entropies","title":"Tsallis (generalized) entropy","text":"","category":"section"},{"location":"entropies/","page":"Entropies","title":"Entropies","text":"entropy_tsallis","category":"page"},{"location":"entropies/#Entropies.entropy_tsallis","page":"Entropies","title":"Entropies.entropy_tsallis","text":"entropy_tsallis(p::Probabilities; k = 1, q = 0, base = MathConstants.e)\n\nCompute the Tsallis entropy of p (Tsallis, 1998)[Tsallis1988].\n\nbase only applies in the limiting case q == 1, in which the Tsallis entropy reduces to Shannon entropy.\n\nentropy_tsallis(x::Array_or_Dataset, est; k = 1, q = 0, base = MathConstants.e)\n\nA convenience syntax, which calls first probabilities(x, est) and then calculates the Tsallis entropy of the result (and thus est can be anything the probabilities function accepts).\n\nDescription\n\nThe Tsallis entropy is a generalization of the Boltzmann-Gibbs entropy, with k standing for the Boltzmann constant. It is defined as\n\nS_q(p) = frackq - 1left(1 - sum_i pi^qright)\n\n[Tsallis1988]: Tsallis, C. (1988). Possible generalization of Boltzmann-Gibbs statistics. Journal of statistical physics, 52(1), 479-487.\n\n\n\n\n\n","category":"function"},{"location":"entropies/#Shannon-entropy-(convenience)","page":"Entropies","title":"Shannon entropy (convenience)","text":"","category":"section"},{"location":"entropies/","page":"Entropies","title":"Entropies","text":"entropy_shannon","category":"page"},{"location":"entropies/#Entropies.entropy_shannon","page":"Entropies","title":"Entropies.entropy_shannon","text":"entropy_shannon(args...; base = MathConstants.e)\n\nEquivalent to entropy_renyi(args...; base = base, q = 1) and provided solely for convenience. Computes the Shannon entropy, given by\n\nH(p) = - sum_i pi log(pi)\n\nSee also: maxentropy_shannon.\n\n\n\n\n\n","category":"function"},{"location":"entropies/#Normalization","page":"Entropies","title":"Normalization","text":"","category":"section"},{"location":"entropies/","page":"Entropies","title":"Entropies","text":"The generic entropy_normalized normalizes any entropy value to the entropy of a uniform distribution. We also provide maximum entropy functions that are useful for manual normalization.","category":"page"},{"location":"entropies/","page":"Entropies","title":"Entropies","text":"entropy_normalized","category":"page"},{"location":"entropies/#Entropies.entropy_normalized","page":"Entropies","title":"Entropies.entropy_normalized","text":"entropy_normalized(f::Function, x, est::ProbabilitiesEstimator, args...; kwargs...)\n\nConvenience syntax for normalizing to the entropy of uniform probability distribution. First estimates probabilities as p::Probabilities = f(x, est, args...; kwargs...), then calls entropy_normalized(f, p, args...; kwargs...).\n\nNormalization is only defined for estimators for which alphabet_length is defined, meaning that the total number of states or symbols is known beforehand.\n\nentropy_normalized(f::Function, p::Probabilities, est::ProbabilitiesEstimator, args...;\n    kwargs...)\n\nNormalize the entropy, as returned by the entropy function f called with the given arguments (i.e. f(p, args...; kwargs...)), to the entropy of a uniform distribution, inferring alphabet_length from est.\n\nentropy_normalized(f::Function, p::Probabilities, args...; kwargs...)\n\nThe same as above, but infers alphabet length from counting how many elements are in p (zero probabilities are counted).\n\nExamples\n\nComputing normalized entropy from scratch:\n\nx = rand(100)\nentropy_normalized(entropy_renyi, x, Dispersion())\n\nComputing normalized entropy from pre-computed probabilities with known parameters:\n\nx = rand(100)\nest = Dispersion(m = 3, symbolization = GaussianSymbolization(c = 4))\np = probabilities(x, est)\nentropy_normalized(entropy_renyi, p, est)\n\nComputing normalized entropy, assumming there are N = 10 total states:\n\nN = 10\np = Probabilities(rand(10))\nentropy_normalized(entropy_renyi, p, est)\n\nnote: Normalized output range\nFor Rényi entropy (e.g. Kumar et al., 1986), and for Tsallis entropy (Tsallis, 1998), normalizing to the uniform distribution ensures that the entropy lies in the interval [0, 1]. For other entropies and parameter choices, the resulting entropy is not guaranteed to lie in [0, 1]. It is up to the user to decide whether normalizing to a uniform distribution makes sense for their use case.\n\n[Kumar1986]: Kumar, U., Kumar, V., & Kapur, J. N. (1986). Normalized measures of entropy. International Journal Of General System, 12(1), 55-69.\n\n[Tsallis1998]: Tsallis, C. (1988). Possible generalization of Boltzmann-Gibbs statistics. Journal of statistical physics, 52(1), 479-487.\n\n\n\n\n\n","category":"function"},{"location":"entropies/#Indirect-entropies","page":"Entropies","title":"Indirect entropies","text":"","category":"section"},{"location":"entropies/","page":"Entropies","title":"Entropies","text":"Here we list functions which compute Shannon entropies via alternate means, without explicitly computing some probability distributions and then using the Shannon formulat.","category":"page"},{"location":"entropies/#Nearest-neighbors-entropy","page":"Entropies","title":"Nearest neighbors entropy","text":"","category":"section"},{"location":"entropies/","page":"Entropies","title":"Entropies","text":"entropy_kraskov\nentropy_kozachenkoleonenko","category":"page"},{"location":"entropies/#Entropies.entropy_kraskov","page":"Entropies","title":"Entropies.entropy_kraskov","text":"entropy_kraskov(x::AbstractDataset{D, T}; k::Int = 1, w::Int = 0,\n    base::Real = MathConstants.e) where {D, T}\n\nEstimate Shannon entropy to the given base using k-th nearest neighbor searches (Kraskov, 2004)[Kraskov2004].\n\nw is the Theiler window, which determines if temporal neighbors are excluded during neighbor searches (defaults to 0, meaning that only the point itself is excluded when searching for neighbours).\n\nSee also: entropy_kozachenkoleonenko.\n\n[Kraskov2004]: Kraskov, A., Stögbauer, H., & Grassberger, P. (2004). Estimating mutual information. Physical review E, 69(6), 066138.\n\n\n\n\n\n","category":"function"},{"location":"entropies/#Entropies.entropy_kozachenkoleonenko","page":"Entropies","title":"Entropies.entropy_kozachenkoleonenko","text":"entropy_kozachenkoleonenko(x::AbstractDataset{D, T}; k::Int = 1, w::Int = 0,\n    base::Real = MathConstants.e) where {D, T}\n\nEstimate Shannon entropy to the given base using k-th nearest neighbor searches, using the method from Kozachenko & Leonenko (1987)[KozachenkoLeonenko1987], as described in Charzyńska and Gambin (2016)[Charzyńska2016].\n\nw is the Theiler window, which determines if temporal neighbors are excluded during neighbor searches (defaults to 0, meaning that only the point itself is excluded when searching for neighbours).\n\nSee also: entropy_kraskov.\n\n[Charzyńska2016]: Charzyńska, A., & Gambin, A. (2016). Improvement of the k-NN entropy estimator with applications in systems biology. Entropy, 18(1), 13.\n\n[KozachenkoLeonenko1987]: Kozachenko, L. F., & Leonenko, N. N. (1987). Sample estimate of the entropy of a random vector. Problemy Peredachi Informatsii, 23(2), 9-16.\n\n\n\n\n\n","category":"function"},{"location":"entropies/#Convenience-functions","page":"Entropies","title":"Convenience functions","text":"","category":"section"},{"location":"entropies/","page":"Entropies","title":"Entropies","text":"In this subsection we expand documentation strings of \"entropy names\" that are used commonly in the literature, such as \"permutation entropy\". As we made clear in API & terminology, these are just the existing Shannon/Rényi/Tsallis entropy with a particularly chosen probability estimator. We have only defined convenience functions for the most used names, and arbitrary more specialized convenience functions can be easily defined in a couple lines of code.","category":"page"},{"location":"entropies/","page":"Entropies","title":"Entropies","text":"entropy_permutation\nentropy_spatial_permutation\nentropy_wavelet\nentropy_dispersion","category":"page"},{"location":"entropies/#Entropies.entropy_permutation","page":"Entropies","title":"Entropies.entropy_permutation","text":"entropy_permutation(x; τ = 1, m = 3, base = MathConstants.e)\n\nCompute the permutation entropy of order m with delay/lag τ. This function is just a convenience call to:\n\nest = SymbolicPermutation(; m, τ)\nentropy_shannon(x, est; base)\n\nSee SymbolicPermutation for more info. Similarly, one can use SymbolicWeightedPermutation or SymbolicAmplitudeAwarePermutation for the weighted/amplitude-aware versions.\n\n\n\n\n\n","category":"function"},{"location":"entropies/#Entropies.entropy_spatial_permutation","page":"Entropies","title":"Entropies.entropy_spatial_permutation","text":"entropy_spatial_permutation(x, stencil, periodic = true; kwargs...)\n\nCompute the spatial permutation entropy of x given the stencil. Here x must be a matrix or higher dimensional Array containing spatial data. This function is just a convenience call to:\n\nest = SpatialSymbolicPermutation(stencil, x, periodic)\nentropy_shannon(x, est; kwargs...)\n\nSee SpatialSymbolicPermutation for more info, or how to encode stencils.\n\n\n\n\n\n","category":"function"},{"location":"entropies/#Entropies.entropy_wavelet","page":"Entropies","title":"Entropies.entropy_wavelet","text":"entropy_wavelet(x; wavelet = Wavelets.WT.Daubechies{12}(), base = MathConstants.e)\n\nCompute the wavelet entropy. This function is just a convenience call to:\n\nest = WaveletOverlap(wavelet)\nentropy_shannon(x, est; base)\n\nSee WaveletOverlap for more info.\n\n\n\n\n\n","category":"function"},{"location":"entropies/#Entropies.entropy_dispersion","page":"Entropies","title":"Entropies.entropy_dispersion","text":"entropy_dispersion(x; m = 2, τ = 1, s = GaussianSymbolization(3),\n    base = MathConstants.e)\n\nCompute the dispersion entropy. This function is just a convenience call to:\n\nest = Dispersion(m = m, τ = τ, s = s)\nentropy_shannon(x, est; base)\n\nSee Dispersion for more info.\n\n\n\n\n\n","category":"function"},{"location":"probabilities/#estimators","page":"Probabilities","title":"Probabilities","text":"","category":"section"},{"location":"probabilities/","page":"Probabilities","title":"Probabilities","text":"For categorical or integer-valued data, probabilities can be estimated by directly counting relative frequencies of data elements. For such data, use probabilities(x::Array_or_Dataset) → p::Probabilities.","category":"page"},{"location":"probabilities/","page":"Probabilities","title":"Probabilities","text":"More advanced estimators computing probabilities by first either discretizing, symbolizing or transforming the data in a way that quantifies some useful properties about the underlying data (e.g. visitation frequencies, wavelet energies, or permutation patterns), from which probability distributions can be estimated. Use probabilities(x::Array_or_Dataset, est::ProbabilitiesEstimator) in combination with any of the estimators listed below.","category":"page"},{"location":"probabilities/","page":"Probabilities","title":"Probabilities","text":"probabilities\nprobabilities!\nProbabilities\nProbabilitiesEstimator","category":"page"},{"location":"probabilities/#Entropies.probabilities","page":"Probabilities","title":"Entropies.probabilities","text":"probabilities(x::Array_or_Dataset) → p::Probabilities\n\nDirectly count probabilities from the elements of x without any discretization, binning, or other processing (mostly useful when x contains categorical or integer data). probabilities always returns a Probabilities container (Vector-like).\n\nprobabilities(x::Array_or_Dataset, est::ProbabilitiesEstimator) → p::Probabilities\n\nCalculate probabilities representing x based on the provided estimator. The probabilities are typically unordered and may or may not contain 0s, see the documentation of the individual estimators for more. Configuration options are always given as arguments to the chosen estimator.\n\nprobabilities(x::Array_or_Dataset, ε::AbstractFloat) → p::Probabilities\n\nConvenience syntax which provides probabilities for x based on rectangular binning (i.e. performing a histogram). In short, the state space is divided into boxes of length ε, and formally we use est = VisitationFrequency(RectangularBinning(ε)) as an estimator, see VisitationFrequency.\n\nThis method has a linearithmic time complexity (n log(n) for n = length(x)) and a linear space complexity (l for l = dimension(x)). This allows computation of probabilities (histograms) of high-dimensional datasets and with small box sizes ε without memory overflow and with maximum performance. To obtain the bin information along with p, use binhist.\n\nprobabilities(x::Array_or_Dataset, n::Integer) → p::Probabilities\n\nSame as the above method, but now each dimension of the data is binned into n::Int equal sized bins instead of bins of length ε::AbstractFloat.\n\n\n\n\n\n","category":"function"},{"location":"probabilities/#Entropies.probabilities!","page":"Probabilities","title":"Entropies.probabilities!","text":"probabilities!(args...)\n\nIdentical to probabilities(args...), but allows pre-allocation of temporarily used containers.\n\nOnly works for certain estimators. See for example SymbolicPermutation.\n\n\n\n\n\n","category":"function"},{"location":"probabilities/#Entropies.Probabilities","page":"Probabilities","title":"Entropies.Probabilities","text":"Probabilities(x) → p\n\nA simple wrapper type around an x::AbstractVector which ensures that p sums to 1. Behaves identically to Vector.\n\n\n\n\n\n","category":"type"},{"location":"probabilities/#Entropies.ProbabilitiesEstimator","page":"Probabilities","title":"Entropies.ProbabilitiesEstimator","text":"An abstract type for probabilities estimators.\n\n\n\n\n\n","category":"type"},{"location":"probabilities/#Count-occurrences-(counting)","page":"Probabilities","title":"Count occurrences (counting)","text":"","category":"section"},{"location":"probabilities/","page":"Probabilities","title":"Probabilities","text":"CountOccurrences","category":"page"},{"location":"probabilities/#Entropies.CountOccurrences","page":"Probabilities","title":"Entropies.CountOccurrences","text":"CountOccurrences\n\nA probabilities/entropy estimator based on straight-forward counting of distinct elements in a univariate time series or multivariate dataset. This is the same as giving no estimator to probabilities.\n\n\n\n\n\n","category":"type"},{"location":"probabilities/#Permutation-(symbolic)","page":"Probabilities","title":"Permutation (symbolic)","text":"","category":"section"},{"location":"probabilities/","page":"Probabilities","title":"Probabilities","text":"SymbolicPermutation\nSpatialSymbolicPermutation","category":"page"},{"location":"probabilities/#Entropies.SymbolicPermutation","page":"Probabilities","title":"Entropies.SymbolicPermutation","text":"SymbolicPermutation(; τ = 1, m = 3, lt = Entropies.isless_rand) <: ProbabilityEstimator\nSymbolicWeightedPermutation(; τ = 1, m = 3, lt = Entropies.isless_rand) <: ProbabilityEstimator\nSymbolicAmplitudeAwarePermutation(; τ = 1, m = 3, A = 0.5, lt = Entropies.isless_rand) <: ProbabilityEstimator\n\nSymbolic, permutation-based probabilities/entropy estimators. m is the permutation order (or the symbol size or the embedding dimension) and τ is the delay time (or lag). They are used to define the permutation entropies[BandtPompe2002].\n\nRepeated values during symbolization\n\nIn the original implementation of permutation entropy [BandtPompe2002], equal values are ordered after their order of appearance, but this can lead to erroneous temporal correlations, especially for data with low-amplitude resolution [Zunino2017]. Here, we resolve this issue by letting the user provide a custom \"less-than\" function. The keyword lt accepts a function that decides which of two state vector elements are smaller. If two elements are equal, the default behaviour is to randomly assign one of them as the largest (lt = Entropies.isless_rand). For data with low amplitude resolution, computing probabilities multiple times using the random approach may reduce these erroneous effects.\n\nTo get the behaviour described in Bandt and Pompe (2002), use lt = Base.isless).\n\nProperties of original signal preserved\n\nSymbolicPermutation: Preserves ordinal patterns of state vectors (sorting information). This   implementation is based on Bandt & Pompe et al. (2002)[BandtPompe2002] and   Berger et al. (2019) [Berger2019].\nSymbolicWeightedPermutation: Like SymbolicPermutation, but also encodes amplitude   information by tracking the variance of the state vectors. This implementation is based   on Fadlallah et al. (2013)[Fadlallah2013].\nSymbolicAmplitudeAwarePermutation: Like SymbolicPermutation, but also encodes   amplitude information by considering a weighted combination of absolute amplitudes   of state vectors, and relative differences between elements of state vectors. See   description below for explanation of the weighting parameter A. This implementation   is based on Azami & Escudero (2016) [Azami2016].\n\nProbability estimation\n\nUnivariate time series\n\nTo estimate probabilities or entropies from univariate time series, use the following methods:\n\nprobabilities(x::AbstractVector, est::SymbolicProbabilityEstimator). Constructs state vectors   from x using embedding lag τ and embedding dimension m, symbolizes state vectors,   and computes probabilities as (weighted) relative frequency of symbols.\nentropy_renyi(x::AbstractVector, est::SymbolicProbabilityEstimator; α=1, base = 2) computes   probabilities by calling probabilities(x::AbstractVector, est),   then computer the order-α generalized entropy to the given base.\n\nSpeeding up repeated computations\n\nA pre-allocated integer symbol array s can be provided to save some memory allocations if the probabilities are to be computed for multiple data sets.\n\nNote: it is not the array that will hold the final probabilities that is pre-allocated, but the temporary integer array containing the symbolized data points. Thus, if provided, it is required that length(x) == length(s) if x is a Dataset, or length(s) == length(x) - (m-1)τ if x is a univariate signal that is to be embedded first.\n\nUse the following signatures (only works for SymbolicPermutation).\n\nprobabilities!(s::Vector{Int}, x::AbstractVector, est::SymbolicPermutation) → ps::Probabilities\nprobabilities!(s::Vector{Int}, x::AbstractDataset, est::SymbolicPermutation) → ps::Probabilities\n\nMultivariate datasets\n\nAlthough not dealt with in the original paper describing the estimators, numerically speaking, permutation entropies can also be computed for multivariate datasets with dimension ≥ 2 (but see caveat below). Such datasets may be, for example, preembedded time series. Then, just skip the delay reconstruction step, compute and symbols directly from the L existing state vectors mathbfx_1 mathbfx_2 ldots mathbfx_L.\n\nprobabilities(x::AbstractDataset, est::SymbolicProbabilityEstimator). Compute ordinal patterns of the   state vectors of x directly (without doing any embedding), symbolize those patterns,   and compute probabilities as (weighted) relative frequencies of symbols.\nentropy_renyi(x::AbstractDataset, est::SymbolicProbabilityEstimator). Computes probabilities from   symbol frequencies using probabilities(x::AbstractDataset, est::SymbolicProbabilityEstimator),   then computes the order-α generalized (permutation) entropy to the given base.\n\nCaveat: A dynamical interpretation of the permutation entropy does not necessarily hold if computing it on generic multivariate datasets. Method signatures for Datasets are provided for convenience, and should only be applied if you understand the relation between your input data, the numerical value for the permutation entropy, and its interpretation.\n\nDescription\n\nAll symbolic estimators use the same underlying approach to estimating probabilities.\n\nEmbedding, ordinal patterns and symbolization\n\nConsider the n-element univariate time series x(t) = x_1 x_2 ldots x_n. Let mathbfx_i^m tau = x_j x_j+tau ldots x_j+(m-1)tau for j = 1 2 ldots n - (m-1)tau be the i-th state vector in a delay reconstruction with embedding dimension m and reconstruction lag tau. There are then N = n - (m-1)tau state vectors.\n\nFor an m-dimensional vector, there are m possible ways of sorting it in ascending order of magnitude. Each such possible sorting ordering is called a motif. Let pi_i^m tau denote the motif associated with the m-dimensional state vector mathbfx_i^m tau, and let R be the number of distinct motifs that can be constructed from the N state vectors. Then there are at most R motifs; R = N precisely when all motifs are unique, and R = 1 when all motifs are the same.\n\nEach unique motif pi_i^m tau can be mapped to a unique integer symbol 0 leq s_i leq M-1. Let S(pi)  mathbbR^m to mathbbN_0 be the function that maps the motif pi to its symbol s, and let Pi denote the set of symbols Pi =  s_i _iin  1 ldots R.\n\nProbability computation\n\nSymbolicPermutation\n\nThe probability of a given motif is its frequency of occurrence, normalized by the total number of motifs (with notation from [Fadlallah2013]),\n\np(pi_i^m tau) = dfracsum_k=1^N mathbf1_uS(u) = s_i left(mathbfx_k^m tau right) sum_k=1^N mathbf1_uS(u) in Pi left(mathbfx_k^m tau right) = dfracsum_k=1^N mathbf1_uS(u) = s_i left(mathbfx_k^m tau right) N\n\nwhere the function mathbf1_A(u) is the indicator function of a set A. That     is, mathbf1_A(u) = 1 if u in A, and mathbf1_A(u) = 0 otherwise.\n\nSymbolicAmplitudeAwarePermutation\n\nAmplitude-aware permutation entropy is computed analogously to regular permutation entropy but probabilities are weighted by amplitude information as follows.\n\np(pi_i^m tau) = dfracsum_k=1^N mathbf1_uS(u) = s_i left( mathbfx_k^m tau right)  a_ksum_k=1^N mathbf1_uS(u) in Pi left( mathbfx_k^m tau right) a_k = dfracsum_k=1^N mathbf1_uS(u) = s_i left( mathbfx_k^m tau right)  a_ksum_k=1^N a_k\n\nThe weights encoding amplitude information about state vector mathbfx_i = (x_1^i x_2^i ldots x_m^i) are\n\na_i = dfracAm sum_k=1^m x_k^i  + dfrac1-Ad-1 sum_k=2^d x_k^i - x_k-1^i\n\nwith 0 leq A leq 1. When A=0 , only internal differences between the elements of mathbfx_i are weighted. Only mean amplitude of the state vector elements are weighted when A=1. With, 0A1, a combined weighting is used.\n\nSymbolicWeightedPermutation\n\nWeighted permutation entropy is also computed analogously to regular permutation entropy, but adds weights that encode amplitude information too:\n\np(pi_i^m tau) = dfracsum_k=1^N mathbf1_uS(u) = s_i\nleft( mathbfx_k^m tau right)\n w_ksum_k=1^N mathbf1_uS(u) in Pi\nleft( mathbfx_k^m tau right) w_k = dfracsum_k=1^N\nmathbf1_uS(u) = s_i\nleft( mathbfx_k^m tau right)  w_ksum_k=1^N w_k\n\nThe weighted permutation entropy is equivalent to regular permutation entropy when weights are positive and identical (w_j = beta  forall  j leq N and beta  0). Weights are dictated by the variance of the state vectors.\n\nLet the aritmetic mean of state vector mathbfx_i be denoted by\n\nmathbfhatx_j^m tau = frac1m sum_k=1^m x_j + (k+1)tau\n\nWeights are then computed as\n\nw_j = dfrac1msum_k=1^m (x_j+(k+1)tau - mathbfhatx_j^m tau)^2\n\nNote: in equation 7, section III, of the original paper, the authors write\n\nw_j = dfrac1msum_k=1^m (x_j-(k-1)tau - mathbfhatx_j^m tau)^2\n\nBut given the formula they give for the arithmetic mean, this is not the variance of mathbfx_i, because the indices are mixed: x_j+(k-1)tau in the weights formula, vs. x_j+(k+1)tau in the arithmetic mean formula. This seems to imply that amplitude information about previous delay vectors are mixed with mean amplitude information about current vectors. The authors also mix the terms \"vector\" and \"neighboring vector\" (but uses the same notation for both), making it hard to interpret whether the sign switch is a typo or intended. Here, we use the notation above, which actually computes the variance for mathbfx_i.\n\n[BandtPompe2002]: Bandt, Christoph, and Bernd Pompe. \"Permutation entropy: a natural complexity measure for time series.\" Physical review letters 88.17 (2002): 174102.\n\n[Berger2019]: Berger, Sebastian, et al. \"Teaching Ordinal Patterns to a Computer: Efficient Encoding Algorithms Based on the Lehmer Code.\" Entropy 21.10 (2019): 1023.\n\n[Fadlallah2013]: Fadlallah, Bilal, et al. \"Weighted-permutation entropy: A complexity measure for time series incorporating amplitude information.\" Physical Review E 87.2 (2013): 022911.\n\n[Rényi1960]: A. Rényi, Proceedings of the fourth Berkeley Symposium on Mathematics, Statistics and Probability, pp 547 (1960)\n\n[Azami2016]: Azami, H., & Escudero, J. (2016). Amplitude-aware permutation entropy: Illustration in spike detection and signal segmentation. Computer methods and programs in biomedicine, 128, 40-51.\n\n[Fadlallah2013]: Fadlallah, Bilal, et al. \"Weighted-permutation entropy: A complexity measure for time series incorporating amplitude information.\" Physical Review E 87.2 (2013): 022911.\n\n[Zunino2017]: Zunino, L., Olivares, F., Scholkmann, F., & Rosso, O. A. (2017). Permutation entropy based time series analysis: Equalities in the input signal can lead to false conclusions. Physics Letters A, 381(22), 1883-1892.\n\n\n\n\n\n","category":"type"},{"location":"probabilities/#Entropies.SpatialSymbolicPermutation","page":"Probabilities","title":"Entropies.SpatialSymbolicPermutation","text":"SpatialSymbolicPermutation(stencil, x, periodic = true)\n\nA symbolic, permutation-based probabilities/entropy estimator for spatiotemporal systems. The data are a high-dimensional array x, such as 2D [Ribeiro2012] or 3D [Schlemmer2018]. This approach is also known as spatiotemporal permutation entropy. x is given because we need to know its size for optimization and bound checking.\n\nA stencil defines what local area around each pixel to consider, and compute the ordinal pattern within the stencil. Stencils are given as vectors of CartesianIndex which encode the offsets of the pixes to include in the stencil, with respect to the current pixel. For example\n\ndata = [rand(50, 50) for _ in 1:50]\nx = data[1] # first \"time slice\" of a spatial system evolution\nstencil = CartesianIndex.([(0,1), (1,1), (1,0)])\nest = SpatialSymbolicPermutation(stencil, x)\n\nHere the stencil creates a 2x2 square extending to the bottom and right of the pixel (directions here correspond to the way Julia prints matrices by default). Notice that no offset (meaning the pixel itself) is always included automatically. The length of the stencil decides the order of the permutation entropy, and the ordering within the stencil dictates the order that pixels are compared with. The pixel without any offset is always first in the order.\n\nAfter having defined est, one calculates the spatial permutation entropy by calling entropy_renyi with est, and with the array data. To apply this to timeseries of spatial data, simply loop over the call, e.g.:\n\nentropy = entropy_renyi(x, est)\nentropy_vs_time = entropy_renyi.(data, est) # broadcasting with `.`\n\nThe argument periodic decides whether the stencil should wrap around at the end of the array. If periodic = false, pixels whose stencil exceeds the array bounds are skipped.\n\n[Ribeiro2012]: Ribeiro et al. (2012). Complexity-entropy causality plane as a complexity measure for two-dimensional patterns. https://doi.org/10.1371/journal.pone.0040689\n\n[Schlemmer2018]: Schlemmer et al. (2018). Spatiotemporal Permutation Entropy as a Measure for Complexity of Cardiac Arrhythmia. https://doi.org/10.3389/fphy.2018.00039\n\n\n\n\n\n","category":"type"},{"location":"probabilities/#Dispersion-(symbolic)","page":"Probabilities","title":"Dispersion (symbolic)","text":"","category":"section"},{"location":"probabilities/","page":"Probabilities","title":"Probabilities","text":"Dispersion","category":"page"},{"location":"probabilities/#Entropies.Dispersion","page":"Probabilities","title":"Entropies.Dispersion","text":"Dispersion(; symbolization = GaussianSymbolization(c = 5), m = 2, τ = 1,\n    check_unique = true)\n\nA probability estimator based on dispersion patterns, originally used by Rostaghi & Azami, 2016[Rostaghi2016] to compute the \"dispersion entropy\", which characterizes the complexity and irregularity of a time series.\n\nRelative frequencies of dispersion patterns are computed using the symbolization scheme s with embedding dimension m and embedding delay τ. Recommended parameter values[Li2018] are m ∈ [2, 3], τ = 1 for the embedding, and c ∈ [3, 4, …, 8] categories for the Gaussian symbol mapping.\n\nDescription\n\nAssume we have a univariate time series X = x_i_i=1^N. First, this time series is symbolized using symbolization, which default to GaussianSymbolization, which uses the normal cumulative distribution function (CDF) for symbolization. Other choices of CDFs are also possible, but Entropies.jl currently only implements GaussianSymbolization, which was used in Rostaghi & Azami (2016). This step results in an integer-valued symbol time series S =  s_i _i=1^N, where s_i in 1 2 ldots c.\n\nNext, the symbol time series S is embedded into an m-dimensional time series, using an embedding lag of tau = 1, which yields a total of N - (m - 1)tau points, or \"dispersion patterns\". Because each z_i can take on c different values, and each embedding point has m values, there are c^m possible dispersion patterns. This number is used for normalization when computing dispersion entropy.\n\nComputing dispersion probabilities and entropy\n\nA probability distribution P = p_i _i=1^c^m, where sum_i^c^m p_i = 1, can then be estimated by counting and sum-normalising the distribution of dispersion patterns among the embedding vectors. Note that dispersion patterns that are not present are not counted. Therefore, you'll always get non-zero probabilities using the Dispersion probability estimator.\n\nTo compute dispersion entropy of order q to a given base on the univariate input time series x, do:\n\nentropy_renyi(x, Dispersion(), base = 2, q = 1)\n\nData requirements and parameters\n\nThe input must have more than one unique element for the Gaussian mapping to be well-defined. Li et al. (2018) recommends that x has at least 1000 data points.\n\nIf check_unique == true (default), then it is checked that the input has more than one unique value. If check_unique == false and the input only has one unique element, then a InexactError is thrown when trying to compute probabilities.\n\nSee also: entropy_dispersion, GaussianSymbolization.\n\nnote: Why 'dispersion patterns'?\nEach embedding vector is called a \"dispersion pattern\". Why? Let's consider the case when m = 5 and c = 3, and use some very imprecise terminology for illustration:When c = 3, values clustering far below mean are in one group, values clustered around the mean are in one group, and values clustering far above the mean are in a third group. Then the embedding vector 2 2 2 2 2 consists of values that are relatively close together (close to the mean), so it represents a set of numbers that are not very spread out (less dispersed). The embedding vector 1 1 2 3 3, however, represents numbers that are much more spread out (more dispersed), because the categories representing \"outliers\" both above and below the mean are represented, not only values close to the mean.\n\n[Rostaghi2016]: Rostaghi, M., & Azami, H. (2016). Dispersion entropy: A measure for time-series analysis. IEEE Signal Processing Letters, 23(5), 610-614.\n\n[Li2018]: Li, G., Guan, Q., & Yang, H. (2018). Noise reduction method of underwater acoustic signals based on CEEMDAN, effort-to-compress complexity, refined composite multiscale dispersion entropy and wavelet threshold denoising. Entropy, 21(1), 11.\n\n\n\n\n\n","category":"type"},{"location":"probabilities/#Visitation-frequency-(binning)","page":"Probabilities","title":"Visitation frequency (binning)","text":"","category":"section"},{"location":"probabilities/","page":"Probabilities","title":"Probabilities","text":"VisitationFrequency","category":"page"},{"location":"probabilities/#Entropies.VisitationFrequency","page":"Probabilities","title":"Entropies.VisitationFrequency","text":"VisitationFrequency(r::RectangularBinning) <: ProbabilitiesEstimator\n\nA probability estimator based on binning data into rectangular boxes dictated by the binning scheme r and then computing the frequencies of points in the bins.\n\nSee also: RectangularBinning.\n\n\n\n\n\n","category":"type"},{"location":"probabilities/#Specifying-binning/boxes","page":"Probabilities","title":"Specifying binning/boxes","text":"","category":"section"},{"location":"probabilities/","page":"Probabilities","title":"Probabilities","text":"RectangularBinning","category":"page"},{"location":"probabilities/#Entropies.RectangularBinning","page":"Probabilities","title":"Entropies.RectangularBinning","text":"RectangularBinning(ϵ) <: BinningScheme\n\nInstructions for creating a rectangular box partition using the binning scheme ϵ. Binning instructions are deduced from the type of ϵ.\n\nRectangular binnings may be automatically adjusted to the data in which the RectangularBinning is applied, as follows:\n\nϵ::Int divides each coordinate axis into ϵ equal-length intervals,  extending the upper bound 1/100th of a bin size to ensure all points are covered.\nϵ::Float64 divides each coordinate axis into intervals of fixed size ϵ, starting  from the axis minima until the data is completely covered by boxes.\nϵ::Vector{Int} divides the i-th coordinate axis into ϵ[i] equal-length  intervals, extending the upper bound 1/100th of a bin size to ensure all points are  covered.\nϵ::Vector{Float64} divides the i-th coordinate axis into intervals of fixed size ϵ[i], starting  from the axis minima until the data is completely covered by boxes.\n\nRectangular binnings may also be specified on arbitrary min-max ranges.\n\nϵ::Tuple{Vector{Tuple{Float64,Float64}},Int64} creates intervals  along each coordinate axis from ranges indicated by a vector of (min, max) tuples, then divides  each coordinate axis into an integer number of equal-length intervals. Note: this does not ensure  that all points are covered by the data (points outside the binning are ignored).\n\nExample 1: Grid deduced automatically from data (partition guaranteed to cover data points)\n\nFlexible box sizes\n\nThe following binning specification finds the minima/maxima along each coordinate axis, then split each of those data ranges (with some tiny padding on the edges) into 10 equal-length intervals. This gives (hyper-)rectangular boxes, and works for data of any dimension.\n\nusing Entropies\nRectangularBinning(10)\n\nNow, assume the data consists of 2-dimensional points, and that we want a finer grid along one of the dimensions than over the other dimension.\n\nThe following binning specification finds the minima/maxima along each coordinate axis, then splits the range along the first coordinate axis (with some tiny padding on the edges) into 10 equal-length intervals, and the range along the second coordinate axis (with some tiny padding on the edges) into 5 equal-length intervals. This gives (hyper-)rectangular boxes.\n\nusing Entropies\nRectangularBinning([10, 5])\n\nFixed box sizes\n\nThe following binning specification finds the minima/maxima along each coordinate axis, then split the axis ranges into equal-length intervals of fixed size 0.5 until the all data points are covered by boxes. This approach yields (hyper-)cubic boxes, and works for data of any dimension.\n\nusing Entropies\nRectangularBinning(0.5)\n\nAgain, assume the data consists of 2-dimensional points, and that we want a finer grid along one of the dimensions than over the other dimension.\n\nThe following binning specification finds the minima/maxima along each coordinate axis, then splits the range along the first coordinate axis into equal-length intervals of size 0.3, and the range along the second axis into equal-length intervals of size 0.1 (in both cases, making sure the data are completely covered by the boxes). This approach gives a (hyper-)rectangular boxes.\n\nusing Entropies\nRectangularBinning([0.3, 0.1])\n\nExample 2: Custom grids (partition not guaranteed to cover data points):\n\nAssume the data consists of 3-dimensional points (x, y, z), and that we want a grid that is fixed over the intervals [x₁, x₂] for the first dimension, over [y₁, y₂] for the second dimension, and over [z₁, z₂] for the third dimension. We when want to split each of those ranges into 4 equal-length pieces. Beware: some points may fall outside the partition if the intervals are not chosen properly (these points are simply discarded).\n\nThe following binning specification produces the desired (hyper-)rectangular boxes.\n\nusing Entropies, DelayEmbeddings\n\nD = Dataset(rand(100, 3));\n\nx₁, x₂ = 0.5, 1 # not completely covering the data, which are on [0, 1]\ny₁, y₂ = -2, 1.5 # covering the data, which are on [0, 1]\nz₁, z₂ = 0, 0.5 # not completely covering the data, which are on [0, 1]\n\nϵ = [(x₁, x₂), (y₁, y₂), (z₁, z₂)], 4 # [interval 1, interval 2, ...], n_subdivisions\n\nRectangularBinning(ϵ)\n\n\n\n\n\n","category":"type"},{"location":"probabilities/#Transfer-operator-(binning)","page":"Probabilities","title":"Transfer operator (binning)","text":"","category":"section"},{"location":"probabilities/","page":"Probabilities","title":"Probabilities","text":"TransferOperator","category":"page"},{"location":"probabilities/#Entropies.TransferOperator","page":"Probabilities","title":"Entropies.TransferOperator","text":"TransferOperator(ϵ::RectangularBinning) <: BinningProbabilitiesEstimator\n\nA probability estimator based on binning data into rectangular boxes dictated by the binning scheme ϵ, then approxmating the transfer (Perron-Frobenius) operator over the bins, then taking the invariant measure associated with that transfer operator as the bin probabilities. Assumes that the input data are sequential (time-ordered).\n\nThis implementation follows the grid estimator approach in Diego et al. (2019)[Diego2019].\n\nDescription\n\nThe transfer operator P^Nis computed as an N-by-N matrix of transition probabilities between the states defined by the partition elements, where N is the number of boxes in the partition that is visited by the orbit/points.\n\nIf  x_t^(D) _n=1^L are the L different D-dimensional points over which the transfer operator is approximated,  C_k=1^N  are the N different partition elements (as dictated by ϵ) that gets visited by the points, and  phi(x_t) = x_t+1, then\n\nP_ij = dfrac\n x_n  phi(x_n) in C_j cap x_n in C_i \n x_m  x_m in C_i \n\nwhere  denotes the cardinal. The element P_ij thus indicates how many points that are initially in box C_i end up in box C_j when the points in C_i are projected one step forward in time. Thus, the row P_ik^N where k in 1 2 ldots N  gives the probability of jumping from the state defined by box C_i to any of the other N states. It follows that sum_k=1^N P_ik = 1 for all i. Thus, P^N is a row/right stochastic matrix.\n\nInvariant measure estimation from transfer operator\n\nThe left invariant distribution mathbfrho^N is a row vector, where mathbfrho^N P^N = mathbfrho^N. Hence, mathbfrho^N is a row eigenvector of the transfer matrix P^N associated with eigenvalue 1. The distribution mathbfrho^N approximates the invariant density of the system subject to the partition ϵ, and can be taken as a probability distribution over the partition elements.\n\nIn practice, the invariant measure mathbfrho^N is computed using invariantmeasure, which also approximates the transfer matrix. The invariant distribution is initialized as a length-N random distribution which is then applied to P^N. The resulting length-N distribution is then applied to P^N again. This process repeats until the difference between the distributions over consecutive iterations is below some threshold.\n\nProbability and entropy estimation\n\nprobabilities(x::AbstractDataset, est::TransferOperator{RectangularBinning}) estimates   probabilities for the bins defined by the provided binning (est.ϵ)\nentropy_renyi(x::AbstractDataset, est::TransferOperator{RectangularBinning}) does the same,   but computes generalized entropy using the probabilities.\n\nSee also: RectangularBinning, invariantmeasure.\n\n[Diego2019]: Diego, D., Haaga, K. A., & Hannisdal, B. (2019). Transfer entropy computation using the Perron-Frobenius operator. Physical Review E, 99(4), 042212.\n\n\n\n\n\n","category":"type"},{"location":"probabilities/#Utility-methods/types","page":"Probabilities","title":"Utility methods/types","text":"","category":"section"},{"location":"probabilities/","page":"Probabilities","title":"Probabilities","text":"InvariantMeasure\ninvariantmeasure\ntransfermatrix","category":"page"},{"location":"probabilities/#Entropies.InvariantMeasure","page":"Probabilities","title":"Entropies.InvariantMeasure","text":"InvariantMeasure(to, ρ)\n\nMinimal return struct for invariantmeasure that contains the estimated invariant measure ρ, as well as the transfer operator to from which it is computed (including bin information).\n\nSee also: invariantmeasure.\n\n\n\n\n\n","category":"type"},{"location":"probabilities/#Entropies.invariantmeasure","page":"Probabilities","title":"Entropies.invariantmeasure","text":"invariantmeasure(x::AbstractDataset, ϵ::RectangularBinning) → iv::InvariantMeasure\n\nEstimate an invariant measure over the points in x based on binning the data into rectangular boxes dictated by the binning scheme ϵ, then approximate the transfer (Perron-Frobenius) operator over the bins. From the approximation to the transfer operator, compute an invariant distribution over the bins. Assumes that the input data are sequential.\n\nDetails on the estimation procedure is found the TransferOperator docstring.\n\nExample\n\nusing DynamicalSystems, Plots, Entropies\nD = 4\nds = Systems.lorenz96(D; F = 32.0)\nN, dt = 20000, 0.1\norbit = trajectory(ds, N*dt; dt = dt, Ttr = 10.0)\n\n# Estimate the invariant measure over some coarse graining of the orbit.\niv = invariantmeasure(orbit, RectangularBinning(15))\n\n# Get the probabilities and bins\ninvariantmeasure(iv)\n\nProbabilities and bin information\n\ninvariantmeasure(iv::InvariantMeasure) → (ρ::Probabilities, bins::Vector{<:SVector})\n\nFrom a pre-computed invariant measure, return the probabilities and associated bins. The element ρ[i] is the probability of visitation to the box bins[i]. Analogous to binhist.\n\nhint: Transfer operator approach vs. naive histogram approach\nWhy bother with the transfer operator instead of using regular histograms to obtain probabilities?In fact, the naive histogram approach and the transfer operator approach are equivalent in the limit of long enough time series (as n to intfy), which is guaranteed by the ergodic theorem. There is a crucial difference, however:The naive histogram approach only gives the long-term probabilities that orbits visit a certain region of the state space. The transfer operator encodes that information too, but comes with the added benefit of knowing the transition probabilities between states (see transfermatrix).\n\nSee also: InvariantMeasure.\n\n\n\n\n\n","category":"function"},{"location":"probabilities/#Entropies.transfermatrix","page":"Probabilities","title":"Entropies.transfermatrix","text":"transfermatrix(iv::InvariantMeasure) → (M::AbstractArray{<:Real, 2}, bins::Vector{<:SVector})\n\nReturn the transfer matrix/operator and corresponding bins. Here, bins[i] corresponds to the i-th row/column of the transfer matrix. Thus, the entry M[i, j] is the probability of jumping from the state defined by bins[i] to the state defined by bins[j].\n\nSee also: TransferOperator.\n\n\n\n\n\n","category":"function"},{"location":"probabilities/#Kernel-density","page":"Probabilities","title":"Kernel density","text":"","category":"section"},{"location":"probabilities/","page":"Probabilities","title":"Probabilities","text":"NaiveKernel","category":"page"},{"location":"probabilities/#Entropies.NaiveKernel","page":"Probabilities","title":"Entropies.NaiveKernel","text":"NaiveKernel(ϵ::Real, ss = KDTree; w = 0, metric = Euclidean()) <: ProbabilitiesEstimator\n\nEstimate probabilities/entropy using a \"naive\" kernel density estimation approach (KDE), as discussed in Prichard and Theiler (1995) [PrichardTheiler1995].\n\nProbabilities P(mathbfx epsilon) are assigned to every point mathbfx by counting how many other points occupy the space spanned by a hypersphere of radius ϵ around mathbfx, according to:\n\nP_i( X epsilon) approx dfrac1N sum_s B(X_i - X_j  epsilon)\n\nwhere B gives 1 if the argument is true. Probabilities are then normalized.\n\nThe search structure ss is any search structure supported by Neighborhood.jl. Specifically, use KDTree to use a tree-based neighbor search, or BruteForce for the direct distances between all points. KDTrees heavily outperform direct distances when the dimensionality of the data is much smaller than the data length.\n\nThe keyword w stands for the Theiler window, and excludes indices s that are within i - s  w from the given point X_i.\n\n[PrichardTheiler1995]: Prichard, D., & Theiler, J. (1995). Generalized redundancies for time series analysis. Physica D: Nonlinear Phenomena, 84(3-4), 476-493.\n\n\n\n\n\n","category":"type"},{"location":"probabilities/#Timescales","page":"Probabilities","title":"Timescales","text":"","category":"section"},{"location":"probabilities/","page":"Probabilities","title":"Probabilities","text":"WaveletOverlap\nPowerSpectrum","category":"page"},{"location":"probabilities/#Entropies.WaveletOverlap","page":"Probabilities","title":"Entropies.WaveletOverlap","text":"WaveletOverlap([wavelet]) <: ProbabilitiesEstimator\n\nApply the maximal overlap discrete wavelet transform (MODWT) to a signal, then compute probabilities/entropy from the energies at different wavelet scales. These probabilities are used to compute the wavelet entropy, according to Rosso et al. (2001)[Rosso2001].\n\nThe probability p[i] is the relative energy for the i-th wavelet scale. To obtain a better understanding of what these probabilities mean, we prepared a notebook you can view online. As such, this estimator only works for timeseries input.\n\nBy default the wavelet Wavelets.WT.Daubechies{12}() is used. Otherwise, you may choose a wavelet from the Wavelets package (it must subtype OrthoWaveletClass).\n\n[Rosso2001]: Rosso et al. (2001). Wavelet entropy: a new tool for analysis of short duration brain electrical signals. Journal of neuroscience methods, 105(1), 65-75.\n\n\n\n\n\n","category":"type"},{"location":"probabilities/#Entropies.PowerSpectrum","page":"Probabilities","title":"Entropies.PowerSpectrum","text":"PowerSpectrum() <: ProbabilitiesEstimator\n\nCalculate the power spectrum of a timeseries (amplitude square of its Fourier transform), and return the spectrum normalized to sum = 1 as probabilities. The Shannon entropy of these probabilities is typically referred in the literature as spectral entropy, e.g. [Llanos2016],[Tian2017].\n\nThe closer the spectrum is to flat, i.e., white noise, the higher the entropy. However, you can't compare entropies of timeseries with different length, because the binning in spectral space depends on the length of the input.\n\n[Llanos2016]: Llanos et al., Power spectral entropy as an information-theoretic correlate of manner of articulation in American English, The Journal of the Acoustical Society of America 141, EL127 (2017)\n\n[Tian2017]: Tian et al, Spectral Entropy Can Predict Changes of Working Memory Performance Reduced by Short-Time Training in the Delayed-Match-to-Sample Task, Front. Hum. Neurosci.\n\n\n\n\n\n","category":"type"},{"location":"examples/#Examples","page":"Entropies.jl examples","title":"Examples","text":"","category":"section"},{"location":"examples/#Nearest-neighbor-direct-entropy-example","page":"Entropies.jl examples","title":"Nearest neighbor direct entropy example","text":"","category":"section"},{"location":"examples/","page":"Entropies.jl examples","title":"Entropies.jl examples","text":"This example reproduces Figure in Charzyńska & Gambin (2016)[Charzyńska2016]. Both estimators nicely converge to the \"true\" entropy with increasing time series length. For a uniform 1D distribution U(0 1), the true entropy is 0.","category":"page"},{"location":"examples/","page":"Entropies.jl examples","title":"Entropies.jl examples","text":"using DynamicalSystems, CairoMakie, Statistics\nusing Distributions: Uniform, Normal\n\nNs = [100:100:500; 1000:1000:10000]\nEkl = Vector{Vector{Float64}}(undef, 0)\nEkr = Vector{Vector{Float64}}(undef, 0)\n\nnreps = 50\nfor N in Ns\n    kl = Float64[]\n    kr = Float64[]\n    for i = 1:nreps\n        pts = Dataset([rand(Uniform(0, 1), 1) for i = 1:N]);\n\n        push!(kl, entropy_kozachenkoleonenko(pts, w = 0, k = 1))\n        # with k = 1, Kraskov is virtually identical to\n        # Kozachenko-Leonenko, so pick a higher number of neighbors\n        push!(kr, entropy_kraskov(pts, w = 0, k = 3))\n    end\n    push!(Ekl, kl)\n    push!(Ekr, kr)\nend\n\nfig = Figure()\nax = Axis(fig[1,1]; ylabel = \"entropy (nats)\", title = \"Kozachenko-Leonenko\")\nlines!(ax, Ns, mean.(Ekl); color = Cycled(1))\nband!(ax, Ns, mean.(Ekl) .+ std.(Ekl), mean.(Ekl) .- std.(Ekl);\ncolor = (Main.COLORS[1], 0.5))\n\nay = Axis(fig[2,1]; xlabel = \"time step\", ylabel = \"entropy (nats)\", title = \"Kraskov\")\nlines!(ay, Ns, mean.(Ekr); color = Cycled(2))\nband!(ay, Ns, mean.(Ekr) .+ std.(Ekr), mean.(Ekr) .- std.(Ekr);\ncolor = (Main.COLORS[2], 0.5))\n\nfig","category":"page"},{"location":"examples/","page":"Entropies.jl examples","title":"Entropies.jl examples","text":"[Charzyńska2016]: Charzyńska, A., & Gambin, A. (2016). Improvement of the k-NN entropy estimator with applications in systems biology. Entropy, 18(1), 13.","category":"page"},{"location":"examples/#Permutation-entropy-example","page":"Entropies.jl examples","title":"Permutation entropy example","text":"","category":"section"},{"location":"examples/","page":"Entropies.jl examples","title":"Entropies.jl examples","text":"This example reproduces an example from Bandt and Pompe (2002), where the permutation entropy is compared with the largest Lyapunov exponents from time series of the chaotic logistic map. Entropy estimates using SymbolicWeightedPermutation and SymbolicAmplitudeAwarePermutation are added here for comparison.","category":"page"},{"location":"examples/","page":"Entropies.jl examples","title":"Entropies.jl examples","text":"using DynamicalSystems, CairoMakie\n\nds = Systems.logistic()\nrs = 3.4:0.001:4\nN_lyap, N_ent = 100000, 10000\nm, τ = 6, 1 # Symbol size/dimension and embedding lag\n\n# Generate one time series for each value of the logistic parameter r\nlyaps = Float64[]\nhs_perm = Float64[]\nhs_wtperm = Float64[]\nhs_ampperm = Float64[]\n\nbase = Base.MathConstants.e\nfor r in rs\n    ds.p[1] = r\n    push!(lyaps, lyapunov(ds, N_lyap))\n\n    x = trajectory(ds, N_ent) # time series\n    hperm = Entropies.entropy_renyi(x, SymbolicPermutation(m = m, τ = τ), base = base)\n    hwtperm = Entropies.entropy_renyi(x, SymbolicWeightedPermutation(m = m, τ = τ), base = base)\n    hampperm = Entropies.entropy_renyi(x, SymbolicAmplitudeAwarePermutation(m = m, τ = τ), base = base)\n\n    push!(hs_perm, hperm); push!(hs_wtperm, hwtperm); push!(hs_ampperm, hampperm)\nend\n\nfig = Figure()\na1 = Axis(fig[1,1]; ylabel = L\"\\lambda\")\nlines!(a1, rs, lyaps); ylims!(a1, (-2, log(2)))\na2 = Axis(fig[2,1]; ylabel = L\"h_6 (SP)\")\nlines!(a2, rs, hs_perm; color = Cycled(2))\na3 = Axis(fig[3,1]; ylabel = L\"h_6 (WT)\")\nlines!(a3, rs, hs_wtperm; color = Cycled(3))\na4 = Axis(fig[4,1]; ylabel = L\"h_6 (SAAP)\")\nlines!(a4, rs, hs_ampperm; color = Cycled(4))\na4.xlabel = L\"r\"\n\nfor a in (a1,a2,a3)\n    hidexdecorations!(a, grid = false)\nend\nfig","category":"page"},{"location":"examples/#Kernel-density-example","page":"Entropies.jl examples","title":"Kernel density example","text":"","category":"section"},{"location":"examples/","page":"Entropies.jl examples","title":"Entropies.jl examples","text":"Here, we draw some random points from a 2D normal distribution. Then, we use kernel density estimation to associate a probability to each point p, measured by how many points are within radius 1.5 of p. Plotting the actual points, along with their associated probabilities estimated by the KDE procedure, we get the following surface plot.","category":"page"},{"location":"examples/","page":"Entropies.jl examples","title":"Entropies.jl examples","text":"using DynamicalSystems, CairoMakie, Distributions\n𝒩 = MvNormal([1, -4], 2)\nN = 500\nD = Dataset(sort([rand(𝒩) for i = 1:N]))\nx, y = columns(D)\np = probabilities(D, NaiveKernel(1.5))\nfig, ax = scatter(D[:, 1], D[:, 2], zeros(N);\n    markersize=8, axis=(type = Axis3,)\n)\nsurface!(ax, x, y, p.p)\nax.zlabel = \"P\"\nax.zticklabelsvisible = false\nfig","category":"page"},{"location":"examples/#Wavelet-entropy-example","page":"Entropies.jl examples","title":"Wavelet entropy example","text":"","category":"section"},{"location":"examples/","page":"Entropies.jl examples","title":"Entropies.jl examples","text":"The scale-resolved wavelet entropy should be lower for very regular signals (most of the energy is contained at one scale) and higher for very irregular signals (energy spread more out across scales).","category":"page"},{"location":"examples/","page":"Entropies.jl examples","title":"Entropies.jl examples","text":"using DynamicalSystems, CairoMakie\nN, a = 1000, 10\nt = LinRange(0, 2*a*π, N)\n\nx = sin.(t);\ny = sin.(t .+ cos.(t/0.5));\nz = sin.(rand(1:15, N) ./ rand(1:10, N))\n\nh_x = entropy_wavelet(x)\nh_y = entropy_wavelet(y)\nh_z = entropy_wavelet(z)\n\nfig = Figure()\nax = Axis(fig[1,1]; ylabel = \"x\")\nlines!(ax, t, x; color = Cycled(1), label = \"h=$(h=round(h_x, sigdigits = 5))\");\nay = Axis(fig[2,1]; ylabel = \"y\")\nlines!(ay, t, y; color = Cycled(2), label = \"h=$(h=round(h_y, sigdigits = 5))\");\naz = Axis(fig[3,1]; ylabel = \"z\", xlabel = \"time\")\nlines!(az, t, z; color = Cycled(3), label = \"h=$(h=round(h_z, sigdigits = 5))\");\nfor a in (ax, ay, az); axislegend(a); end\nfor a in (ax, ay); hidexdecorations!(a; grid=false); end\nfig","category":"page"},{"location":"examples/#dispersion_examples","page":"Entropies.jl examples","title":"Dispersion and reverse dispersion entropy","text":"","category":"section"},{"location":"examples/","page":"Entropies.jl examples","title":"Entropies.jl examples","text":"Here we reproduce parts of figure 3 in Li et al. (2019), computing reverse and regular dispersion entropy for a time series consisting of normally distributed noise with a single spike in the middle of the signal. We compute the entropies over a range subsets of the data, using a sliding window consisting of 70 data points, stepping the window 10 time steps at a time.","category":"page"},{"location":"examples/","page":"Entropies.jl examples","title":"Entropies.jl examples","text":"Note: the results here are not exactly the same as in the original paper, because Li et  al. (2019) base their examples on randomly generated numbers and do not provide code that  specify random number seeds.","category":"page"},{"location":"examples/","page":"Entropies.jl examples","title":"Entropies.jl examples","text":"using Entropies, DynamicalSystems, Random, CairoMakie, Distributions\n\nn = 1000\nts = 1:n\nx = [i == n ÷ 2 ? 50.0 : 0.0 for i in ts]\nrng = Random.default_rng()\ns = rand(rng, Normal(0, 1), n)\ny = x .+ s\n\nws = 70\nwindows = [t:t+ws for t in 1:10:n-ws]\nrdes = zeros(length(windows))\ndes = zeros(length(windows))\npes = zeros(length(windows))\n\nm, c = 2, 6\nest_de = Dispersion(symbolization = GaussianSymbolization(c), m = m, τ = 1)\n\nfor (i, window) in enumerate(windows)\n    rdes[i] = reverse_dispersion(y[window], est_de; normalize = true)\n    des[i] = entropy_renyi_norm(y[window], est_de)\nend\n\nfig = Figure()\n\na1 = Axis(fig[1,1]; xlabel = \"Time step\", ylabel = \"Value\")\nlines!(a1, ts, y)\ndisplay(fig)\n\na2 = Axis(fig[2, 1]; xlabel = \"Time step\", ylabel = \"Value\")\np_rde = scatterlines!([first(w) for w in windows], rdes,\n    label = \"Reverse dispersion entropy\",\n    color = :black,\n    markercolor = :black, marker = '●')\np_de = scatterlines!([first(w) for w in windows], des,\n    label = \"Dispersion entropy\",\n    color = :red,\n    markercolor = :red, marker = 'x', markersize = 20)\n\naxislegend(position = :rc)\nylims!(0, max(maximum(pes), 1))\nfig","category":"page"},{"location":"examples/","page":"Entropies.jl examples","title":"Entropies.jl examples","text":"[Rostaghi2016]: Rostaghi, M., & Azami, H. (2016). Dispersion entropy: A measure for time-series analysis. IEEE Signal Processing Letters, 23(5), 610-614.","category":"page"},{"location":"examples/","page":"Entropies.jl examples","title":"Entropies.jl examples","text":"[Li2019]: Li, Y., Gao, X., & Wang, L. (2019). Reverse dispersion entropy: a new complexity measure for sensor signal. Sensors, 19(23), 5203.","category":"page"},{"location":"complexity_measures/#complexity_measures","page":"Complexity measures","title":"Complexity measures","text":"","category":"section"},{"location":"complexity_measures/#Sample-entropy","page":"Complexity measures","title":"Sample entropy","text":"","category":"section"},{"location":"complexity_measures/#Approximate-entropy","page":"Complexity measures","title":"Approximate entropy","text":"","category":"section"},{"location":"complexity_measures/#Reverse-dispersion-entropy","page":"Complexity measures","title":"Reverse dispersion entropy","text":"","category":"section"},{"location":"complexity_measures/","page":"Complexity measures","title":"Complexity measures","text":"reverse_dispersion\ndistance_to_whitenoise","category":"page"},{"location":"complexity_measures/#Entropies.reverse_dispersion","page":"Complexity measures","title":"Entropies.reverse_dispersion","text":"reverse_dispersion(x::AbstractVector{T}, est::Dispersion = Dispersion();\n    normalize = true) where T <: Real\n\nCompute the reverse dispersion entropy complexity measure (Li et al., 2019)[Li2019].\n\nDescription\n\nLi et al. (2021)[Li2019] defines the reverse dispersion entropy as\n\nH_rde = sum_i = 1^c^m left(p_i - dfrac1c^m right)^2 =\nleft( sum_i=1^c^m p_i^2 right) - dfrac1c^m\n\nwhere the probabilities p_i are obtained precisely as for the Dispersion probability estimator. Relative frequencies of dispersion patterns are computed using the given symbolization scheme , which defaults to symbolization using the normal cumulative distribution function (NCDF), as implemented by GaussianSymbolization, using embedding dimension m and embedding delay τ. Recommended parameter values[Li2018] are m ∈ [2, 3], τ = 1 for the embedding, and c ∈ [3, 4, …, 8] categories for the Gaussian mapping.\n\nIf normalize == true, then the reverse dispersion entropy is normalized to [0, 1].\n\nThe minimum value of H_rde is zero and occurs precisely when the dispersion pattern distribution is flat, which occurs when all p_is are equal to 1c^m. Because H_rde geq 0, H_rde can therefore be said to be a measure of how far the dispersion pattern probability distribution is from white noise.\n\n[Li2019]: Li, Y., Gao, X., & Wang, L. (2019). Reverse dispersion entropy: a new complexity measure for sensor signal. Sensors, 19(23), 5203.\n\n\n\n\n\n","category":"function"},{"location":"complexity_measures/#Entropies.distance_to_whitenoise","page":"Complexity measures","title":"Entropies.distance_to_whitenoise","text":"distance_to_whitenoise(p::Probabilities, estimator::Dispersion; normalize = false)\n\nCompute the distance of the probability distribution p from a uniform distribution, given the parameters of estimator (which must be known beforehand).\n\nIf normalize == true, then normalize the value to the interval [0, 1] by using the parameters of estimator.\n\nUsed to compute reverse dispersion entropy(reverse_dispersion; Li et al., 2019[Li2019]).\n\n[Li2019]: Li, Y., Gao, X., & Wang, L. (2019). Reverse dispersion entropy: a new complexity measure for sensor signal. Sensors, 19(23), 5203.\n\n\n\n\n\n","category":"function"},{"location":"complexity_measures/#Disequilibrium","page":"Complexity measures","title":"Disequilibrium","text":"","category":"section"},{"location":"#Entropies.jl","page":"Entropies.jl","title":"Entropies.jl","text":"","category":"section"},{"location":"","page":"Entropies.jl","title":"Entropies.jl","text":"Entropies","category":"page"},{"location":"#Entropies","page":"Entropies.jl","title":"Entropies","text":"A Julia package that provides estimators for probabilities, entropies, and complexity measures for timeseries, nonlinear dynamics and complex systems. It can be used as a standalone package, or as part of several projects in the JuliaDynamics organization, such as DynamicalSystems.jl or CausalityTools.jl.\n\n\n\n\n\n","category":"module"},{"location":"#API-and-terminology","page":"Entropies.jl","title":"API & terminology","text":"","category":"section"},{"location":"","page":"Entropies.jl","title":"Entropies.jl","text":"note: Note\nThe documentation here follows (loosely) chapter 5 of Nonlinear Dynamics, Datseris & Parlitz, Springer 2022.","category":"page"},{"location":"","page":"Entropies.jl","title":"Entropies.jl","text":"In the literature, the term \"entropy\" is used (and abused) in multiple contexts. The API and documentation of Entropies.jl aim to clarify some aspects and provide a simple way to obtain probabilities, entropies, or other complexity measures.","category":"page"},{"location":"#Probabilities","page":"Entropies.jl","title":"Probabilities","text":"","category":"section"},{"location":"","page":"Entropies.jl","title":"Entropies.jl","text":"Entropies and other complexity measures are typically computed based on probability distributions. These are obtained from Input data by a plethora of different ways. The central API function that returns a probability distribution (actual, just a vector of probabilities) is probabilities, which takes in a subtype of ProbabilityEstimator to specify how the probabilities are computed. All estimators available in Entropies.jl can be found in the estimators page.","category":"page"},{"location":"#Entropies","page":"Entropies.jl","title":"Entropies","text":"","category":"section"},{"location":"","page":"Entropies.jl","title":"Entropies.jl","text":"Entropy is an established concept in statistics, information theory, and nonlinear dynamics. However it is also an umbrella term that may mean several computationally different quantities.","category":"page"},{"location":"","page":"Entropies.jl","title":"Entropies.jl","text":"Generalized entropies are theoretically well-founded and in Entropies.jl we have the","category":"page"},{"location":"","page":"Entropies.jl","title":"Entropies.jl","text":"Rényi entropy entropy_renyi.\nTsallis entropy entropy_tsallis.\nShannon entropy entropy_shannon, which is just a subcase of either of the above two.","category":"page"},{"location":"","page":"Entropies.jl","title":"Entropies.jl","text":"Computing such an entropy most of the time boils down to two simple steps: first estimating a probability distribution, and then applying one of the generalized entropy formulas to the distributions. Thus, any of the implemented probabilities estimators can be used to compute generalized entropies.","category":"page"},{"location":"","page":"Entropies.jl","title":"Entropies.jl","text":"tip: There aren't many entropies, really.\nA crucial thing to clarify is that many quantities that are named as entropies (e.g., permutation entropy entropy_permutation, wavelet entropy entropy_wavelet, etc.), are not really new entropies. They are new probability estimators. They simply devise a new way to calculate probabilities from data, and then plug those probabilities into formal entropy formulas such as the Shannon entropy. The probability estimators are smartly created so that they elegantly highlight important aspects of the data relevant to complexity.While in Entropies.jl we provide convenience functions like entropy_wavelet, they really aren't anything more than 2-lines-of-code wrappers that call entropy_shannon with the appropriate ProbabilityEstimator.There are only a few exceptions to this rule, which are quantities that are able to compute Shannon entropies via alternate means, without explicitly computing some probability distributions, such as entropy_kraskov.","category":"page"},{"location":"#Complexity-measures","page":"Entropies.jl","title":"Complexity measures","text":"","category":"section"},{"location":"","page":"Entropies.jl","title":"Entropies.jl","text":"Other complexity measures, which strictly speaking don't compute entropies, and may or may not explicitly compute probability distributions, appear in the Complexity measures section.","category":"page"},{"location":"#Input-data","page":"Entropies.jl","title":"Input data","text":"","category":"section"},{"location":"","page":"Entropies.jl","title":"Entropies.jl","text":"The input data type typically depend on the probability estimator chosen. In general though, the standard DynamicalSystems.jl approach is taken and as such we have three types of input data:","category":"page"},{"location":"","page":"Entropies.jl","title":"Entropies.jl","text":"Timeseries, which are AbstractVector{<:Real}, used in e.g. with WaveletOverlap.\nMulti-dimensional timeseries, or datasets, or state space sets, which are Dataset, used e.g. with NaiveKernel.\nSpatial data, which are higher dimensional standard Arrays, used e.g. with  SpatialSymbolicPermutation.","category":"page"},{"location":"utils/#Utility-methods","page":"Utility methods","title":"Utility methods","text":"","category":"section"},{"location":"utils/#Fast-histograms","page":"Utility methods","title":"Fast histograms","text":"","category":"section"},{"location":"utils/","page":"Utility methods","title":"Utility methods","text":"Entropies.binhist","category":"page"},{"location":"utils/#Entropies.binhist","page":"Utility methods","title":"Entropies.binhist","text":"binhist(x::AbstractDataset, ε::Real) → p, bins\nbinhist(x::AbstractDataset, ε::RectangularBinning) → p, bins\n\nHyper-optimized histogram calculation for x with rectangular binning ε. Returns the probabilities p of each bin of the histogram as well as the bins. Notice that bins are the starting corners of each bin. If ε isa Real, then the actual bin size is ε across each dimension. If ε isa RectangularBinning, then the bin size for each dimension will depend on the binning scheme.\n\nSee also: RectangularBinning.\n\n\n\n\n\n","category":"function"},{"location":"utils/#Symbolization","page":"Utility methods","title":"Symbolization","text":"","category":"section"},{"location":"utils/","page":"Utility methods","title":"Utility methods","text":"symbolize","category":"page"},{"location":"utils/#Entropies.symbolize","page":"Utility methods","title":"Entropies.symbolize","text":"symbolize(x, scheme::SymbolizationScheme) → Vector{Int}\nsymbolize!(s, x, scheme::SymbolizationScheme) → Vector{Int}\n\nSymbolize x using the provided symbolization scheme, optionally writing symbols into the pre-allocated symbol vector s. For usage examples, see individual symbolization scheme docstrings.\n\nSee also: OrdinalPattern, GaussianSymbolization.\n\n\n\n\n\n","category":"function"},{"location":"utils/#Symbolization-schemes","page":"Utility methods","title":"Symbolization schemes","text":"","category":"section"},{"location":"utils/","page":"Utility methods","title":"Utility methods","text":"The following symbolization schemes are currently implemented.","category":"page"},{"location":"utils/","page":"Utility methods","title":"Utility methods","text":"GaussianSymbolization\nOrdinalPattern","category":"page"},{"location":"utils/#Entropies.GaussianSymbolization","page":"Utility methods","title":"Entropies.GaussianSymbolization","text":"GaussianSymbolization(; c::Int = 3)\n\nA symbolization scheme where the elements of x are symbolized into c distinct integer categories using the normal cumulative distribution function (NCDF).\n\nAlgorithm\n\nAssume we have a univariate time series X = x_i_i=1^N. GaussianSymbolization first maps each x_i to a new real number y_i in 0 1 by using the normal cumulative distribution function (CDF), x_i to y_i  y_i = dfrac1 sigma     sqrt2 pi int_-infty^x_i e^(-(x_i - mu)^2)(2 sigma^2) dx, where mu and sigma are the empirical mean and standard deviation of X.\n\nNext, each y_i is linearly mapped to an integer z_i in 1 2 ldots c using the map y_i to z_i  z_i = R(y_i(c-1) + 05), where R indicates rounding up to the nearest integer. This procedure subdivides the interval 0 1 into c different subintervals that form a covering of 0 1, and assigns each y_i to one of these subintervals. The original time series X is thus transformed to a symbol time series S =  s_i _i=1^N, where s_i in 1 2 ldots c.\n\nUsage\n\nsymbolize(x::AbstractVector, s::GaussianSymbolization)\n\nMap the elements of x to a symbol time series according to the Gaussian symbolization scheme s.\n\nExamples\n\njulia> x = [0.1, 0.4, 0.7, -2.1, 8.0, 0.9, -5.2];\n\njulia> Entropies.symbolize(x, GaussianSymbolization(c = 5))\n7-element Vector{Int64}:\n 3\n 3\n 3\n 2\n 5\n 3\n 1\n\nSee also: symbolize.\n\n\n\n\n\n","category":"type"},{"location":"utils/#Entropies.OrdinalPattern","page":"Utility methods","title":"Entropies.OrdinalPattern","text":"OrdinalPattern(m = 3, τ = 1; lt = est.lt)\n\nA symbolization scheme that converts the input time series to ordinal patterns, which are then encoded to integers using encode_motif.\n\nnote: Note\nOrdinalPattern is intended for symbolizing time series. If providing a short vector, say x = [2, 5, 2, 1, 3, 4], then symbolize(x, OrdinalPattern(m = 2, τ = 1) will first embed x, then encode/symbolize each resulting state vector, not the original input. For symbolizing a single vector, use sortperm on it and use encode_motif on the resulting permutation indices.\n\nUsage\n\nsymbolize(x, scheme::OrdinalPattern) → Vector{Int}\nsymbolize!(s, x, scheme::OrdinalPattern) → Vector{Int}\n\nIf applied to an m-dimensional Dataset x, then m and τ are ignored, and m-dimensional permutation patterns are obtained directly for each xᵢ ∈ x. Permutation patterns are then encoded as integers using encode_motif. Optionally, symbols can be written directly into a pre-allocated integer vector s, where length(s) == length(x) using symbolize!.\n\nIf applied to a univariate vector x, then x is first converted to a delay reconstruction using embedding dimension m and lag τ. Permutation patterns are then computed for each of the resulting m-dimensional xᵢ ∈ x, and each permutation is then encoded as an integer using encode_motif. If using the in-place variant with univariate input, s must obey length(s) == length(x)-(est.m-1)*est.τ.\n\nExamples\n\nusing DelayEmbeddings, Entropies\nD = Dataset([rand(7) for i = 1:1000])\ns = symbolize(D, OrdinalPattern())\n\nSee also: symbolize.\n\n\n\n\n\n","category":"type"},{"location":"utils/#Encoding-ordinal-patterns","page":"Utility methods","title":"Encoding ordinal patterns","text":"","category":"section"},{"location":"utils/","page":"Utility methods","title":"Utility methods","text":"Entropies.encode_motif","category":"page"},{"location":"utils/#Entropies.encode_motif","page":"Utility methods","title":"Entropies.encode_motif","text":"encode_motif(x, m::Int = length(x)) → s::Int\n\nEncode the length-m motif x (a vector of indices that would sort some vector v in ascending order) into its unique integer symbol s in 1 2 ldots m - 1 , using Algorithm 1 in Berger et al. (2019)[Berger2019].\n\nExample\n\nv = rand(5)\n\n# The indices that would sort `v` in ascending order. This is now a permutation\n# of the index permutation (1, 2, ..., 5)\nx = sortperm(v)\n\n# Encode this permutation as an integer.\nencode_motif(x)\n\n[Berger2019]: Berger, Sebastian, et al. \"Teaching Ordinal Patterns to a Computer: Efficient Encoding Algorithms Based on the Lehmer Code.\" Entropy 21.10 (2019): 1023.\n\n\n\n\n\n","category":"function"},{"location":"utils/#Normalization","page":"Utility methods","title":"Normalization","text":"","category":"section"},{"location":"utils/","page":"Utility methods","title":"Utility methods","text":"alphabet_length","category":"page"},{"location":"utils/#Entropies.alphabet_length","page":"Utility methods","title":"Entropies.alphabet_length","text":"alphabet_length(estimator) → Int\n\nReturns the total number of possible symbols/states implied by estimator. If the total number of states cannot be known a priori, an error is thrown. Primarily used for normalization of entropy values computed using symbolic estimators.\n\nExamples\n\njulia> est = SymbolicPermutation(m = 4)\nSymbolicPermutation{typeof(Entropies.isless_rand)}(1, 4, Entropies.isless_rand)\n\njulia> alphabet_length(est)\n24\n\n\n\n\n\n","category":"function"},{"location":"utils/#maximum_entropy","page":"Utility methods","title":"Maximum entropy","text":"","category":"section"},{"location":"utils/","page":"Utility methods","title":"Utility methods","text":"maxentropy_tsallis\nmaxentropy_renyi\nmaxentropy_shannon","category":"page"},{"location":"utils/#Entropies.maxentropy_tsallis","page":"Utility methods","title":"Entropies.maxentropy_tsallis","text":"maxentropy_tsallis(N::Int, q; k = 1, base = MathConstants.e)\n\nConvenience function that computes the maximum value of the generalized Tsallis entropy with parameters q and k for an N-element probability distribution, i.e. dfracN^1 - q - 1(1 - q), which is useful for normalization when N and q is known.\n\nIf q == 1, then log(base, N) is returned.\n\nSee also entropy_tsallis, entropy_normalized, maxentropy_tsallis.\n\n\n\n\n\n","category":"function"},{"location":"utils/#Entropies.maxentropy_renyi","page":"Utility methods","title":"Entropies.maxentropy_renyi","text":"maxentropy_renyi(N::Int, base = MathConstants.e)\n\nConvenience function that computes the maximum value of the order-q generalized Rényi entropy for an N-element probability distribution, i.e. log_base(N), which is useful for normalization when N is known.\n\nSee also entropy_renyi, entropy_normalized.\n\n\n\n\n\n","category":"function"},{"location":"utils/#Entropies.maxentropy_shannon","page":"Utility methods","title":"Entropies.maxentropy_shannon","text":"maxentropy_shannon(N::Int, q; base = MathConstants.e)\n\nConvenience function that computes the maximum value of the Shannon entropy, i.e. log_base(N), which is useful for normalization when N is known.\n\nSee also entropy_shannon, entropy_normalized.\n\n\n\n\n\n","category":"function"}]
}
