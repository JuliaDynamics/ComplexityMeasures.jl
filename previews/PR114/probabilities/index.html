<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Probabilities · Entropies.jl</title><script data-outdated-warner src="../assets/warner.js"></script><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.045/juliamono.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.24/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL=".."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../assets/documenter.js"></script><script src="../siteinfo.js"></script><script src="../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../assets/themeswap.js"></script><link href="https://fonts.googleapis.com/css?family=Montserrat|Source+Code+Pro&amp;display=swap" rel="stylesheet" type="text/css"/></head><body><div id="documenter"><nav class="docs-sidebar"><div class="docs-package-name"><span class="docs-autofit"><a href="../">Entropies.jl</a></span></div><form class="docs-search" action="../search/"><input class="docs-search-query" id="documenter-search-query" name="q" type="text" placeholder="Search docs"/></form><ul class="docs-menu"><li><a class="tocitem" href="../">Entropies.jl</a></li><li class="is-active"><a class="tocitem" href>Probabilities</a><ul class="internal"><li><a class="tocitem" href="#Count-occurrences-(counting)"><span>Count occurrences (counting)</span></a></li><li><a class="tocitem" href="#Permutation-(symbolic)"><span>Permutation (symbolic)</span></a></li><li><a class="tocitem" href="#Dispersion-(symbolic)"><span>Dispersion (symbolic)</span></a></li><li><a class="tocitem" href="#Visitation-frequency-(binning)"><span>Visitation frequency (binning)</span></a></li><li><a class="tocitem" href="#Transfer-operator-(binning)"><span>Transfer operator (binning)</span></a></li><li><a class="tocitem" href="#Kernel-density"><span>Kernel density</span></a></li><li><a class="tocitem" href="#Timescales"><span>Timescales</span></a></li></ul></li><li><a class="tocitem" href="../entropies/">Entropies</a></li><li><a class="tocitem" href="../complexity_measures/">Complexity measures</a></li><li><a class="tocitem" href="../examples/">Entropies.jl examples</a></li><li><a class="tocitem" href="../utils/">Utility methods</a></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><nav class="breadcrumb"><ul class="is-hidden-mobile"><li class="is-active"><a href>Probabilities</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>Probabilities</a></li></ul></nav><div class="docs-right"><a class="docs-edit-link" href="https://github.com/JuliaDynamics/Entropies.jl/blob/main/docs/src/probabilities.md" title="Edit on GitHub"><span class="docs-icon fab"></span><span class="docs-label is-hidden-touch">Edit on GitHub</span></a><a class="docs-settings-button fas fa-cog" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-sidebar-button fa fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a></div></header><article class="content" id="documenter-page"><h1 id="probabilities_estimators"><a class="docs-heading-anchor" href="#probabilities_estimators">Probabilities</a><a id="probabilities_estimators-1"></a><a class="docs-heading-anchor-permalink" href="#probabilities_estimators" title="Permalink"></a></h1><article class="docstring"><header><a class="docstring-binding" id="Entropies.probabilities" href="#Entropies.probabilities"><code>Entropies.probabilities</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">probabilities(x::Array_or_Dataset) → p::Probabilities</code></pre><p>Directly count probabilities from the elements of <code>x</code> without any discretization, binning, symbolizing, or any other common processing. This is mostly useful when <code>x</code> contains categorical or integer data.</p><p><code>probabilities</code> always returns a <a href="#Entropies.Probabilities"><code>Probabilities</code></a> container (<code>Vector</code>-like).</p><p><code>x</code> is typically an <code>Array</code> or a <a href="@ref"><code>Dataset</code></a>, see <a href="../#Input-data-for-Entropies.jl">Input data for Entropies.jl</a>.</p><pre><code class="nohighlight hljs">probabilities(x::Array_or_Dataset, est::ProbabilitiesEstimator) → p::Probabilities</code></pre><p>Calculate probabilities representing <code>x</code> based on the provided estimator. The probabilities are typically unordered and may or may not contain 0s, see the documentation of the individual estimators for more. Configuration options are always given as arguments to the chosen estimator.</p><pre><code class="nohighlight hljs">probabilities(x::Array_or_Dataset, ε::AbstractFloat) → p::Probabilities</code></pre><p>Convenience syntax which provides probabilities for <code>x</code> based on rectangular binning (i.e. performing a histogram). In short, the state space is divided into boxes of length <code>ε</code>, and formally we use <code>est = VisitationFrequency(RectangularBinning(ε))</code> as an estimator, see <a href="#Entropies.VisitationFrequency"><code>VisitationFrequency</code></a>.</p><pre><code class="nohighlight hljs">probabilities(x::Array_or_Dataset, n::Integer) → p::Probabilities</code></pre><p>Same as the above method, but now each dimension of the data is binned into <code>n::Int</code> equal sized bins instead of bins of length <code>ε::AbstractFloat</code>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaDynamics/Entropies.jl/blob/724ffe63408f8255bf9334373f30d4b8ab747341/src/probabilities.jl#L38-L70">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Entropies.probabilities!" href="#Entropies.probabilities!"><code>Entropies.probabilities!</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">probabilities!(s, args...)</code></pre><p>Similar to <code>probabilities(args...)</code>, but allows pre-allocation of temporarily used containers <code>s</code>.</p><p>Only works for certain estimators. See for example <a href="#Entropies.SymbolicPermutation"><code>SymbolicPermutation</code></a>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaDynamics/Entropies.jl/blob/724ffe63408f8255bf9334373f30d4b8ab747341/src/probabilities.jl#L76-L83">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Entropies.Probabilities" href="#Entropies.Probabilities"><code>Entropies.Probabilities</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">Probabilities(x) → p</code></pre><p>A simple wrapper type around an <code>x::AbstractVector</code> which ensures that <code>p</code> sums to 1. Behaves identically to <code>Vector</code>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaDynamics/Entropies.jl/blob/724ffe63408f8255bf9334373f30d4b8ab747341/src/probabilities.jl#L5-L10">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Entropies.ProbabilitiesEstimator" href="#Entropies.ProbabilitiesEstimator"><code>Entropies.ProbabilitiesEstimator</code></a> — <span class="docstring-category">Type</span></header><section><div><p>An abstract type for probabilities estimators.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaDynamics/Entropies.jl/blob/724ffe63408f8255bf9334373f30d4b8ab747341/src/probabilities.jl#L32-L34">source</a></section></article><h2 id="Count-occurrences-(counting)"><a class="docs-heading-anchor" href="#Count-occurrences-(counting)">Count occurrences (counting)</a><a id="Count-occurrences-(counting)-1"></a><a class="docs-heading-anchor-permalink" href="#Count-occurrences-(counting)" title="Permalink"></a></h2><article class="docstring"><header><a class="docstring-binding" id="Entropies.CountOccurrences" href="#Entropies.CountOccurrences"><code>Entropies.CountOccurrences</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">CountOccurrences</code></pre><p>A probabilities/entropy estimator based on straight-forward counting of distinct elements in a univariate time series or multivariate dataset. This is the same as giving no estimator to <a href="#Entropies.probabilities"><code>probabilities</code></a>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaDynamics/Entropies.jl/blob/724ffe63408f8255bf9334373f30d4b8ab747341/src/probabilities_estimators/count_occurences.jl#L3-L9">source</a></section></article><h2 id="Permutation-(symbolic)"><a class="docs-heading-anchor" href="#Permutation-(symbolic)">Permutation (symbolic)</a><a id="Permutation-(symbolic)-1"></a><a class="docs-heading-anchor-permalink" href="#Permutation-(symbolic)" title="Permalink"></a></h2><article class="docstring"><header><a class="docstring-binding" id="Entropies.SymbolicPermutation" href="#Entropies.SymbolicPermutation"><code>Entropies.SymbolicPermutation</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">SymbolicPermutation(; m = 3, τ = 1, lt::Function = Entropies.isless_rand)</code></pre><p>A probabilities estimator based on ordinal permutation patterns, originally used by Bandt &amp; Pompe (2002)<sup class="footnote-reference"><a id="citeref-BandtPompe2002" href="#footnote-BandtPompe2002">[BandtPompe2002]</a></sup> to compute permutation entropy.</p><p>If applied to a univariate time series, then the time series is first embedded using embedding delay <code>τ</code> and dimension <code>m</code>, and then converted to a symbol time series using <a href="../utils/#Entropies.symbolize"><code>symbolize</code></a> with <a href="../utils/#Entropies.OrdinalPattern"><code>OrdinalPattern</code></a>, from which probabilities are estimated. If applied to a <code>Dataset</code>, then <code>τ</code> and <code>m</code> are ignored, and probabilities are computed directly from the state vectors.</p><div class="admonition is-info"><header class="admonition-header">Handling equal values in ordinal patterns</header><div class="admonition-body"><p>In Bandt &amp; Pompe (2002), equal values are ordered after their order of appearance, but this can lead to erroneous temporal correlations, especially for data with low-amplitude resolution <sup class="footnote-reference"><a id="citeref-Zunino2017" href="#footnote-Zunino2017">[Zunino2017]</a></sup>. Here, by default, if two values are equal, then one of the is randomly assigned as &quot;the largest&quot;, using <code>lt = Entropies.isless_rand</code>. To get the behaviour from Bandt and Pompe (2002), use <code>lt = Base.isless</code>).</p></div></div><p><strong>In-place symbolization</strong></p><p><code>SymbolicPermutation</code> also implements the in-place <a href="../entropies/#Entropies.entropy!"><code>entropy!</code></a> and <a href="#Entropies.probabilities!"><code>probabilities!</code></a>. The length of the pre-allocated symbol vector must match the length of the embedding: <code>N - (m-1)τ</code> for univariate time series, and <code>M</code> for length-<code>M</code> <code>Dataset</code>s), i.e.</p><pre><code class="language-julia hljs">using DelayEmbeddings, Entropies
m, τ, N = 2, 1, 100
est = SymbolicPermutation(; m, τ)

# For a time series
x_ts = rand(N)
s_ts = zeros(Int, N - (m - 1)*τ)
p = probabilities!(s_ts, x_ts, est)
h = entropy!(s_ts, Renyi(),  x_ts, est)

# For a pre-symbolized `Dataset`
x_symb = symbolize(x_ts, OrdinalPattern(m = 2, τ = 1))
x_d = genembed(x_symb, (0, -1, -2))
s_d = zeros(Int, length(x_d))
p = probabilities!(s_d, x_d, est)
h = entropy!(s_d, Renyi(), x_d, est)</code></pre><p>See <a href="#Entropies.SymbolicWeightedPermutation"><code>SymbolicWeightedPermutation</code></a> and <a href="#Entropies.SymbolicAmplitudeAwarePermutation"><code>SymbolicAmplitudeAwarePermutation</code></a> for estimators that not only consider ordinal (sorting) patterns, but also incorporate information about within-state-vector amplitudes.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaDynamics/Entropies.jl/blob/724ffe63408f8255bf9334373f30d4b8ab747341/src/probabilities_estimators/permutation_ordinal/SymbolicPermutation.jl#L8-L63">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Entropies.SymbolicWeightedPermutation" href="#Entropies.SymbolicWeightedPermutation"><code>Entropies.SymbolicWeightedPermutation</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">SymbolicWeightedPermutation(; τ = 1, m = 3, lt = Entropies.isless_rand)</code></pre><p>A variant of <a href="#Entropies.SymbolicPermutation"><code>SymbolicPermutation</code></a> that also incorporates amplitude information, based on the weighted permutation entropy (Fadlallah et al., 2013).</p><p>Probabilities are computed as</p><p class="math-container">\[p(\pi_i^{m, \tau}) = \dfrac{\sum_{k=1}^N \mathbf{1}_{u:S(u) = s_i}
\left( \mathbf{x}_k^{m, \tau} \right)
\, w_k}{\sum_{k=1}^N \mathbf{1}_{u:S(u) \in \Pi}
\left( \mathbf{x}_k^{m, \tau} \right) \,w_k} = \dfrac{\sum_{k=1}^N
\mathbf{1}_{u:S(u) = s_i}
\left( \mathbf{x}_k^{m, \tau} \right) \, w_k}{\sum_{k=1}^N w_k},\]</p><p>where weights are computed based on the variance of the state vectors as</p><p class="math-container">\[w_j = \dfrac{1}{m}\sum_{k=1}^m (x_{j+(k+1)\tau} - \mathbf{\hat{x}}_j^{m, \tau})^2,\]</p><p>and <span>$\mathbf{x}_i$</span> is the aritmetic mean of state vector:</p><p class="math-container">\[\mathbf{\hat{x}}_j^{m, \tau} = \frac{1}{m} \sum_{k=1}^m x_{j + (k+1)\tau}.\]</p><p>The weighted permutation entropy is equivalent to regular permutation entropy when weights are positive and identical (<span>$w_j = \beta \,\,\, \forall \,\,\, j \leq N$</span> and <span>$\beta &gt; 0)$</span>.</p><div class="admonition is-info"><header class="admonition-header">An implementation note</header><div class="admonition-body"><p><em>Note: in equation 7, section III, of the original paper, the authors write</em></p><p class="math-container">\[w_j = \dfrac{1}{m}\sum_{k=1}^m (x_{j-(k-1)\tau} - \mathbf{\hat{x}}_j^{m, \tau})^2.\]</p><p><em>But given the formula they give for the arithmetic mean, this is <strong>not</strong> the variance of <span>$\mathbf{x}_i$</span>, because the indices are mixed: <span>$x_{j+(k-1)\tau}$</span> in the weights formula, vs. <span>$x_{j+(k+1)\tau}$</span> in the arithmetic mean formula. This seems to imply that amplitude information about previous delay vectors are mixed with mean amplitude information about current vectors. The authors also mix the terms &quot;vector&quot; and &quot;neighboring vector&quot; (but uses the same notation for both), making it hard to interpret whether the sign switch is a typo or intended. Here, we use the notation above, which actually computes the variance for <span>$\mathbf{x}_i$</span></em>.</p></div></div><p>See <a href="#Entropies.SymbolicPermutation"><code>SymbolicPermutation</code></a> for an estimator that only incorporates ordinal/sorting information and disregards amplitudes, and <a href="#Entropies.SymbolicAmplitudeAwarePermutation"><code>SymbolicAmplitudeAwarePermutation</code></a> for another estimator that incorporates amplitude information.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaDynamics/Entropies.jl/blob/724ffe63408f8255bf9334373f30d4b8ab747341/src/probabilities_estimators/permutation_ordinal/SymbolicWeightedPermutation.jl#L6-L61">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Entropies.SymbolicAmplitudeAwarePermutation" href="#Entropies.SymbolicAmplitudeAwarePermutation"><code>Entropies.SymbolicAmplitudeAwarePermutation</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">SymbolicAmplitudeAwarePermutation(; τ = 1, m = 3, A = 0.5, lt = Entropies.isless_rand)</code></pre><p>A variant of <a href="#Entropies.SymbolicPermutation"><code>SymbolicPermutation</code></a> that also incorporates amplitude information, based on the amplitude-aware permutation entropy (Azami &amp; Escudero, 2016).</p><p>Probabilities are computed as</p><p class="math-container">\[p(\pi_i^{m, \tau}) =
\dfrac{\sum_{k=1}^N
\mathbf{1}_{u:S(u) = s_i} \left( \mathbf{x}_k^{m, \tau} \right) \, a_k}{\sum_{k=1}^N
\mathbf{1}_{u:S(u) \in \Pi} \left( \mathbf{x}_k^{m, \tau} \right) \,a_k} =
\dfrac{\sum_{k=1}^N \mathbf{1}_{u:S(u) = s_i}
\left( \mathbf{x}_k^{m, \tau} \right) \, a_k}{\sum_{k=1}^N a_k}.\]</p><p>The weights encoding amplitude information about state vector <span>$\mathbf{x}_i = (x_1^i, x_2^i, \ldots, x_m^i)$</span> are</p><p class="math-container">\[a_i = \dfrac{A}{m} \sum_{k=1}^m |x_k^i | + \dfrac{1-A}{d-1}
\sum_{k=2}^d |x_{k}^i - x_{k-1}^i|,\]</p><p>with <span>$0 \leq A \leq 1$</span>. When <span>$A=0$</span> , only internal differences between the elements of <span>$\mathbf{x}_i$</span> are weighted. Only mean amplitude of the state vector elements are weighted when <span>$A=1$</span>. With, <span>$0&lt;A&lt;1$</span>, a combined weighting is used.</p><p>See <a href="#Entropies.SymbolicPermutation"><code>SymbolicPermutation</code></a> for an estimator that only incorporates ordinal/sorting information and disregards amplitudes, and <a href="#Entropies.SymbolicWeightedPermutation"><code>SymbolicWeightedPermutation</code></a> for another estimator that incorporates amplitude information.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaDynamics/Entropies.jl/blob/724ffe63408f8255bf9334373f30d4b8ab747341/src/probabilities_estimators/permutation_ordinal/SymbolicAmplitudeAware.jl#L3-L40">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Entropies.SpatialSymbolicPermutation" href="#Entropies.SpatialSymbolicPermutation"><code>Entropies.SpatialSymbolicPermutation</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">SpatialSymbolicPermutation(stencil, x, periodic = true)</code></pre><p>A symbolic, permutation-based probabilities/entropy estimator for spatiotemporal systems.</p><p>The input data <code>x</code> are high-dimensional arrays, for example 2D arrays <sup class="footnote-reference"><a id="citeref-Ribeiro2012" href="#footnote-Ribeiro2012">[Ribeiro2012]</a></sup> or 3D arrays <sup class="footnote-reference"><a id="citeref-Schlemmer2018" href="#footnote-Schlemmer2018">[Schlemmer2018]</a></sup>. This approach is also known as <em>spatiotemporal permutation entropy</em>. <code>x</code> is given because we need to know its size for optimization and bound checking.</p><p>A <em>stencil</em> defines what local area around each pixel to consider, and compute the ordinal pattern within the stencil. Stencils are given as vectors of <code>CartesianIndex</code> which encode the <em>offsets</em> of the pixes to include in the stencil, with respect to the current pixel. For example</p><pre><code class="language-julia hljs">data = [rand(50, 50) for _ in 1:50]
x = data[1] # first &quot;time slice&quot; of a spatial system evolution
stencil = CartesianIndex.([(0,1), (1,1), (1,0)])
est = SpatialSymbolicPermutation(stencil, x)</code></pre><p>Here the stencil creates a 2x2 square extending to the bottom and right of the pixel (directions here correspond to the way Julia prints matrices by default). Notice that no offset (meaning the pixel itself) is always included automatically. The length of the stencil decides the order of the permutation entropy, and the ordering within the stencil dictates the order that pixels are compared with. The pixel without any offset is always first in the order.</p><p>After having defined <code>est</code>, one calculates the spatial permutation entropy by calling <a href="../entropies/#Entropies.entropy"><code>entropy</code></a> with <code>est</code>, and with the array data. To apply this to timeseries of spatial data, simply loop over the call, e.g.:</p><pre><code class="language-julia hljs">h = entropy(x, est)
h_vs_t = entropy.(data, est) # broadcasting with `.`</code></pre><p>The argument <code>periodic</code> decides whether the stencil should wrap around at the end of the array. If <code>periodic = false</code>, pixels whose stencil exceeds the array bounds are skipped.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaDynamics/Entropies.jl/blob/724ffe63408f8255bf9334373f30d4b8ab747341/src/probabilities_estimators/permutation_ordinal/spatial_permutation.jl#L3-L50">source</a></section></article><h2 id="Dispersion-(symbolic)"><a class="docs-heading-anchor" href="#Dispersion-(symbolic)">Dispersion (symbolic)</a><a id="Dispersion-(symbolic)-1"></a><a class="docs-heading-anchor-permalink" href="#Dispersion-(symbolic)" title="Permalink"></a></h2><article class="docstring"><header><a class="docstring-binding" id="Entropies.Dispersion" href="#Entropies.Dispersion"><code>Entropies.Dispersion</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">Dispersion(; symbolization = GaussianSymbolization(c = 5), m = 2, τ = 1,
    check_unique = true)</code></pre><p>A probability estimator based on dispersion patterns, originally used by Rostaghi &amp; Azami, 2016<sup class="footnote-reference"><a id="citeref-Rostaghi2016" href="#footnote-Rostaghi2016">[Rostaghi2016]</a></sup> to compute the &quot;dispersion entropy&quot;, which characterizes the complexity and irregularity of a time series.</p><p>Relative frequencies of dispersion patterns are computed using the symbolization scheme <code>s</code> with embedding dimension <code>m</code> and embedding delay <code>τ</code>. Recommended parameter values<sup class="footnote-reference"><a id="citeref-Li2018" href="#footnote-Li2018">[Li2018]</a></sup> are <code>m ∈ [2, 3]</code>, <code>τ = 1</code> for the embedding, and <code>c ∈ [3, 4, …, 8]</code> categories for the Gaussian symbol mapping.</p><p><strong>Description</strong></p><p>Assume we have a univariate time series <span>$X = \{x_i\}_{i=1}^N$</span>. First, this time series is symbolized using <code>symbolization</code>, which default to <a href="../utils/#Entropies.GaussianSymbolization"><code>GaussianSymbolization</code></a>, which uses the normal cumulative distribution function (CDF) for symbolization. Other choices of CDFs are also possible, but Entropies.jl currently only implements <a href="../utils/#Entropies.GaussianSymbolization"><code>GaussianSymbolization</code></a>, which was used in Rostaghi &amp; Azami (2016). This step results in an integer-valued symbol time series <span>$S = \{ s_i \}_{i=1}^N$</span>, where <span>$s_i \in [1, 2, \ldots, c]$</span>.</p><p>Next, the symbol time series <span>$S$</span> is embedded into an <span>$m$</span>-dimensional time series, using an embedding lag of <span>$\tau = 1$</span>, which yields a total of <span>$N - (m - 1)\tau$</span> points, or &quot;dispersion patterns&quot;. Because each <span>$z_i$</span> can take on <span>$c$</span> different values, and each embedding point has <span>$m$</span> values, there are <span>$c^m$</span> possible dispersion patterns. This number is used for normalization when computing dispersion entropy.</p><p><strong>Computing dispersion probabilities and entropy</strong></p><p>A probability distribution <span>$P = \{p_i \}_{i=1}^{c^m}$</span>, where <span>$\sum_i^{c^m} p_i = 1$</span>, can then be estimated by counting and sum-normalising the distribution of dispersion patterns among the embedding vectors. Note that dispersion patterns that are not present are not counted. Therefore, you&#39;ll always get non-zero probabilities using the <code>Dispersion</code> probability estimator.</p><p>To compute dispersion entropy of order <code>q</code> to a given <code>base</code> on the univariate input time series <code>x</code>, do:</p><pre><code class="language-julia hljs">entropy_renyi(x, Dispersion(), base = 2, q = 1)</code></pre><p><strong>Data requirements and parameters</strong></p><p>The input must have more than one unique element for the Gaussian mapping to be well-defined. Li et al. (2018) recommends that <code>x</code> has at least 1000 data points.</p><p>If <code>check_unique == true</code> (default), then it is checked that the input has more than one unique value. If <code>check_unique == false</code> and the input only has one unique element, then a <code>InexactError</code> is thrown when trying to compute probabilities.</p><p>See also: <a href="../entropies/#Entropies.entropy_dispersion"><code>entropy_dispersion</code></a>, <a href="../utils/#Entropies.GaussianSymbolization"><code>GaussianSymbolization</code></a>.</p><div class="admonition is-info"><header class="admonition-header">Why &#39;dispersion patterns&#39;?</header><div class="admonition-body"><p>Each embedding vector is called a &quot;dispersion pattern&quot;. Why? Let&#39;s consider the case when <span>$m = 5$</span> and <span>$c = 3$</span>, and use some very imprecise terminology for illustration:</p><p>When <span>$c = 3$</span>, values clustering far below mean are in one group, values clustered around the mean are in one group, and values clustering far above the mean are in a third group. Then the embedding vector <span>$[2, 2, 2, 2, 2]$</span> consists of values that are relatively close together (close to the mean), so it represents a set of numbers that are not very spread out (less dispersed). The embedding vector <span>$[1, 1, 2, 3, 3]$</span>, however, represents numbers that are much more spread out (more dispersed), because the categories representing &quot;outliers&quot; both above and below the mean are represented, not only values close to the mean.</p></div></div></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaDynamics/Entropies.jl/blob/724ffe63408f8255bf9334373f30d4b8ab747341/src/probabilities_estimators/dispersion/dispersion.jl#L4-L75">source</a></section></article><h2 id="Visitation-frequency-(binning)"><a class="docs-heading-anchor" href="#Visitation-frequency-(binning)">Visitation frequency (binning)</a><a id="Visitation-frequency-(binning)-1"></a><a class="docs-heading-anchor-permalink" href="#Visitation-frequency-(binning)" title="Permalink"></a></h2><article class="docstring"><header><a class="docstring-binding" id="Entropies.VisitationFrequency" href="#Entropies.VisitationFrequency"><code>Entropies.VisitationFrequency</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">VisitationFrequency(r::RectangularBinning) &lt;: ProbabilitiesEstimator</code></pre><p>A probability estimator based on binning data into rectangular boxes dictated by the binning scheme <code>r</code> and then computing the frequencies of points in the bins.</p><p>This method has a linearithmic time complexity (<code>n log(n)</code> for <code>n = length(x)</code>) and a linear space complexity (<code>l</code> for <code>l = dimension(x)</code>). This allows computation of probabilities (histograms) of high-dimensional datasets and with small box sizes <code>ε</code> without memory overflow and with maximum performance. To obtain the bin information along with the probabilities, use <a href="../utils/#Entropies.binhist"><code>binhist</code></a>.</p><p>See also: <a href="#Entropies.RectangularBinning"><code>RectangularBinning</code></a>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaDynamics/Entropies.jl/blob/724ffe63408f8255bf9334373f30d4b8ab747341/src/probabilities_estimators/rectangular_binning/VisitationFrequency.jl#L5-L18">source</a></section></article><h3 id="Specifying-binning/boxes"><a class="docs-heading-anchor" href="#Specifying-binning/boxes">Specifying binning/boxes</a><a id="Specifying-binning/boxes-1"></a><a class="docs-heading-anchor-permalink" href="#Specifying-binning/boxes" title="Permalink"></a></h3><article class="docstring"><header><a class="docstring-binding" id="Entropies.RectangularBinning" href="#Entropies.RectangularBinning"><code>Entropies.RectangularBinning</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">RectangularBinning(ϵ) &lt;: BinningScheme</code></pre><p>Instructions for creating a rectangular box partition using the binning scheme <code>ϵ</code>. Binning instructions are deduced from the type of <code>ϵ</code>.</p><p>Rectangular binnings may be automatically adjusted to the data in which the <code>RectangularBinning</code> is applied, as follows:</p><ol><li><p><code>ϵ::Int</code> divides each coordinate axis into <code>ϵ</code> equal-length intervals,  extending the upper bound 1/100th of a bin size to ensure all points are covered.</p></li><li><p><code>ϵ::Float64</code> divides each coordinate axis into intervals of fixed size <code>ϵ</code>, starting  from the axis minima until the data is completely covered by boxes.</p></li><li><p><code>ϵ::Vector{Int}</code> divides the i-th coordinate axis into <code>ϵ[i]</code> equal-length  intervals, extending the upper bound 1/100th of a bin size to ensure all points are  covered.</p></li><li><p><code>ϵ::Vector{Float64}</code> divides the i-th coordinate axis into intervals of fixed size <code>ϵ[i]</code>, starting  from the axis minima until the data is completely covered by boxes.</p></li></ol><p>Rectangular binnings may also be specified on arbitrary min-max ranges.</p><ol><li><code>ϵ::Tuple{Vector{Tuple{Float64,Float64}},Int64}</code> creates intervals  along each coordinate axis from ranges indicated by a vector of <code>(min, max)</code> tuples, then divides  each coordinate axis into an integer number of equal-length intervals. <em>Note: this does not ensure  that all points are covered by the data (points outside the binning are ignored)</em>.</li></ol><p><strong>Example 1: Grid deduced automatically from data (partition guaranteed to cover data points)</strong></p><p><strong>Flexible box sizes</strong></p><p>The following binning specification finds the minima/maxima along each coordinate axis, then split each of those data ranges (with some tiny padding on the edges) into <code>10</code> equal-length intervals. This gives (hyper-)rectangular boxes, and works for data of any dimension.</p><pre><code class="language-julia hljs">using Entropies
RectangularBinning(10)</code></pre><p>Now, assume the data consists of 2-dimensional points, and that we want a finer grid along one of the dimensions than over the other dimension.</p><p>The following binning specification finds the minima/maxima along each coordinate axis, then splits the range along the first coordinate axis (with some tiny padding on the edges) into <code>10</code> equal-length intervals, and the range along the second coordinate axis (with some tiny padding on the edges) into <code>5</code> equal-length intervals. This gives (hyper-)rectangular boxes.</p><pre><code class="language-julia hljs">using Entropies
RectangularBinning([10, 5])</code></pre><p><strong>Fixed box sizes</strong></p><p>The following binning specification finds the minima/maxima along each coordinate axis, then split the axis ranges into equal-length intervals of fixed size <code>0.5</code> until the all data points are covered by boxes. This approach yields (hyper-)cubic boxes, and works for data of any dimension.</p><pre><code class="language-julia hljs">using Entropies
RectangularBinning(0.5)</code></pre><p>Again, assume the data consists of 2-dimensional points, and that we want a finer grid along one of the dimensions than over the other dimension.</p><p>The following binning specification finds the minima/maxima along each coordinate axis, then splits the range along the first coordinate axis into equal-length intervals of size <code>0.3</code>, and the range along the second axis into equal-length intervals of size <code>0.1</code> (in both cases, making sure the data are completely covered by the boxes). This approach gives a (hyper-)rectangular boxes.</p><pre><code class="language-julia hljs">using Entropies
RectangularBinning([0.3, 0.1])</code></pre><p><strong>Example 2: Custom grids (partition not guaranteed to cover data points):</strong></p><p>Assume the data consists of 3-dimensional points <code>(x, y, z)</code>, and that we want a grid that is fixed over the intervals <code>[x₁, x₂]</code> for the first dimension, over <code>[y₁, y₂]</code> for the second dimension, and over <code>[z₁, z₂]</code> for the third dimension. We when want to split each of those ranges into 4 equal-length pieces. <em>Beware: some points may fall outside the partition if the intervals are not chosen properly (these points are simply discarded)</em>.</p><p>The following binning specification produces the desired (hyper-)rectangular boxes.</p><pre><code class="language-julia hljs">using Entropies, DelayEmbeddings

D = Dataset(rand(100, 3));

x₁, x₂ = 0.5, 1 # not completely covering the data, which are on [0, 1]
y₁, y₂ = -2, 1.5 # covering the data, which are on [0, 1]
z₁, z₂ = 0, 0.5 # not completely covering the data, which are on [0, 1]

ϵ = [(x₁, x₂), (y₁, y₂), (z₁, z₂)], 4 # [interval 1, interval 2, ...], n_subdivisions

RectangularBinning(ϵ)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaDynamics/Entropies.jl/blob/724ffe63408f8255bf9334373f30d4b8ab747341/src/probabilities_estimators/rectangular_binning/binning_schemes.jl#L20-L127">source</a></section></article><h2 id="Transfer-operator-(binning)"><a class="docs-heading-anchor" href="#Transfer-operator-(binning)">Transfer operator (binning)</a><a id="Transfer-operator-(binning)-1"></a><a class="docs-heading-anchor-permalink" href="#Transfer-operator-(binning)" title="Permalink"></a></h2><article class="docstring"><header><a class="docstring-binding" id="Entropies.TransferOperator" href="#Entropies.TransferOperator"><code>Entropies.TransferOperator</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">TransferOperator(ϵ::RectangularBinning) &lt;: BinningProbabilitiesEstimator</code></pre><p>A probability estimator based on binning data into rectangular boxes dictated by the binning scheme <code>ϵ</code>, then approxmating the transfer (Perron-Frobenius) operator over the bins, then taking the invariant measure associated with that transfer operator as the bin probabilities. Assumes that the input data are sequential (time-ordered).</p><p>This implementation follows the grid estimator approach in Diego et al. (2019)<sup class="footnote-reference"><a id="citeref-Diego2019" href="#footnote-Diego2019">[Diego2019]</a></sup>.</p><p><strong>Description</strong></p><p>The transfer operator <span>$P^{N}$</span>is computed as an <code>N</code>-by-<code>N</code> matrix of transition probabilities between the states defined by the partition elements, where <code>N</code> is the number of boxes in the partition that is visited by the orbit/points.</p><p>If  <span>$\{x_t^{(D)} \}_{n=1}^L$</span> are the <span>$L$</span> different <span>$D$</span>-dimensional points over which the transfer operator is approximated, <span>$\{ C_{k=1}^N \}$</span> are the <span>$N$</span> different partition elements (as dictated by <code>ϵ</code>) that gets visited by the points, and  <span>$\phi(x_t) = x_{t+1}$</span>, then</p><p class="math-container">\[P_{ij} = \dfrac
{\#\{ x_n | \phi(x_n) \in C_j \cap x_n \in C_i \}}
{\#\{ x_m | x_m \in C_i \}},\]</p><p>where <span>$\#$</span> denotes the cardinal. The element <span>$P_{ij}$</span> thus indicates how many points that are initially in box <span>$C_i$</span> end up in box <span>$C_j$</span> when the points in <span>$C_i$</span> are projected one step forward in time. Thus, the row <span>$P_{ik}^N$</span> where <span>$k \in \{1, 2, \ldots, N \}$</span> gives the probability of jumping from the state defined by box <span>$C_i$</span> to any of the other <span>$N$</span> states. It follows that <span>$\sum_{k=1}^{N} P_{ik} = 1$</span> for all <span>$i$</span>. Thus, <span>$P^N$</span> is a row/right stochastic matrix.</p><p><strong>Invariant measure estimation from transfer operator</strong></p><p>The left invariant distribution <span>$\mathbf{\rho}^N$</span> is a row vector, where <span>$\mathbf{\rho}^N P^{N} = \mathbf{\rho}^N$</span>. Hence, <span>$\mathbf{\rho}^N$</span> is a row eigenvector of the transfer matrix <span>$P^{N}$</span> associated with eigenvalue 1. The distribution <span>$\mathbf{\rho}^N$</span> approximates the invariant density of the system subject to the partition <code>ϵ</code>, and can be taken as a probability distribution over the partition elements.</p><p>In practice, the invariant measure <span>$\mathbf{\rho}^N$</span> is computed using <a href="#Entropies.invariantmeasure"><code>invariantmeasure</code></a>, which also approximates the transfer matrix. The invariant distribution is initialized as a length-<code>N</code> random distribution which is then applied to <span>$P^{N}$</span>. The resulting length-<code>N</code> distribution is then applied to <span>$P^{N}$</span> again. This process repeats until the difference between the distributions over consecutive iterations is below some threshold.</p><p><strong>Probability and entropy estimation</strong></p><ul><li><code>probabilities(x::AbstractDataset, est::TransferOperator{RectangularBinning})</code> estimates   probabilities for the bins defined by the provided binning (<code>est.ϵ</code>)</li><li><code>entropy_renyi(x::AbstractDataset, est::TransferOperator{RectangularBinning})</code> does the same,   but computes generalized entropy using the probabilities.</li></ul><p>See also: <a href="#Entropies.RectangularBinning"><code>RectangularBinning</code></a>, <a href="#Entropies.invariantmeasure"><code>invariantmeasure</code></a>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaDynamics/Entropies.jl/blob/724ffe63408f8255bf9334373f30d4b8ab747341/src/probabilities_estimators/transfer_operator/transfer_operator.jl#L10-L71">source</a></section></article><h3 id="Utility-methods/types"><a class="docs-heading-anchor" href="#Utility-methods/types">Utility methods/types</a><a id="Utility-methods/types-1"></a><a class="docs-heading-anchor-permalink" href="#Utility-methods/types" title="Permalink"></a></h3><article class="docstring"><header><a class="docstring-binding" id="Entropies.InvariantMeasure" href="#Entropies.InvariantMeasure"><code>Entropies.InvariantMeasure</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">InvariantMeasure(to, ρ)</code></pre><p>Minimal return struct for <a href="#Entropies.invariantmeasure"><code>invariantmeasure</code></a> that contains the estimated invariant measure <code>ρ</code>, as well as the transfer operator <code>to</code> from which it is computed (including bin information).</p><p>See also: <a href="#Entropies.invariantmeasure"><code>invariantmeasure</code></a>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaDynamics/Entropies.jl/blob/724ffe63408f8255bf9334373f30d4b8ab747341/src/probabilities_estimators/transfer_operator/transfer_operator.jl#L297-L305">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Entropies.invariantmeasure" href="#Entropies.invariantmeasure"><code>Entropies.invariantmeasure</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">invariantmeasure(x::AbstractDataset, ϵ::RectangularBinning) → iv::InvariantMeasure</code></pre><p>Estimate an invariant measure over the points in <code>x</code> based on binning the data into rectangular boxes dictated by the binning scheme <code>ϵ</code>, then approximate the transfer (Perron-Frobenius) operator over the bins. From the approximation to the transfer operator, compute an invariant distribution over the bins. Assumes that the input data are sequential.</p><p>Details on the estimation procedure is found the <a href="#Entropies.TransferOperator"><code>TransferOperator</code></a> docstring.</p><p><strong>Example</strong></p><pre><code class="language-julia hljs">using DynamicalSystems, Plots, Entropies
D = 4
ds = Systems.lorenz96(D; F = 32.0)
N, dt = 20000, 0.1
orbit = trajectory(ds, N*dt; dt = dt, Ttr = 10.0)

# Estimate the invariant measure over some coarse graining of the orbit.
iv = invariantmeasure(orbit, RectangularBinning(15))

# Get the probabilities and bins
invariantmeasure(iv)</code></pre><p><strong>Probabilities and bin information</strong></p><pre><code class="nohighlight hljs">invariantmeasure(iv::InvariantMeasure) → (ρ::Probabilities, bins::Vector{&lt;:SVector})</code></pre><p>From a pre-computed invariant measure, return the probabilities and associated bins. The element <code>ρ[i]</code> is the probability of visitation to the box <code>bins[i]</code>. Analogous to <a href="../utils/#Entropies.binhist"><code>binhist</code></a>.</p><div class="admonition is-category-hint"><header class="admonition-header">Transfer operator approach vs. naive histogram approach</header><div class="admonition-body"><p>Why bother with the transfer operator instead of using regular histograms to obtain probabilities?</p><p>In fact, the naive histogram approach and the transfer operator approach are equivalent in the limit of long enough time series (as <span>$n \to \intfy$</span>), which is guaranteed by the ergodic theorem. There is a crucial difference, however:</p><p>The naive histogram approach only gives the long-term probabilities that orbits visit a certain region of the state space. The transfer operator encodes that information too, but comes with the added benefit of knowing the <em>transition probabilities</em> between states (see <a href="#Entropies.transfermatrix"><code>transfermatrix</code></a>).</p></div></div><p>See also: <a href="#Entropies.InvariantMeasure"><code>InvariantMeasure</code></a>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaDynamics/Entropies.jl/blob/724ffe63408f8255bf9334373f30d4b8ab747341/src/probabilities_estimators/transfer_operator/transfer_operator.jl#L317-L368">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Entropies.transfermatrix" href="#Entropies.transfermatrix"><code>Entropies.transfermatrix</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">transfermatrix(iv::InvariantMeasure) → (M::AbstractArray{&lt;:Real, 2}, bins::Vector{&lt;:SVector})</code></pre><p>Return the transfer matrix/operator and corresponding bins. Here, <code>bins[i]</code> corresponds to the i-th row/column of the transfer matrix. Thus, the entry <code>M[i, j]</code> is the probability of jumping from the state defined by <code>bins[i]</code> to the state defined by <code>bins[j]</code>.</p><p>See also: <a href="#Entropies.TransferOperator"><code>TransferOperator</code></a>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaDynamics/Entropies.jl/blob/724ffe63408f8255bf9334373f30d4b8ab747341/src/probabilities_estimators/transfer_operator/transfer_operator.jl#L445-L454">source</a></section></article><h2 id="Kernel-density"><a class="docs-heading-anchor" href="#Kernel-density">Kernel density</a><a id="Kernel-density-1"></a><a class="docs-heading-anchor-permalink" href="#Kernel-density" title="Permalink"></a></h2><article class="docstring"><header><a class="docstring-binding" id="Entropies.NaiveKernel" href="#Entropies.NaiveKernel"><code>Entropies.NaiveKernel</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">NaiveKernel(ϵ::Real, ss = KDTree; w = 0, metric = Euclidean()) &lt;: ProbabilitiesEstimator</code></pre><p>Estimate probabilities/entropy using a &quot;naive&quot; kernel density estimation approach (KDE), as discussed in Prichard and Theiler (1995) <sup class="footnote-reference"><a id="citeref-PrichardTheiler1995" href="#footnote-PrichardTheiler1995">[PrichardTheiler1995]</a></sup>.</p><p>Probabilities <span>$P(\mathbf{x}, \epsilon)$</span> are assigned to every point <span>$\mathbf{x}$</span> by counting how many other points occupy the space spanned by a hypersphere of radius <code>ϵ</code> around <span>$\mathbf{x}$</span>, according to:</p><p class="math-container">\[P_i( X, \epsilon) \approx \dfrac{1}{N} \sum_{s} B(||X_i - X_j|| &lt; \epsilon),\]</p><p>where <span>$B$</span> gives 1 if the argument is <code>true</code>. Probabilities are then normalized.</p><p>The search structure <code>ss</code> is any search structure supported by Neighborhood.jl. Specifically, use <code>KDTree</code> to use a tree-based neighbor search, or <code>BruteForce</code> for the direct distances between all points. KDTrees heavily outperform direct distances when the dimensionality of the data is much smaller than the data length.</p><p>The keyword <code>w</code> stands for the Theiler window, and excludes indices <span>$s$</span> that are within <span>$|i - s| ≤ w$</span> from the given point <span>$X_i$</span>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaDynamics/Entropies.jl/blob/724ffe63408f8255bf9334373f30d4b8ab747341/src/probabilities_estimators/kernel_density.jl#L5-L32">source</a></section></article><h2 id="Timescales"><a class="docs-heading-anchor" href="#Timescales">Timescales</a><a id="Timescales-1"></a><a class="docs-heading-anchor-permalink" href="#Timescales" title="Permalink"></a></h2><article class="docstring"><header><a class="docstring-binding" id="Entropies.WaveletOverlap" href="#Entropies.WaveletOverlap"><code>Entropies.WaveletOverlap</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">WaveletOverlap([wavelet]) &lt;: ProbabilitiesEstimator</code></pre><p>Apply the maximal overlap discrete wavelet transform (MODWT) to a signal, then compute probabilities/entropy from the energies at different wavelet scales. These probabilities are used to compute the wavelet entropy, according to Rosso et al. (2001)<sup class="footnote-reference"><a id="citeref-Rosso2001" href="#footnote-Rosso2001">[Rosso2001]</a></sup>.</p><p>The probability <code>p[i]</code> is the relative energy for the <code>i</code>-th wavelet scale. To obtain a better understanding of what these probabilities mean, we prepared a notebook you can <a href="https://github.com/kahaaga/waveletentropy_example/blob/main/wavelet_entropy_example.ipynb">view online</a>. As such, this estimator only works for timeseries input.</p><p>By default the wavelet <code>Wavelets.WT.Daubechies{12}()</code> is used. Otherwise, you may choose a wavelet from the <code>Wavelets</code> package (it must subtype <code>OrthoWaveletClass</code>).</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaDynamics/Entropies.jl/blob/724ffe63408f8255bf9334373f30d4b8ab747341/src/probabilities_estimators/timescales/wavelet_overlap.jl#L4-L25">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Entropies.PowerSpectrum" href="#Entropies.PowerSpectrum"><code>Entropies.PowerSpectrum</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">PowerSpectrum() &lt;: ProbabilitiesEstimator</code></pre><p>Calculate the power spectrum of a timeseries (amplitude square of its Fourier transform), and return the spectrum normalized to sum = 1 as probabilities. The Shannon entropy of these probabilities is typically referred in the literature as <em>spectral entropy</em>, e.g. <sup class="footnote-reference"><a id="citeref-Llanos2016" href="#footnote-Llanos2016">[Llanos2016]</a></sup>,<sup class="footnote-reference"><a id="citeref-Tian2017" href="#footnote-Tian2017">[Tian2017]</a></sup>.</p><p>The closer the spectrum is to flat, i.e., white noise, the higher the entropy. However, you can&#39;t compare entropies of timeseries with different length, because the binning in spectral space depends on the length of the input.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaDynamics/Entropies.jl/blob/724ffe63408f8255bf9334373f30d4b8ab747341/src/probabilities_estimators/timescales/power_spectrum.jl#L4-L25">source</a></section></article><section class="footnotes is-size-7"><ul><li class="footnote" id="footnote-BandtPompe2002"><a class="tag is-link" href="#citeref-BandtPompe2002">BandtPompe2002</a>Bandt, Christoph, and Bernd Pompe. &quot;Permutation entropy: a natural complexity measure for time series.&quot; Physical review letters 88.17 (2002): 174102.</li><li class="footnote" id="footnote-Zunino2017"><a class="tag is-link" href="#citeref-Zunino2017">Zunino2017</a>Zunino, L., Olivares, F., Scholkmann, F., &amp; Rosso, O. A. (2017). Permutation entropy based time series analysis: Equalities in the input signal can lead to false conclusions. Physics Letters A, 381(22), 1883-1892.</li><li class="footnote" id="footnote-Fadlallah2013"><a class="tag is-link" href="#citeref-Fadlallah2013">Fadlallah2013</a>Fadlallah, Bilal, et al. &quot;Weighted-permutation entropy: A complexity measure for time series incorporating amplitude information.&quot; Physical Review E 87.2 (2013): 022911.</li><li class="footnote" id="footnote-Azami2016"><a class="tag is-link" href="#citeref-Azami2016">Azami2016</a>Azami, H., &amp; Escudero, J. (2016). Amplitude-aware permutation entropy: Illustration in spike detection and signal segmentation. Computer methods and programs in biomedicine, 128, 40-51.</li><li class="footnote" id="footnote-Ribeiro2012"><a class="tag is-link" href="#citeref-Ribeiro2012">Ribeiro2012</a>Ribeiro et al. (2012). Complexity-entropy causality plane as a complexity measure for two-dimensional patterns. https://doi.org/10.1371/journal.pone.0040689</li><li class="footnote" id="footnote-Schlemmer2018"><a class="tag is-link" href="#citeref-Schlemmer2018">Schlemmer2018</a>Schlemmer et al. (2018). Spatiotemporal Permutation Entropy as a Measure for Complexity of Cardiac Arrhythmia. https://doi.org/10.3389/fphy.2018.00039</li><li class="footnote" id="footnote-Rostaghi2016"><a class="tag is-link" href="#citeref-Rostaghi2016">Rostaghi2016</a>Rostaghi, M., &amp; Azami, H. (2016). Dispersion entropy: A measure for time-series analysis. IEEE Signal Processing Letters, 23(5), 610-614.</li><li class="footnote" id="footnote-Li2018"><a class="tag is-link" href="#citeref-Li2018">Li2018</a>Li, G., Guan, Q., &amp; Yang, H. (2018). Noise reduction method of underwater acoustic signals based on CEEMDAN, effort-to-compress complexity, refined composite multiscale dispersion entropy and wavelet threshold denoising. Entropy, 21(1), 11.</li><li class="footnote" id="footnote-Diego2019"><a class="tag is-link" href="#citeref-Diego2019">Diego2019</a>Diego, D., Haaga, K. A., &amp; Hannisdal, B. (2019). Transfer entropy computation using the Perron-Frobenius operator. Physical Review E, 99(4), 042212.</li><li class="footnote" id="footnote-PrichardTheiler1995"><a class="tag is-link" href="#citeref-PrichardTheiler1995">PrichardTheiler1995</a>Prichard, D., &amp; Theiler, J. (1995). Generalized redundancies for time series analysis. Physica D: Nonlinear Phenomena, 84(3-4), 476-493.</li><li class="footnote" id="footnote-Rosso2001"><a class="tag is-link" href="#citeref-Rosso2001">Rosso2001</a>Rosso et al. (2001). Wavelet entropy: a new tool for analysis of short duration brain electrical signals. Journal of neuroscience methods, 105(1), 65-75.</li><li class="footnote" id="footnote-Llanos2016"><a class="tag is-link" href="#citeref-Llanos2016">Llanos2016</a>Llanos et al., <em>Power spectral entropy as an information-theoretic correlate of manner of articulation in American English</em>, <a href="https://doi.org/10.1121/1.4976109">The Journal of the Acoustical Society of America 141, EL127 (2017)</a></li><li class="footnote" id="footnote-Tian2017"><a class="tag is-link" href="#citeref-Tian2017">Tian2017</a>Tian et al, <em>Spectral Entropy Can Predict Changes of Working Memory Performance Reduced by Short-Time Training in the Delayed-Match-to-Sample Task</em>, <a href="https://doi.org/10.3389/fnhum.2017.00437">Front. Hum. Neurosci.</a></li></ul></section></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../">« Entropies.jl</a><a class="docs-footer-nextpage" href="../entropies/">Entropies »</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 0.27.23 on <span class="colophon-date" title="Monday 3 October 2022 19:31">Monday 3 October 2022</span>. Using Julia version 1.8.2.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
