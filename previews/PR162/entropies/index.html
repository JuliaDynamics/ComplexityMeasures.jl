<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Entropies · Entropies.jl</title><script data-outdated-warner src="../assets/warner.js"></script><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.045/juliamono.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.24/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL=".."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../assets/documenter.js"></script><script src="../siteinfo.js"></script><script src="../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../assets/themeswap.js"></script><link href="https://fonts.googleapis.com/css?family=Montserrat|Source+Code+Pro&amp;display=swap" rel="stylesheet" type="text/css"/></head><body><div id="documenter"><nav class="docs-sidebar"><div class="docs-package-name"><span class="docs-autofit"><a href="../">Entropies.jl</a></span></div><form class="docs-search" action="../search/"><input class="docs-search-query" id="documenter-search-query" name="q" type="text" placeholder="Search docs"/></form><ul class="docs-menu"><li><a class="tocitem" href="../">Entropies.jl</a></li><li><a class="tocitem" href="../probabilities/">Probabilities</a></li><li class="is-active"><a class="tocitem" href>Entropies</a><ul class="internal"><li><a class="tocitem" href="#Rényi-(generalized)-entropy"><span>Rényi (generalized) entropy</span></a></li><li><a class="tocitem" href="#Tsallis-(generalized)-entropy"><span>Tsallis (generalized) entropy</span></a></li><li><a class="tocitem" href="#Shannon-entropy-(convenience)"><span>Shannon entropy (convenience)</span></a></li><li><a class="tocitem" href="#Curado-entropy"><span>Curado entropy</span></a></li><li><a class="tocitem" href="#Stretched-exponental-entropy"><span>Stretched exponental entropy</span></a></li><li><a class="tocitem" href="#Normalized-entropies"><span>Normalized entropies</span></a></li><li><a class="tocitem" href="#Indirect-entropies"><span>Indirect entropies</span></a></li><li><a class="tocitem" href="#Convenience-functions"><span>Convenience functions</span></a></li></ul></li><li><a class="tocitem" href="../examples/">Entropies.jl Examples</a></li><li><a class="tocitem" href="../utils/">Utility methods</a></li><li><a class="tocitem" href="../devdocs/">Entropies.jl Dev Docs</a></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><nav class="breadcrumb"><ul class="is-hidden-mobile"><li class="is-active"><a href>Entropies</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>Entropies</a></li></ul></nav><div class="docs-right"><a class="docs-edit-link" href="https://github.com/JuliaDynamics/Entropies.jl/blob/main/docs/src/entropies.md" title="Edit on GitHub"><span class="docs-icon fab"></span><span class="docs-label is-hidden-touch">Edit on GitHub</span></a><a class="docs-settings-button fas fa-cog" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-sidebar-button fa fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a></div></header><article class="content" id="documenter-page"><h1 id="entropies"><a class="docs-heading-anchor" href="#entropies">Entropies</a><a id="entropies-1"></a><a class="docs-heading-anchor-permalink" href="#entropies" title="Permalink"></a></h1><article class="docstring"><header><a class="docstring-binding" id="Entropies.entropy" href="#Entropies.entropy"><code>Entropies.entropy</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">entropy([e::Entropy,] x, est::ProbabilitiesEstimator) → h::Real
entropy([e::Entropy,] probs::Probabilities) → h::Real</code></pre><p>Compute a (generalized) entropy <code>h</code> from <code>x</code> according to the specified entropy type <code>e</code> and the given probability estimator <code>est</code>. Alternatively compute the entropy directly from the existing probabilities <code>probs</code>. In fact, the first method is a 2-lines-of-code wrapper that calls <a href="../probabilities/#Entropies.probabilities"><code>probabilities</code></a> and gives the result to the second method.</p><p><code>x</code> is typically an <code>Array</code> or a <code>Dataset</code>, see <a href="@ref">Input data for Entropies.jl</a>.</p><p>The entropy types that support this interface are &quot;direct&quot; entropies. They always yield an entropy value given a probability distribution. Such entropies are theoretically well-founded and are typically called &quot;generalized entropies&quot;. Currently implemented types are:</p><ul><li><a href="#Entropies.Renyi"><code>Renyi</code></a>.</li><li><a href="#Entropies.Tsallis"><code>Tsallis</code></a>.</li><li><a href="#Entropies.Shannon"><code>Shannon</code></a>, which is a subcase of the above two in the limit <code>q → 1</code>.</li><li><a href="#Entropies.Curado"><code>Curado</code></a>.</li><li><a href="#Entropies.StretchedExponential"><code>StretchedExponential</code></a>.</li></ul><p>The entropy (first argument) is optional: if not given, <code>Shannon()</code> is used instead.</p><p>These entropies also have a well defined maximum value for a given probability estimator. To obtain this value one only needs to call the <a href="#Entropies.entropy_maximum"><code>entropy_maximum</code></a> function with the chosen entropy type and probability estimator. Or, one can use <a href="#Entropies.entropy_normalized"><code>entropy_normalized</code></a> to obtain the normalized form of the entropy (divided by the maximum).</p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">x = [rand(Bool) for _ in 1:10000] # coin toss
ps = probabilities(x) # gives about [0.5, 0.5] by definition
h = entropy(ps) # gives 1, about 1 bit by definition
h = entropy(Shannon(), ps) # syntactically equivalent to above
h = entropy(Shannon(), x, CountOccurrences()) # syntactically equivalent to above
h = entropy(x, SymbolicPermutation(;m=3)) # gives about 2, again by definition
h = entropy(Renyi(2.0), ps) # also gives 1, order `q` doesn&#39;t matter for coin toss</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaDynamics/Entropies.jl/blob/9fc0b4af1362963b7ea294fca5a89d1978c7512c/src/entropy.jl#L16-L56">source</a></section><section><div><pre><code class="nohighlight hljs">entropy(e::IndirectEntropy, x) → h::Real</code></pre><p>Compute the entropy of <code>x</code>, here named <code>h</code>, according to the specified indirect entropy estimator <code>e</code>.</p><p>In contrast to the &quot;typical&quot; way one obtains entropies in the above methods, indirect entropy estimators compute Shannon entropies via alternate means, without explicitly computing probability distributions. The available indirect entropies are:</p><ul><li><a href="#Entropies.Kraskov"><code>Kraskov</code></a>.</li><li><a href="#Entropies.KozachenkoLeonenko"><code>KozachenkoLeonenko</code></a>.</li><li><a href="#Entropies.Vasicek"><code>Vasicek</code></a>.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaDynamics/Entropies.jl/blob/9fc0b4af1362963b7ea294fca5a89d1978c7512c/src/entropy.jl#L130-L143">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Entropies.entropy!" href="#Entropies.entropy!"><code>Entropies.entropy!</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">entropy!(s, [e::Entropy,] x, est::ProbabilitiesEstimator)</code></pre><p>Similar to <code>probabilities!</code>, this is an in-place version of <a href="#Entropies.entropy"><code>entropy</code></a> that allows pre-allocation of temporarily used containers.</p><p>The entropy (second argument) is optional: if not given, <code>Shannon()</code> is used instead.</p><p>Only works for certain estimators. See for example <a href="../probabilities/#Entropies.SymbolicPermutation"><code>SymbolicPermutation</code></a>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaDynamics/Entropies.jl/blob/9fc0b4af1362963b7ea294fca5a89d1978c7512c/src/entropy.jl#L67-L76">source</a></section></article><h2 id="Rényi-(generalized)-entropy"><a class="docs-heading-anchor" href="#Rényi-(generalized)-entropy">Rényi (generalized) entropy</a><a id="Rényi-(generalized)-entropy-1"></a><a class="docs-heading-anchor-permalink" href="#Rényi-(generalized)-entropy" title="Permalink"></a></h2><article class="docstring"><header><a class="docstring-binding" id="Entropies.Renyi" href="#Entropies.Renyi"><code>Entropies.Renyi</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">Renyi &lt;: Entropy
Renyi(q, base = 2)
Renyi(; q = 1.0, base = 2)</code></pre><p>The Rényi<sup class="footnote-reference"><a id="citeref-Rényi1960" href="#footnote-Rényi1960">[Rényi1960]</a></sup> generalized order-<code>q</code> entropy, used with <a href="#Entropies.entropy"><code>entropy</code></a> to compute an entropy with units given by <code>base</code> (typically <code>2</code> or <code>MathConstants.e</code>).</p><p><strong>Description</strong></p><p>Let <span>$p$</span> be an array of probabilities (summing to 1). Then the Rényi generalized entropy is</p><p class="math-container">\[H_q(p) = \frac{1}{1-q} \log \left(\sum_i p[i]^q\right)\]</p><p>and generalizes other known entropies, like e.g. the information entropy (<span>$q = 1$</span>, see <sup class="footnote-reference"><a id="citeref-Shannon1948" href="#footnote-Shannon1948">[Shannon1948]</a></sup>), the maximum entropy (<span>$q=0$</span>, also known as Hartley entropy), or the correlation entropy (<span>$q = 2$</span>, also known as collision entropy).</p><p>If the probability estimator has known alphabet length <span>$L$</span>, then the maximum value of the Rényi entropy is <span>$\log_{base}(L)$</span>, which is the entropy of the uniform distribution with given alphabet length.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaDynamics/Entropies.jl/blob/9fc0b4af1362963b7ea294fca5a89d1978c7512c/src/entropies/renyi.jl#L3-L34">source</a></section></article><h2 id="Tsallis-(generalized)-entropy"><a class="docs-heading-anchor" href="#Tsallis-(generalized)-entropy">Tsallis (generalized) entropy</a><a id="Tsallis-(generalized)-entropy-1"></a><a class="docs-heading-anchor-permalink" href="#Tsallis-(generalized)-entropy" title="Permalink"></a></h2><article class="docstring"><header><a class="docstring-binding" id="Entropies.Tsallis" href="#Entropies.Tsallis"><code>Entropies.Tsallis</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">Tsallis &lt;: Entropy
Tsallis(q; k = 1.0, base = 2)
Tsallis(; q = 1.0, k = 1.0, base = 2)</code></pre><p>The Tsallis<sup class="footnote-reference"><a id="citeref-Tsallis1988" href="#footnote-Tsallis1988">[Tsallis1988]</a></sup> generalized order-<code>q</code> entropy, used with <a href="#Entropies.entropy"><code>entropy</code></a> to compute an entropy.</p><p><code>base</code> only applies in the limiting case <code>q == 1</code>, in which the Tsallis entropy reduces to Shannon entropy.</p><p><strong>Description</strong></p><p>The Tsallis entropy is a generalization of the Boltzmann-Gibbs entropy, with <code>k</code> standing for the Boltzmann constant. It is defined as</p><p class="math-container">\[S_q(p) = \frac{k}{q - 1}\left(1 - \sum_{i} p[i]^q\right)\]</p><p>If the probability estimator has known alphabet length <span>$L$</span>, then the maximum value of the Tsallis entropy is <span>$k(L^{1 - q} - 1)/(1 - q)$</span>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaDynamics/Entropies.jl/blob/9fc0b4af1362963b7ea294fca5a89d1978c7512c/src/entropies/tsallis.jl#L3-L29">source</a></section></article><h2 id="Shannon-entropy-(convenience)"><a class="docs-heading-anchor" href="#Shannon-entropy-(convenience)">Shannon entropy (convenience)</a><a id="Shannon-entropy-(convenience)-1"></a><a class="docs-heading-anchor-permalink" href="#Shannon-entropy-(convenience)" title="Permalink"></a></h2><article class="docstring"><header><a class="docstring-binding" id="Entropies.Shannon" href="#Entropies.Shannon"><code>Entropies.Shannon</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">Shannon(; base = 2)</code></pre><p>The Shannon<sup class="footnote-reference"><a id="citeref-Shannon1948" href="#footnote-Shannon1948">[Shannon1948]</a></sup> entropy, used with <a href="#Entropies.entropy"><code>entropy</code></a> to compute:</p><p class="math-container">\[H(p) = - \sum_i p[i] \log(p[i])\]</p><p>with the <span>$log$</span> at the given <code>base</code>.</p><p><code>Shannon(base)</code> is syntactically equivalent to <code>Renyi(; base)</code>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaDynamics/Entropies.jl/blob/9fc0b4af1362963b7ea294fca5a89d1978c7512c/src/entropies/renyi.jl#L60-L73">source</a></section></article><h2 id="Curado-entropy"><a class="docs-heading-anchor" href="#Curado-entropy">Curado entropy</a><a id="Curado-entropy-1"></a><a class="docs-heading-anchor-permalink" href="#Curado-entropy" title="Permalink"></a></h2><article class="docstring"><header><a class="docstring-binding" id="Entropies.Curado" href="#Entropies.Curado"><code>Entropies.Curado</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">Curado &lt;: Entropy
Curado(; b = 1.0)</code></pre><p>The Curado entropy (Curado &amp; Nobre, 2004)<sup class="footnote-reference"><a id="citeref-Curado2004" href="#footnote-Curado2004">[Curado2004]</a></sup>, used with <a href="#Entropies.entropy"><code>entropy</code></a> to compute</p><p class="math-container">\[H_C(p) = \left( \sum_{i=1}^N e^{-b p_i} \right) + e^{-b} - 1,\]</p><p>with <code>b ∈ ℛ, b &gt; 0</code>, where the terms outside the sum ensures that <span>$H_C(0) = H_C(1) = 0$</span>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaDynamics/Entropies.jl/blob/9fc0b4af1362963b7ea294fca5a89d1978c7512c/src/entropies/curado.jl#L3-L18">source</a></section></article><h2 id="Stretched-exponental-entropy"><a class="docs-heading-anchor" href="#Stretched-exponental-entropy">Stretched exponental entropy</a><a id="Stretched-exponental-entropy-1"></a><a class="docs-heading-anchor-permalink" href="#Stretched-exponental-entropy" title="Permalink"></a></h2><article class="docstring"><header><a class="docstring-binding" id="Entropies.StretchedExponential" href="#Entropies.StretchedExponential"><code>Entropies.StretchedExponential</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">StretchedExponential &lt;: Entropy
StretchedExponential(; η = 2.0, base = 2)</code></pre><p>The stretched exponential, or Anteneodo-Plastino, entropy (Anteneodo &amp; Plastino, 1999<sup class="footnote-reference"><a id="citeref-Anteneodo1999" href="#footnote-Anteneodo1999">[Anteneodo1999]</a></sup>), used with <a href="#Entropies.entropy"><code>entropy</code></a> to compute</p><p class="math-container">\[S_{\eta}(p) = \sum_{i = 1}^N
\Gamma \left( \dfrac{\eta + 1}{\eta}, - \log_{base}(p_i) \right) -
p_i \Gamma \left( \dfrac{\eta + 1}{\eta} \right),\]</p><p>where <span>$\eta \geq 0$</span>, <span>$\Gamma(\cdot, \cdot)$</span> is the upper incomplete Gamma function, and <span>$\Gamma(\cdot) = \Gamma(\cdot, 0)$</span> is the Gamma function. Reduces to <a href="@ref">Shannon</a> entropy for <code>η = 1.0</code>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaDynamics/Entropies.jl/blob/9fc0b4af1362963b7ea294fca5a89d1978c7512c/src/entropies/streched_exponential.jl#L5-L25">source</a></section></article><h2 id="Normalized-entropies"><a class="docs-heading-anchor" href="#Normalized-entropies">Normalized entropies</a><a id="Normalized-entropies-1"></a><a class="docs-heading-anchor-permalink" href="#Normalized-entropies" title="Permalink"></a></h2><article class="docstring"><header><a class="docstring-binding" id="Entropies.entropy_maximum" href="#Entropies.entropy_maximum"><code>Entropies.entropy_maximum</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">entropy_maximum(e::Entropy, x, est::ProbabilitiesEstimator) → m::Real</code></pre><p>Return the maximum value <code>m</code> of the given entropy type based on the given estimator and the given input <code>x</code> (whose values are not important, but layout and type are).</p><p>This function only works if the maximum value is deducable, which is possible only when the estimator has a known <a href="../probabilities/#Entropies.total_outcomes"><code>total_outcomes</code></a>.</p><pre><code class="nohighlight hljs">entropy_maximum(e::Entropy, L::Int) → m::Real</code></pre><p>Alternatively, compute the maximum entropy from the total outcomes <code>L</code> directly.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaDynamics/Entropies.jl/blob/9fc0b4af1362963b7ea294fca5a89d1978c7512c/src/entropy.jl#L88-L100">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Entropies.entropy_normalized" href="#Entropies.entropy_normalized"><code>Entropies.entropy_normalized</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">entropy_normalized([e::Entropy,] x, est::ProbabilitiesEstimator) → h̃ ∈ [0, 1]</code></pre><p>Return the normalized entropy of <code>x</code>, i.e., the value of <a href="#Entropies.entropy"><code>entropy</code></a> divided by the maximum value for <code>e</code>, according to the given probability estimator. If <code>e</code> is not given, it defaults to <code>Shannon()</code>.</p><p>Notice that unlike for <a href="#Entropies.entropy"><code>entropy</code></a>, here there is no method <code>entropy_normalized(e::Entropy, probs::Probabilities)</code> because there is no way to know the amount of <em>possible</em> events (i.e., the <a href="../probabilities/#Entropies.total_outcomes"><code>total_outcomes</code></a>) from <code>probs</code>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaDynamics/Entropies.jl/blob/9fc0b4af1362963b7ea294fca5a89d1978c7512c/src/entropy.jl#L109-L119">source</a></section></article><h2 id="Indirect-entropies"><a class="docs-heading-anchor" href="#Indirect-entropies">Indirect entropies</a><a id="Indirect-entropies-1"></a><a class="docs-heading-anchor-permalink" href="#Indirect-entropies" title="Permalink"></a></h2><p>Here we list functions which compute Shannon entropies via alternate means, without explicitly computing some probability distributions and then using the Shannon formula.</p><h3 id="Nearest-neighbors-entropy"><a class="docs-heading-anchor" href="#Nearest-neighbors-entropy">Nearest neighbors entropy</a><a id="Nearest-neighbors-entropy-1"></a><a class="docs-heading-anchor-permalink" href="#Nearest-neighbors-entropy" title="Permalink"></a></h3><article class="docstring"><header><a class="docstring-binding" id="Entropies.Kraskov" href="#Entropies.Kraskov"><code>Entropies.Kraskov</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">Kraskov &lt;: IndirectEntropy
Kraskov(; k::Int = 1, w::Int = 1, base = 2)</code></pre><p>An indirect entropy used in <a href="#Entropies.entropy"><code>entropy</code></a><code>(Kraskov(), x)</code> to estimate the Shannon entropy of <code>x</code> (a multi-dimensional <code>Dataset</code>) to the given <code>base</code> using <code>k</code>-th nearest neighbor searches as in <sup class="footnote-reference"><a id="citeref-Kraskov2004" href="#footnote-Kraskov2004">[Kraskov2004]</a></sup>.</p><p><code>w</code> is the Theiler window, which determines if temporal neighbors are excluded during neighbor searches (defaults to <code>0</code>, meaning that only the point itself is excluded when searching for neighbours).</p><p>See also: <a href="#Entropies.KozachenkoLeonenko"><code>KozachenkoLeonenko</code></a>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaDynamics/Entropies.jl/blob/9fc0b4af1362963b7ea294fca5a89d1978c7512c/src/entropies/direct_entropies/nearest_neighbors/Kraskov.jl#L3-L20">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Entropies.KozachenkoLeonenko" href="#Entropies.KozachenkoLeonenko"><code>Entropies.KozachenkoLeonenko</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">KozachenkoLeonenko &lt;: IndirectEntropy
KozachenkoLeonenko(; k::Int = 1, w::Int = 1, base = 2)</code></pre><p>An indirect entropy estimator used in <a href="#Entropies.entropy"><code>entropy</code></a><code>(KozachenkoLeonenko(), x)</code> to estimate the Shannon entropy of <code>x</code> (a multi-dimensional <code>Dataset</code>) to the given <code>base</code> using nearest neighbor searches using the method from Kozachenko &amp; Leonenko<sup class="footnote-reference"><a id="citeref-KozachenkoLeonenko1987" href="#footnote-KozachenkoLeonenko1987">[KozachenkoLeonenko1987]</a></sup>, as described in Charzyńska and Gambin<sup class="footnote-reference"><a id="citeref-Charzyńska2016" href="#footnote-Charzyńska2016">[Charzyńska2016]</a></sup>.</p><p><code>w</code> is the Theiler window, which determines if temporal neighbors are excluded during neighbor searches (defaults to <code>0</code>, meaning that only the point itself is excluded when searching for neighbours).</p><p>In contrast to <a href="#Entropies.Kraskov"><code>Kraskov</code></a>, this estimator uses only the <em>closest</em> neighbor.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaDynamics/Entropies.jl/blob/9fc0b4af1362963b7ea294fca5a89d1978c7512c/src/entropies/direct_entropies/nearest_neighbors/KozachenkoLeonenko.jl#L3-L22">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Entropies.Zhu" href="#Entropies.Zhu"><code>Entropies.Zhu</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">Zhu &lt;: IndirectEntropy
Zhu(k = 1, w = 0, base = MathConstants.e)</code></pre><p>The <code>Zhu</code> indirect entropy estimator (Zhu et al., 2015)<sup class="footnote-reference"><a id="citeref-Zhu2015" href="#footnote-Zhu2015">[Zhu2015]</a></sup> estimates the Shannon entropy of <code>x</code> (a multi-dimensional <code>Dataset</code>) to the given <code>base</code>, by approximating probabilities within hyperrectangles surrounding each point <code>xᵢ ∈ x</code> using using <code>k</code> nearest neighbor searches.</p><p>This estimator is an extension to <a href="#Entropies.KozachenkoLeonenko"><code>KozachenkoLeonenko</code></a>.</p><p><code>w</code> is the Theiler window, which determines if temporal neighbors are excluded during neighbor searches (defaults to <code>0</code>, meaning that only the point itself is excluded when searching for neighbours).</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaDynamics/Entropies.jl/blob/9fc0b4af1362963b7ea294fca5a89d1978c7512c/src/entropies/direct_entropies/nearest_neighbors/Zhu.jl#L3-L22">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Entropies.ZhuSingh" href="#Entropies.ZhuSingh"><code>Entropies.ZhuSingh</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">ZhuSingh &lt;: IndirectEntropy
ZhuSingh(k = 1, w = 0, base = MathConstants.e)</code></pre><p>The <code>ZhuSingh</code> indirect entropy estimator (Zhu et al., 2015)<sup class="footnote-reference"><a id="citeref-Zhu2015" href="#footnote-Zhu2015">[Zhu2015]</a></sup> estimates the Shannon entropy of <code>x</code> (a multi-dimensional <code>Dataset</code>) to the given <code>base</code>.</p><p>Like <a href="#Entropies.Zhu"><code>Zhu</code></a>, this estimator approximates probabilities within hyperrectangles surrounding each point <code>xᵢ ∈ x</code> using using <code>k</code> nearest neighbor searches. However, it also considers the number of neighbors falling on the borders of these hyperrectangles. This estimator is an extension to the entropy estimator in Singh et al. (2003).</p><p><code>w</code> is the Theiler window, which determines if temporal neighbors are excluded during neighbor searches (defaults to <code>0</code>, meaning that only the point itself is excluded when searching for neighbours).</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaDynamics/Entropies.jl/blob/9fc0b4af1362963b7ea294fca5a89d1978c7512c/src/entropies/direct_entropies/nearest_neighbors/ZhuSingh.jl#L8-L32">source</a></section></article><h3 id="Order-statistics-entropy"><a class="docs-heading-anchor" href="#Order-statistics-entropy">Order statistics entropy</a><a id="Order-statistics-entropy-1"></a><a class="docs-heading-anchor-permalink" href="#Order-statistics-entropy" title="Permalink"></a></h3><article class="docstring"><header><a class="docstring-binding" id="Entropies.Vasicek" href="#Entropies.Vasicek"><code>Entropies.Vasicek</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">Vasicek &lt;: IndirectEntropy
Vasicek(; m::Int = 1, base = 2)</code></pre><p>An indirect entropy estimator used in <a href="#Entropies.entropy"><code>entropy</code></a><code>(Vasicek(), x)</code> to estimate the Shannon entropy of the timeseries <code>x</code> to the given <code>base</code> using the method from Vasicek (1976)<sup class="footnote-reference"><a id="citeref-Vasicek1976" href="#footnote-Vasicek1976">[Vasicek1976]</a></sup>.</p><p><strong>Description</strong></p><p>The Vasicek entropy estimator first computes the <a href="https://en.wikipedia.org/wiki/Order_statistic">order statistics</a> <span>$X_{(1)} \leq X_{(2)} \leq \cdots \leq X_{(n)}$</span> for a random sample of length <span>$n$</span>, i.e. the input timeseries. The <a href="#Entropies.Shannon"><code>Shannon</code></a> entropy is then estimated as</p><p class="math-container">\[H_V(m) =
\dfrac{1}{n} \sum_{i = 1}^n \log \left[ \dfrac{n}{2m} (X_{(i+m)} - X_{(i-m)}) \right]\]</p><p><strong>Usage</strong></p><p>In practice, choice of <code>m</code> influences how fast the entropy converges to the true value. For small value of <code>m</code>, convergence is slow, so we recommend to scale <code>m</code> according to the time series length <code>n</code> and use <code>m &gt;= n/100</code> (this is just a heuristic based on the tests written for this package).</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaDynamics/Entropies.jl/blob/9fc0b4af1362963b7ea294fca5a89d1978c7512c/src/entropies/direct_entropies/order_statistics/Vasicek.jl#L3-L33">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Entropies.AlizadehArghami" href="#Entropies.AlizadehArghami"><code>Entropies.AlizadehArghami</code></a> — <span class="docstring-category">Type</span></header><section><div><p>AlizadehArghami &lt;: IndirectEntropy     AlizadehArghami(; m::Int = 1, base = 2)</p><p>An indirect entropy estimator used in <a href="#Entropies.entropy"><code>entropy</code></a><code>(Alizadeh(), x)</code> to estimate the Shannon entropy of the timeseries <code>x</code> to the given <code>base</code> using the method from Alizadeh &amp; Arghami (2010)<sup class="footnote-reference"><a id="citeref-Alizadeh2010" href="#footnote-Alizadeh2010">[Alizadeh2010]</a></sup>.</p><p><strong>Description</strong></p><p>The Alizadeh entropy estimator first computes the order statistics <span>$X_{(1)} \leq X_{(2)} \leq \cdots \leq X_{(n)}$</span> for a random sample of length <span>$n$</span>, i.e. the input timeseries. The entropy for the length-<code>n</code> sample is then estimated as the <a href="#Entropies.Vasicek"><code>Vasicek</code></a> entropy estimate, plus a correction factor</p><p class="math-container">\[H_{A}(m, n) = H_{V}(m, n) + \dfrac{2}{n}\left(m \log_{base}(2) \right).\]</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaDynamics/Entropies.jl/blob/9fc0b4af1362963b7ea294fca5a89d1978c7512c/src/entropies/direct_entropies/order_statistics/AlizadehArghami.jl#L3-L25">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Entropies.Ebrahimi" href="#Entropies.Ebrahimi"><code>Entropies.Ebrahimi</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">Ebrahimi &lt;: IndirectEntropy
Ebrahimi(; m::Int = 1, base = 2)</code></pre><p>An indirect entropy estimator used in <a href="#Entropies.entropy"><code>entropy</code></a><code>(Ebrahimi(), x)</code> to estimate the Shannon entropy of the timeseries <code>x</code> to the given <code>base</code> using the method from Ebrahimi (1994)<sup class="footnote-reference"><a id="citeref-Ebrahimi1994" href="#footnote-Ebrahimi1994">[Ebrahimi1994]</a></sup>.</p><p><strong>Description</strong></p><p>The Ebrahimi entropy estimator first computes the order statistics <span>$X_{(1)} \leq X_{(2)} \leq \cdots \leq X_{(n)}$</span> for a random sample of length <span>$n$</span>, i.e. the input timeseries. The <a href="#Entropies.Shannon"><code>Shannon</code></a> entropy is then estimated as</p><p class="math-container">\[H_{E}(m) =
\dfrac{1}{n} \sum_{i = 1}^n \log \left[ \dfrac{n}{c_i m} (X_{(i+m)} - X_{(i-m)}) \right],\]</p><p>where</p><p class="math-container">\[c_i =
\begin{cases}
    1 + \frac{i - 1}{m}, &amp; 1 \geq i \geq m \
    2,                    &amp; m + 1 \geq i \geq n - m \
    1 + \frac{n - i}{m} &amp; n - m + 1 \geq i \geq n
\end{cases}.\]</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaDynamics/Entropies.jl/blob/9fc0b4af1362963b7ea294fca5a89d1978c7512c/src/entropies/direct_entropies/order_statistics/Ebrahimi.jl#L3-L36">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Entropies.Correa" href="#Entropies.Correa"><code>Entropies.Correa</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">Correa &lt;: IndirectEntropy
Correa(; m::Int = 1, base = 2)</code></pre><p>An indirect entropy estimator used in <a href="#Entropies.entropy"><code>entropy</code></a><code>(Correa(), x)</code> to estimate the Shannon entropy of the timeseries <code>x</code> to the given <code>base</code> using the method from Correa (1995)<sup class="footnote-reference"><a id="citeref-Correa1995" href="#footnote-Correa1995">[Correa1995]</a></sup>.</p><p><strong>Description</strong></p><p>The Correa entropy estimator first computes the order statistics like <a href="#Entropies.Vasicek"><code>Vasicek</code></a>, ensuring that edge points are included, then estimates entropy as</p><p class="math-container">\[H_C(m, n) =
\dfrac{1}{n} \sum_{i = 1}^n \log
\left[ \dfrac{ \sum_{j=i-m}^{i+m}(X_{(j)} - \bar{X}_{(i)})(j - i)}{n \sum_{j=i-m}^{i+m} (X_{(j)} - \bar{X}_{(i)})^2} \right],\]</p><p>where</p><p class="math-container">\[\bar{X}_{(i)} = \dfrac{1}{2m + 1} \sum_{j = i - m}^{i + m} X_{(j)}.\]</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaDynamics/Entropies.jl/blob/9fc0b4af1362963b7ea294fca5a89d1978c7512c/src/entropies/direct_entropies/order_statistics/Correa.jl#L3-L31">source</a></section></article><h2 id="Convenience-functions"><a class="docs-heading-anchor" href="#Convenience-functions">Convenience functions</a><a id="Convenience-functions-1"></a><a class="docs-heading-anchor-permalink" href="#Convenience-functions" title="Permalink"></a></h2><p>In this subsection we expand documentation strings of &quot;entropy names&quot; that are used commonly in the literature, such as &quot;permutation entropy&quot;. As we made clear in <a href="../#API-and-terminology">API &amp; terminology</a>, these are just the existing Shannon entropy with a particularly chosen probability estimator. We have only defined convenience functions for the most used names, and arbitrary more specialized convenience functions can be easily defined in a couple lines of code.</p><article class="docstring"><header><a class="docstring-binding" id="Entropies.entropy_permutation" href="#Entropies.entropy_permutation"><code>Entropies.entropy_permutation</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">entropy_permutation(x; τ = 1, m = 3, base = 2)</code></pre><p>Compute the permutation entropy of <code>x</code> of order <code>m</code> with delay/lag <code>τ</code>. This function is just a convenience call to:</p><pre><code class="language-julia hljs">est = SymbolicPermutation(; m, τ)
entropy(Shannon(base), x, est)</code></pre><p>See <a href="../probabilities/#Entropies.SymbolicPermutation"><code>SymbolicPermutation</code></a> for more info. Similarly, one can use <code>SymbolicWeightedPermutation</code> or <code>SymbolicAmplitudeAwarePermutation</code> for the weighted/amplitude-aware versions.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaDynamics/Entropies.jl/blob/9fc0b4af1362963b7ea294fca5a89d1978c7512c/src/entropies/convenience_definitions.jl#L9-L23">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Entropies.entropy_spatial_permutation" href="#Entropies.entropy_spatial_permutation"><code>Entropies.entropy_spatial_permutation</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">entropy_spatial_permutation(x, stencil, periodic = true; kwargs...)</code></pre><p>Compute the spatial permutation entropy of <code>x</code> given the <code>stencil</code>. Here <code>x</code> must be a matrix or higher dimensional <code>Array</code> containing spatial data. This function is just a convenience call to:</p><pre><code class="language-julia hljs">est = SpatialSymbolicPermutation(stencil, x, periodic)
entropy(Renyi(;kwargs...), x, est)</code></pre><p>See <a href="../probabilities/#Entropies.SpatialSymbolicPermutation"><code>SpatialSymbolicPermutation</code></a> for more info, or how to encode stencils.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaDynamics/Entropies.jl/blob/9fc0b4af1362963b7ea294fca5a89d1978c7512c/src/entropies/convenience_definitions.jl#L29-L42">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Entropies.entropy_wavelet" href="#Entropies.entropy_wavelet"><code>Entropies.entropy_wavelet</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">entropy_wavelet(x; wavelet = Wavelets.WT.Daubechies{12}(), base = 2)</code></pre><p>Compute the wavelet entropy. This function is just a convenience call to:</p><pre><code class="language-julia hljs">est = WaveletOverlap(wavelet)
entropy(Shannon(base), x, est)</code></pre><p>See <a href="../probabilities/#Entropies.WaveletOverlap"><code>WaveletOverlap</code></a> for more info.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaDynamics/Entropies.jl/blob/9fc0b4af1362963b7ea294fca5a89d1978c7512c/src/entropies/convenience_definitions.jl#L48-L59">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Entropies.entropy_dispersion" href="#Entropies.entropy_dispersion"><code>Entropies.entropy_dispersion</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">entropy_dispersion(x; base = 2, kwargs...)</code></pre><p>Compute the dispersion entropy. This function is just a convenience call to:</p><pre><code class="language-julia hljs">est = Dispersion(kwargs...)
entropy(Shannon(base), x, est)</code></pre><p>See <a href="../probabilities/#Entropies.Dispersion"><code>Dispersion</code></a> for more info.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaDynamics/Entropies.jl/blob/9fc0b4af1362963b7ea294fca5a89d1978c7512c/src/entropies/convenience_definitions.jl#L65-L76">source</a></section></article><section class="footnotes is-size-7"><ul><li class="footnote" id="footnote-Rényi1960"><a class="tag is-link" href="#citeref-Rényi1960">Rényi1960</a>A. Rényi, <em>Proceedings of the fourth Berkeley Symposium on Mathematics, Statistics and Probability</em>, pp 547 (1960)</li><li class="footnote" id="footnote-Shannon1948"><a class="tag is-link" href="#citeref-Shannon1948">Shannon1948</a>C. E. Shannon, Bell Systems Technical Journal <strong>27</strong>, pp 379 (1948)</li><li class="footnote" id="footnote-Tsallis1988"><a class="tag is-link" href="#citeref-Tsallis1988">Tsallis1988</a>Tsallis, C. (1988). Possible generalization of Boltzmann-Gibbs statistics. Journal of statistical physics, 52(1), 479-487.</li><li class="footnote" id="footnote-Shannon1948"><a class="tag is-link" href="#citeref-Shannon1948">Shannon1948</a>C. E. Shannon, Bell Systems Technical Journal <strong>27</strong>, pp 379 (1948)</li><li class="footnote" id="footnote-Curado2004"><a class="tag is-link" href="#citeref-Curado2004">Curado2004</a>Curado, E. M., &amp; Nobre, F. D. (2004). On the stability of analytic entropic forms. Physica A: Statistical Mechanics and its Applications, 335(1-2), 94-106.</li><li class="footnote" id="footnote-Anteneodo1999"><a class="tag is-link" href="#citeref-Anteneodo1999">Anteneodo1999</a>Anteneodo, C., &amp; Plastino, A. R. (1999). Maximum entropy approach to stretched exponential probability distributions. Journal of Physics A: Mathematical and General, 32(7), 1089.</li><li class="footnote" id="footnote-Kraskov2004"><a class="tag is-link" href="#citeref-Kraskov2004">Kraskov2004</a>Kraskov, A., Stögbauer, H., &amp; Grassberger, P. (2004). Estimating mutual information. Physical review E, 69(6), 066138.</li><li class="footnote" id="footnote-Charzyńska2016"><a class="tag is-link" href="#citeref-Charzyńska2016">Charzyńska2016</a>Charzyńska, A., &amp; Gambin, A. (2016). Improvement of the k-NN entropy estimator with applications in systems biology. Entropy, 18(1), 13.</li><li class="footnote" id="footnote-KozachenkoLeonenko1987"><a class="tag is-link" href="#citeref-KozachenkoLeonenko1987">KozachenkoLeonenko1987</a>Kozachenko, L. F., &amp; Leonenko, N. N. (1987). Sample estimate of the entropy of a random vector. Problemy Peredachi Informatsii, 23(2), 9-16.</li><li class="footnote" id="footnote-Zhu2015"><a class="tag is-link" href="#citeref-Zhu2015">Zhu2015</a>Zhu, J., Bellanger, J. J., Shu, H., &amp; Le Bouquin Jeannès, R. (2015). Contribution to transfer entropy estimation via the k-nearest-neighbors approach. Entropy, 17(6), 4173-4201.</li><li class="footnote" id="footnote-Zhu2015"><a class="tag is-link" href="#citeref-Zhu2015">Zhu2015</a>Zhu, J., Bellanger, J. J., Shu, H., &amp; Le Bouquin Jeannès, R. (2015). Contribution to transfer entropy estimation via the k-nearest-neighbors approach. Entropy, 17(6), 4173-4201.</li><li class="footnote" id="footnote-Singh2003"><a class="tag is-link" href="#citeref-Singh2003">Singh2003</a>Singh, H., Misra, N., Hnizdo, V., Fedorowicz, A., &amp; Demchuk, E. (2003). Nearest neighbor estimates of entropy. American journal of mathematical and management sciences, 23(3-4), 301-321.</li><li class="footnote" id="footnote-Vasicek1976"><a class="tag is-link" href="#citeref-Vasicek1976">Vasicek1976</a>Vasicek, O. (1976). A test for normality based on sample entropy. Journal of the Royal Statistical Society: Series B (Methodological), 38(1), 54-59.</li><li class="footnote" id="footnote-Alizadeh2010"><a class="tag is-link" href="#citeref-Alizadeh2010">Alizadeh2010</a>Alizadeh, N. H., &amp; Arghami, N. R. (2010). A new estimator of entropy. Journal of the Iranian Statistical Society (JIRSS).</li><li class="footnote" id="footnote-Ebrahimi1994"><a class="tag is-link" href="#citeref-Ebrahimi1994">Ebrahimi1994</a>Ebrahimi, N., Pflughoeft, K., &amp; Soofi, E. S. (1994). Two measures of sample entropy. Statistics &amp; Probability Letters, 20(3), 225-234.</li><li class="footnote" id="footnote-Correa1995"><a class="tag is-link" href="#citeref-Correa1995">Correa1995</a>Correa, J. C. (1995). A new estimator of entropy. Communications in Statistics-Theory and Methods, 24(10), 2439-2449.</li></ul></section></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../probabilities/">« Probabilities</a><a class="docs-footer-nextpage" href="../examples/">Entropies.jl Examples »</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 0.27.23 on <span class="colophon-date" title="Wednesday 9 November 2022 15:32">Wednesday 9 November 2022</span>. Using Julia version 1.8.2.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
